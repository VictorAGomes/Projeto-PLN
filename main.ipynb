{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63cfcefb",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "Nosso grupo enfrentou algumas dificuldades na definição do projeto, equilibrando a busca por uma proposta relevante com o receio de estabelecer objetivos além de nossas capacidades. Após diversas discussões e ideias, decidimos desenvolver uma solução que compara dois textos e, por meio de análises semânticas e sintáticas, calcula a similaridade entre eles em forma de porcentagem. Dessa maneira, buscamos inferir se os textos foram ou não produzidos pelo mesmo autor. \n",
    "Após a definição da nossa ideia, iniciamos pesquisas sobre o plano de ação para lidar com o problema proposto. Primeiramente, percebemos que o pré-processamento é uma etapa extremamente relevante e que difere consideravelmente de outros casos de PLN. Isso ocorre porque um autor pode expressar seus maneirismos linguísticos e estilísticos de maneira sutil, o que torna características geralmente descartadas — como o início e o fim de sentenças, a pontuação e o uso de stopwords — potencialmente relevantes para a análise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb403de",
   "metadata": {},
   "source": [
    "## Coleta de dados \n",
    "Considerando o problema abordado, optamos inicialmente por buscar textos de autores em comum, mas com temáticas distintas. Dessa forma, conseguimos testar a funcionalidade da nossa solução minimizando possíveis interferências decorrentes da similaridade temática entre os textos. Para isso, utilizamos o site Domínio Público, onde foi possível encontrar obras de diversos autores em língua portuguesa.\n",
    "Selecionamos, nesta etapa, quatro textos: Dom Casmurro, de Machado de Assis, e O Cortiço, de Aluísio Azevedo e Iracema de José de Alencar. Atualmente, estamos utilizando esses 3 textos para testes, mas nossa intenção é expandir consideravelmente o conjunto de dados ao longo do desenvolvimento. Cada texto teve 4 capítulos extraídos e separados para análise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1805bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo: data/caps_processados\\bras_cubas_machado_cap_1.txt\n",
      "Arquivo salvo: data/caps_processados\\bras_cubas_machado_cap_2.txt\n",
      "Arquivo salvo: data/caps_processados\\bras_cubas_machado_cap_3.txt\n",
      "Arquivo salvo: data/caps_processados\\bras_cubas_machado_cap_4.txt\n",
      "Arquivo salvo: data/caps_processados\\dom_casmurro_machado_cap_1.txt\n",
      "Arquivo salvo: data/caps_processados\\dom_casmurro_machado_cap_2.txt\n",
      "Arquivo salvo: data/caps_processados\\dom_casmurro_machado_cap_3.txt\n",
      "Arquivo salvo: data/caps_processados\\dom_casmurro_machado_cap_4.txt\n",
      "Arquivo salvo: data/caps_processados\\iracema_jose_de_alencar_cap_1.txt\n",
      "Arquivo salvo: data/caps_processados\\iracema_jose_de_alencar_cap_2.txt\n",
      "Arquivo salvo: data/caps_processados\\iracema_jose_de_alencar_cap_3.txt\n",
      "Arquivo salvo: data/caps_processados\\iracema_jose_de_alencar_cap_4.txt\n",
      "Arquivo salvo: data/caps_processados\\o_cortico_aluisio_azevedo_cap_1.txt\n",
      "Arquivo salvo: data/caps_processados\\o_cortico_aluisio_azevedo_cap_2.txt\n",
      "Arquivo salvo: data/caps_processados\\o_cortico_aluisio_azevedo_cap_3.txt\n",
      "Arquivo salvo: data/caps_processados\\o_cortico_aluisio_azevedo_cap_4.txt\n",
      "Arquivo salvo: data/caps_processados\\o_mulato_aluisio_azevedo_cap_1.txt\n",
      "Arquivo salvo: data/caps_processados\\o_mulato_aluisio_azevedo_cap_2.txt\n",
      "Arquivo salvo: data/caps_processados\\o_mulato_aluisio_azevedo_cap_3.txt\n",
      "Arquivo salvo: data/caps_processados\\o_mulato_aluisio_azevedo_cap_4.txt\n",
      "Arquivo salvo: data/caps_processados\\senhora_jose_de_alencar_cap_1.txt\n",
      "Arquivo salvo: data/caps_processados\\senhora_jose_de_alencar_cap_2.txt\n",
      "Arquivo salvo: data/caps_processados\\senhora_jose_de_alencar_cap_3.txt\n",
      "Arquivo salvo: data/caps_processados\\senhora_jose_de_alencar_cap_4.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Diretórios de origem e destino\n",
    "origem = \"data/caps\"\n",
    "destino = \"data/caps_processados\"\n",
    "os.makedirs(destino, exist_ok=True)\n",
    "\n",
    "for nome_arquivo in os.listdir(origem):\n",
    "    caminho_origem = os.path.join(origem, nome_arquivo)\n",
    "    caminho_destino = os.path.join(destino, os.path.splitext(nome_arquivo)[0] + \".txt\")\n",
    "    if os.path.isfile(caminho_origem):\n",
    "        with open(caminho_origem, 'r', encoding='utf-8') as f_origem:\n",
    "            conteudo = f_origem.read()\n",
    "        with open(caminho_destino, 'w', encoding='utf-8') as f_destino:\n",
    "            f_destino.write(conteudo)\n",
    "        print(f\"Arquivo salvo: {caminho_destino}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46a5c5",
   "metadata": {},
   "source": [
    "## Limpeza dos textos\n",
    "\n",
    "Na etapa de pré-processamento definimos uma pipeline de limpeza de dados, essa é responsável por tornar os dados mais homogêneos, sem perder a complexidade. Além disso, aplicamos tokenização no texto, tomando cuidado para manter a divisão entre sentenças e manter pontuação e stopwords que podem ser relevantes para análise.\n",
    "Abaixo definimos as configurações e funções usadas para limpeza do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6798610",
   "metadata": {},
   "source": [
    "A célula abaixo corrige um problma com a biblioteca nltk que estávamos tendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "95fc800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ./.venv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./.venv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.data.path.append('./.venv/nltk_data')  \n",
    "\n",
    "nltk.download('punkt', download_dir='./.venv/nltk_data')\n",
    "nltk.download('stopwords', download_dir='./.venv/nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a99cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def limpeza_texto(texto):\n",
    "\n",
    "    # 1. Corrigir quebras de linha excessivas\n",
    "    texto = texto.replace('\\n', '')\n",
    "    \n",
    "    # 2. Corrigir múltiplos espaços\n",
    "    texto = re.sub(r'[ \\t]+', ' ', texto)\n",
    "    \n",
    "    # 3. Remover pontuação desnecessária (mas manter . , ? — e aspas)\n",
    "    texto = re.sub(r'[!;:()*[\\]{}<>]', '', texto)\n",
    "    \n",
    "    # 4. Converter para minúsculas\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    # 5. Remover acentos\n",
    "    texto = unicodedata.normalize('NFKD', texto)\n",
    "    texto = ''.join(c for c in texto if not unicodedata.combining(c))\n",
    "    \n",
    "    return texto\n",
    "\n",
    "def limpar_e_tokenizar_texto(texto):\n",
    "    # Limpa o texto primeiro\n",
    "    texto_limpo = limpeza_texto(texto)\n",
    "    \n",
    "    sentences = sent_tokenize(texto_limpo, language='portuguese')\n",
    "    # Remove sentenças vazias\n",
    "    sentences = [s for s in sentences if s.strip()]\n",
    "    # Tokeniza as sentenças e achata a lista de listas\n",
    "    tokens = [token for s in sentences for token in word_tokenize(s, language='portuguese')]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def extrair_titulo_autor(nome_arquivo):\n",
    "    # Remove \"preprocessado_\" e \".txt\" ou \".json\"\n",
    "    nome_limpo = re.sub(r'^preprocessado_', '', nome_arquivo)\n",
    "    nome_limpo = re.sub(r'\\.(txt|json)$', '', nome_limpo)\n",
    "    \n",
    "    # Separar título e autor\n",
    "    match = re.match(r'(.+?) \\((.+?)\\)', nome_limpo)\n",
    "    if match:\n",
    "        titulo, autor = match.groups()\n",
    "        return titulo.strip(), autor.strip()\n",
    "    else:\n",
    "        return nome_limpo.strip(), None  # Se não bater, só retorna o título"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d6134",
   "metadata": {},
   "source": [
    "### Tokenização e Armazenamento\n",
    "\n",
    "Na célula abaixo realizamos a tokenização e armazenamento dos textos em um json com título do texto, nome do autor, e conteúdo tokenizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cd992666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processando frase original (após split e strip): 'Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em\n",
      "primeiro lugar o meu nascimento ou a minha morte'\n",
      "Tokens gerados: ['algum', 'tempo', 'hesitei', 'se', 'devia', 'abrir', 'estas', 'memorias', 'pelo', 'principio', 'ou', 'pelo', 'fim', ',', 'isto', 'e', ',', 'se', 'poria', 'emprimeiro', 'lugar', 'o', 'meu', 'nascimento', 'ou', 'a', 'minha', 'morte']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Suposto o uso vulgar seja começar pelo nascimento,\n",
      "duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente\n",
      "um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito\n",
      "ficaria assim mais galante e mais novo'\n",
      "Tokens gerados: ['suposto', 'o', 'uso', 'vulgar', 'seja', 'comecar', 'pelo', 'nascimento', ',', 'duas', 'consideracoes', 'me', 'levaram', 'a', 'adotar', 'diferente', 'metodo', 'a', 'primeira', 'e', 'que', 'eu', 'nao', 'sou', 'propriamenteum', 'autor', 'defunto', ',', 'mas', 'um', 'defunto', 'autor', ',', 'para', 'quem', 'a', 'campa', 'foi', 'outro', 'berco', 'a', 'segunda', 'e', 'que', 'o', 'escritoficaria', 'assim', 'mais', 'galante', 'e', 'mais', 'novo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Moisés, que também contou a sua morte, não a pôs no intróito,\n",
      "mas no cabo; diferença radical entre este livro e o Pentateuco'\n",
      "Tokens gerados: ['moises', ',', 'que', 'tambem', 'contou', 'a', 'sua', 'morte', ',', 'nao', 'a', 'pos', 'no', 'introito', ',', 'mas', 'no', 'cabo', 'diferenca', 'radical', 'entre', 'este', 'livro', 'e', 'o', 'pentateuco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dito isto, expirei às duas horas da tarde de uma sexta-feira do mês de agosto de 1869, na minha bela chácara de Catumbi'\n",
      "Tokens gerados: ['dito', 'isto', ',', 'expirei', 'as', 'duas', 'horas', 'da', 'tarde', 'de', 'uma', 'sexta-feira', 'do', 'mes', 'de', 'agosto', 'de', '1869', ',', 'na', 'minha', 'bela', 'chacara', 'de', 'catumbi']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha uns sessenta e quatro anos, rijos e prósperos, era solteiro, possuía cerca de trezentos contos e fui acompanhado ao\n",
      "cemitério por onze amigos'\n",
      "Tokens gerados: ['tinha', 'uns', 'sessenta', 'e', 'quatro', 'anos', ',', 'rijos', 'e', 'prosperos', ',', 'era', 'solteiro', ',', 'possuia', 'cerca', 'de', 'trezentos', 'contos', 'e', 'fui', 'acompanhado', 'aocemiterio', 'por', 'onze', 'amigos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Onze amigos! Verdade é que não houve cartas nem anúncios'\n",
      "Tokens gerados: ['onze', 'amigos', 'verdade', 'e', 'que', 'nao', 'houve', 'cartas', 'nem', 'anuncios']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Acresce que chovia — peneirava\n",
      "— uma chuvinha miúda, triste e constante, tão constante e tão triste, que levou um daqueles fiéis da última hora a intercalar\n",
      "esta engenhosa idéia no discurso que proferiu à beira de minha cova: — “Vós, que o conhecestes, meus senhores, vós podeis\n",
      "dizer comigo que a natureza parece estar chorando a perda irreparável de um dos mais belos caracteres que tem honrado a\n",
      "humanidade'\n",
      "Tokens gerados: ['acresce', 'que', 'chovia', '—', 'peneirava—', 'uma', 'chuvinha', 'miuda', ',', 'triste', 'e', 'constante', ',', 'tao', 'constante', 'e', 'tao', 'triste', ',', 'que', 'levou', 'um', 'daqueles', 'fieis', 'da', 'ultima', 'hora', 'a', 'intercalaresta', 'engenhosa', 'ideia', 'no', 'discurso', 'que', 'proferiu', 'a', 'beira', 'de', 'minha', 'cova', '—', '“', 'vos', ',', 'que', 'o', 'conhecestes', ',', 'meus', 'senhores', ',', 'vos', 'podeisdizer', 'comigo', 'que', 'a', 'natureza', 'parece', 'estar', 'chorando', 'a', 'perda', 'irreparavel', 'de', 'um', 'dos', 'mais', 'belos', 'caracteres', 'que', 'tem', 'honrado', 'ahumanidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Este ar sombrio, estas gotas do céu, aquelas nuvens escuras que cobrem o azul como um crepe funéreo, tudo\n",
      "isso é a dor crua e má que lhe rói à natureza as mais íntimas entranhas; tudo isso é um sublime louvor ao nosso ilustre\n",
      "finado'\n",
      "Tokens gerados: ['este', 'ar', 'sombrio', ',', 'estas', 'gotas', 'do', 'ceu', ',', 'aquelas', 'nuvens', 'escuras', 'que', 'cobrem', 'o', 'azul', 'como', 'um', 'crepe', 'funereo', ',', 'tudoisso', 'e', 'a', 'dor', 'crua', 'e', 'ma', 'que', 'lhe', 'roi', 'a', 'natureza', 'as', 'mais', 'intimas', 'entranhas', 'tudo', 'isso', 'e', 'um', 'sublime', 'louvor', 'ao', 'nosso', 'ilustrefinado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      " Bom e fiel amigo! Não, não me arrependo das vinte apólices que lhe deixei'\n",
      "Tokens gerados: ['”', 'bom', 'e', 'fiel', 'amigo', 'nao', ',', 'nao', 'me', 'arrependo', 'das', 'vinte', 'apolices', 'que', 'lhe', 'deixei']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E foi assim que cheguei\n",
      "à cláusula dos meus dias; foi assim que me encaminhei para o undiscovered country de Hamlet, sem as\n",
      "ânsias nem as dúvidas do moço príncipe, mas pausado e trôpego, como quem se retira tarde do espetáculo'\n",
      "Tokens gerados: ['e', 'foi', 'assim', 'que', 'chegueia', 'clausula', 'dos', 'meus', 'dias', 'foi', 'assim', 'que', 'me', 'encaminhei', 'para', 'o', 'undiscovered', 'country', 'de', 'hamlet', ',', 'sem', 'asansias', 'nem', 'as', 'duvidas', 'do', 'moco', 'principe', ',', 'mas', 'pausado', 'e', 'tropego', ',', 'como', 'quem', 'se', 'retira', 'tarde', 'do', 'espetaculo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tarde e aborrecido'\n",
      "Tokens gerados: ['tarde', 'e', 'aborrecido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Viram-me ir umas nove ou dez pessoas, entre elas três senhoras, minha irmã Sabina,\n",
      "casada com o Cotrim, — a filha, um lírio-do-vale, — e'\n",
      "Tokens gerados: ['viram-me', 'ir', 'umas', 'nove', 'ou', 'dez', 'pessoas', ',', 'entre', 'elas', 'tres', 'senhoras', ',', 'minha', 'irma', 'sabina', ',', 'casada', 'com', 'o', 'cotrim', ',', '—', 'a', 'filha', ',', 'um', 'lirio-do-vale', ',', '—', 'e']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tenham paciência! daqui a pouco lhes direi\n",
      "quem era a terceira senhora'\n",
      "Tokens gerados: ['tenham', 'paciencia', 'daqui', 'a', 'pouco', 'lhes', 'direiquem', 'era', 'a', 'terceira', 'senhora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Contentem-se de saber que essa anônima, ainda que não parenta, padeceu\n",
      "mais do que as parentas'\n",
      "Tokens gerados: ['contentem-se', 'de', 'saber', 'que', 'essa', 'anonima', ',', 'ainda', 'que', 'nao', 'parenta', ',', 'padeceumais', 'do', 'que', 'as', 'parentas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É verdade, padeceu mais'\n",
      "Tokens gerados: ['e', 'verdade', ',', 'padeceu', 'mais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não digo que se carpisse, não digo que se deixasse\n",
      "rolar pelo chão, convulsa'\n",
      "Tokens gerados: ['nao', 'digo', 'que', 'se', 'carpisse', ',', 'nao', 'digo', 'que', 'se', 'deixasserolar', 'pelo', 'chao', ',', 'convulsa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nem o meu óbito era coisa altamente dramática'\n",
      "Tokens gerados: ['nem', 'o', 'meu', 'obito', 'era', 'coisa', 'altamente', 'dramatica']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um solteirão que expira\n",
      "aos sessenta e quatro anos, não parece que reúna em si todos os elementos de uma tragédia'\n",
      "Tokens gerados: ['um', 'solteirao', 'que', 'expiraaos', 'sessenta', 'e', 'quatro', 'anos', ',', 'nao', 'parece', 'que', 'reuna', 'em', 'si', 'todos', 'os', 'elementos', 'de', 'uma', 'tragedia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E dado que\n",
      "sim, o que menos convinha a essa anônima era aparentá-lo'\n",
      "Tokens gerados: ['e', 'dado', 'quesim', ',', 'o', 'que', 'menos', 'convinha', 'a', 'essa', 'anonima', 'era', 'aparenta-lo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De pé, à cabeceira da cama, com os olhos\n",
      "estúpidos, a boca entreaberta, a triste senhora mal podia crer na minha extinção'\n",
      "Tokens gerados: ['de', 'pe', ',', 'a', 'cabeceira', 'da', 'cama', ',', 'com', 'os', 'olhosestupidos', ',', 'a', 'boca', 'entreaberta', ',', 'a', 'triste', 'senhora', 'mal', 'podia', 'crer', 'na', 'minha', 'extincao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Morto! morto! dizia consigo'\n",
      "Tokens gerados: ['—', 'morto', 'morto', 'dizia', 'consigo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É a imaginação dela, como as cegonhas que um ilustre viajante viu desferirem o vôo desde o Ilisso\n",
      "às ribas africanas, sem embargo das ruínas e dos tempos, — a imaginação dessa senhora também voou\n",
      "por sobre os destroços presentes até às ribas de uma África juvenil'\n",
      "Tokens gerados: ['e', 'a', 'imaginacao', 'dela', ',', 'como', 'as', 'cegonhas', 'que', 'um', 'ilustre', 'viajante', 'viu', 'desferirem', 'o', 'voo', 'desde', 'o', 'ilissoas', 'ribas', 'africanas', ',', 'sem', 'embargo', 'das', 'ruinas', 'e', 'dos', 'tempos', ',', '—', 'a', 'imaginacao', 'dessa', 'senhora', 'tambem', 'vooupor', 'sobre', 'os', 'destrocos', 'presentes', 'ate', 'as', 'ribas', 'de', 'uma', 'africa', 'juvenil']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deixá-la ir; lá iremos mais tarde;\n",
      "lá iremos quando eu me restituir aos primeiros anos'\n",
      "Tokens gerados: ['deixa-la', 'ir', 'la', 'iremos', 'mais', 'tardela', 'iremos', 'quando', 'eu', 'me', 'restituir', 'aos', 'primeiros', 'anos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Agora, quero morrer tranqüilamente, metodicamente,\n",
      "ouvindo os soluços das damas, as falas baixas dos homens, a chuva que tamborila nas folhas de tinhorão\n",
      "da chácara, e o som estrídulo de uma navalha que um amolador está afiando lá fora, à porta de um\n",
      "correeiro'\n",
      "Tokens gerados: ['agora', ',', 'quero', 'morrer', 'tranquilamente', ',', 'metodicamente', ',', 'ouvindo', 'os', 'solucos', 'das', 'damas', ',', 'as', 'falas', 'baixas', 'dos', 'homens', ',', 'a', 'chuva', 'que', 'tamborila', 'nas', 'folhas', 'de', 'tinhoraoda', 'chacara', ',', 'e', 'o', 'som', 'estridulo', 'de', 'uma', 'navalha', 'que', 'um', 'amolador', 'esta', 'afiando', 'la', 'fora', ',', 'a', 'porta', 'de', 'umcorreeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Juro-lhes que essa orquestra da morte foi muito menos triste do que podia parecer'\n",
      "Tokens gerados: ['juro-lhes', 'que', 'essa', 'orquestra', 'da', 'morte', 'foi', 'muito', 'menos', 'triste', 'do', 'que', 'podia', 'parecer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De certo\n",
      "ponto em diante chegou a ser deliciosa'\n",
      "Tokens gerados: ['de', 'certoponto', 'em', 'diante', 'chegou', 'a', 'ser', 'deliciosa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A vida estrebuchava-me no peito, com uns ímpetos de vaga\n",
      "marinha, esvaía-se-me a consciência, eu descia à imobilidade física e moral, e o corpo fazia-se-me\n",
      "planta, e pedra, e lodo, e coisa nenhuma'\n",
      "Tokens gerados: ['a', 'vida', 'estrebuchava-me', 'no', 'peito', ',', 'com', 'uns', 'impetos', 'de', 'vagamarinha', ',', 'esvaia-se-me', 'a', 'consciencia', ',', 'eu', 'descia', 'a', 'imobilidade', 'fisica', 'e', 'moral', ',', 'e', 'o', 'corpo', 'fazia-se-meplanta', ',', 'e', 'pedra', ',', 'e', 'lodo', ',', 'e', 'coisa', 'nenhuma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Morri de uma pneumonia; mas se lhe disser que foi menos a pneumonia, do que uma idéia grandiosa\n",
      "e útil, a causa da minha morte, é possível que o leitor me não creia, e todavia é verdade'\n",
      "Tokens gerados: ['morri', 'de', 'uma', 'pneumonia', 'mas', 'se', 'lhe', 'disser', 'que', 'foi', 'menos', 'a', 'pneumonia', ',', 'do', 'que', 'uma', 'ideia', 'grandiosae', 'util', ',', 'a', 'causa', 'da', 'minha', 'morte', ',', 'e', 'possivel', 'que', 'o', 'leitor', 'me', 'nao', 'creia', ',', 'e', 'todavia', 'e', 'verdade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vou expor-lhe\n",
      "sumariamente o caso'\n",
      "Tokens gerados: ['vou', 'expor-lhesumariamente', 'o', 'caso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Julgue-o por si mesmo'\n",
      "Tokens gerados: ['julgue-o', 'por', 'si', 'mesmo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_bras_cubas_machado_cap_1.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Com efeito, um dia de manhã, estando a passear na chácara, pendurou-se-me uma idéia no trapézio que eu tinha no\n",
      "cérebro'\n",
      "Tokens gerados: ['com', 'efeito', ',', 'um', 'dia', 'de', 'manha', ',', 'estando', 'a', 'passear', 'na', 'chacara', ',', 'pendurou-se-me', 'uma', 'ideia', 'no', 'trapezio', 'que', 'eu', 'tinha', 'nocerebro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma vez pendurada, entrou a bracejar, a pernear, a fazer as mais arrojadas cabriolas de volatim, que é possível crer'\n",
      "Tokens gerados: ['uma', 'vez', 'pendurada', ',', 'entrou', 'a', 'bracejar', ',', 'a', 'pernear', ',', 'a', 'fazer', 'as', 'mais', 'arrojadas', 'cabriolas', 'de', 'volatim', ',', 'que', 'e', 'possivel', 'crer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eu deixei-me estar a contemplá-la'\n",
      "Tokens gerados: ['eu', 'deixei-me', 'estar', 'a', 'contempla-la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Súbito, deu um grande salto, estendeu os braços e as pernas, até tomar a forma de um X:\n",
      "decifra-me ou devoro-te'\n",
      "Tokens gerados: ['subito', ',', 'deu', 'um', 'grande', 'salto', ',', 'estendeu', 'os', 'bracos', 'e', 'as', 'pernas', ',', 'ate', 'tomar', 'a', 'forma', 'de', 'um', 'xdecifra-me', 'ou', 'devoro-te']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Essa idéia era nada menos que a invenção de um medicamento sublime, um emplasto antihipocondríaco, destinado a aliviar a nossa melancólica humanidade'\n",
      "Tokens gerados: ['essa', 'ideia', 'era', 'nada', 'menos', 'que', 'a', 'invencao', 'de', 'um', 'medicamento', 'sublime', ',', 'um', 'emplasto', 'antihipocondriaco', ',', 'destinado', 'a', 'aliviar', 'a', 'nossa', 'melancolica', 'humanidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na petição de privilégio que então\n",
      "redigi, chamei a atenção do governo para esse resultado, verdadeiramente cristão'\n",
      "Tokens gerados: ['na', 'peticao', 'de', 'privilegio', 'que', 'entaoredigi', ',', 'chamei', 'a', 'atencao', 'do', 'governo', 'para', 'esse', 'resultado', ',', 'verdadeiramente', 'cristao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Todavia, não neguei\n",
      "aos amigos as vantagens pecuniárias que deviam resultar da distribuição de um produto de tamanhos e\n",
      "tão profundos efeitos'\n",
      "Tokens gerados: ['todavia', ',', 'nao', 'negueiaos', 'amigos', 'as', 'vantagens', 'pecuniarias', 'que', 'deviam', 'resultar', 'da', 'distribuicao', 'de', 'um', 'produto', 'de', 'tamanhos', 'etao', 'profundos', 'efeitos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Agora, porém, que estou cá do outro lado da vida, posso confessar tudo: o que me\n",
      "influiu principalmente foi o gosto de ver impressas nos jornais, mostradores, folhetos, esquinas, e\n",
      "enfim nas caixinhas do remédio, estas três palavras: Emplasto Brás Cubas'\n",
      "Tokens gerados: ['agora', ',', 'porem', ',', 'que', 'estou', 'ca', 'do', 'outro', 'lado', 'da', 'vida', ',', 'posso', 'confessar', 'tudo', 'o', 'que', 'meinfluiu', 'principalmente', 'foi', 'o', 'gosto', 'de', 'ver', 'impressas', 'nos', 'jornais', ',', 'mostradores', ',', 'folhetos', ',', 'esquinas', ',', 'eenfim', 'nas', 'caixinhas', 'do', 'remedio', ',', 'estas', 'tres', 'palavras', 'emplasto', 'bras', 'cubas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Para que negá-lo? Eu tinha\n",
      "a paixão do arruído, do cartaz, do foguete de lágrimas'\n",
      "Tokens gerados: ['para', 'que', 'nega-lo', '?', 'eu', 'tinhaa', 'paixao', 'do', 'arruido', ',', 'do', 'cartaz', ',', 'do', 'foguete', 'de', 'lagrimas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Talvez os modestos me arguam esse defeito; fio,\n",
      "porém, que esse talento me hão de reconhecer os hábeis'\n",
      "Tokens gerados: ['talvez', 'os', 'modestos', 'me', 'arguam', 'esse', 'defeito', 'fio', ',', 'porem', ',', 'que', 'esse', 'talento', 'me', 'hao', 'de', 'reconhecer', 'os', 'habeis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Assim, a minha idéia trazia duas faces, como as medalhas, uma virada para o público, outra para mim'\n",
      "Tokens gerados: ['assim', ',', 'a', 'minha', 'ideia', 'trazia', 'duas', 'faces', ',', 'como', 'as', 'medalhas', ',', 'uma', 'virada', 'para', 'o', 'publico', ',', 'outra', 'para', 'mim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De um lado,\n",
      "filantropia e lucro; de outro lado, sede de nomeada'\n",
      "Tokens gerados: ['de', 'um', 'lado', ',', 'filantropia', 'e', 'lucro', 'de', 'outro', 'lado', ',', 'sede', 'de', 'nomeada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Digamos: — amor da glória'\n",
      "Tokens gerados: ['digamos', '—', 'amor', 'da', 'gloria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um tio meu, cônego de prebenda inteira, costumava dizer que o amor da glória temporal era a perdição das almas, que só\n",
      "devem cobiçar a glória eterna'\n",
      "Tokens gerados: ['um', 'tio', 'meu', ',', 'conego', 'de', 'prebenda', 'inteira', ',', 'costumava', 'dizer', 'que', 'o', 'amor', 'da', 'gloria', 'temporal', 'era', 'a', 'perdicao', 'das', 'almas', ',', 'que', 'sodevem', 'cobicar', 'a', 'gloria', 'eterna']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao que retorquia outro tio, oficial de um dos antigos terços de infantaria, que o amor da glória\n",
      "era a coisa mais verdadeiramente humana que há no homem, e, conseguintemente, a sua mais genuína feição'\n",
      "Tokens gerados: ['ao', 'que', 'retorquia', 'outro', 'tio', ',', 'oficial', 'de', 'um', 'dos', 'antigos', 'tercos', 'de', 'infantaria', ',', 'que', 'o', 'amor', 'da', 'gloriaera', 'a', 'coisa', 'mais', 'verdadeiramente', 'humana', 'que', 'ha', 'no', 'homem', ',', 'e', ',', 'conseguintemente', ',', 'a', 'sua', 'mais', 'genuina', 'feicao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Decida o leitor entre o militar e o cônego; eu volto ao emplasto'\n",
      "Tokens gerados: ['decida', 'o', 'leitor', 'entre', 'o', 'militar', 'e', 'o', 'conego', 'eu', 'volto', 'ao', 'emplasto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_bras_cubas_machado_cap_2.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, já que falei nos meus dois tios, deixa-me fazer aqui um curto esboço genealógico'\n",
      "Tokens gerados: ['mas', ',', 'ja', 'que', 'falei', 'nos', 'meus', 'dois', 'tios', ',', 'deixa-me', 'fazer', 'aqui', 'um', 'curto', 'esboco', 'genealogico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O fundador de minha família foi um certo Damião Cubas, que floresceu na primeira metade do\n",
      "século XVIII'\n",
      "Tokens gerados: ['o', 'fundador', 'de', 'minha', 'familia', 'foi', 'um', 'certo', 'damiao', 'cubas', ',', 'que', 'floresceu', 'na', 'primeira', 'metade', 'doseculo', 'xviii']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era tanoeiro de ofício, natural do Rio de Janeiro, onde teria morrido na penúria e na\n",
      "obscuridade, se somente exercesse a tanoaria'\n",
      "Tokens gerados: ['era', 'tanoeiro', 'de', 'oficio', ',', 'natural', 'do', 'rio', 'de', 'janeiro', ',', 'onde', 'teria', 'morrido', 'na', 'penuria', 'e', 'naobscuridade', ',', 'se', 'somente', 'exercesse', 'a', 'tanoaria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas não; fez-se lavrador, plantou, colheu, permutou o\n",
      "seu produto por boas e honradas patacas, até que morreu, deixando grosso cabedal a um filho, o licenciado\n",
      "Luís Cubas'\n",
      "Tokens gerados: ['mas', 'nao', 'fez-se', 'lavrador', ',', 'plantou', ',', 'colheu', ',', 'permutou', 'oseu', 'produto', 'por', 'boas', 'e', 'honradas', 'patacas', ',', 'ate', 'que', 'morreu', ',', 'deixando', 'grosso', 'cabedal', 'a', 'um', 'filho', ',', 'o', 'licenciadoluis', 'cubas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Neste rapaz é que verdadeiramente começa a série de meus avós — dos avós que a minha\n",
      "família sempre confessou —, porque o Damião Cubas era afinal de contas um tanoeiro, e talvez mau\n",
      "tanoeiro, ao passo que o Luís Cubas estudou em Coimbra, primou no Estado, e foi um dos amigos\n",
      "particulares do vice-rei conde da Cunha'\n",
      "Tokens gerados: ['neste', 'rapaz', 'e', 'que', 'verdadeiramente', 'comeca', 'a', 'serie', 'de', 'meus', 'avos', '—', 'dos', 'avos', 'que', 'a', 'minhafamilia', 'sempre', 'confessou', '—', ',', 'porque', 'o', 'damiao', 'cubas', 'era', 'afinal', 'de', 'contas', 'um', 'tanoeiro', ',', 'e', 'talvez', 'mautanoeiro', ',', 'ao', 'passo', 'que', 'o', 'luis', 'cubas', 'estudou', 'em', 'coimbra', ',', 'primou', 'no', 'estado', ',', 'e', 'foi', 'um', 'dos', 'amigosparticulares', 'do', 'vice-rei', 'conde', 'da', 'cunha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Como este apelido de Cubas lhe cheirasse excessivamente a tanoaria, alegava meu pai, bisneto do\n",
      "Damião, que o dito apelido fora dado a um cavaleiro, herói nas jornadas da Africa, em prêmio da\n",
      "façanha que praticou arrebatando trezentas cubas aos mouros'\n",
      "Tokens gerados: ['como', 'este', 'apelido', 'de', 'cubas', 'lhe', 'cheirasse', 'excessivamente', 'a', 'tanoaria', ',', 'alegava', 'meu', 'pai', ',', 'bisneto', 'dodamiao', ',', 'que', 'o', 'dito', 'apelido', 'fora', 'dado', 'a', 'um', 'cavaleiro', ',', 'heroi', 'nas', 'jornadas', 'da', 'africa', ',', 'em', 'premio', 'dafacanha', 'que', 'praticou', 'arrebatando', 'trezentas', 'cubas', 'aos', 'mouros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Meu pai era homem de imaginação;\n",
      "escapou à tanoaria nas asas de um calembour'\n",
      "Tokens gerados: ['meu', 'pai', 'era', 'homem', 'de', 'imaginacaoescapou', 'a', 'tanoaria', 'nas', 'asas', 'de', 'um', 'calembour']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era um bom caráter, meu pai, varão digno e leal como\n",
      "poucos'\n",
      "Tokens gerados: ['era', 'um', 'bom', 'carater', ',', 'meu', 'pai', ',', 'varao', 'digno', 'e', 'leal', 'comopoucos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha, é verdade, uns fumos de pacholice; mas quem não é um pouco pachola nesse mundo?\n",
      "Releva notar que ele não recorreu à inventiva senão depois de experimentar a falsificação; primeiramente,\n",
      "entroncou-se na família daquele meu famoso homônimo, o capitão-mor Brás Cubas, que fundou a vila\n",
      "de São Vicente, onde morreu em 1592, e por esse motivo é que me deu o nome de Brás'\n",
      "Tokens gerados: ['tinha', ',', 'e', 'verdade', ',', 'uns', 'fumos', 'de', 'pacholice', 'mas', 'quem', 'nao', 'e', 'um', 'pouco', 'pachola', 'nesse', 'mundo', '?', 'releva', 'notar', 'que', 'ele', 'nao', 'recorreu', 'a', 'inventiva', 'senao', 'depois', 'de', 'experimentar', 'a', 'falsificacao', 'primeiramente', ',', 'entroncou-se', 'na', 'familia', 'daquele', 'meu', 'famoso', 'homonimo', ',', 'o', 'capitao-mor', 'bras', 'cubas', ',', 'que', 'fundou', 'a', 'vilade', 'sao', 'vicente', ',', 'onde', 'morreu', 'em', '1592', ',', 'e', 'por', 'esse', 'motivo', 'e', 'que', 'me', 'deu', 'o', 'nome', 'de', 'bras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Opôs-se-lhe,\n",
      "porém, a família do capitão-mor, e foi então que ele imaginou as trezentas cubas mouriscas'\n",
      "Tokens gerados: ['opos-se-lhe', ',', 'porem', ',', 'a', 'familia', 'do', 'capitao-mor', ',', 'e', 'foi', 'entao', 'que', 'ele', 'imaginou', 'as', 'trezentas', 'cubas', 'mouriscas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vivem ainda alguns membros da minha família, minha sobrinha Venância, por exemplo, o lírio-dovale, que é a flor das damas do seu tempo; vive o pai, o Cotrim, um sujeito que'\n",
      "Tokens gerados: ['vivem', 'ainda', 'alguns', 'membros', 'da', 'minha', 'familia', ',', 'minha', 'sobrinha', 'venancia', ',', 'por', 'exemplo', ',', 'o', 'lirio-dovale', ',', 'que', 'e', 'a', 'flor', 'das', 'damas', 'do', 'seu', 'tempo', 'vive', 'o', 'pai', ',', 'o', 'cotrim', ',', 'um', 'sujeito', 'que']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas não antecipemos\n",
      "os sucessos; acabemos de uma vez como nosso emplasto'\n",
      "Tokens gerados: ['mas', 'nao', 'antecipemosos', 'sucessos', 'acabemos', 'de', 'uma', 'vez', 'como', 'nosso', 'emplasto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_bras_cubas_machado_cap_3.json\n",
      "\n",
      "Processando frase original (após split e strip): 'A minha idéia, depois de tantas cabriolas, constituíra-se idéia fixa'\n",
      "Tokens gerados: ['a', 'minha', 'ideia', ',', 'depois', 'de', 'tantas', 'cabriolas', ',', 'constituira-se', 'ideia', 'fixa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deus te livre, leitor, de uma idéia\n",
      "fixa; antes um argueiro, antes uma trave no olho'\n",
      "Tokens gerados: ['deus', 'te', 'livre', ',', 'leitor', ',', 'de', 'uma', 'ideiafixa', 'antes', 'um', 'argueiro', ',', 'antes', 'uma', 'trave', 'no', 'olho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vê o Cavour; foi a idéia fixa da unidade italiana que\n",
      "o matou'\n",
      "Tokens gerados: ['ve', 'o', 'cavour', 'foi', 'a', 'ideia', 'fixa', 'da', 'unidade', 'italiana', 'queo', 'matou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Verdade é que Bismarck não morreu; mas cumpre advertir que a natureza é uma grande\n",
      "caprichosa e a história uma eterna loureira'\n",
      "Tokens gerados: ['verdade', 'e', 'que', 'bismarck', 'nao', 'morreu', 'mas', 'cumpre', 'advertir', 'que', 'a', 'natureza', 'e', 'uma', 'grandecaprichosa', 'e', 'a', 'historia', 'uma', 'eterna', 'loureira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por exemplo, Suetônio deu-nos um Cláudio, que era um simplório, — ou “uma abóbora” como lhe\n",
      "chamou Sêneca, e um Tito, que mereceu ser as delícias de Roma'\n",
      "Tokens gerados: ['por', 'exemplo', ',', 'suetonio', 'deu-nos', 'um', 'claudio', ',', 'que', 'era', 'um', 'simplorio', ',', '—', 'ou', '“', 'uma', 'abobora', '”', 'como', 'lhechamou', 'seneca', ',', 'e', 'um', 'tito', ',', 'que', 'mereceu', 'ser', 'as', 'delicias', 'de', 'roma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Veio modernamente um professor e\n",
      "achou meio de demonstrar que dos dois césares, o delicioso, o verdadeiramente delicioso, foi o “abóbora”\n",
      "de Sêneca'\n",
      "Tokens gerados: ['veio', 'modernamente', 'um', 'professor', 'eachou', 'meio', 'de', 'demonstrar', 'que', 'dos', 'dois', 'cesares', ',', 'o', 'delicioso', ',', 'o', 'verdadeiramente', 'delicioso', ',', 'foi', 'o', '“', 'abobora', '”', 'de', 'seneca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E tu, madama Lucrécia, flor dos Bórgias, se um poeta te pintou como a Messalina católica,\n",
      "apareceu um Gregorovius incrédulo que te apagou muito essa qualidade, e, se não vieste a lírio, também\n",
      "não ficaste pântano'\n",
      "Tokens gerados: ['e', 'tu', ',', 'madama', 'lucrecia', ',', 'flor', 'dos', 'borgias', ',', 'se', 'um', 'poeta', 'te', 'pintou', 'como', 'a', 'messalina', 'catolica', ',', 'apareceu', 'um', 'gregorovius', 'incredulo', 'que', 'te', 'apagou', 'muito', 'essa', 'qualidade', ',', 'e', ',', 'se', 'nao', 'vieste', 'a', 'lirio', ',', 'tambemnao', 'ficaste', 'pantano']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eu deixo-me estar entre o poeta e o sábio'\n",
      "Tokens gerados: ['eu', 'deixo-me', 'estar', 'entre', 'o', 'poeta', 'e', 'o', 'sabio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Viva pois a história, a volúvel história que dá para tudo; e, tomando à idéia fixa, direi que é ela a que faz os varões fortes\n",
      "e os doidos; a idéia móbil, vaga ou furta-cor é a que faz os Cláudios, — fórmula Suetônio'\n",
      "Tokens gerados: ['viva', 'pois', 'a', 'historia', ',', 'a', 'voluvel', 'historia', 'que', 'da', 'para', 'tudo', 'e', ',', 'tomando', 'a', 'ideia', 'fixa', ',', 'direi', 'que', 'e', 'ela', 'a', 'que', 'faz', 'os', 'varoes', 'fortese', 'os', 'doidos', 'a', 'ideia', 'mobil', ',', 'vaga', 'ou', 'furta-cor', 'e', 'a', 'que', 'faz', 'os', 'claudios', ',', '—', 'formula', 'suetonio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era fixa a minha idéia, fixa como'\n",
      "Tokens gerados: ['era', 'fixa', 'a', 'minha', 'ideia', ',', 'fixa', 'como']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não me ocorre nada que seja assaz fixo nesse mundo: talvez a\n",
      "lua, talvez as pirâmides do Egito, talvez a finada dieta germânica'\n",
      "Tokens gerados: ['nao', 'me', 'ocorre', 'nada', 'que', 'seja', 'assaz', 'fixo', 'nesse', 'mundo', 'talvez', 'alua', ',', 'talvez', 'as', 'piramides', 'do', 'egito', ',', 'talvez', 'a', 'finada', 'dieta', 'germanica']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Veja o leitor a comparação que\n",
      "melhor lhe quadrar, veja-a e não esteja daí a torcer-me o nariz, só porque ainda não chegamos à parte\n",
      "narrativa destas memórias'\n",
      "Tokens gerados: ['veja', 'o', 'leitor', 'a', 'comparacao', 'quemelhor', 'lhe', 'quadrar', ',', 'veja-a', 'e', 'nao', 'esteja', 'dai', 'a', 'torcer-me', 'o', 'nariz', ',', 'so', 'porque', 'ainda', 'nao', 'chegamos', 'a', 'partenarrativa', 'destas', 'memorias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lá iremos'\n",
      "Tokens gerados: ['la', 'iremos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Creio que prefere a anedota à reflexão, como os outros leitores,\n",
      "seus confrades, e acho que faz muito bem'\n",
      "Tokens gerados: ['creio', 'que', 'prefere', 'a', 'anedota', 'a', 'reflexao', ',', 'como', 'os', 'outros', 'leitores', ',', 'seus', 'confrades', ',', 'e', 'acho', 'que', 'faz', 'muito', 'bem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois lá iremos'\n",
      "Tokens gerados: ['pois', 'la', 'iremos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Todavia, importa dizer que este livro é escrito\n",
      "com pachorra, com a pachorra de um homem já desafrontado da brevidade do século, obra supinamente\n",
      "filosófica, de uma filosofia desigual, agora austera, logo brincalhona, coisa que não edifica nem destrói,\n",
      "não inflama nem regela, e é todavia mais do que passatempo e menos do que apostolado'\n",
      "Tokens gerados: ['todavia', ',', 'importa', 'dizer', 'que', 'este', 'livro', 'e', 'escritocom', 'pachorra', ',', 'com', 'a', 'pachorra', 'de', 'um', 'homem', 'ja', 'desafrontado', 'da', 'brevidade', 'do', 'seculo', ',', 'obra', 'supinamentefilosofica', ',', 'de', 'uma', 'filosofia', 'desigual', ',', 'agora', 'austera', ',', 'logo', 'brincalhona', ',', 'coisa', 'que', 'nao', 'edifica', 'nem', 'destroi', ',', 'nao', 'inflama', 'nem', 'regela', ',', 'e', 'e', 'todavia', 'mais', 'do', 'que', 'passatempo', 'e', 'menos', 'do', 'que', 'apostolado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vamos lá; retifique o seu nariz, e tornemos ao emplasto'\n",
      "Tokens gerados: ['vamos', 'la', 'retifique', 'o', 'seu', 'nariz', ',', 'e', 'tornemos', 'ao', 'emplasto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deixemos a história com os seus caprichos\n",
      "de dama elegante'\n",
      "Tokens gerados: ['deixemos', 'a', 'historia', 'com', 'os', 'seus', 'caprichosde', 'dama', 'elegante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nenhum de nós pelejou a batalha de Salamina, nenhum escreveu a confissão de\n",
      "Augsburgo; pela minha parte, se alguma vez me lembro de Cromwell, é só pela idéia de que Sua Alteza,\n",
      "com a mesma mão que trancara o parlamento, teria imposto aos ingleses o emplasto Brás Cubas'\n",
      "Tokens gerados: ['nenhum', 'de', 'nos', 'pelejou', 'a', 'batalha', 'de', 'salamina', ',', 'nenhum', 'escreveu', 'a', 'confissao', 'deaugsburgo', 'pela', 'minha', 'parte', ',', 'se', 'alguma', 'vez', 'me', 'lembro', 'de', 'cromwell', ',', 'e', 'so', 'pela', 'ideia', 'de', 'que', 'sua', 'alteza', ',', 'com', 'a', 'mesma', 'mao', 'que', 'trancara', 'o', 'parlamento', ',', 'teria', 'imposto', 'aos', 'ingleses', 'o', 'emplasto', 'bras', 'cubas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não\n",
      "se riam dessa vitória comum da farmácia e do puritanismo'\n",
      "Tokens gerados: ['naose', 'riam', 'dessa', 'vitoria', 'comum', 'da', 'farmacia', 'e', 'do', 'puritanismo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quem não sabe que ao pé de cada bandeira\n",
      "grande, pública, ostensiva, há muitas vezes várias outras bandeiras modestamente particulares, que se\n",
      "hasteiam e flutuam à sombra daquela, e não poucas vezes lhe sobrevivem? Mal comparando, é como a\n",
      "arraia-miúda, que se acolhia à sombra do castelo-feudal; caiu este e a arraia ficou'\n",
      "Tokens gerados: ['quem', 'nao', 'sabe', 'que', 'ao', 'pe', 'de', 'cada', 'bandeiragrande', ',', 'publica', ',', 'ostensiva', ',', 'ha', 'muitas', 'vezes', 'varias', 'outras', 'bandeiras', 'modestamente', 'particulares', ',', 'que', 'sehasteiam', 'e', 'flutuam', 'a', 'sombra', 'daquela', ',', 'e', 'nao', 'poucas', 'vezes', 'lhe', 'sobrevivem', '?', 'mal', 'comparando', ',', 'e', 'como', 'aarraia-miuda', ',', 'que', 'se', 'acolhia', 'a', 'sombra', 'do', 'castelo-feudal', 'caiu', 'este', 'e', 'a', 'arraia', 'ficou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Verdade é que se fez\n",
      "graúda e castelã'\n",
      "Tokens gerados: ['verdade', 'e', 'que', 'se', 'fezgrauda', 'e', 'castela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não, a comparação não presta'\n",
      "Tokens gerados: ['nao', ',', 'a', 'comparacao', 'nao', 'presta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_bras_cubas_machado_cap_4.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma noite destas, vindo da cidade para o Engenho Novo, encontrei num trem da Central um rapaz \n",
      "aqui do bairro, que eu conheço de vista e de chapéu'\n",
      "Tokens gerados: ['uma', 'noite', 'destas', ',', 'vindo', 'da', 'cidade', 'para', 'o', 'engenho', 'novo', ',', 'encontrei', 'num', 'trem', 'da', 'central', 'um', 'rapaz', 'aqui', 'do', 'bairro', ',', 'que', 'eu', 'conheco', 'de', 'vista', 'e', 'de', 'chapeu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cumprimentou-me, sentou-se ao pé de mim, \n",
      "falou da lua e dos ministros, e acabou recitando-me versos'\n",
      "Tokens gerados: ['cumprimentou-me', ',', 'sentou-se', 'ao', 'pe', 'de', 'mim', ',', 'falou', 'da', 'lua', 'e', 'dos', 'ministros', ',', 'e', 'acabou', 'recitando-me', 'versos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A viagem era curta, e os versos pode ser \n",
      "que não fossem inteiramente maus'\n",
      "Tokens gerados: ['a', 'viagem', 'era', 'curta', ',', 'e', 'os', 'versos', 'pode', 'ser', 'que', 'nao', 'fossem', 'inteiramente', 'maus']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sucedeu, porém, que, como eu estava cansado, fechei os olhos \n",
      "três ou quatro vezes; tanto bastou para que ele interrompesse a leitura e metesse os versos no bolso'\n",
      "Tokens gerados: ['sucedeu', ',', 'porem', ',', 'que', ',', 'como', 'eu', 'estava', 'cansado', ',', 'fechei', 'os', 'olhos', 'tres', 'ou', 'quatro', 'vezes', 'tanto', 'bastou', 'para', 'que', 'ele', 'interrompesse', 'a', 'leitura', 'e', 'metesse', 'os', 'versos', 'no', 'bolso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '-- Continue, disse eu acordando'\n",
      "Tokens gerados: ['--', 'continue', ',', 'disse', 'eu', 'acordando']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '-- Já acabei, murmurou ele'\n",
      "Tokens gerados: ['--', 'ja', 'acabei', ',', 'murmurou', 'ele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '-- São muito bonitos'\n",
      "Tokens gerados: ['--', 'sao', 'muito', 'bonitos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vi-lhe fazer um gesto para tirá-los outra vez do bolso, mas não passou do gesto; estava amuado'\n",
      "Tokens gerados: ['vi-lhe', 'fazer', 'um', 'gesto', 'para', 'tira-los', 'outra', 'vez', 'do', 'bolso', ',', 'mas', 'nao', 'passou', 'do', 'gesto', 'estava', 'amuado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No \n",
      "dia seguinte entrou a dizer de mim nomes feios, e acabou alcunhando-me Dom Casmurro'\n",
      "Tokens gerados: ['no', 'dia', 'seguinte', 'entrou', 'a', 'dizer', 'de', 'mim', 'nomes', 'feios', ',', 'e', 'acabou', 'alcunhando-me', 'dom', 'casmurro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os \n",
      "vizinhos, que não gostam dos meus hábitos reclusos e calados, deram curso à alcunha, que afinal \n",
      "pegou'\n",
      "Tokens gerados: ['os', 'vizinhos', ',', 'que', 'nao', 'gostam', 'dos', 'meus', 'habitos', 'reclusos', 'e', 'calados', ',', 'deram', 'curso', 'a', 'alcunha', ',', 'que', 'afinal', 'pegou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nem por isso me zanguei'\n",
      "Tokens gerados: ['nem', 'por', 'isso', 'me', 'zanguei']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Contei a anedota aos amigos da cidade, e eles, por graça, \n",
      "chamam-me assim, alguns em bilhetes: \"Dom Casmurro, domingo vou jantar com você'\n",
      "Tokens gerados: ['contei', 'a', 'anedota', 'aos', 'amigos', 'da', 'cidade', ',', 'e', 'eles', ',', 'por', 'graca', ',', 'chamam-me', 'assim', ',', 'alguns', 'em', 'bilhetes', '``', 'dom', 'casmurro', ',', 'domingo', 'vou', 'jantar', 'com', 'voce']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '\"--\"Vou \n",
      "para Petrópolis, Dom Casmurro; a casa é a mesma da Renania; vê se deixas essa caverna do \n",
      "Engenho Novo, e vai lá passar uns quinze dias comigo'\n",
      "Tokens gerados: ['``', '--', \"''\", 'vou', 'para', 'petropolis', ',', 'dom', 'casmurro', 'a', 'casa', 'e', 'a', 'mesma', 'da', 'renania', 've', 'se', 'deixas', 'essa', 'caverna', 'do', 'engenho', 'novo', ',', 'e', 'vai', 'la', 'passar', 'uns', 'quinze', 'dias', 'comigo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '\"--\"Meu caro Dom Casmurro, não cuide que \n",
      "o dispenso do teatro amanhã; venha e dormirá aqui na cidade; dou-lhe camarote, dou-lhe chá, dou-\n",
      "lhe cama; só não lhe dou moça'\n",
      "Tokens gerados: ['``', '--', \"''\", 'meu', 'caro', 'dom', 'casmurro', ',', 'nao', 'cuide', 'que', 'o', 'dispenso', 'do', 'teatro', 'amanha', 'venha', 'e', 'dormira', 'aqui', 'na', 'cidade', 'dou-lhe', 'camarote', ',', 'dou-lhe', 'cha', ',', 'dou-lhe', 'cama', 'so', 'nao', 'lhe', 'dou', 'moca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '\" \n",
      "Não consultes dicionários'\n",
      "Tokens gerados: ['``', 'nao', 'consultes', 'dicionarios']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Casmurro não está aqui no sentido que eles lhe dão, mas no que lhe pôs \n",
      "o vulgo de homem calado e metido consigo'\n",
      "Tokens gerados: ['casmurro', 'nao', 'esta', 'aqui', 'no', 'sentido', 'que', 'eles', 'lhe', 'dao', ',', 'mas', 'no', 'que', 'lhe', 'pos', 'o', 'vulgo', 'de', 'homem', 'calado', 'e', 'metido', 'consigo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dom veio por ironia, para atribuir-me fumos de \n",
      "fidalgo'\n",
      "Tokens gerados: ['dom', 'veio', 'por', 'ironia', ',', 'para', 'atribuir-me', 'fumos', 'de', 'fidalgo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tudo por estar cochilando! Também não achei melhor título para a minha narração - se não \n",
      "tiver outro daqui até ao fim do livro, vai este mesmo'\n",
      "Tokens gerados: ['tudo', 'por', 'estar', 'cochilando', 'tambem', 'nao', 'achei', 'melhor', 'titulo', 'para', 'a', 'minha', 'narracao', '-', 'se', 'nao', 'tiver', 'outro', 'daqui', 'ate', 'ao', 'fim', 'do', 'livro', ',', 'vai', 'este', 'mesmo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O meu poeta do trem ficará sabendo que não \n",
      "lhe guardo rancor'\n",
      "Tokens gerados: ['o', 'meu', 'poeta', 'do', 'trem', 'ficara', 'sabendo', 'que', 'nao', 'lhe', 'guardo', 'rancor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E com pequeno esforço, sendo o título seu, poderá cuidar que a obra é sua'\n",
      "Tokens gerados: ['e', 'com', 'pequeno', 'esforco', ',', 'sendo', 'o', 'titulo', 'seu', ',', 'podera', 'cuidar', 'que', 'a', 'obra', 'e', 'sua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Há \n",
      "livros que apenas terão isso dos seus autores; alguns nem tanto'\n",
      "Tokens gerados: ['ha', 'livros', 'que', 'apenas', 'terao', 'isso', 'dos', 'seus', 'autores', 'alguns', 'nem', 'tanto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_dom_casmurro_machado_cap_1.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Agora que expliquei o título, passo a escrever o livro'\n",
      "Tokens gerados: ['agora', 'que', 'expliquei', 'o', 'titulo', ',', 'passo', 'a', 'escrever', 'o', 'livro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Antes disso, porém, digamos os motivos que \n",
      "me põem a pena na mão'\n",
      "Tokens gerados: ['antes', 'disso', ',', 'porem', ',', 'digamos', 'os', 'motivos', 'que', 'me', 'poem', 'a', 'pena', 'na', 'mao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vivo só, com um criado'\n",
      "Tokens gerados: ['vivo', 'so', ',', 'com', 'um', 'criado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A casa em que moro é própria; fi-la construir de propósito, levado de um \n",
      "desejo tão particular que me vexa imprimi-lo, mas vá lá'\n",
      "Tokens gerados: ['a', 'casa', 'em', 'que', 'moro', 'e', 'propria', 'fi-la', 'construir', 'de', 'proposito', ',', 'levado', 'de', 'um', 'desejo', 'tao', 'particular', 'que', 'me', 'vexa', 'imprimi-lo', ',', 'mas', 'va', 'la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um dia'\n",
      "Tokens gerados: ['um', 'dia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'há bastantes anos, lembrou-me \n",
      "reproduzir no Engenho Novo a casa em que me criei na antiga Rua de Mata-cavalos, dando-lhe o \n",
      "mesmo aspecto e economia daquela outra, que desapareceu'\n",
      "Tokens gerados: ['ha', 'bastantes', 'anos', ',', 'lembrou-me', 'reproduzir', 'no', 'engenho', 'novo', 'a', 'casa', 'em', 'que', 'me', 'criei', 'na', 'antiga', 'rua', 'de', 'mata-cavalos', ',', 'dando-lhe', 'o', 'mesmo', 'aspecto', 'e', 'economia', 'daquela', 'outra', ',', 'que', 'desapareceu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Construtor e pintor entenderam bem as \n",
      "indicações que lhes fiz: é o mesmo prédio assobradado, três janelas de frente, varanda ao fundo, as \n",
      "mesmas alcovas e salas'\n",
      "Tokens gerados: ['construtor', 'e', 'pintor', 'entenderam', 'bem', 'as', 'indicacoes', 'que', 'lhes', 'fiz', 'e', 'o', 'mesmo', 'predio', 'assobradado', ',', 'tres', 'janelas', 'de', 'frente', ',', 'varanda', 'ao', 'fundo', ',', 'as', 'mesmas', 'alcovas', 'e', 'salas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na principal destas, a pintura do tecto e das paredes é mais ou menos igual, \n",
      "umas grinaldas de flores miúdas e grandes pássaros que as tomam nos blocos, de espaço a espaço'\n",
      "Tokens gerados: ['na', 'principal', 'destas', ',', 'a', 'pintura', 'do', 'tecto', 'e', 'das', 'paredes', 'e', 'mais', 'ou', 'menos', 'igual', ',', 'umas', 'grinaldas', 'de', 'flores', 'miudas', 'e', 'grandes', 'passaros', 'que', 'as', 'tomam', 'nos', 'blocos', ',', 'de', 'espaco', 'a', 'espaco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nos quatro cantos do tecto as figuras das estações, e ao centro das paredes os medalhões de César, \n",
      "Augusto, Nero e Massinissa, com os nomes por baixo'\n",
      "Tokens gerados: ['nos', 'quatro', 'cantos', 'do', 'tecto', 'as', 'figuras', 'das', 'estacoes', ',', 'e', 'ao', 'centro', 'das', 'paredes', 'os', 'medalhoes', 'de', 'cesar', ',', 'augusto', ',', 'nero', 'e', 'massinissa', ',', 'com', 'os', 'nomes', 'por', 'baixo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não alcanço a razão de tais personagens'\n",
      "Tokens gerados: ['nao', 'alcanco', 'a', 'razao', 'de', 'tais', 'personagens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando fomos para a casa de Mata-cavalos, já ela estava assim decorada; vinha do decênio anterior'\n",
      "Tokens gerados: ['quando', 'fomos', 'para', 'a', 'casa', 'de', 'mata-cavalos', ',', 'ja', 'ela', 'estava', 'assim', 'decorada', 'vinha', 'do', 'decenio', 'anterior']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Naturalmente era gosto do tempo meter sabor clássico e figuras antigas em pinturas americanas'\n",
      "Tokens gerados: ['naturalmente', 'era', 'gosto', 'do', 'tempo', 'meter', 'sabor', 'classico', 'e', 'figuras', 'antigas', 'em', 'pinturas', 'americanas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O \n",
      "mais é também análogo e parecido'\n",
      "Tokens gerados: ['o', 'mais', 'e', 'tambem', 'analogo', 'e', 'parecido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tenho chacarinha, flores, legume, uma casuarina, um poço e \n",
      "lavadouro'\n",
      "Tokens gerados: ['tenho', 'chacarinha', ',', 'flores', ',', 'legume', ',', 'uma', 'casuarina', ',', 'um', 'poco', 'e', 'lavadouro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uso louça velha e mobília velha'\n",
      "Tokens gerados: ['uso', 'louca', 'velha', 'e', 'mobilia', 'velha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Enfim, agora, como outrora, há aqui o mesmo contraste \n",
      "da vida interior, que é pacata, com a exterior, que é ruidosa'\n",
      "Tokens gerados: ['enfim', ',', 'agora', ',', 'como', 'outrora', ',', 'ha', 'aqui', 'o', 'mesmo', 'contraste', 'da', 'vida', 'interior', ',', 'que', 'e', 'pacata', ',', 'com', 'a', 'exterior', ',', 'que', 'e', 'ruidosa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O meu fim evidente era atar as duas pontas da vida, e restaurar na velhice a adolescência'\n",
      "Tokens gerados: ['o', 'meu', 'fim', 'evidente', 'era', 'atar', 'as', 'duas', 'pontas', 'da', 'vida', ',', 'e', 'restaurar', 'na', 'velhice', 'a', 'adolescencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois, \n",
      "senhor, não consegui recompor o que foi nem o que fui'\n",
      "Tokens gerados: ['pois', ',', 'senhor', ',', 'nao', 'consegui', 'recompor', 'o', 'que', 'foi', 'nem', 'o', 'que', 'fui']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em tudo, se o rosto é igual, a fisionomia é \n",
      "diferente'\n",
      "Tokens gerados: ['em', 'tudo', ',', 'se', 'o', 'rosto', 'e', 'igual', ',', 'a', 'fisionomia', 'e', 'diferente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se só me faltassem os outros, vá um homem consola-se mais ou menos das pessoas que \n",
      "perde; mais falto eu mesmo, e esta lacuna é tudo'\n",
      "Tokens gerados: ['se', 'so', 'me', 'faltassem', 'os', 'outros', ',', 'va', 'um', 'homem', 'consola-se', 'mais', 'ou', 'menos', 'das', 'pessoas', 'que', 'perde', 'mais', 'falto', 'eu', 'mesmo', ',', 'e', 'esta', 'lacuna', 'e', 'tudo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O que aqui está é, mal comparando, semelhante à \n",
      "pintura que se põe na barba e nos cabelos, e que apenas conserva o hábito externo, como se diz nas \n",
      "autópsias; o interno não agüenta tinta'\n",
      "Tokens gerados: ['o', 'que', 'aqui', 'esta', 'e', ',', 'mal', 'comparando', ',', 'semelhante', 'a', 'pintura', 'que', 'se', 'poe', 'na', 'barba', 'e', 'nos', 'cabelos', ',', 'e', 'que', 'apenas', 'conserva', 'o', 'habito', 'externo', ',', 'como', 'se', 'diz', 'nas', 'autopsias', 'o', 'interno', 'nao', 'aguenta', 'tinta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma certidão que me desse vinte anos de idade poderia \n",
      "enganar os estranhos, como todos os documentos falsos, mas não a mim'\n",
      "Tokens gerados: ['uma', 'certidao', 'que', 'me', 'desse', 'vinte', 'anos', 'de', 'idade', 'poderia', 'enganar', 'os', 'estranhos', ',', 'como', 'todos', 'os', 'documentos', 'falsos', ',', 'mas', 'nao', 'a', 'mim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os amigos que me restam \n",
      "são de data recente; todos os antigos foram estudar a geologia dos campos-santos'\n",
      "Tokens gerados: ['os', 'amigos', 'que', 'me', 'restam', 'sao', 'de', 'data', 'recente', 'todos', 'os', 'antigos', 'foram', 'estudar', 'a', 'geologia', 'dos', 'campos-santos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quanto às \n",
      "amigas, algumas datam de quinze anos, outras de menos, e quase todas crêem na mocidade'\n",
      "Tokens gerados: ['quanto', 'as', 'amigas', ',', 'algumas', 'datam', 'de', 'quinze', 'anos', ',', 'outras', 'de', 'menos', ',', 'e', 'quase', 'todas', 'creem', 'na', 'mocidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Duas \n",
      "ou três fariam crer nela aos outros, mas a língua que falam obriga muita vez a consultar os \n",
      "dicionários, e tal freqüência é cansativa'\n",
      "Tokens gerados: ['duas', 'ou', 'tres', 'fariam', 'crer', 'nela', 'aos', 'outros', ',', 'mas', 'a', 'lingua', 'que', 'falam', 'obriga', 'muita', 'vez', 'a', 'consultar', 'os', 'dicionarios', ',', 'e', 'tal', 'frequencia', 'e', 'cansativa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, vida diferente não quer dizer vida pior, é outra cousa a certos respeitos, aquela vida \n",
      "antiga aparece-me despida de muitos encantos que lhe achei; mas é também exato que perdeu muito \n",
      "espinho que a fez molesta, e, de memória, conservo alguma recordação doce e feiticeira'\n",
      "Tokens gerados: ['entretanto', ',', 'vida', 'diferente', 'nao', 'quer', 'dizer', 'vida', 'pior', ',', 'e', 'outra', 'cousa', 'a', 'certos', 'respeitos', ',', 'aquela', 'vida', 'antiga', 'aparece-me', 'despida', 'de', 'muitos', 'encantos', 'que', 'lhe', 'achei', 'mas', 'e', 'tambem', 'exato', 'que', 'perdeu', 'muito', 'espinho', 'que', 'a', 'fez', 'molesta', ',', 'e', ',', 'de', 'memoria', ',', 'conservo', 'alguma', 'recordacao', 'doce', 'e', 'feiticeira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em \n",
      "verdade, pouco apareço e menos falo'\n",
      "Tokens gerados: ['em', 'verdade', ',', 'pouco', 'apareco', 'e', 'menos', 'falo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Distrações raras'\n",
      "Tokens gerados: ['distracoes', 'raras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O mais do tempo é gasto em hortar, jardinar \n",
      "e ler; como bem e não durmo mal'\n",
      "Tokens gerados: ['o', 'mais', 'do', 'tempo', 'e', 'gasto', 'em', 'hortar', ',', 'jardinar', 'e', 'ler', 'como', 'bem', 'e', 'nao', 'durmo', 'mal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ora, como tudo cansa, esta monotonia acabou por exaurir-me também'\n",
      "Tokens gerados: ['ora', ',', 'como', 'tudo', 'cansa', ',', 'esta', 'monotonia', 'acabou', 'por', 'exaurir-me', 'tambem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quis variar, e lembrou-me \n",
      "escrever um livro'\n",
      "Tokens gerados: ['quis', 'variar', ',', 'e', 'lembrou-me', 'escrever', 'um', 'livro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Jurisprudência'\n",
      "Tokens gerados: ['jurisprudencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'filosofia e política acudiram-me, mas não me acudiram as forças \n",
      "necessárias'\n",
      "Tokens gerados: ['filosofia', 'e', 'politica', 'acudiram-me', ',', 'mas', 'nao', 'me', 'acudiram', 'as', 'forcas', 'necessarias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois, pensei em fazer uma \"História dos Subúrbios\" menos seca que as memórias do \n",
      "Padre Luís Gonçalves dos Santos relativas à cidade; era obra modesta, mas exigia documentos e \n",
      "datas como preliminares, tudo árido e longo'\n",
      "Tokens gerados: ['depois', ',', 'pensei', 'em', 'fazer', 'uma', '``', 'historia', 'dos', 'suburbios', \"''\", 'menos', 'seca', 'que', 'as', 'memorias', 'do', 'padre', 'luis', 'goncalves', 'dos', 'santos', 'relativas', 'a', 'cidade', 'era', 'obra', 'modesta', ',', 'mas', 'exigia', 'documentos', 'e', 'datas', 'como', 'preliminares', ',', 'tudo', 'arido', 'e', 'longo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi então que os bustos pintados nas paredes entraram \n",
      "a falar-me e a dizer-me que, uma vez que eles não alcançavam reconstituir-me os tempos idos, \n",
      "pegasse da pena e contasse alguns'\n",
      "Tokens gerados: ['foi', 'entao', 'que', 'os', 'bustos', 'pintados', 'nas', 'paredes', 'entraram', 'a', 'falar-me', 'e', 'a', 'dizer-me', 'que', ',', 'uma', 'vez', 'que', 'eles', 'nao', 'alcancavam', 'reconstituir-me', 'os', 'tempos', 'idos', ',', 'pegasse', 'da', 'pena', 'e', 'contasse', 'alguns']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Talvez a narração me desse a ilusão, e as sombras viessem \n",
      "perpassar ligeiras, como ao poeta, não o do trem, mas o do Fausto: Aí vindes outra vez, inquietas \n",
      "sombras?'\n",
      "Tokens gerados: ['talvez', 'a', 'narracao', 'me', 'desse', 'a', 'ilusao', ',', 'e', 'as', 'sombras', 'viessem', 'perpassar', 'ligeiras', ',', 'como', 'ao', 'poeta', ',', 'nao', 'o', 'do', 'trem', ',', 'mas', 'o', 'do', 'fausto', 'ai', 'vindes', 'outra', 'vez', ',', 'inquietas', 'sombras', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fiquei tão alegre com esta idéia, que ainda agora me treme a pena na mão'\n",
      "Tokens gerados: ['fiquei', 'tao', 'alegre', 'com', 'esta', 'ideia', ',', 'que', 'ainda', 'agora', 'me', 'treme', 'a', 'pena', 'na', 'mao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sim, Nero, Augusto, \n",
      "Massinissa, e tu, grande César, que me incitas a fazer os meus comentários, agradeço-vos o \n",
      "conselho, e vou deitar ao papel as reminiscências que me vierem vindo'\n",
      "Tokens gerados: ['sim', ',', 'nero', ',', 'augusto', ',', 'massinissa', ',', 'e', 'tu', ',', 'grande', 'cesar', ',', 'que', 'me', 'incitas', 'a', 'fazer', 'os', 'meus', 'comentarios', ',', 'agradeco-vos', 'o', 'conselho', ',', 'e', 'vou', 'deitar', 'ao', 'papel', 'as', 'reminiscencias', 'que', 'me', 'vierem', 'vindo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deste modo, viverei o que \n",
      "vivi, e assentarei a mão para alguma obra de maior tomo'\n",
      "Tokens gerados: ['deste', 'modo', ',', 'viverei', 'o', 'que', 'vivi', ',', 'e', 'assentarei', 'a', 'mao', 'para', 'alguma', 'obra', 'de', 'maior', 'tomo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eia, comecemos a evocação por uma \n",
      "célebre tarde de novembro, que nunca me esqueceu'\n",
      "Tokens gerados: ['eia', ',', 'comecemos', 'a', 'evocacao', 'por', 'uma', 'celebre', 'tarde', 'de', 'novembro', ',', 'que', 'nunca', 'me', 'esqueceu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tive outras muitas, melhores, e piores, mas \n",
      "aquela nunca se me apagou do espírito'\n",
      "Tokens gerados: ['tive', 'outras', 'muitas', ',', 'melhores', ',', 'e', 'piores', ',', 'mas', 'aquela', 'nunca', 'se', 'me', 'apagou', 'do', 'espirito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É o que vais entender, lendo'\n",
      "Tokens gerados: ['e', 'o', 'que', 'vais', 'entender', ',', 'lendo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_dom_casmurro_machado_cap_2.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Ia entrar na sala de visitas, quando ouvi proferir o meu nome e escondi-me atrás da porta'\n",
      "Tokens gerados: ['ia', 'entrar', 'na', 'sala', 'de', 'visitas', ',', 'quando', 'ouvi', 'proferir', 'o', 'meu', 'nome', 'e', 'escondi-me', 'atras', 'da', 'porta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A casa \n",
      "era a da Rua de Mata-cavalos, o mês novembro, o ano é que é um tanto remoto, mas eu não hei de \n",
      "trocar as datas à minha vida só para agradar às pessoas que não amam histórias velhas; o ano era de \n",
      "1857'\n",
      "Tokens gerados: ['a', 'casa', 'era', 'a', 'da', 'rua', 'de', 'mata-cavalos', ',', 'o', 'mes', 'novembro', ',', 'o', 'ano', 'e', 'que', 'e', 'um', 'tanto', 'remoto', ',', 'mas', 'eu', 'nao', 'hei', 'de', 'trocar', 'as', 'datas', 'a', 'minha', 'vida', 'so', 'para', 'agradar', 'as', 'pessoas', 'que', 'nao', 'amam', 'historias', 'velhas', 'o', 'ano', 'era', 'de', '1857']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--D'\n",
      "Tokens gerados: ['--', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Glória, a senhora persiste na idéia de meter o nosso Bentinho no seminário? É mais que tempo, \n",
      "e já agora pode haver uma dificuldade'\n",
      "Tokens gerados: ['gloria', ',', 'a', 'senhora', 'persiste', 'na', 'ideia', 'de', 'meter', 'o', 'nosso', 'bentinho', 'no', 'seminario', '?', 'e', 'mais', 'que', 'tempo', ',', 'e', 'ja', 'agora', 'pode', 'haver', 'uma', 'dificuldade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Que dificuldade? \n",
      "--Uma grande dificuldade'\n",
      "Tokens gerados: ['--', 'que', 'dificuldade', '?', '--', 'uma', 'grande', 'dificuldade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Minha mãe quis saber o que era'\n",
      "Tokens gerados: ['minha', 'mae', 'quis', 'saber', 'o', 'que', 'era']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José Dias, depois de alguns instantes de concentração, veio ver se \n",
      "havia alguém no corredor; não deu por mim, voltou e, abafando a voz, disse que a dificuldade \n",
      "estava na casa ao pé, a gente do Pádua'\n",
      "Tokens gerados: ['jose', 'dias', ',', 'depois', 'de', 'alguns', 'instantes', 'de', 'concentracao', ',', 'veio', 'ver', 'se', 'havia', 'alguem', 'no', 'corredor', 'nao', 'deu', 'por', 'mim', ',', 'voltou', 'e', ',', 'abafando', 'a', 'voz', ',', 'disse', 'que', 'a', 'dificuldade', 'estava', 'na', 'casa', 'ao', 'pe', ',', 'a', 'gente', 'do', 'padua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--A gente do Pádua? \n",
      "--Há algum tempo estou para lhe dizer isto, mas não me atrevia'\n",
      "Tokens gerados: ['--', 'a', 'gente', 'do', 'padua', '?', '--', 'ha', 'algum', 'tempo', 'estou', 'para', 'lhe', 'dizer', 'isto', ',', 'mas', 'nao', 'me', 'atrevia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não me parece bonito que o nosso \n",
      "Bentinho ande metido nos cantos com a filha do Tartaruga, e esta é a dificuldade, porque se eles \n",
      "pegam de namoro, a senhora terá muito que lutar para separá-los'\n",
      "Tokens gerados: ['nao', 'me', 'parece', 'bonito', 'que', 'o', 'nosso', 'bentinho', 'ande', 'metido', 'nos', 'cantos', 'com', 'a', 'filha', 'do', 'tartaruga', ',', 'e', 'esta', 'e', 'a', 'dificuldade', ',', 'porque', 'se', 'eles', 'pegam', 'de', 'namoro', ',', 'a', 'senhora', 'tera', 'muito', 'que', 'lutar', 'para', 'separa-los']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Não acho'\n",
      "Tokens gerados: ['--', 'nao', 'acho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Metidos nos cantos? \n",
      "--É um modo de falar'\n",
      "Tokens gerados: ['metidos', 'nos', 'cantos', '?', '--', 'e', 'um', 'modo', 'de', 'falar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em segredinhos, sempre juntos'\n",
      "Tokens gerados: ['em', 'segredinhos', ',', 'sempre', 'juntos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bentinho quase que não sai de lá'\n",
      "Tokens gerados: ['bentinho', 'quase', 'que', 'nao', 'sai', 'de', 'la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A pequena \n",
      "é uma desmiolada; o pai faz que não vê; tomara ele que as cousas corressem de maneira, que'\n",
      "Tokens gerados: ['a', 'pequena', 'e', 'uma', 'desmiolada', 'o', 'pai', 'faz', 'que', 'nao', 've', 'tomara', 'ele', 'que', 'as', 'cousas', 'corressem', 'de', 'maneira', ',', 'que']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Compreendo o seu gesto; a senhora não crê em tais cálculos, parece-lhe que todos têm a alma \n",
      "candida'\n",
      "Tokens gerados: ['compreendo', 'o', 'seu', 'gesto', 'a', 'senhora', 'nao', 'cre', 'em', 'tais', 'calculos', ',', 'parece-lhe', 'que', 'todos', 'tem', 'a', 'alma', 'candida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Mas, Sr'\n",
      "Tokens gerados: ['--', 'mas', ',', 'sr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José Dias, tenho visto os pequenos brincando, e nunca vi nada que faça desconfiar'\n",
      "Tokens gerados: ['jose', 'dias', ',', 'tenho', 'visto', 'os', 'pequenos', 'brincando', ',', 'e', 'nunca', 'vi', 'nada', 'que', 'faca', 'desconfiar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Basta \n",
      "a idade; Bentinho mal tem quinze anos'\n",
      "Tokens gerados: ['basta', 'a', 'idade', 'bentinho', 'mal', 'tem', 'quinze', 'anos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Capitu fez quatorze à semana passada; são dous criançolas'\n",
      "Tokens gerados: ['capitu', 'fez', 'quatorze', 'a', 'semana', 'passada', 'sao', 'dous', 'criancolas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não se esqueça que foram criados juntos, desde aquela grande enchente, há dez anos, em que a \n",
      "família Pádua perdeu tanta cousa; daí vieram as nossas relações'\n",
      "Tokens gerados: ['nao', 'se', 'esqueca', 'que', 'foram', 'criados', 'juntos', ',', 'desde', 'aquela', 'grande', 'enchente', ',', 'ha', 'dez', 'anos', ',', 'em', 'que', 'a', 'familia', 'padua', 'perdeu', 'tanta', 'cousa', 'dai', 'vieram', 'as', 'nossas', 'relacoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois eu hei de crer?'\n",
      "Tokens gerados: ['pois', 'eu', 'hei', 'de', 'crer', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mano \n",
      "Cosme, você que acha? Tio Cosme respondeu com um \"Ora!\" que, traduzido em vulgar, queria \n",
      "dizer: \"São imaginações do José Dias os pequenos divertem-se, eu divirto-me; onde está o gamão?\" \n",
      "--Sim, creio que o senhor está enganado'\n",
      "Tokens gerados: ['mano', 'cosme', ',', 'voce', 'que', 'acha', '?', 'tio', 'cosme', 'respondeu', 'com', 'um', '``', 'ora', \"''\", 'que', ',', 'traduzido', 'em', 'vulgar', ',', 'queria', 'dizer', '``', 'sao', 'imaginacoes', 'do', 'jose', 'dias', 'os', 'pequenos', 'divertem-se', ',', 'eu', 'divirto-me', 'onde', 'esta', 'o', 'gamao', '?', \"''\", '--', 'sim', ',', 'creio', 'que', 'o', 'senhor', 'esta', 'enganado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Pode ser minha senhora'\n",
      "Tokens gerados: ['--', 'pode', 'ser', 'minha', 'senhora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Oxalá tenham razão; mas creia que não falei senão depois de muito \n",
      "examinar'\n",
      "Tokens gerados: ['oxala', 'tenham', 'razao', 'mas', 'creia', 'que', 'nao', 'falei', 'senao', 'depois', 'de', 'muito', 'examinar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Em todo caso, vai sendo tempo, interrompeu minha mãe; vou tratar de metê-lo no seminário \n",
      "quanto antes'\n",
      "Tokens gerados: ['--', 'em', 'todo', 'caso', ',', 'vai', 'sendo', 'tempo', ',', 'interrompeu', 'minha', 'mae', 'vou', 'tratar', 'de', 'mete-lo', 'no', 'seminario', 'quanto', 'antes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Bem, uma vez que não perdeu a idéia de o fazer padre, tem-se ganho o principal'\n",
      "Tokens gerados: ['--', 'bem', ',', 'uma', 'vez', 'que', 'nao', 'perdeu', 'a', 'ideia', 'de', 'o', 'fazer', 'padre', ',', 'tem-se', 'ganho', 'o', 'principal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bentinho há de \n",
      "satisfazer os desejos de sua mãe e depois a igreja brasileira tem altos destinos'\n",
      "Tokens gerados: ['bentinho', 'ha', 'de', 'satisfazer', 'os', 'desejos', 'de', 'sua', 'mae', 'e', 'depois', 'a', 'igreja', 'brasileira', 'tem', 'altos', 'destinos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não esqueçamos que \n",
      "um bispo presidiu a Constituinte, e que o Padre Feijó governou o Império'\n",
      "Tokens gerados: ['nao', 'esquecamos', 'que', 'um', 'bispo', 'presidiu', 'a', 'constituinte', ',', 'e', 'que', 'o', 'padre', 'feijo', 'governou', 'o', 'imperio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '-- Governo como a cara dele! atalhou tio Cosme, cedendo a antigos rancores políticos'\n",
      "Tokens gerados: ['--', 'governo', 'como', 'a', 'cara', 'dele', 'atalhou', 'tio', 'cosme', ',', 'cedendo', 'a', 'antigos', 'rancores', 'politicos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Perdão, doutor, não estou defendendo ninguém, estou citando O que eu quero é dizer que o clero \n",
      "ainda tem grande papel no Brasil'\n",
      "Tokens gerados: ['--', 'perdao', ',', 'doutor', ',', 'nao', 'estou', 'defendendo', 'ninguem', ',', 'estou', 'citando', 'o', 'que', 'eu', 'quero', 'e', 'dizer', 'que', 'o', 'clero', 'ainda', 'tem', 'grande', 'papel', 'no', 'brasil']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Você o que quer é um capote; ande, vá buscar o gamão'\n",
      "Tokens gerados: ['--', 'voce', 'o', 'que', 'quer', 'e', 'um', 'capote', 'ande', ',', 'va', 'buscar', 'o', 'gamao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quanto ao pequeno, se tem de ser padre, \n",
      "realmente é melhor que não comece a dizer missa atrás das portas'\n",
      "Tokens gerados: ['quanto', 'ao', 'pequeno', ',', 'se', 'tem', 'de', 'ser', 'padre', ',', 'realmente', 'e', 'melhor', 'que', 'nao', 'comece', 'a', 'dizer', 'missa', 'atras', 'das', 'portas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, olhe cá, mana Glória, há \n",
      "mesmo necessidade de fazê-lo padre? \n",
      "-- É promessa, há de cumprir-se'\n",
      "Tokens gerados: ['mas', ',', 'olhe', 'ca', ',', 'mana', 'gloria', ',', 'ha', 'mesmo', 'necessidade', 'de', 'faze-lo', 'padre', '?', '--', 'e', 'promessa', ',', 'ha', 'de', 'cumprir-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Sei que você fez promessa'\n",
      "Tokens gerados: ['--', 'sei', 'que', 'voce', 'fez', 'promessa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'mas uma promessa assim'\n",
      "Tokens gerados: ['mas', 'uma', 'promessa', 'assim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'não sei'\n",
      "Tokens gerados: ['nao', 'sei']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Creio que, bem pensado'\n",
      "Tokens gerados: ['creio', 'que', ',', 'bem', 'pensado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Você \n",
      "que acha, prima Justina? \n",
      "-- Eu? \n",
      "--Verdade é que cada um sabe melhor de si, continuou tio Cosme- Deus é que sabe de todos'\n",
      "Tokens gerados: ['voce', 'que', 'acha', ',', 'prima', 'justina', '?', '--', 'eu', '?', '--', 'verdade', 'e', 'que', 'cada', 'um', 'sabe', 'melhor', 'de', 'si', ',', 'continuou', 'tio', 'cosme-', 'deus', 'e', 'que', 'sabe', 'de', 'todos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Contudo, uma promessa de tantos anos'\n",
      "Tokens gerados: ['contudo', ',', 'uma', 'promessa', 'de', 'tantos', 'anos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, que é isso, mana Glória? Está chorando? Ora esta \n",
      "pois isto é cousa de lágrimas? \n",
      "Minha mãe assoou-se sem responder'\n",
      "Tokens gerados: ['mas', ',', 'que', 'e', 'isso', ',', 'mana', 'gloria', '?', 'esta', 'chorando', '?', 'ora', 'esta', 'pois', 'isto', 'e', 'cousa', 'de', 'lagrimas', '?', 'minha', 'mae', 'assoou-se', 'sem', 'responder']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prima Justina creio que se levantou e foi ter com ela'\n",
      "Tokens gerados: ['prima', 'justina', 'creio', 'que', 'se', 'levantou', 'e', 'foi', 'ter', 'com', 'ela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seguiu-\n",
      "se um alto silêncio, durante o qual estive a pique de entrar na sala, mas outra força maior, outra \n",
      "emoção'\n",
      "Tokens gerados: ['seguiu-se', 'um', 'alto', 'silencio', ',', 'durante', 'o', 'qual', 'estive', 'a', 'pique', 'de', 'entrar', 'na', 'sala', ',', 'mas', 'outra', 'forca', 'maior', ',', 'outra', 'emocao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não pude ouvir as palavras que tio Cosme entrou a dizer'\n",
      "Tokens gerados: ['nao', 'pude', 'ouvir', 'as', 'palavras', 'que', 'tio', 'cosme', 'entrou', 'a', 'dizer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prima Justina exortava: \"Prima \n",
      "Glória! Prima Glória!\" José Dias desculpava-se: \"Se soubesse, não teria falado, mas falei pela \n",
      "veneração, pela estima, pelo afeto, para cumprir um dever amargo, um dever amaríssimo'\n",
      "Tokens gerados: ['prima', 'justina', 'exortava', '``', 'prima', 'gloria', 'prima', 'gloria', \"''\", 'jose', 'dias', 'desculpava-se', '``', 'se', 'soubesse', ',', 'nao', 'teria', 'falado', ',', 'mas', 'falei', 'pela', 'veneracao', ',', 'pela', 'estima', ',', 'pelo', 'afeto', ',', 'para', 'cumprir', 'um', 'dever', 'amargo', ',', 'um', 'dever', 'amarissimo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '\"'\n",
      "Tokens gerados: ['``']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_dom_casmurro_machado_cap_3.json\n",
      "\n",
      "Processando frase original (após split e strip): 'José Dias amava os superlativos'\n",
      "Tokens gerados: ['jose', 'dias', 'amava', 'os', 'superlativos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era um modo de dar feição monumental às idéias; não as havendo, \n",
      "servia a prolongar as frases'\n",
      "Tokens gerados: ['era', 'um', 'modo', 'de', 'dar', 'feicao', 'monumental', 'as', 'ideias', 'nao', 'as', 'havendo', ',', 'servia', 'a', 'prolongar', 'as', 'frases']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Levantou-se para ir buscar o gamão, que estava no interior da casa'\n",
      "Tokens gerados: ['levantou-se', 'para', 'ir', 'buscar', 'o', 'gamao', ',', 'que', 'estava', 'no', 'interior', 'da', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cosi-me muito à parede, e vi-o passar com as suas calças brancas engomadas, presilhas, rodaque e \n",
      "gravata de mola'\n",
      "Tokens gerados: ['cosi-me', 'muito', 'a', 'parede', ',', 'e', 'vi-o', 'passar', 'com', 'as', 'suas', 'calcas', 'brancas', 'engomadas', ',', 'presilhas', ',', 'rodaque', 'e', 'gravata', 'de', 'mola']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi dos últimos que usaram presilhas no Rio de Janeiro, e talvez neste mundo'\n",
      "Tokens gerados: ['foi', 'dos', 'ultimos', 'que', 'usaram', 'presilhas', 'no', 'rio', 'de', 'janeiro', ',', 'e', 'talvez', 'neste', 'mundo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Trazia as calças curtas para que lhe ficassem bem esticadas'\n",
      "Tokens gerados: ['trazia', 'as', 'calcas', 'curtas', 'para', 'que', 'lhe', 'ficassem', 'bem', 'esticadas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A gravata de cetim preto, com um arco \n",
      "de aço por dentro, imobilizava-lhe o pescoço; era então moda'\n",
      "Tokens gerados: ['a', 'gravata', 'de', 'cetim', 'preto', ',', 'com', 'um', 'arco', 'de', 'aco', 'por', 'dentro', ',', 'imobilizava-lhe', 'o', 'pescoco', 'era', 'entao', 'moda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O rodaque de chita, veste caseira e \n",
      "leve, parecia nele uma casaca de cerimônia'\n",
      "Tokens gerados: ['o', 'rodaque', 'de', 'chita', ',', 'veste', 'caseira', 'e', 'leve', ',', 'parecia', 'nele', 'uma', 'casaca', 'de', 'cerimonia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era magro, chupado, com um princípio de calva; teria \n",
      "os seus cinqüenta e cinco anos'\n",
      "Tokens gerados: ['era', 'magro', ',', 'chupado', ',', 'com', 'um', 'principio', 'de', 'calva', 'teria', 'os', 'seus', 'cinquenta', 'e', 'cinco', 'anos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Levantou-se com o passo vagaroso do costume, não aquele vagar \n",
      "arrastado se era dos preguiçosos, mas um vagar calculado e deduzido, um silogismo completo, a \n",
      "premissa antes da conseqüência, a conseqüência antes da conclusão'\n",
      "Tokens gerados: ['levantou-se', 'com', 'o', 'passo', 'vagaroso', 'do', 'costume', ',', 'nao', 'aquele', 'vagar', 'arrastado', 'se', 'era', 'dos', 'preguicosos', ',', 'mas', 'um', 'vagar', 'calculado', 'e', 'deduzido', ',', 'um', 'silogismo', 'completo', ',', 'a', 'premissa', 'antes', 'da', 'consequencia', ',', 'a', 'consequencia', 'antes', 'da', 'conclusao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um dever amaríssimo!'\n",
      "Tokens gerados: ['um', 'dever', 'amarissimo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_dom_casmurro_machado_cap_4.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Verdes mares bravios de minha terra natal, onde canta a jandaia nas frondes da carnaúba; Verdes  mares  que  brilhais  como  líquida  esmeralda  aos  raios  do  Sol  nascente,  perlongando  as  alvas  praias  \n",
      "ensombradas de coqueiros'\n",
      "Tokens gerados: ['verdes', 'mares', 'bravios', 'de', 'minha', 'terra', 'natal', ',', 'onde', 'canta', 'a', 'jandaia', 'nas', 'frondes', 'da', 'carnauba', 'verdes', 'mares', 'que', 'brilhais', 'como', 'liquida', 'esmeralda', 'aos', 'raios', 'do', 'sol', 'nascente', ',', 'perlongando', 'as', 'alvas', 'praias', 'ensombradas', 'de', 'coqueiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Serenai verdes  mares, e alisai  docemente a  vaga  impetuosa, para que o barco aventureiro manso resvale à flor \n",
      "das águas'\n",
      "Tokens gerados: ['serenai', 'verdes', 'mares', ',', 'e', 'alisai', 'docemente', 'a', 'vaga', 'impetuosa', ',', 'para', 'que', 'o', 'barco', 'aventureiro', 'manso', 'resvale', 'a', 'flor', 'das', 'aguas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Onde vai a afouta jangada, que deixa rápida a costa cearense, aberta ao fresco terral a grande vela? \n",
      "Onde vai como branca alcíone buscando o rochedo pátrio nas solidões do oceano? \n",
      "Três entes respiram sobre o frágil lenho que vai singrando veloce, mar em fora; \n",
      "Um jovem guerreiro cuja tez branca não cora o sangue americano; uma criança e um rafeiro que viram a luz no \n",
      "berço das florestas, e brincam irmãos, filhos ambos da mesma terra selvagem'\n",
      "Tokens gerados: ['onde', 'vai', 'a', 'afouta', 'jangada', ',', 'que', 'deixa', 'rapida', 'a', 'costa', 'cearense', ',', 'aberta', 'ao', 'fresco', 'terral', 'a', 'grande', 'vela', '?', 'onde', 'vai', 'como', 'branca', 'alcione', 'buscando', 'o', 'rochedo', 'patrio', 'nas', 'solidoes', 'do', 'oceano', '?', 'tres', 'entes', 'respiram', 'sobre', 'o', 'fragil', 'lenho', 'que', 'vai', 'singrando', 'veloce', ',', 'mar', 'em', 'fora', 'um', 'jovem', 'guerreiro', 'cuja', 'tez', 'branca', 'nao', 'cora', 'o', 'sangue', 'americano', 'uma', 'crianca', 'e', 'um', 'rafeiro', 'que', 'viram', 'a', 'luz', 'no', 'berco', 'das', 'florestas', ',', 'e', 'brincam', 'irmaos', ',', 'filhos', 'ambos', 'da', 'mesma', 'terra', 'selvagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A lufada intermitente traz da praia um eco vibrante, que ressoa entre o marulho das vagas: \n",
      "— Iracema!'\n",
      "Tokens gerados: ['a', 'lufada', 'intermitente', 'traz', 'da', 'praia', 'um', 'eco', 'vibrante', ',', 'que', 'ressoa', 'entre', 'o', 'marulho', 'das', 'vagas', '—', 'iracema']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O  moço  guerreiro,  encostado  ao  mastro,  leva  os  olhos  presos  na  sombra  fugitiva  da  terra;  a  espaços  o  olhar  \n",
      "empanado  por  tênue  lágrima  cai  sobre  o  jirau,  onde  folgam  as  duas  inocentes  criaturas,  companheiras  de  seu  \n",
      "infortúnio'\n",
      "Tokens gerados: ['o', 'moco', 'guerreiro', ',', 'encostado', 'ao', 'mastro', ',', 'leva', 'os', 'olhos', 'presos', 'na', 'sombra', 'fugitiva', 'da', 'terra', 'a', 'espacos', 'o', 'olhar', 'empanado', 'por', 'tenue', 'lagrima', 'cai', 'sobre', 'o', 'jirau', ',', 'onde', 'folgam', 'as', 'duas', 'inocentes', 'criaturas', ',', 'companheiras', 'de', 'seu', 'infortunio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nesse momento o lábio arranca d’alma um agro sorriso'\n",
      "Tokens gerados: ['nesse', 'momento', 'o', 'labio', 'arranca', 'd', '’', 'alma', 'um', 'agro', 'sorriso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que deixara ele na terra do exílio? \n",
      "Uma história que me contaram nas lindas várzeas onde nasci, à calada da noite, quando a Lua passeava no céu \n",
      "argenteando os campos, e a brisa rugitava nos palmares'\n",
      "Tokens gerados: ['que', 'deixara', 'ele', 'na', 'terra', 'do', 'exilio', '?', 'uma', 'historia', 'que', 'me', 'contaram', 'nas', 'lindas', 'varzeas', 'onde', 'nasci', ',', 'a', 'calada', 'da', 'noite', ',', 'quando', 'a', 'lua', 'passeava', 'no', 'ceu', 'argenteando', 'os', 'campos', ',', 'e', 'a', 'brisa', 'rugitava', 'nos', 'palmares']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Refresca o vento'\n",
      "Tokens gerados: ['refresca', 'o', 'vento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O  rulo  das  vagas  precipita'\n",
      "Tokens gerados: ['o', 'rulo', 'das', 'vagas', 'precipita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O  barco  salta  sobre  as  ondas;  desaparece  no  horizonte'\n",
      "Tokens gerados: ['o', 'barco', 'salta', 'sobre', 'as', 'ondas', 'desaparece', 'no', 'horizonte']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Abre-se  a  imensidade  dos  \n",
      "mares; e a borrasca enverga, como o condor, as foscas asas sobre o abismo'\n",
      "Tokens gerados: ['abre-se', 'a', 'imensidade', 'dos', 'mares', 'e', 'a', 'borrasca', 'enverga', ',', 'como', 'o', 'condor', ',', 'as', 'foscas', 'asas', 'sobre', 'o', 'abismo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deus  te  leve  a  salvo,  brioso  e  altivo  barco,  por  entre    as  vagas  revoltas,  e  te  poje  nalguma  enseada  amiga'\n",
      "Tokens gerados: ['deus', 'te', 'leve', 'a', 'salvo', ',', 'brioso', 'e', 'altivo', 'barco', ',', 'por', 'entre', 'as', 'vagas', 'revoltas', ',', 'e', 'te', 'poje', 'nalguma', 'enseada', 'amiga']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Soprem para ti as brandas  auras; e para ti jaspeie a bonança mares de leite'\n",
      "Tokens gerados: ['soprem', 'para', 'ti', 'as', 'brandas', 'auras', 'e', 'para', 'ti', 'jaspeie', 'a', 'bonanca', 'mares', 'de', 'leite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Enquanto vogas assim à discrição do vento, airoso barco, volva às brancas areias a saudade, que te acompanha, \n",
      "mas não se parte da terra onde revoa'\n",
      "Tokens gerados: ['enquanto', 'vogas', 'assim', 'a', 'discricao', 'do', 'vento', ',', 'airoso', 'barco', ',', 'volva', 'as', 'brancas', 'areias', 'a', 'saudade', ',', 'que', 'te', 'acompanha', ',', 'mas', 'nao', 'se', 'parte', 'da', 'terra', 'onde', 'revoa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_iracema_jose_de_alencar_cap_1.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Além, muito além daquela serra, que ainda azula no horizonte, nasceu Iracema'\n",
      "Tokens gerados: ['alem', ',', 'muito', 'alem', 'daquela', 'serra', ',', 'que', 'ainda', 'azula', 'no', 'horizonte', ',', 'nasceu', 'iracema']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Iracema, a virgem dos lábios de mel, que tinha os cabelos mais negros que a asa da graúna, e mais longos que\n",
      "seu talhe de palmeira'\n",
      "Tokens gerados: ['iracema', ',', 'a', 'virgem', 'dos', 'labios', 'de', 'mel', ',', 'que', 'tinha', 'os', 'cabelos', 'mais', 'negros', 'que', 'a', 'asa', 'da', 'grauna', ',', 'e', 'mais', 'longos', 'queseu', 'talhe', 'de', 'palmeira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O favo da jati não era doce como seu sorriso; nem a baunilha recendia no bosque como seu hálito perfumado'\n",
      "Tokens gerados: ['o', 'favo', 'da', 'jati', 'nao', 'era', 'doce', 'como', 'seu', 'sorriso', 'nem', 'a', 'baunilha', 'recendia', 'no', 'bosque', 'como', 'seu', 'halito', 'perfumado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mais rápida que a corça selvagem, a morena virgem corria o sertão e as matas do Ipu, onde campeava sua\n",
      "guerreira tribo, da grande nação tabajara'\n",
      "Tokens gerados: ['mais', 'rapida', 'que', 'a', 'corca', 'selvagem', ',', 'a', 'morena', 'virgem', 'corria', 'o', 'sertao', 'e', 'as', 'matas', 'do', 'ipu', ',', 'onde', 'campeava', 'suaguerreira', 'tribo', ',', 'da', 'grande', 'nacao', 'tabajara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O pé grácil e nu, mal roçando, alisava apenas a verde pelúcia que vestia a\n",
      "terra com as primeiras águas'\n",
      "Tokens gerados: ['o', 'pe', 'gracil', 'e', 'nu', ',', 'mal', 'rocando', ',', 'alisava', 'apenas', 'a', 'verde', 'pelucia', 'que', 'vestia', 'aterra', 'com', 'as', 'primeiras', 'aguas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um dia, ao pino do Sol, ela repousava em um claro da floresta'\n",
      "Tokens gerados: ['um', 'dia', ',', 'ao', 'pino', 'do', 'sol', ',', 'ela', 'repousava', 'em', 'um', 'claro', 'da', 'floresta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Banhava-lhe o corpo a sombra da oiticica, mais\n",
      "fresca do que o orvalho da noite'\n",
      "Tokens gerados: ['banhava-lhe', 'o', 'corpo', 'a', 'sombra', 'da', 'oiticica', ',', 'maisfresca', 'do', 'que', 'o', 'orvalho', 'da', 'noite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os ramos da acácia silvestre esparziam flores sobre os úmidos cabelos'\n",
      "Tokens gerados: ['os', 'ramos', 'da', 'acacia', 'silvestre', 'esparziam', 'flores', 'sobre', 'os', 'umidos', 'cabelos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Escondidos\n",
      "na folhagem os pássaros ameigavam o canto'\n",
      "Tokens gerados: ['escondidosna', 'folhagem', 'os', 'passaros', 'ameigavam', 'o', 'canto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Iracema saiu do banho: o aljôfar d’água ainda a roreja, como à doce mangaba que corou em manhã de chuva'\n",
      "Tokens gerados: ['iracema', 'saiu', 'do', 'banho', 'o', 'aljofar', 'd', '’', 'agua', 'ainda', 'a', 'roreja', ',', 'como', 'a', 'doce', 'mangaba', 'que', 'corou', 'em', 'manha', 'de', 'chuva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Enquanto repousa, empluma das penas do gará as flechas de seu arco, e concerta com o sabiá da mata, pousado no\n",
      "galho próximo, o canto agreste'\n",
      "Tokens gerados: ['enquanto', 'repousa', ',', 'empluma', 'das', 'penas', 'do', 'gara', 'as', 'flechas', 'de', 'seu', 'arco', ',', 'e', 'concerta', 'com', 'o', 'sabia', 'da', 'mata', ',', 'pousado', 'nogalho', 'proximo', ',', 'o', 'canto', 'agreste']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A graciosa ará, sua companheira e amiga, brinca junto dela'\n",
      "Tokens gerados: ['a', 'graciosa', 'ara', ',', 'sua', 'companheira', 'e', 'amiga', ',', 'brinca', 'junto', 'dela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Às vezes sobe aos ramos da árvore e de lá chama a\n",
      "virgem pelo nome; outras remexe o uru de palha matizada, onde traz a selvagem seus perfumes, os alvos fios do\n",
      "crautá, as agulhas da juçara com que tece a renda, e as tintas de que matiza o algodão'\n",
      "Tokens gerados: ['as', 'vezes', 'sobe', 'aos', 'ramos', 'da', 'arvore', 'e', 'de', 'la', 'chama', 'avirgem', 'pelo', 'nome', 'outras', 'remexe', 'o', 'uru', 'de', 'palha', 'matizada', ',', 'onde', 'traz', 'a', 'selvagem', 'seus', 'perfumes', ',', 'os', 'alvos', 'fios', 'docrauta', ',', 'as', 'agulhas', 'da', 'jucara', 'com', 'que', 'tece', 'a', 'renda', ',', 'e', 'as', 'tintas', 'de', 'que', 'matiza', 'o', 'algodao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Rumor suspeito quebra a doce harmonia da sesta'\n",
      "Tokens gerados: ['rumor', 'suspeito', 'quebra', 'a', 'doce', 'harmonia', 'da', 'sesta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ergue a virgem os olhos, que o sol não deslumbra; sua vista\n",
      "perturba-se'\n",
      "Tokens gerados: ['ergue', 'a', 'virgem', 'os', 'olhos', ',', 'que', 'o', 'sol', 'nao', 'deslumbra', 'sua', 'vistaperturba-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Diante dela e todo a contemplá-la está um guerreiro estranho, se é guerreiro e não algum mau espírito da\n",
      "floresta'\n",
      "Tokens gerados: ['diante', 'dela', 'e', 'todo', 'a', 'contempla-la', 'esta', 'um', 'guerreiro', 'estranho', ',', 'se', 'e', 'guerreiro', 'e', 'nao', 'algum', 'mau', 'espirito', 'dafloresta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tem nas faces o branco das areias que bordam o mar; nos olhos o azul triste das águas profundas'\n",
      "Tokens gerados: ['tem', 'nas', 'faces', 'o', 'branco', 'das', 'areias', 'que', 'bordam', 'o', 'mar', 'nos', 'olhos', 'o', 'azul', 'triste', 'das', 'aguas', 'profundas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ignotas\n",
      "armas e tecidos ignotos cobrem-lhe o corpo'\n",
      "Tokens gerados: ['ignotasarmas', 'e', 'tecidos', 'ignotos', 'cobrem-lhe', 'o', 'corpo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi rápido, como o olhar, o gesto de Iracema'\n",
      "Tokens gerados: ['foi', 'rapido', ',', 'como', 'o', 'olhar', ',', 'o', 'gesto', 'de', 'iracema']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A flecha embebida no arco partiu'\n",
      "Tokens gerados: ['a', 'flecha', 'embebida', 'no', 'arco', 'partiu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Gotas de sangue borbulham na\n",
      "face do desconhecido'\n",
      "Tokens gerados: ['gotas', 'de', 'sangue', 'borbulham', 'naface', 'do', 'desconhecido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De primeiro ímpeto, a mão lesta caiu sobre a cruz da espada; mas logo sorriu'\n",
      "Tokens gerados: ['de', 'primeiro', 'impeto', ',', 'a', 'mao', 'lesta', 'caiu', 'sobre', 'a', 'cruz', 'da', 'espada', 'mas', 'logo', 'sorriu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O moço guerreiro aprendeu na\n",
      "religião de sua mãe, onde a mulher é símbolo de ternura e amor'\n",
      "Tokens gerados: ['o', 'moco', 'guerreiro', 'aprendeu', 'nareligiao', 'de', 'sua', 'mae', ',', 'onde', 'a', 'mulher', 'e', 'simbolo', 'de', 'ternura', 'e', 'amor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sofreu mais d’alma que da ferida'\n",
      "Tokens gerados: ['sofreu', 'mais', 'd', '’', 'alma', 'que', 'da', 'ferida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O sentimento que ele pôs nos olhos e no rosto, não o sei eu'\n",
      "Tokens gerados: ['o', 'sentimento', 'que', 'ele', 'pos', 'nos', 'olhos', 'e', 'no', 'rosto', ',', 'nao', 'o', 'sei', 'eu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Porém a virgem lançou de si o arco e a uiraçaba, e\n",
      "correu para o guerreiro, sentida da mágoa que causara'\n",
      "Tokens gerados: ['porem', 'a', 'virgem', 'lancou', 'de', 'si', 'o', 'arco', 'e', 'a', 'uiracaba', ',', 'ecorreu', 'para', 'o', 'guerreiro', ',', 'sentida', 'da', 'magoa', 'que', 'causara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mão que rápida ferira, estancou mais rápida e compassiva o sangue que gotejava'\n",
      "Tokens gerados: ['a', 'mao', 'que', 'rapida', 'ferira', ',', 'estancou', 'mais', 'rapida', 'e', 'compassiva', 'o', 'sangue', 'que', 'gotejava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois Iracema quebrou a\n",
      "flecha homicida: deu a haste ao desconhecido, guardando consigo a ponta farpada'\n",
      "Tokens gerados: ['depois', 'iracema', 'quebrou', 'aflecha', 'homicida', 'deu', 'a', 'haste', 'ao', 'desconhecido', ',', 'guardando', 'consigo', 'a', 'ponta', 'farpada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O guerreiro falou:\n",
      "— Quebras comigo a flecha da paz?\n",
      "— Quem te ensinou, guerreiro branco, a linguagem de meus irmãos? Donde vieste a estas matas, que nunca\n",
      "viram outro guerreiro como tu?\n",
      "— Venho de bem longe, filha das florestas'\n",
      "Tokens gerados: ['o', 'guerreiro', 'falou—', 'quebras', 'comigo', 'a', 'flecha', 'da', 'paz', '?', '—', 'quem', 'te', 'ensinou', ',', 'guerreiro', 'branco', ',', 'a', 'linguagem', 'de', 'meus', 'irmaos', '?', 'donde', 'vieste', 'a', 'estas', 'matas', ',', 'que', 'nuncaviram', 'outro', 'guerreiro', 'como', 'tu', '?', '—', 'venho', 'de', 'bem', 'longe', ',', 'filha', 'das', 'florestas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Venho das terras que teus irmãos já possuíram, e hoje têm os meus'\n",
      "Tokens gerados: ['venho', 'das', 'terras', 'que', 'teus', 'irmaos', 'ja', 'possuiram', ',', 'e', 'hoje', 'tem', 'os', 'meus']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Bem-vindo seja o estrangeiro aos campos dos tabajaras, senhores das aldeias, e à cabana de Araquém, pai de\n",
      "Iracema'\n",
      "Tokens gerados: ['—', 'bem-vindo', 'seja', 'o', 'estrangeiro', 'aos', 'campos', 'dos', 'tabajaras', ',', 'senhores', 'das', 'aldeias', ',', 'e', 'a', 'cabana', 'de', 'araquem', ',', 'pai', 'deiracema']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_iracema_jose_de_alencar_cap_2.json\n",
      "\n",
      "Processando frase original (após split e strip): 'O estrangeiro seguiu a virgem através da floresta'\n",
      "Tokens gerados: ['o', 'estrangeiro', 'seguiu', 'a', 'virgem', 'atraves', 'da', 'floresta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando o Sol descambava sobre a crista dos montes, e a rola desatava do fundo da mata os primeiros arrulhos,\n",
      "eles descobriram no vale a grande taba; e mais longe, pendurada no rochedo, à sombra dos altos juazeiros, a cabana\n",
      "do pajé'\n",
      "Tokens gerados: ['quando', 'o', 'sol', 'descambava', 'sobre', 'a', 'crista', 'dos', 'montes', ',', 'e', 'a', 'rola', 'desatava', 'do', 'fundo', 'da', 'mata', 'os', 'primeiros', 'arrulhos', ',', 'eles', 'descobriram', 'no', 'vale', 'a', 'grande', 'taba', 'e', 'mais', 'longe', ',', 'pendurada', 'no', 'rochedo', ',', 'a', 'sombra', 'dos', 'altos', 'juazeiros', ',', 'a', 'cabanado', 'paje']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O ancião fumava à porta, sentado na esteira de carnaúba, meditando os sagrados ritos de Tupã'\n",
      "Tokens gerados: ['o', 'anciao', 'fumava', 'a', 'porta', ',', 'sentado', 'na', 'esteira', 'de', 'carnauba', ',', 'meditando', 'os', 'sagrados', 'ritos', 'de', 'tupa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O tênue sopro da\n",
      "brisa carmeava, como frocos de algodão, os compridos e raros cabelos brancos'\n",
      "Tokens gerados: ['o', 'tenue', 'sopro', 'dabrisa', 'carmeava', ',', 'como', 'frocos', 'de', 'algodao', ',', 'os', 'compridos', 'e', 'raros', 'cabelos', 'brancos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De imóvel que estava, sumia a vida\n",
      "nos olhos cavos e nas rugas profundas'\n",
      "Tokens gerados: ['de', 'imovel', 'que', 'estava', ',', 'sumia', 'a', 'vidanos', 'olhos', 'cavos', 'e', 'nas', 'rugas', 'profundas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O pajé lobrigou os dois vultos que avançavam; cuidou ver a sombra de uma árvore solitária que vinha\n",
      "alongando-se pelo vale fora'\n",
      "Tokens gerados: ['o', 'paje', 'lobrigou', 'os', 'dois', 'vultos', 'que', 'avancavam', 'cuidou', 'ver', 'a', 'sombra', 'de', 'uma', 'arvore', 'solitaria', 'que', 'vinhaalongando-se', 'pelo', 'vale', 'fora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando os viajantes entraram na densa penumbra do bosque, então seu olhar como o do tigre, afeito às trevas,\n",
      "conheceu Iracema e viu que a seguia um jovem guerreiro, de estranha raça e longes terras'\n",
      "Tokens gerados: ['quando', 'os', 'viajantes', 'entraram', 'na', 'densa', 'penumbra', 'do', 'bosque', ',', 'entao', 'seu', 'olhar', 'como', 'o', 'do', 'tigre', ',', 'afeito', 'as', 'trevas', ',', 'conheceu', 'iracema', 'e', 'viu', 'que', 'a', 'seguia', 'um', 'jovem', 'guerreiro', ',', 'de', 'estranha', 'raca', 'e', 'longes', 'terras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As tribos tabajaras, d’além Ibiapaba, falavam de uma nova raça de guerreiros, alvos como flores de borrasca, e\n",
      "vindos de remota plaga às margens do Mearim'\n",
      "Tokens gerados: ['as', 'tribos', 'tabajaras', ',', 'd', '’', 'alem', 'ibiapaba', ',', 'falavam', 'de', 'uma', 'nova', 'raca', 'de', 'guerreiros', ',', 'alvos', 'como', 'flores', 'de', 'borrasca', ',', 'evindos', 'de', 'remota', 'plaga', 'as', 'margens', 'do', 'mearim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O ancião pensou que fosse um guerreiro semelhante, aquele que\n",
      "pisava os campos nativos'\n",
      "Tokens gerados: ['o', 'anciao', 'pensou', 'que', 'fosse', 'um', 'guerreiro', 'semelhante', ',', 'aquele', 'quepisava', 'os', 'campos', 'nativos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tranqüilo, esperou'\n",
      "Tokens gerados: ['tranquilo', ',', 'esperou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A virgem aponta para o estrangeiro e diz:\n",
      "— Ele veio, pai'\n",
      "Tokens gerados: ['a', 'virgem', 'aponta', 'para', 'o', 'estrangeiro', 'e', 'diz—', 'ele', 'veio', ',', 'pai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Veio bem'\n",
      "Tokens gerados: ['—', 'veio', 'bem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É Tupã que traz o hóspede à cabana de Araquém'\n",
      "Tokens gerados: ['e', 'tupa', 'que', 'traz', 'o', 'hospede', 'a', 'cabana', 'de', 'araquem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Assim dizendo, o pajé passou o cachimbo ao estrangeiro; e entraram ambos na cabana'\n",
      "Tokens gerados: ['assim', 'dizendo', ',', 'o', 'paje', 'passou', 'o', 'cachimbo', 'ao', 'estrangeiro', 'e', 'entraram', 'ambos', 'na', 'cabana']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O mancebo sentou-se na rede principal, suspensa no centro da habitação'\n",
      "Tokens gerados: ['o', 'mancebo', 'sentou-se', 'na', 'rede', 'principal', ',', 'suspensa', 'no', 'centro', 'da', 'habitacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Iracema acendeu o fogo da hospitalidade; e trouxe o que havia de provisões para satisfazer a fome e a sede:\n",
      "trouxe o resto da caça, a farinha-d’água, os frutos silvestres, os favos de mel e o vinho de caju e ananás'\n",
      "Tokens gerados: ['iracema', 'acendeu', 'o', 'fogo', 'da', 'hospitalidade', 'e', 'trouxe', 'o', 'que', 'havia', 'de', 'provisoes', 'para', 'satisfazer', 'a', 'fome', 'e', 'a', 'sedetrouxe', 'o', 'resto', 'da', 'caca', ',', 'a', 'farinha-d', '’', 'agua', ',', 'os', 'frutos', 'silvestres', ',', 'os', 'favos', 'de', 'mel', 'e', 'o', 'vinho', 'de', 'caju', 'e', 'ananas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois a virgem entrou com a igaçaba, que enchera na fonte próxima de água fresca para lavar o rosto e as mãos\n",
      "do estrangeiro'\n",
      "Tokens gerados: ['depois', 'a', 'virgem', 'entrou', 'com', 'a', 'igacaba', ',', 'que', 'enchera', 'na', 'fonte', 'proxima', 'de', 'agua', 'fresca', 'para', 'lavar', 'o', 'rosto', 'e', 'as', 'maosdo', 'estrangeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando o guerreiro terminou a refeição, o velho pajé apagou o cachimbo e falou:\n",
      "— Vieste?\n",
      "— Vim, respondeu o desconhecido'\n",
      "Tokens gerados: ['quando', 'o', 'guerreiro', 'terminou', 'a', 'refeicao', ',', 'o', 'velho', 'paje', 'apagou', 'o', 'cachimbo', 'e', 'falou—', 'vieste', '?', '—', 'vim', ',', 'respondeu', 'o', 'desconhecido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Bem vieste'\n",
      "Tokens gerados: ['—', 'bem', 'vieste']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O estrangeiro é senhor na cabana de Araquém'\n",
      "Tokens gerados: ['o', 'estrangeiro', 'e', 'senhor', 'na', 'cabana', 'de', 'araquem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os tabajaras têm mil guerreiros para defendê-lo, e\n",
      "mulheres sem conta para servi-lo'\n",
      "Tokens gerados: ['os', 'tabajaras', 'tem', 'mil', 'guerreiros', 'para', 'defende-lo', ',', 'emulheres', 'sem', 'conta', 'para', 'servi-lo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dize, e todos te obedecerão'\n",
      "Tokens gerados: ['dize', ',', 'e', 'todos', 'te', 'obedecerao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pajé, eu te agradeço o agasalho que me deste'\n",
      "Tokens gerados: ['—', 'paje', ',', 'eu', 'te', 'agradeco', 'o', 'agasalho', 'que', 'me', 'deste']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Logo que o Sol nascer, deixarei tua cabana e teus campos\n",
      "aonde vim perdido; mas não devo deixá-los sem dizer-te quem é o guerreiro, que fizeste amigo'\n",
      "Tokens gerados: ['logo', 'que', 'o', 'sol', 'nascer', ',', 'deixarei', 'tua', 'cabana', 'e', 'teus', 'camposaonde', 'vim', 'perdido', 'mas', 'nao', 'devo', 'deixa-los', 'sem', 'dizer-te', 'quem', 'e', 'o', 'guerreiro', ',', 'que', 'fizeste', 'amigo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Foi a Tupã que o pajé serviu: ele te trouxe, ele te levará'\n",
      "Tokens gerados: ['—', 'foi', 'a', 'tupa', 'que', 'o', 'paje', 'serviu', 'ele', 'te', 'trouxe', ',', 'ele', 'te', 'levara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Araquém nada fez pelo hóspede; não pergunta donde\n",
      "vem, e quando vai'\n",
      "Tokens gerados: ['araquem', 'nada', 'fez', 'pelo', 'hospede', 'nao', 'pergunta', 'dondevem', ',', 'e', 'quando', 'vai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se queres dormir, desçam sobre ti os sonhos alegres; se queres falar, teu hóspede escuta'\n",
      "Tokens gerados: ['se', 'queres', 'dormir', ',', 'descam', 'sobre', 'ti', 'os', 'sonhos', 'alegres', 'se', 'queres', 'falar', ',', 'teu', 'hospede', 'escuta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O estrangeiro disse:\n",
      "— Sou dos guerreiros brancos, que levantaram a taba nas margens do Jaguaribe, perto do mar, onde habitam os\n",
      "pitiguaras, inimigos de tua nação'\n",
      "Tokens gerados: ['o', 'estrangeiro', 'disse—', 'sou', 'dos', 'guerreiros', 'brancos', ',', 'que', 'levantaram', 'a', 'taba', 'nas', 'margens', 'do', 'jaguaribe', ',', 'perto', 'do', 'mar', ',', 'onde', 'habitam', 'ospitiguaras', ',', 'inimigos', 'de', 'tua', 'nacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Meu nome é Martim, que na tua língua diz como filho de guerreiro; meu sangue, o\n",
      "do grande povo que primeiro viu as terras de tua pátria'\n",
      "Tokens gerados: ['meu', 'nome', 'e', 'martim', ',', 'que', 'na', 'tua', 'lingua', 'diz', 'como', 'filho', 'de', 'guerreiro', 'meu', 'sangue', ',', 'odo', 'grande', 'povo', 'que', 'primeiro', 'viu', 'as', 'terras', 'de', 'tua', 'patria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Já meus destroçados companheiros voltaram por mar às\n",
      "margens do Paraíba, de onde vieram; e o chefe, desamparado dos seus, atravessa agora os vastos sertões do Apodi'\n",
      "Tokens gerados: ['ja', 'meus', 'destrocados', 'companheiros', 'voltaram', 'por', 'mar', 'asmargens', 'do', 'paraiba', ',', 'de', 'onde', 'vieram', 'e', 'o', 'chefe', ',', 'desamparado', 'dos', 'seus', ',', 'atravessa', 'agora', 'os', 'vastos', 'sertoes', 'do', 'apodi']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Só eu de tantos fiquei, porque estava entre os pitiguaras de Acaraú, na cabana do bravo Poti, irmão de Jacaúna, que\n",
      "plantou comigo a árvore da amizade'\n",
      "Tokens gerados: ['so', 'eu', 'de', 'tantos', 'fiquei', ',', 'porque', 'estava', 'entre', 'os', 'pitiguaras', 'de', 'acarau', ',', 'na', 'cabana', 'do', 'bravo', 'poti', ',', 'irmao', 'de', 'jacauna', ',', 'queplantou', 'comigo', 'a', 'arvore', 'da', 'amizade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Há três sóis partimos para a caça; e perdido dos meus, vim aos campos dos\n",
      "tabajaras'\n",
      "Tokens gerados: ['ha', 'tres', 'sois', 'partimos', 'para', 'a', 'caca', 'e', 'perdido', 'dos', 'meus', ',', 'vim', 'aos', 'campos', 'dostabajaras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Foi algum mau espírito da floresta que cegou o guerreiro branco no escuro da mata, respondeu o ancião'\n",
      "Tokens gerados: ['—', 'foi', 'algum', 'mau', 'espirito', 'da', 'floresta', 'que', 'cegou', 'o', 'guerreiro', 'branco', 'no', 'escuro', 'da', 'mata', ',', 'respondeu', 'o', 'anciao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A cauã piou, além, na extrema do vale'\n",
      "Tokens gerados: ['a', 'caua', 'piou', ',', 'alem', ',', 'na', 'extrema', 'do', 'vale']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Caía a noite'\n",
      "Tokens gerados: ['caia', 'a', 'noite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_iracema_jose_de_alencar_cap_3.json\n",
      "\n",
      "Processando frase original (após split e strip): 'O pajé vibrou o maracá, e saiu da cabana, porém o estrangeiro não ficou só'\n",
      "Tokens gerados: ['o', 'paje', 'vibrou', 'o', 'maraca', ',', 'e', 'saiu', 'da', 'cabana', ',', 'porem', 'o', 'estrangeiro', 'nao', 'ficou', 'so']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Iracema voltara com as mulheres chamadas para servir o hóspede de Araquém, e os guerreiros vindos para\n",
      "obedecer-lhe'\n",
      "Tokens gerados: ['iracema', 'voltara', 'com', 'as', 'mulheres', 'chamadas', 'para', 'servir', 'o', 'hospede', 'de', 'araquem', ',', 'e', 'os', 'guerreiros', 'vindos', 'paraobedecer-lhe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Guerreiro branco, disse a virgem, o prazer embale tua rede durante a noite; e o Sol traga luz a teus olhos,\n",
      "alegria à tua alma'\n",
      "Tokens gerados: ['—', 'guerreiro', 'branco', ',', 'disse', 'a', 'virgem', ',', 'o', 'prazer', 'embale', 'tua', 'rede', 'durante', 'a', 'noite', 'e', 'o', 'sol', 'traga', 'luz', 'a', 'teus', 'olhos', ',', 'alegria', 'a', 'tua', 'alma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E assim dizendo, Iracema tinha o lábio trêmulo, e úmida a pálpebra'\n",
      "Tokens gerados: ['e', 'assim', 'dizendo', ',', 'iracema', 'tinha', 'o', 'labio', 'tremulo', ',', 'e', 'umida', 'a', 'palpebra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Tu me deixas? perguntou Martim'\n",
      "Tokens gerados: ['—', 'tu', 'me', 'deixas', '?', 'perguntou', 'martim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— As mais belas mulheres da grande taba contigo ficam'\n",
      "Tokens gerados: ['—', 'as', 'mais', 'belas', 'mulheres', 'da', 'grande', 'taba', 'contigo', 'ficam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Para elas a filha de Araquém não devia ter conduzido o hóspede à cabana do pajé'\n",
      "Tokens gerados: ['—', 'para', 'elas', 'a', 'filha', 'de', 'araquem', 'nao', 'devia', 'ter', 'conduzido', 'o', 'hospede', 'a', 'cabana', 'do', 'paje']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Estrangeiro, Iracema não pode ser tua serva'\n",
      "Tokens gerados: ['—', 'estrangeiro', ',', 'iracema', 'nao', 'pode', 'ser', 'tua', 'serva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É ela que guarda o segredo da jurema e o mistério do sonho'\n",
      "Tokens gerados: ['e', 'ela', 'que', 'guarda', 'o', 'segredo', 'da', 'jurema', 'e', 'o', 'misterio', 'do', 'sonho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sua\n",
      "mão fabrica para o pajé a bebida de Tupã'\n",
      "Tokens gerados: ['suamao', 'fabrica', 'para', 'o', 'paje', 'a', 'bebida', 'de', 'tupa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O guerreiro cristão atravessou a cabana e sumiu-se na treva'\n",
      "Tokens gerados: ['o', 'guerreiro', 'cristao', 'atravessou', 'a', 'cabana', 'e', 'sumiu-se', 'na', 'treva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A grande taba erguia-se no fundo do vale, iluminada pelos fachos da alegria'\n",
      "Tokens gerados: ['a', 'grande', 'taba', 'erguia-se', 'no', 'fundo', 'do', 'vale', ',', 'iluminada', 'pelos', 'fachos', 'da', 'alegria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Rugia o maracá; ao quebro lento do\n",
      "canto selvagem, batia a dança em trono a rude cadência'\n",
      "Tokens gerados: ['rugia', 'o', 'maraca', 'ao', 'quebro', 'lento', 'docanto', 'selvagem', ',', 'batia', 'a', 'danca', 'em', 'trono', 'a', 'rude', 'cadencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O pajé inspirado conduzia o sagrado tripúdio e dizia ao\n",
      "povo crente os segredos de Tupã'\n",
      "Tokens gerados: ['o', 'paje', 'inspirado', 'conduzia', 'o', 'sagrado', 'tripudio', 'e', 'dizia', 'aopovo', 'crente', 'os', 'segredos', 'de', 'tupa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O maior chefe da nação tabajara, Irapuã, descera do alto da serra Ibiapaba, para levar as tribos do sertão contra o\n",
      "inimigo pitiguara'\n",
      "Tokens gerados: ['o', 'maior', 'chefe', 'da', 'nacao', 'tabajara', ',', 'irapua', ',', 'descera', 'do', 'alto', 'da', 'serra', 'ibiapaba', ',', 'para', 'levar', 'as', 'tribos', 'do', 'sertao', 'contra', 'oinimigo', 'pitiguara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os guerreiros do vale festejam a vinda do chefe, e o próximo combate'\n",
      "Tokens gerados: ['os', 'guerreiros', 'do', 'vale', 'festejam', 'a', 'vinda', 'do', 'chefe', ',', 'e', 'o', 'proximo', 'combate']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O mancebo cristão viu longe o clarão da festa, e passou além, e olhou o céu azul sem nuvens'\n",
      "Tokens gerados: ['o', 'mancebo', 'cristao', 'viu', 'longe', 'o', 'clarao', 'da', 'festa', ',', 'e', 'passou', 'alem', ',', 'e', 'olhou', 'o', 'ceu', 'azul', 'sem', 'nuvens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A estrela morta,\n",
      "que então brilhava sobre a cúpula da floresta, guiou seu passo firme para as frescas margens do Acaraú'\n",
      "Tokens gerados: ['a', 'estrela', 'morta', ',', 'que', 'entao', 'brilhava', 'sobre', 'a', 'cupula', 'da', 'floresta', ',', 'guiou', 'seu', 'passo', 'firme', 'para', 'as', 'frescas', 'margens', 'do', 'acarau']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando ele transmontou o vale e ia penetrar na mata, o vulto de Iracema surgiu'\n",
      "Tokens gerados: ['quando', 'ele', 'transmontou', 'o', 'vale', 'e', 'ia', 'penetrar', 'na', 'mata', ',', 'o', 'vulto', 'de', 'iracema', 'surgiu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A virgem seguira o estrangeiro\n",
      "como a brisa sutil que resvala sem murmurejar por entre a ramagem'\n",
      "Tokens gerados: ['a', 'virgem', 'seguira', 'o', 'estrangeirocomo', 'a', 'brisa', 'sutil', 'que', 'resvala', 'sem', 'murmurejar', 'por', 'entre', 'a', 'ramagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Por que, disse ela, o estrangeiro abandona a cabana hospedeira sem levar o presente da volta? Quem fez mal\n",
      "ao guerreiro branco na terra dos tabajaras?\n",
      "O cristão sentiu quanto era justa a queixa; e achou-se ingrato'\n",
      "Tokens gerados: ['—', 'por', 'que', ',', 'disse', 'ela', ',', 'o', 'estrangeiro', 'abandona', 'a', 'cabana', 'hospedeira', 'sem', 'levar', 'o', 'presente', 'da', 'volta', '?', 'quem', 'fez', 'malao', 'guerreiro', 'branco', 'na', 'terra', 'dos', 'tabajaras', '?', 'o', 'cristao', 'sentiu', 'quanto', 'era', 'justa', 'a', 'queixa', 'e', 'achou-se', 'ingrato']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ninguém fez mal ao teu hóspede, filha de Araquém'\n",
      "Tokens gerados: ['—', 'ninguem', 'fez', 'mal', 'ao', 'teu', 'hospede', ',', 'filha', 'de', 'araquem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era o desejo de ver seus amigos que o afastava dos\n",
      "campos dos tabajaras'\n",
      "Tokens gerados: ['era', 'o', 'desejo', 'de', 'ver', 'seus', 'amigos', 'que', 'o', 'afastava', 'doscampos', 'dos', 'tabajaras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não levava o presente da volta; mas leva em sua alma a lembrança de Iracema'\n",
      "Tokens gerados: ['nao', 'levava', 'o', 'presente', 'da', 'volta', 'mas', 'leva', 'em', 'sua', 'alma', 'a', 'lembranca', 'de', 'iracema']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Se a lembrança de Iracema estivesse n’alma do estrangeiro, ela não o deixaria partir'\n",
      "Tokens gerados: ['—', 'se', 'a', 'lembranca', 'de', 'iracema', 'estivesse', 'n', '’', 'alma', 'do', 'estrangeiro', ',', 'ela', 'nao', 'o', 'deixaria', 'partir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O vento não leva a areia\n",
      "da várzea, quando a areia bebe a água da chuva'\n",
      "Tokens gerados: ['o', 'vento', 'nao', 'leva', 'a', 'areiada', 'varzea', ',', 'quando', 'a', 'areia', 'bebe', 'a', 'agua', 'da', 'chuva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A virgem suspirou:\n",
      "— Guerreiro branco, espera que Caubi volte da caça'\n",
      "Tokens gerados: ['a', 'virgem', 'suspirou—', 'guerreiro', 'branco', ',', 'espera', 'que', 'caubi', 'volte', 'da', 'caca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O irmão de Iracema tem o ouvido sutil que pressente a\n",
      "boicininga entre os rumores da mata; e o olhar do oitibó que vê melhor na treva'\n",
      "Tokens gerados: ['o', 'irmao', 'de', 'iracema', 'tem', 'o', 'ouvido', 'sutil', 'que', 'pressente', 'aboicininga', 'entre', 'os', 'rumores', 'da', 'mata', 'e', 'o', 'olhar', 'do', 'oitibo', 'que', 've', 'melhor', 'na', 'treva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ele te guiará às margens do rio das\n",
      "garças'\n",
      "Tokens gerados: ['ele', 'te', 'guiara', 'as', 'margens', 'do', 'rio', 'dasgarcas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Quanto tempo se passará antes que o irmão de Iracema esteja de volta na cabana de Araquém?\n",
      "— O Sol, que vai nascer, tornará com o guerreiro Caubi aos campos do Ipu'\n",
      "Tokens gerados: ['—', 'quanto', 'tempo', 'se', 'passara', 'antes', 'que', 'o', 'irmao', 'de', 'iracema', 'esteja', 'de', 'volta', 'na', 'cabana', 'de', 'araquem', '?', '—', 'o', 'sol', ',', 'que', 'vai', 'nascer', ',', 'tornara', 'com', 'o', 'guerreiro', 'caubi', 'aos', 'campos', 'do', 'ipu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Teu hóspede espera, filha de Araquém; mas se o Sol tornando não trouxer o irmão de Iracema, ele levará o\n",
      "guerreiro branco à taba dos pitiguaras'\n",
      "Tokens gerados: ['—', 'teu', 'hospede', 'espera', ',', 'filha', 'de', 'araquem', 'mas', 'se', 'o', 'sol', 'tornando', 'nao', 'trouxer', 'o', 'irmao', 'de', 'iracema', ',', 'ele', 'levara', 'oguerreiro', 'branco', 'a', 'taba', 'dos', 'pitiguaras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Martim voltou à cabana do pajé'\n",
      "Tokens gerados: ['martim', 'voltou', 'a', 'cabana', 'do', 'paje']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A alva rede que Iracema perfumara com a resina do benjoim guardava-lhe um sono calmo e doce'\n",
      "Tokens gerados: ['a', 'alva', 'rede', 'que', 'iracema', 'perfumara', 'com', 'a', 'resina', 'do', 'benjoim', 'guardava-lhe', 'um', 'sono', 'calmo', 'e', 'doce']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cristão adormeceu ouvindo suspirar, entre os murmúrios da floresta, o canto mavioso da virgem indiana'\n",
      "Tokens gerados: ['o', 'cristao', 'adormeceu', 'ouvindo', 'suspirar', ',', 'entre', 'os', 'murmurios', 'da', 'floresta', ',', 'o', 'canto', 'mavioso', 'da', 'virgem', 'indiana']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_iracema_jose_de_alencar_cap_4.json\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão foi, dos treze aos vinte e cinco anos, empregado de um vendeiro que enriqueceu entre as quatro \n",
      "paredes de uma suja e obscura taverna nos refolhos do bairro do Botafogo; e tanto economizou do pouco que ganhara \n",
      "nessa dúzia de anos, que, ao retirar-se o patrão para a terra, lhe deixou, em pagamento de ordenados vencidos, nem só a \n",
      "venda com o que estava dentro, como ainda um conto e quinhentos em dinheiro'\n",
      "Tokens gerados: ['joao', 'romao', 'foi', ',', 'dos', 'treze', 'aos', 'vinte', 'e', 'cinco', 'anos', ',', 'empregado', 'de', 'um', 'vendeiro', 'que', 'enriqueceu', 'entre', 'as', 'quatro', 'paredes', 'de', 'uma', 'suja', 'e', 'obscura', 'taverna', 'nos', 'refolhos', 'do', 'bairro', 'do', 'botafogo', 'e', 'tanto', 'economizou', 'do', 'pouco', 'que', 'ganhara', 'nessa', 'duzia', 'de', 'anos', ',', 'que', ',', 'ao', 'retirar-se', 'o', 'patrao', 'para', 'a', 'terra', ',', 'lhe', 'deixou', ',', 'em', 'pagamento', 'de', 'ordenados', 'vencidos', ',', 'nem', 'so', 'a', 'venda', 'com', 'o', 'que', 'estava', 'dentro', ',', 'como', 'ainda', 'um', 'conto', 'e', 'quinhentos', 'em', 'dinheiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Proprietário e estabelecido por sua conta, o rapaz atirou-se à labutação ainda com mais ardor, possuindo-se de tal \n",
      "delírio de enriquecer, que afrontava resignado as mais duras privações'\n",
      "Tokens gerados: ['proprietario', 'e', 'estabelecido', 'por', 'sua', 'conta', ',', 'o', 'rapaz', 'atirou-se', 'a', 'labutacao', 'ainda', 'com', 'mais', 'ardor', ',', 'possuindo-se', 'de', 'tal', 'delirio', 'de', 'enriquecer', ',', 'que', 'afrontava', 'resignado', 'as', 'mais', 'duras', 'privacoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dormia sobre o balcão da própria venda, em \n",
      "cima de uma esteira, fazendo travesseiro de um saco de estopa cheio de palha'\n",
      "Tokens gerados: ['dormia', 'sobre', 'o', 'balcao', 'da', 'propria', 'venda', ',', 'em', 'cima', 'de', 'uma', 'esteira', ',', 'fazendo', 'travesseiro', 'de', 'um', 'saco', 'de', 'estopa', 'cheio', 'de', 'palha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A comida arranjava-lha, mediante \n",
      "quatrocentos réis por dia, uma quitandeira sua vizinha, a Bertoleza, crioula trintona, escrava de um velho cego residente \n",
      "em Juiz de Fora e amigada com um português que tinha uma carroça de mão e fazia fretes na cidade'\n",
      "Tokens gerados: ['a', 'comida', 'arranjava-lha', ',', 'mediante', 'quatrocentos', 'reis', 'por', 'dia', ',', 'uma', 'quitandeira', 'sua', 'vizinha', ',', 'a', 'bertoleza', ',', 'crioula', 'trintona', ',', 'escrava', 'de', 'um', 'velho', 'cego', 'residente', 'em', 'juiz', 'de', 'fora', 'e', 'amigada', 'com', 'um', 'portugues', 'que', 'tinha', 'uma', 'carroca', 'de', 'mao', 'e', 'fazia', 'fretes', 'na', 'cidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bertoleza também trabalhava forte; a sua quitanda era a mais bem afreguesada do bairro'\n",
      "Tokens gerados: ['bertoleza', 'tambem', 'trabalhava', 'forte', 'a', 'sua', 'quitanda', 'era', 'a', 'mais', 'bem', 'afreguesada', 'do', 'bairro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De manhã vendia angu, \n",
      "e à noite peixe frito e iscas de fígado; pagava de jornal a seu dono vinte mil-réis por mês, e, apesar disso, tinha de parte \n",
      "quase que o necessário para a alforria'\n",
      "Tokens gerados: ['de', 'manha', 'vendia', 'angu', ',', 'e', 'a', 'noite', 'peixe', 'frito', 'e', 'iscas', 'de', 'figado', 'pagava', 'de', 'jornal', 'a', 'seu', 'dono', 'vinte', 'mil-reis', 'por', 'mes', ',', 'e', ',', 'apesar', 'disso', ',', 'tinha', 'de', 'parte', 'quase', 'que', 'o', 'necessario', 'para', 'a', 'alforria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um dia, porém, o seu homem, depois de correr meia légua, puxando uma carga \n",
      "superior às suas forças, caiu morto na rua, ao lado da carroça, estrompado como uma besta'\n",
      "Tokens gerados: ['um', 'dia', ',', 'porem', ',', 'o', 'seu', 'homem', ',', 'depois', 'de', 'correr', 'meia', 'legua', ',', 'puxando', 'uma', 'carga', 'superior', 'as', 'suas', 'forcas', ',', 'caiu', 'morto', 'na', 'rua', ',', 'ao', 'lado', 'da', 'carroca', ',', 'estrompado', 'como', 'uma', 'besta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão mostrou grande interesse por esta desgraça, fez-se até participante direto dos sofrimentos da vizinha, \n",
      "e com tamanho empenho a lamentou, que a boa mulher o escolheu para confidente das suas desventuras'\n",
      "Tokens gerados: ['joao', 'romao', 'mostrou', 'grande', 'interesse', 'por', 'esta', 'desgraca', ',', 'fez-se', 'ate', 'participante', 'direto', 'dos', 'sofrimentos', 'da', 'vizinha', ',', 'e', 'com', 'tamanho', 'empenho', 'a', 'lamentou', ',', 'que', 'a', 'boa', 'mulher', 'o', 'escolheu', 'para', 'confidente', 'das', 'suas', 'desventuras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Abriu-se com \n",
      "ele, contou-lhe a sua vida de amofinações e dificuldades'\n",
      "Tokens gerados: ['abriu-se', 'com', 'ele', ',', 'contou-lhe', 'a', 'sua', 'vida', 'de', 'amofinacoes', 'e', 'dificuldades']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Seu senhor comia-lhe a pele do corpo! Não era brinquedo \n",
      "para uma pobre mulher ter de escarrar pr’ali, todos os meses, vinte mil-réis em dinheiro!” E segredou-lhe então o que \n",
      "tinha juntado para a sua liberdade e acabou pedindo ao vendeiro que lhe guardasse as economias, porque já de certa vez \n",
      "fora roubada por gatunos que lhe entraram na quitanda pelos fundos'\n",
      "Tokens gerados: ['“', 'seu', 'senhor', 'comia-lhe', 'a', 'pele', 'do', 'corpo', 'nao', 'era', 'brinquedo', 'para', 'uma', 'pobre', 'mulher', 'ter', 'de', 'escarrar', 'pr', '’', 'ali', ',', 'todos', 'os', 'meses', ',', 'vinte', 'mil-reis', 'em', 'dinheiro', '”', 'e', 'segredou-lhe', 'entao', 'o', 'que', 'tinha', 'juntado', 'para', 'a', 'sua', 'liberdade', 'e', 'acabou', 'pedindo', 'ao', 'vendeiro', 'que', 'lhe', 'guardasse', 'as', 'economias', ',', 'porque', 'ja', 'de', 'certa', 'vez', 'fora', 'roubada', 'por', 'gatunos', 'que', 'lhe', 'entraram', 'na', 'quitanda', 'pelos', 'fundos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Daí em diante, João Romão tornou-se o caixa, o procurador e o conselheiro da crioula'\n",
      "Tokens gerados: ['dai', 'em', 'diante', ',', 'joao', 'romao', 'tornou-se', 'o', 'caixa', ',', 'o', 'procurador', 'e', 'o', 'conselheiro', 'da', 'crioula']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No fim de pouco tempo \n",
      "era ele quem tomava conta de tudo que ela produzia e era também quem punha e dispunha dos seus pecúlios, e quem se \n",
      "encarregava de remeter ao senhor os vinte mil-réis mensais'\n",
      "Tokens gerados: ['no', 'fim', 'de', 'pouco', 'tempo', 'era', 'ele', 'quem', 'tomava', 'conta', 'de', 'tudo', 'que', 'ela', 'produzia', 'e', 'era', 'tambem', 'quem', 'punha', 'e', 'dispunha', 'dos', 'seus', 'peculios', ',', 'e', 'quem', 'se', 'encarregava', 'de', 'remeter', 'ao', 'senhor', 'os', 'vinte', 'mil-reis', 'mensais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Abriu-lhe logo uma conta corrente, e a quitandeira, quando \n",
      "precisava de dinheiro para qualquer coisa, dava um pulo até à venda e recebia-o das mãos do vendeiro, de “Seu João”, \n",
      "como ela dizia'\n",
      "Tokens gerados: ['abriu-lhe', 'logo', 'uma', 'conta', 'corrente', ',', 'e', 'a', 'quitandeira', ',', 'quando', 'precisava', 'de', 'dinheiro', 'para', 'qualquer', 'coisa', ',', 'dava', 'um', 'pulo', 'ate', 'a', 'venda', 'e', 'recebia-o', 'das', 'maos', 'do', 'vendeiro', ',', 'de', '“', 'seu', 'joao', '”', ',', 'como', 'ela', 'dizia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seu João debitava metodicamente essas pequenas quantias num caderninho, em cuja capa de papel \n",
      "pardo lia-se, mal escrito e em letras cortadas de jornal: “Ativo e passivo de Bertoleza”'\n",
      "Tokens gerados: ['seu', 'joao', 'debitava', 'metodicamente', 'essas', 'pequenas', 'quantias', 'num', 'caderninho', ',', 'em', 'cuja', 'capa', 'de', 'papel', 'pardo', 'lia-se', ',', 'mal', 'escrito', 'e', 'em', 'letras', 'cortadas', 'de', 'jornal', '“', 'ativo', 'e', 'passivo', 'de', 'bertoleza', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E por tal forma foi o taverneiro ganhando confiança no espírito da mulher, que esta afinal nada mais resolvia só \n",
      "por si, e aceitava dele, cegamente, todo e qualquer arbítrio'\n",
      "Tokens gerados: ['e', 'por', 'tal', 'forma', 'foi', 'o', 'taverneiro', 'ganhando', 'confianca', 'no', 'espirito', 'da', 'mulher', ',', 'que', 'esta', 'afinal', 'nada', 'mais', 'resolvia', 'so', 'por', 'si', ',', 'e', 'aceitava', 'dele', ',', 'cegamente', ',', 'todo', 'e', 'qualquer', 'arbitrio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por último, se alguém precisava tratar com ela qualquer \n",
      "negócio, nem mais se dava ao trabalho de procurá-la, ia logo direito a João Romão'\n",
      "Tokens gerados: ['por', 'ultimo', ',', 'se', 'alguem', 'precisava', 'tratar', 'com', 'ela', 'qualquer', 'negocio', ',', 'nem', 'mais', 'se', 'dava', 'ao', 'trabalho', 'de', 'procura-la', ',', 'ia', 'logo', 'direito', 'a', 'joao', 'romao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando deram fé estavam amigados'\n",
      "Tokens gerados: ['quando', 'deram', 'fe', 'estavam', 'amigados']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ele propôs-lhe morarem juntos e ela concordou de braços abertos, feliz em meter-se de novo com um português, \n",
      "porque, como toda a cafuza, Bertoleza não queria sujeitar-se a negros e procurava instintivamente o homem numa raça \n",
      "superior à sua'\n",
      "Tokens gerados: ['ele', 'propos-lhe', 'morarem', 'juntos', 'e', 'ela', 'concordou', 'de', 'bracos', 'abertos', ',', 'feliz', 'em', 'meter-se', 'de', 'novo', 'com', 'um', 'portugues', ',', 'porque', ',', 'como', 'toda', 'a', 'cafuza', ',', 'bertoleza', 'nao', 'queria', 'sujeitar-se', 'a', 'negros', 'e', 'procurava', 'instintivamente', 'o', 'homem', 'numa', 'raca', 'superior', 'a', 'sua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão comprou então, com as economias da amiga, alguns palmos de terreno ao lado esquerdo da venda, e \n",
      "levantou uma casinha de duas portas, dividida ao meio paralelamente à rua, sendo a parte da frente destinada à quitanda \n",
      "e a do fundo para um dormitório que se arranjou com os cacarecos de Bertoleza'\n",
      "Tokens gerados: ['joao', 'romao', 'comprou', 'entao', ',', 'com', 'as', 'economias', 'da', 'amiga', ',', 'alguns', 'palmos', 'de', 'terreno', 'ao', 'lado', 'esquerdo', 'da', 'venda', ',', 'e', 'levantou', 'uma', 'casinha', 'de', 'duas', 'portas', ',', 'dividida', 'ao', 'meio', 'paralelamente', 'a', 'rua', ',', 'sendo', 'a', 'parte', 'da', 'frente', 'destinada', 'a', 'quitanda', 'e', 'a', 'do', 'fundo', 'para', 'um', 'dormitorio', 'que', 'se', 'arranjou', 'com', 'os', 'cacarecos', 'de', 'bertoleza']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia, além da cama, uma cômoda de \n",
      "jacarandá muito velha com maçanetas de metal amarelo já mareadas, um oratório cheio de santos e forrado de papel de \n",
      "cor, um baú grande de couro cru tacheado, dois banquinhos de pau feitos de uma só peça e um formidável cabide de \n",
      "pregar na parede, com a sua competente coberta de retalhos de chita'\n",
      "Tokens gerados: ['havia', ',', 'alem', 'da', 'cama', ',', 'uma', 'comoda', 'de', 'jacaranda', 'muito', 'velha', 'com', 'macanetas', 'de', 'metal', 'amarelo', 'ja', 'mareadas', ',', 'um', 'oratorio', 'cheio', 'de', 'santos', 'e', 'forrado', 'de', 'papel', 'de', 'cor', ',', 'um', 'bau', 'grande', 'de', 'couro', 'cru', 'tacheado', ',', 'dois', 'banquinhos', 'de', 'pau', 'feitos', 'de', 'uma', 'so', 'peca', 'e', 'um', 'formidavel', 'cabide', 'de', 'pregar', 'na', 'parede', ',', 'com', 'a', 'sua', 'competente', 'coberta', 'de', 'retalhos', 'de', 'chita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O vendeiro nunca tivera tanta mobília'\n",
      "Tokens gerados: ['o', 'vendeiro', 'nunca', 'tivera', 'tanta', 'mobilia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Agora, disse ele à crioula, as coisas vão correr melhor para você'\n",
      "Tokens gerados: ['—', 'agora', ',', 'disse', 'ele', 'a', 'crioula', ',', 'as', 'coisas', 'vao', 'correr', 'melhor', 'para', 'voce']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Você vai ficar forra; eu entro com o que falta'\n",
      "Tokens gerados: ['voce', 'vai', 'ficar', 'forra', 'eu', 'entro', 'com', 'o', 'que', 'falta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nesse dia ele saiu muito à rua, e uma semana depois apareceu com uma folha de papel toda escrita, que leu em \n",
      "voz alta à companheira'\n",
      "Tokens gerados: ['nesse', 'dia', 'ele', 'saiu', 'muito', 'a', 'rua', ',', 'e', 'uma', 'semana', 'depois', 'apareceu', 'com', 'uma', 'folha', 'de', 'papel', 'toda', 'escrita', ',', 'que', 'leu', 'em', 'voz', 'alta', 'a', 'companheira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Você agora não tem mais senhor! declarou em seguida à leitura, que ela ouviu entre lágrimas agradecidas'\n",
      "Tokens gerados: ['—', 'voce', 'agora', 'nao', 'tem', 'mais', 'senhor', 'declarou', 'em', 'seguida', 'a', 'leitura', ',', 'que', 'ela', 'ouviu', 'entre', 'lagrimas', 'agradecidas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Agora está livre'\n",
      "Tokens gerados: ['agora', 'esta', 'livre']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Doravante o que você fizer é só seu e mais de seus filhos, se os tiver'\n",
      "Tokens gerados: ['doravante', 'o', 'que', 'voce', 'fizer', 'e', 'so', 'seu', 'e', 'mais', 'de', 'seus', 'filhos', ',', 'se', 'os', 'tiver']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Acabou-se o cativeiro de pagar \n",
      "os vinte mil-réis à peste do cego! \n",
      "— Coitado! A gente se queixa é da sorte! Ele, como meu senhor, exigia o jornal, exigia o que era seu! \n",
      "— Seu ou não seu, acabou-se! E vida nova! \n",
      "Contra todo o costume, abriu-se nesse dia uma garrafa de vinho do Porto, e os dois beberam-na em honra ao \n",
      "grande acontecimento'\n",
      "Tokens gerados: ['acabou-se', 'o', 'cativeiro', 'de', 'pagar', 'os', 'vinte', 'mil-reis', 'a', 'peste', 'do', 'cego', '—', 'coitado', 'a', 'gente', 'se', 'queixa', 'e', 'da', 'sorte', 'ele', ',', 'como', 'meu', 'senhor', ',', 'exigia', 'o', 'jornal', ',', 'exigia', 'o', 'que', 'era', 'seu', '—', 'seu', 'ou', 'nao', 'seu', ',', 'acabou-se', 'e', 'vida', 'nova', 'contra', 'todo', 'o', 'costume', ',', 'abriu-se', 'nesse', 'dia', 'uma', 'garrafa', 'de', 'vinho', 'do', 'porto', ',', 'e', 'os', 'dois', 'beberam-na', 'em', 'honra', 'ao', 'grande', 'acontecimento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, a tal carta de liberdade era obra do próprio João Romão, e nem mesmo o selo, que \n",
      "ele entendeu de pespegar-lhe em cima, para dar à burla maior formalidade, representava despesa porque o esperto \n",
      "aproveitara uma estampilha já servida'\n",
      "Tokens gerados: ['entretanto', ',', 'a', 'tal', 'carta', 'de', 'liberdade', 'era', 'obra', 'do', 'proprio', 'joao', 'romao', ',', 'e', 'nem', 'mesmo', 'o', 'selo', ',', 'que', 'ele', 'entendeu', 'de', 'pespegar-lhe', 'em', 'cima', ',', 'para', 'dar', 'a', 'burla', 'maior', 'formalidade', ',', 'representava', 'despesa', 'porque', 'o', 'esperto', 'aproveitara', 'uma', 'estampilha', 'ja', 'servida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O senhor de Bertoleza não teve sequer conhecimento do fato; o que lhe constou, \n",
      "sim, foi que a sua escrava lhe havia fugido para a Bahia depois da morte do amigo'\n",
      "Tokens gerados: ['o', 'senhor', 'de', 'bertoleza', 'nao', 'teve', 'sequer', 'conhecimento', 'do', 'fato', 'o', 'que', 'lhe', 'constou', ',', 'sim', ',', 'foi', 'que', 'a', 'sua', 'escrava', 'lhe', 'havia', 'fugido', 'para', 'a', 'bahia', 'depois', 'da', 'morte', 'do', 'amigo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— O cego que venha buscá-la aqui, se for capaz'\n",
      "Tokens gerados: ['—', 'o', 'cego', 'que', 'venha', 'busca-la', 'aqui', ',', 'se', 'for', 'capaz']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'desafiou o vendeiro de si para si'\n",
      "Tokens gerados: ['desafiou', 'o', 'vendeiro', 'de', 'si', 'para', 'si']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ele que caia nessa e verá se \n",
      "tem ou não pra pêras! \n",
      "Não obstante, só ficou tranqüilo de todo daí a três meses, quando lhe constou a morte do velho'\n",
      "Tokens gerados: ['ele', 'que', 'caia', 'nessa', 'e', 'vera', 'se', 'tem', 'ou', 'nao', 'pra', 'peras', 'nao', 'obstante', ',', 'so', 'ficou', 'tranquilo', 'de', 'todo', 'dai', 'a', 'tres', 'meses', ',', 'quando', 'lhe', 'constou', 'a', 'morte', 'do', 'velho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A escrava passara \n",
      "naturalmente em herança a qualquer dos filhos do morto; mas, por estes, nada havia que recear: dois pândegos de marca \n",
      "maior que, empolgada a legitima, cuidariam de tudo, menos de atirar-se na pista de uma crioula a quem não viam de \n",
      "muitos anos àquela parte'\n",
      "Tokens gerados: ['a', 'escrava', 'passara', 'naturalmente', 'em', 'heranca', 'a', 'qualquer', 'dos', 'filhos', 'do', 'morto', 'mas', ',', 'por', 'estes', ',', 'nada', 'havia', 'que', 'recear', 'dois', 'pandegos', 'de', 'marca', 'maior', 'que', ',', 'empolgada', 'a', 'legitima', ',', 'cuidariam', 'de', 'tudo', ',', 'menos', 'de', 'atirar-se', 'na', 'pista', 'de', 'uma', 'crioula', 'a', 'quem', 'nao', 'viam', 'de', 'muitos', 'anos', 'aquela', 'parte']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Ora! bastava já, e não era pouco, o que lhe tinham sugado durante tanto tempo!” \n",
      "Bertoleza representava agora ao lado de João Romão o papel tríplice de caixeiro, de criada e de amante'\n",
      "Tokens gerados: ['“', 'ora', 'bastava', 'ja', ',', 'e', 'nao', 'era', 'pouco', ',', 'o', 'que', 'lhe', 'tinham', 'sugado', 'durante', 'tanto', 'tempo', '”', 'bertoleza', 'representava', 'agora', 'ao', 'lado', 'de', 'joao', 'romao', 'o', 'papel', 'triplice', 'de', 'caixeiro', ',', 'de', 'criada', 'e', 'de', 'amante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mourejava a valer, mas de cara alegre; às quatro da madrugada estava já na faina de todos os dias, aviando o café para \n",
      "os fregueses e depois preparando o almoço para os trabalhadores de uma pedreira que havia para além de um grande \n",
      "capinzal aos fundos da venda'\n",
      "Tokens gerados: ['mourejava', 'a', 'valer', ',', 'mas', 'de', 'cara', 'alegre', 'as', 'quatro', 'da', 'madrugada', 'estava', 'ja', 'na', 'faina', 'de', 'todos', 'os', 'dias', ',', 'aviando', 'o', 'cafe', 'para', 'os', 'fregueses', 'e', 'depois', 'preparando', 'o', 'almoco', 'para', 'os', 'trabalhadores', 'de', 'uma', 'pedreira', 'que', 'havia', 'para', 'alem', 'de', 'um', 'grande', 'capinzal', 'aos', 'fundos', 'da', 'venda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Varria a casa, cozinhava, vendia ao balcão na taverna, quando o amigo andava ocupado \n",
      "lá por fora; fazia a sua quitanda durante o dia no intervalo de outros serviços, e à noite passava-se para a porta da venda, \n",
      "e, defronte de um fogareiro de barro, fritava fígado e frigia sardinhas, que Romão ia pela manhã, em mangas de camisa, \n",
      "de tamancos e sem meias, comprar à praia do Peixe'\n",
      "Tokens gerados: ['varria', 'a', 'casa', ',', 'cozinhava', ',', 'vendia', 'ao', 'balcao', 'na', 'taverna', ',', 'quando', 'o', 'amigo', 'andava', 'ocupado', 'la', 'por', 'fora', 'fazia', 'a', 'sua', 'quitanda', 'durante', 'o', 'dia', 'no', 'intervalo', 'de', 'outros', 'servicos', ',', 'e', 'a', 'noite', 'passava-se', 'para', 'a', 'porta', 'da', 'venda', ',', 'e', ',', 'defronte', 'de', 'um', 'fogareiro', 'de', 'barro', ',', 'fritava', 'figado', 'e', 'frigia', 'sardinhas', ',', 'que', 'romao', 'ia', 'pela', 'manha', ',', 'em', 'mangas', 'de', 'camisa', ',', 'de', 'tamancos', 'e', 'sem', 'meias', ',', 'comprar', 'a', 'praia', 'do', 'peixe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o demônio da mulher ainda encontrava tempo para lavar e \n",
      "consertar, além da sua, a roupa do seu homem, que esta, valha a verdade, não era tanta e nunca passava em todo o mês \n",
      "de alguns pares de calças de zuarte e outras tantas camisas de riscado'\n",
      "Tokens gerados: ['e', 'o', 'demonio', 'da', 'mulher', 'ainda', 'encontrava', 'tempo', 'para', 'lavar', 'e', 'consertar', ',', 'alem', 'da', 'sua', ',', 'a', 'roupa', 'do', 'seu', 'homem', ',', 'que', 'esta', ',', 'valha', 'a', 'verdade', ',', 'nao', 'era', 'tanta', 'e', 'nunca', 'passava', 'em', 'todo', 'o', 'mes', 'de', 'alguns', 'pares', 'de', 'calcas', 'de', 'zuarte', 'e', 'outras', 'tantas', 'camisas', 'de', 'riscado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão não saia nunca a passeio, nem ia à missa aos domingos; tudo que rendia a sua venda e mais a \n",
      "quitanda seguia direitinho para a caixa econômica e daí então para o banco'\n",
      "Tokens gerados: ['joao', 'romao', 'nao', 'saia', 'nunca', 'a', 'passeio', ',', 'nem', 'ia', 'a', 'missa', 'aos', 'domingos', 'tudo', 'que', 'rendia', 'a', 'sua', 'venda', 'e', 'mais', 'a', 'quitanda', 'seguia', 'direitinho', 'para', 'a', 'caixa', 'economica', 'e', 'dai', 'entao', 'para', 'o', 'banco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tanto assim que, um ano depois da \n",
      "aquisição da crioula, indo em hasta pública algumas braças de terra situadas ao fundo da taverna, arrematou-as logo e \n",
      "tratou, sem perda de tempo, de construir três casinhas de porta e janela'\n",
      "Tokens gerados: ['tanto', 'assim', 'que', ',', 'um', 'ano', 'depois', 'da', 'aquisicao', 'da', 'crioula', ',', 'indo', 'em', 'hasta', 'publica', 'algumas', 'bracas', 'de', 'terra', 'situadas', 'ao', 'fundo', 'da', 'taverna', ',', 'arrematou-as', 'logo', 'e', 'tratou', ',', 'sem', 'perda', 'de', 'tempo', ',', 'de', 'construir', 'tres', 'casinhas', 'de', 'porta', 'e', 'janela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que milagres de esperteza e de economia não realizou ele nessa construção! Servia de pedreiro, amassava e \n",
      "carregava barro, quebrava pedra; pedra, que o velhaco, fora de horas, junto com a amiga, furtavam à pedreira do fundo, \n",
      "da mesma forma que subtraiam o material das casas em obra que havia por ali perto'\n",
      "Tokens gerados: ['que', 'milagres', 'de', 'esperteza', 'e', 'de', 'economia', 'nao', 'realizou', 'ele', 'nessa', 'construcao', 'servia', 'de', 'pedreiro', ',', 'amassava', 'e', 'carregava', 'barro', ',', 'quebrava', 'pedra', 'pedra', ',', 'que', 'o', 'velhaco', ',', 'fora', 'de', 'horas', ',', 'junto', 'com', 'a', 'amiga', ',', 'furtavam', 'a', 'pedreira', 'do', 'fundo', ',', 'da', 'mesma', 'forma', 'que', 'subtraiam', 'o', 'material', 'das', 'casas', 'em', 'obra', 'que', 'havia', 'por', 'ali', 'perto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estes furtos eram feitos com todas as cautelas e sempre coroados do melhor sucesso, graças à circunstância de que \n",
      "nesse tempo a polícia não se mostrava muito por aquelas alturas'\n",
      "Tokens gerados: ['estes', 'furtos', 'eram', 'feitos', 'com', 'todas', 'as', 'cautelas', 'e', 'sempre', 'coroados', 'do', 'melhor', 'sucesso', ',', 'gracas', 'a', 'circunstancia', 'de', 'que', 'nesse', 'tempo', 'a', 'policia', 'nao', 'se', 'mostrava', 'muito', 'por', 'aquelas', 'alturas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão observava durante o dia quais as obras em \n",
      "que ficava material para o dia seguinte, e à noite lá estava ele rente, mais a Bertoleza, a removerem tábuas, tijolos, \n",
      "telhas, sacos de cal, para o meio da rua, com tamanha habilidade que se não ouvia vislumbre de rumor'\n",
      "Tokens gerados: ['joao', 'romao', 'observava', 'durante', 'o', 'dia', 'quais', 'as', 'obras', 'em', 'que', 'ficava', 'material', 'para', 'o', 'dia', 'seguinte', ',', 'e', 'a', 'noite', 'la', 'estava', 'ele', 'rente', ',', 'mais', 'a', 'bertoleza', ',', 'a', 'removerem', 'tabuas', ',', 'tijolos', ',', 'telhas', ',', 'sacos', 'de', 'cal', ',', 'para', 'o', 'meio', 'da', 'rua', ',', 'com', 'tamanha', 'habilidade', 'que', 'se', 'nao', 'ouvia', 'vislumbre', 'de', 'rumor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois, um \n",
      "tomava uma carga e partia para casa, enquanto o outro ficava de alcatéia ao lado do resto, pronto a dar sinal, em caso de \n",
      "perigo; e, quando o que tinha ido voltava, seguia então o companheiro, carregado por sua vez'\n",
      "Tokens gerados: ['depois', ',', 'um', 'tomava', 'uma', 'carga', 'e', 'partia', 'para', 'casa', ',', 'enquanto', 'o', 'outro', 'ficava', 'de', 'alcateia', 'ao', 'lado', 'do', 'resto', ',', 'pronto', 'a', 'dar', 'sinal', ',', 'em', 'caso', 'de', 'perigo', 'e', ',', 'quando', 'o', 'que', 'tinha', 'ido', 'voltava', ',', 'seguia', 'entao', 'o', 'companheiro', ',', 'carregado', 'por', 'sua', 'vez']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nada lhes escapava, nem mesmo as escadas dos pedreiros, os cavalos de pau, o banco ou a ferramenta dos \n",
      "marceneiros'\n",
      "Tokens gerados: ['nada', 'lhes', 'escapava', ',', 'nem', 'mesmo', 'as', 'escadas', 'dos', 'pedreiros', ',', 'os', 'cavalos', 'de', 'pau', ',', 'o', 'banco', 'ou', 'a', 'ferramenta', 'dos', 'marceneiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o fato é que aquelas três casinhas, tão engenhosamente construídas, foram o ponto de partida do grande cortiço \n",
      "de São Romão'\n",
      "Tokens gerados: ['e', 'o', 'fato', 'e', 'que', 'aquelas', 'tres', 'casinhas', ',', 'tao', 'engenhosamente', 'construidas', ',', 'foram', 'o', 'ponto', 'de', 'partida', 'do', 'grande', 'cortico', 'de', 'sao', 'romao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Hoje quatro braças de terra, amanhã seis, depois mais outras, ia o vendeiro conquistando todo o terreno que se \n",
      "estendia pelos fundos da sua bodega; e, à proporção que o conquistava, reproduziam-se os quartos e o número de \n",
      "moradores'\n",
      "Tokens gerados: ['hoje', 'quatro', 'bracas', 'de', 'terra', ',', 'amanha', 'seis', ',', 'depois', 'mais', 'outras', ',', 'ia', 'o', 'vendeiro', 'conquistando', 'todo', 'o', 'terreno', 'que', 'se', 'estendia', 'pelos', 'fundos', 'da', 'sua', 'bodega', 'e', ',', 'a', 'proporcao', 'que', 'o', 'conquistava', ',', 'reproduziam-se', 'os', 'quartos', 'e', 'o', 'numero', 'de', 'moradores']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sempre em mangas de camisa, sem domingo nem dia santo, não perdendo nunca a ocasião de assenhorear-se do \n",
      "alheio, deixando de pagar todas as vezes que podia e nunca deixando de receber, enganando os fregueses, roubando nos \n",
      "pesos e nas medidas, comprando por dez réis de mel coado o que os escravos furtavam da casa dos seus senhores, \n",
      "apertando cada vez mais as próprias despesas, empilhando privações sobre privações, trabalhando e mais a amiga como \n",
      "uma junta de bois, João Romão veio afinal a comprar uma boa parte da bela pedreira, que ele, todos os dias, ao cair da \n",
      "tarde, assentado um instante à porta da venda, contemplava de longe com um resignado olhar de cobiça'\n",
      "Tokens gerados: ['sempre', 'em', 'mangas', 'de', 'camisa', ',', 'sem', 'domingo', 'nem', 'dia', 'santo', ',', 'nao', 'perdendo', 'nunca', 'a', 'ocasiao', 'de', 'assenhorear-se', 'do', 'alheio', ',', 'deixando', 'de', 'pagar', 'todas', 'as', 'vezes', 'que', 'podia', 'e', 'nunca', 'deixando', 'de', 'receber', ',', 'enganando', 'os', 'fregueses', ',', 'roubando', 'nos', 'pesos', 'e', 'nas', 'medidas', ',', 'comprando', 'por', 'dez', 'reis', 'de', 'mel', 'coado', 'o', 'que', 'os', 'escravos', 'furtavam', 'da', 'casa', 'dos', 'seus', 'senhores', ',', 'apertando', 'cada', 'vez', 'mais', 'as', 'proprias', 'despesas', ',', 'empilhando', 'privacoes', 'sobre', 'privacoes', ',', 'trabalhando', 'e', 'mais', 'a', 'amiga', 'como', 'uma', 'junta', 'de', 'bois', ',', 'joao', 'romao', 'veio', 'afinal', 'a', 'comprar', 'uma', 'boa', 'parte', 'da', 'bela', 'pedreira', ',', 'que', 'ele', ',', 'todos', 'os', 'dias', ',', 'ao', 'cair', 'da', 'tarde', ',', 'assentado', 'um', 'instante', 'a', 'porta', 'da', 'venda', ',', 'contemplava', 'de', 'longe', 'com', 'um', 'resignado', 'olhar', 'de', 'cobica']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pôs lá seis homens a quebrarem pedra e outros seis a fazerem lajedos e paralelepípedos, e então principiou a \n",
      "ganhar em grosso, tão em grosso que, dentro de ano e meio, arrematava já todo o espaço compreendido entre as suas \n",
      "casinhas e a pedreira, isto é, umas oitenta braças de fundo sobre vinte de frente em plano enxuto e magnífico para \n",
      "construir'\n",
      "Tokens gerados: ['pos', 'la', 'seis', 'homens', 'a', 'quebrarem', 'pedra', 'e', 'outros', 'seis', 'a', 'fazerem', 'lajedos', 'e', 'paralelepipedos', ',', 'e', 'entao', 'principiou', 'a', 'ganhar', 'em', 'grosso', ',', 'tao', 'em', 'grosso', 'que', ',', 'dentro', 'de', 'ano', 'e', 'meio', ',', 'arrematava', 'ja', 'todo', 'o', 'espaco', 'compreendido', 'entre', 'as', 'suas', 'casinhas', 'e', 'a', 'pedreira', ',', 'isto', 'e', ',', 'umas', 'oitenta', 'bracas', 'de', 'fundo', 'sobre', 'vinte', 'de', 'frente', 'em', 'plano', 'enxuto', 'e', 'magnifico', 'para', 'construir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Justamente por essa ocasião vendeu-se também um sobrado que ficava à direita da venda, separado desta apenas \n",
      "por aquelas vinte braças; de sorte que todo o flanco esquerdo do prédio, coisa de uns vinte e tantos metros, despejava \n",
      "para o terreno do vendeiro as suas nove janelas de peitoril'\n",
      "Tokens gerados: ['justamente', 'por', 'essa', 'ocasiao', 'vendeu-se', 'tambem', 'um', 'sobrado', 'que', 'ficava', 'a', 'direita', 'da', 'venda', ',', 'separado', 'desta', 'apenas', 'por', 'aquelas', 'vinte', 'bracas', 'de', 'sorte', 'que', 'todo', 'o', 'flanco', 'esquerdo', 'do', 'predio', ',', 'coisa', 'de', 'uns', 'vinte', 'e', 'tantos', 'metros', ',', 'despejava', 'para', 'o', 'terreno', 'do', 'vendeiro', 'as', 'suas', 'nove', 'janelas', 'de', 'peitoril']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Comprou-o um tal Miranda, negociante português, \n",
      "estabelecido na Rua do Hospício com uma loja de fazendas por atacado'\n",
      "Tokens gerados: ['comprou-o', 'um', 'tal', 'miranda', ',', 'negociante', 'portugues', ',', 'estabelecido', 'na', 'rua', 'do', 'hospicio', 'com', 'uma', 'loja', 'de', 'fazendas', 'por', 'atacado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Corrida uma limpeza geral no casarão, \n",
      "mudar-se-ia ele para lá com a família, pois que a mulher, Dona Estela, senhora pretensiosa e com fumaças de nobreza, \n",
      "já não podia suportar a residência no centro da cidade, como também sua menina, a Zulmirinha, crescia muito pálida e \n",
      "precisava de largueza para enrijar e tomar corpo'\n",
      "Tokens gerados: ['corrida', 'uma', 'limpeza', 'geral', 'no', 'casarao', ',', 'mudar-se-ia', 'ele', 'para', 'la', 'com', 'a', 'familia', ',', 'pois', 'que', 'a', 'mulher', ',', 'dona', 'estela', ',', 'senhora', 'pretensiosa', 'e', 'com', 'fumacas', 'de', 'nobreza', ',', 'ja', 'nao', 'podia', 'suportar', 'a', 'residencia', 'no', 'centro', 'da', 'cidade', ',', 'como', 'tambem', 'sua', 'menina', ',', 'a', 'zulmirinha', ',', 'crescia', 'muito', 'palida', 'e', 'precisava', 'de', 'largueza', 'para', 'enrijar', 'e', 'tomar', 'corpo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Isto foi o que disse o Miranda aos colegas, porém a verdadeira causa da mudança estava na necessidade, que ele \n",
      "reconhecia urgente, de afastar Dona Estela do alcance dos seus caixeiros'\n",
      "Tokens gerados: ['isto', 'foi', 'o', 'que', 'disse', 'o', 'miranda', 'aos', 'colegas', ',', 'porem', 'a', 'verdadeira', 'causa', 'da', 'mudanca', 'estava', 'na', 'necessidade', ',', 'que', 'ele', 'reconhecia', 'urgente', ',', 'de', 'afastar', 'dona', 'estela', 'do', 'alcance', 'dos', 'seus', 'caixeiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dona Estela era uma mulherzinha levada da \n",
      "breca: achava-se casada havia treze anos e durante esse tempo dera ao marido toda sorte de desgostos'\n",
      "Tokens gerados: ['dona', 'estela', 'era', 'uma', 'mulherzinha', 'levada', 'da', 'breca', 'achava-se', 'casada', 'havia', 'treze', 'anos', 'e', 'durante', 'esse', 'tempo', 'dera', 'ao', 'marido', 'toda', 'sorte', 'de', 'desgostos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ainda antes de \n",
      "terminar o segundo ano de matrimônio, o Miranda pilhou-a em flagrante delito de adultério; ficou furioso e o seu \n",
      "primeiro impulso foi de mandá-la para o diabo junto com o cúmplice; mas a sua casa comercial garantia-se com o dote \n",
      "que ela trouxera, uns oitenta contos em prédios e ações da divida publica, de que se utilizava o desgraçado tanto quanto \n",
      "lhe permitia o regime dotal'\n",
      "Tokens gerados: ['ainda', 'antes', 'de', 'terminar', 'o', 'segundo', 'ano', 'de', 'matrimonio', ',', 'o', 'miranda', 'pilhou-a', 'em', 'flagrante', 'delito', 'de', 'adulterio', 'ficou', 'furioso', 'e', 'o', 'seu', 'primeiro', 'impulso', 'foi', 'de', 'manda-la', 'para', 'o', 'diabo', 'junto', 'com', 'o', 'cumplice', 'mas', 'a', 'sua', 'casa', 'comercial', 'garantia-se', 'com', 'o', 'dote', 'que', 'ela', 'trouxera', ',', 'uns', 'oitenta', 'contos', 'em', 'predios', 'e', 'acoes', 'da', 'divida', 'publica', ',', 'de', 'que', 'se', 'utilizava', 'o', 'desgracado', 'tanto', 'quanto', 'lhe', 'permitia', 'o', 'regime', 'dotal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Além de que, um rompimento brusco seria obra para escândalo, e, segundo a sua opinião, \n",
      "qualquer escândalo doméstico ficava muito mal a um negociante de certa ordem'\n",
      "Tokens gerados: ['alem', 'de', 'que', ',', 'um', 'rompimento', 'brusco', 'seria', 'obra', 'para', 'escandalo', ',', 'e', ',', 'segundo', 'a', 'sua', 'opiniao', ',', 'qualquer', 'escandalo', 'domestico', 'ficava', 'muito', 'mal', 'a', 'um', 'negociante', 'de', 'certa', 'ordem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prezava, acima de tudo, a sua posição \n",
      "social e tremia só com a idéia de ver-se novamente pobre, sem recursos e sem coragem para recomeçar a vida, depois de \n",
      "se haver habituado a umas tantas regalias e afeito à hombridade de português rico que já não tem pátria na Europa'\n",
      "Tokens gerados: ['prezava', ',', 'acima', 'de', 'tudo', ',', 'a', 'sua', 'posicao', 'social', 'e', 'tremia', 'so', 'com', 'a', 'ideia', 'de', 'ver-se', 'novamente', 'pobre', ',', 'sem', 'recursos', 'e', 'sem', 'coragem', 'para', 'recomecar', 'a', 'vida', ',', 'depois', 'de', 'se', 'haver', 'habituado', 'a', 'umas', 'tantas', 'regalias', 'e', 'afeito', 'a', 'hombridade', 'de', 'portugues', 'rico', 'que', 'ja', 'nao', 'tem', 'patria', 'na', 'europa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Acovardado defronte destes raciocínios, contentou-se com uma simples separação de leitos, e os dois passaram a \n",
      "dormir em quartos separados'\n",
      "Tokens gerados: ['acovardado', 'defronte', 'destes', 'raciocinios', ',', 'contentou-se', 'com', 'uma', 'simples', 'separacao', 'de', 'leitos', ',', 'e', 'os', 'dois', 'passaram', 'a', 'dormir', 'em', 'quartos', 'separados']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não comiam juntos, e mal trocavam entre si uma ou outra palavra constrangida, quando \n",
      "qualquer inesperado acaso os reunia a contragosto'\n",
      "Tokens gerados: ['nao', 'comiam', 'juntos', ',', 'e', 'mal', 'trocavam', 'entre', 'si', 'uma', 'ou', 'outra', 'palavra', 'constrangida', ',', 'quando', 'qualquer', 'inesperado', 'acaso', 'os', 'reunia', 'a', 'contragosto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Odiavam-se'\n",
      "Tokens gerados: ['odiavam-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cada qual sentia pelo outro um profundo desprezo, que pouco a pouco se foi transformando em \n",
      "repugnância completa'\n",
      "Tokens gerados: ['cada', 'qual', 'sentia', 'pelo', 'outro', 'um', 'profundo', 'desprezo', ',', 'que', 'pouco', 'a', 'pouco', 'se', 'foi', 'transformando', 'em', 'repugnancia', 'completa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O nascimento de Zulmira veio agravar ainda mais a situação; a pobre criança, em vez de servir \n",
      "de elo aos dois infelizes, foi antes um novo isolador que se estabeleceu entre eles'\n",
      "Tokens gerados: ['o', 'nascimento', 'de', 'zulmira', 'veio', 'agravar', 'ainda', 'mais', 'a', 'situacao', 'a', 'pobre', 'crianca', ',', 'em', 'vez', 'de', 'servir', 'de', 'elo', 'aos', 'dois', 'infelizes', ',', 'foi', 'antes', 'um', 'novo', 'isolador', 'que', 'se', 'estabeleceu', 'entre', 'eles']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estela amava-a menos do que lhe \n",
      "pedia o instinto materno por supô-la filha do marido, e este a detestava porque tinha convicção de não ser seu pai'\n",
      "Tokens gerados: ['estela', 'amava-a', 'menos', 'do', 'que', 'lhe', 'pedia', 'o', 'instinto', 'materno', 'por', 'supo-la', 'filha', 'do', 'marido', ',', 'e', 'este', 'a', 'detestava', 'porque', 'tinha', 'conviccao', 'de', 'nao', 'ser', 'seu', 'pai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma bela noite, porém, o Miranda, que era homem de sangue esperto e orçava então pelos seus trinta e cinco \n",
      "anos, sentiu-se em insuportável estado de lubricidade'\n",
      "Tokens gerados: ['uma', 'bela', 'noite', ',', 'porem', ',', 'o', 'miranda', ',', 'que', 'era', 'homem', 'de', 'sangue', 'esperto', 'e', 'orcava', 'entao', 'pelos', 'seus', 'trinta', 'e', 'cinco', 'anos', ',', 'sentiu-se', 'em', 'insuportavel', 'estado', 'de', 'lubricidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era tarde já e não havia em casa alguma criada que lhe pudesse \n",
      "valer'\n",
      "Tokens gerados: ['era', 'tarde', 'ja', 'e', 'nao', 'havia', 'em', 'casa', 'alguma', 'criada', 'que', 'lhe', 'pudesse', 'valer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lembrou-se da mulher, mas repeliu logo esta idéia com escrupulosa repugnância'\n",
      "Tokens gerados: ['lembrou-se', 'da', 'mulher', ',', 'mas', 'repeliu', 'logo', 'esta', 'ideia', 'com', 'escrupulosa', 'repugnancia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Continuava a odiá-la'\n",
      "Tokens gerados: ['continuava', 'a', 'odia-la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto este mesmo fato de obrigação em que ele se colocou de não servir-se dela, a responsabilidade de desprezá-la, \n",
      "como que ainda mais lhe assanhava o desejo da carne, fazendo da esposa infiel um fruto proibido'\n",
      "Tokens gerados: ['entretanto', 'este', 'mesmo', 'fato', 'de', 'obrigacao', 'em', 'que', 'ele', 'se', 'colocou', 'de', 'nao', 'servir-se', 'dela', ',', 'a', 'responsabilidade', 'de', 'despreza-la', ',', 'como', 'que', 'ainda', 'mais', 'lhe', 'assanhava', 'o', 'desejo', 'da', 'carne', ',', 'fazendo', 'da', 'esposa', 'infiel', 'um', 'fruto', 'proibido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Afinal, coisa \n",
      "singular, posto que moralmente nada diminuísse a sua repugnância pela perjura, foi ter ao quarto dela'\n",
      "Tokens gerados: ['afinal', ',', 'coisa', 'singular', ',', 'posto', 'que', 'moralmente', 'nada', 'diminuisse', 'a', 'sua', 'repugnancia', 'pela', 'perjura', ',', 'foi', 'ter', 'ao', 'quarto', 'dela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mulher dormia a sono solto'\n",
      "Tokens gerados: ['a', 'mulher', 'dormia', 'a', 'sono', 'solto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Miranda entrou pé ante pé e aproximou-se da cama'\n",
      "Tokens gerados: ['miranda', 'entrou', 'pe', 'ante', 'pe', 'e', 'aproximou-se', 'da', 'cama']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Devia voltar!'\n",
      "Tokens gerados: ['“', 'devia', 'voltar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'pensou'\n",
      "Tokens gerados: ['pensou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não \n",
      "lhe ficava bem aquilo!'\n",
      "Tokens gerados: ['nao', 'lhe', 'ficava', 'bem', 'aquilo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” Mas o sangue latejava-lhe, reclamando-a'\n",
      "Tokens gerados: ['”', 'mas', 'o', 'sangue', 'latejava-lhe', ',', 'reclamando-a']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ainda hesitou um instante, imóvel, a contemplá-la \n",
      "no seu desejo'\n",
      "Tokens gerados: ['ainda', 'hesitou', 'um', 'instante', ',', 'imovel', ',', 'a', 'contempla-la', 'no', 'seu', 'desejo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estela, como se o olhar do marido lhe apalpasse o corpo, torceu-se sobre o quadril da esquerda, repuxando com as \n",
      "coxas o lençol para a frente e patenteando uma nesga de nudez estofada e branca'\n",
      "Tokens gerados: ['estela', ',', 'como', 'se', 'o', 'olhar', 'do', 'marido', 'lhe', 'apalpasse', 'o', 'corpo', ',', 'torceu-se', 'sobre', 'o', 'quadril', 'da', 'esquerda', ',', 'repuxando', 'com', 'as', 'coxas', 'o', 'lencol', 'para', 'a', 'frente', 'e', 'patenteando', 'uma', 'nesga', 'de', 'nudez', 'estofada', 'e', 'branca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Miranda não pôde resistir, atirou-se \n",
      "contra ela, que, num pequeno sobressalto, mais de surpresa que de revolta, desviou-se, tornando logo e enfrentando com \n",
      "o marido'\n",
      "Tokens gerados: ['o', 'miranda', 'nao', 'pode', 'resistir', ',', 'atirou-se', 'contra', 'ela', ',', 'que', ',', 'num', 'pequeno', 'sobressalto', ',', 'mais', 'de', 'surpresa', 'que', 'de', 'revolta', ',', 'desviou-se', ',', 'tornando', 'logo', 'e', 'enfrentando', 'com', 'o', 'marido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E deixou-se empolgar pelos rins, de olhos fechados, fingindo que continuava a dormir, sem a menor \n",
      "consciência de tudo aquilo'\n",
      "Tokens gerados: ['e', 'deixou-se', 'empolgar', 'pelos', 'rins', ',', 'de', 'olhos', 'fechados', ',', 'fingindo', 'que', 'continuava', 'a', 'dormir', ',', 'sem', 'a', 'menor', 'consciencia', 'de', 'tudo', 'aquilo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah! ela contava como certo que o esposo, desde que não teve coragem de separar-se de casa, havia, mais cedo ou \n",
      "mais tarde, de procurá-la de novo'\n",
      "Tokens gerados: ['ah', 'ela', 'contava', 'como', 'certo', 'que', 'o', 'esposo', ',', 'desde', 'que', 'nao', 'teve', 'coragem', 'de', 'separar-se', 'de', 'casa', ',', 'havia', ',', 'mais', 'cedo', 'ou', 'mais', 'tarde', ',', 'de', 'procura-la', 'de', 'novo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Conhecia-lhe o temperamento, forte para desejar e fraco para resistir ao desejo'\n",
      "Tokens gerados: ['conhecia-lhe', 'o', 'temperamento', ',', 'forte', 'para', 'desejar', 'e', 'fraco', 'para', 'resistir', 'ao', 'desejo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Consumado o delito, o honrado negociante sentiu-se tolhido de vergonha e arrependimento'\n",
      "Tokens gerados: ['consumado', 'o', 'delito', ',', 'o', 'honrado', 'negociante', 'sentiu-se', 'tolhido', 'de', 'vergonha', 'e', 'arrependimento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não teve animo de \n",
      "dar palavra, e retirou-se tristonho e murcho para o seu quarto de desquitado'\n",
      "Tokens gerados: ['nao', 'teve', 'animo', 'de', 'dar', 'palavra', ',', 'e', 'retirou-se', 'tristonho', 'e', 'murcho', 'para', 'o', 'seu', 'quarto', 'de', 'desquitado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Oh! como lhe doía agora o que acabava de praticar na cegueira da sua sensualidade'\n",
      "Tokens gerados: ['oh', 'como', 'lhe', 'doia', 'agora', 'o', 'que', 'acabava', 'de', 'praticar', 'na', 'cegueira', 'da', 'sua', 'sensualidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que cabeçada!'\n",
      "Tokens gerados: ['—', 'que', 'cabecada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'dizia ele agitado'\n",
      "Tokens gerados: ['dizia', 'ele', 'agitado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que formidável cabeçada!'\n",
      "Tokens gerados: ['que', 'formidavel', 'cabecada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No dia seguinte, os dois viram-se e evitaram-se em silêncio, como se nada de extraordinário houvera entre eles \n",
      "acontecido na véspera'\n",
      "Tokens gerados: ['no', 'dia', 'seguinte', ',', 'os', 'dois', 'viram-se', 'e', 'evitaram-se', 'em', 'silencio', ',', 'como', 'se', 'nada', 'de', 'extraordinario', 'houvera', 'entre', 'eles', 'acontecido', 'na', 'vespera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dir-se-ia até que, depois daquela ocorrência, o Miranda sentia crescer o seu ódio contra a \n",
      "esposa'\n",
      "Tokens gerados: ['dir-se-ia', 'ate', 'que', ',', 'depois', 'daquela', 'ocorrencia', ',', 'o', 'miranda', 'sentia', 'crescer', 'o', 'seu', 'odio', 'contra', 'a', 'esposa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, à noite desse mesmo dia, quando se achou sozinho na sua cama estreita, jurou mil vezes aos seus brios nunca \n",
      "mais, nunca mais, praticar semelhante loucura'\n",
      "Tokens gerados: ['e', ',', 'a', 'noite', 'desse', 'mesmo', 'dia', ',', 'quando', 'se', 'achou', 'sozinho', 'na', 'sua', 'cama', 'estreita', ',', 'jurou', 'mil', 'vezes', 'aos', 'seus', 'brios', 'nunca', 'mais', ',', 'nunca', 'mais', ',', 'praticar', 'semelhante', 'loucura']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, daí a um mês, o pobre homem, acometido de um novo acesso de luxúria, voltou ao quarto da mulher'\n",
      "Tokens gerados: ['mas', ',', 'dai', 'a', 'um', 'mes', ',', 'o', 'pobre', 'homem', ',', 'acometido', 'de', 'um', 'novo', 'acesso', 'de', 'luxuria', ',', 'voltou', 'ao', 'quarto', 'da', 'mulher']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estela recebeu-o desta vez como da primeira, fingindo que não acordava; na ocasião, porém, em que ele se \n",
      "apoderava dela febrilmente, a leviana, sem se poder conter, soltou-lhe em cheio contra o rosto uma gargalhada que a \n",
      "custo sopeava'\n",
      "Tokens gerados: ['estela', 'recebeu-o', 'desta', 'vez', 'como', 'da', 'primeira', ',', 'fingindo', 'que', 'nao', 'acordava', 'na', 'ocasiao', ',', 'porem', ',', 'em', 'que', 'ele', 'se', 'apoderava', 'dela', 'febrilmente', ',', 'a', 'leviana', ',', 'sem', 'se', 'poder', 'conter', ',', 'soltou-lhe', 'em', 'cheio', 'contra', 'o', 'rosto', 'uma', 'gargalhada', 'que', 'a', 'custo', 'sopeava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O pobre-diabo desnorteou, deveras escandalizado, soerguendo-se, brusco, num estremunhamento de \n",
      "sonâmbulo acordado com violência'\n",
      "Tokens gerados: ['o', 'pobre-diabo', 'desnorteou', ',', 'deveras', 'escandalizado', ',', 'soerguendo-se', ',', 'brusco', ',', 'num', 'estremunhamento', 'de', 'sonambulo', 'acordado', 'com', 'violencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mulher percebeu a situação e não lhe deu tempo para fugir; passou-lhe rápido as pernas por cima e, \n",
      "grudando-se-lhe ao corpo, cegou-o com uma metralhada de beijos'\n",
      "Tokens gerados: ['a', 'mulher', 'percebeu', 'a', 'situacao', 'e', 'nao', 'lhe', 'deu', 'tempo', 'para', 'fugir', 'passou-lhe', 'rapido', 'as', 'pernas', 'por', 'cima', 'e', ',', 'grudando-se-lhe', 'ao', 'corpo', ',', 'cegou-o', 'com', 'uma', 'metralhada', 'de', 'beijos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não se falaram'\n",
      "Tokens gerados: ['nao', 'se', 'falaram']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Miranda nunca a tivera, nem nunca a vira, assim tão violenta no prazer'\n",
      "Tokens gerados: ['miranda', 'nunca', 'a', 'tivera', ',', 'nem', 'nunca', 'a', 'vira', ',', 'assim', 'tao', 'violenta', 'no', 'prazer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estranhou-a'\n",
      "Tokens gerados: ['estranhou-a']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Afigurou-se-lhe estar nos \n",
      "braços de uma amante apaixonada: descobriu nela o capitoso encanto com que nos embebedam as cortesãs amestradas \n",
      "na ciência do gozo venéreo'\n",
      "Tokens gerados: ['afigurou-se-lhe', 'estar', 'nos', 'bracos', 'de', 'uma', 'amante', 'apaixonada', 'descobriu', 'nela', 'o', 'capitoso', 'encanto', 'com', 'que', 'nos', 'embebedam', 'as', 'cortesas', 'amestradas', 'na', 'ciencia', 'do', 'gozo', 'venereo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Descobriu-lhe no cheiro da pele e no cheiro dos cabelos perfumes que nunca lhe sentira; \n",
      "notou-lhe outro hálito, outro som nos gemidos e nos suspiros'\n",
      "Tokens gerados: ['descobriu-lhe', 'no', 'cheiro', 'da', 'pele', 'e', 'no', 'cheiro', 'dos', 'cabelos', 'perfumes', 'que', 'nunca', 'lhe', 'sentira', 'notou-lhe', 'outro', 'halito', ',', 'outro', 'som', 'nos', 'gemidos', 'e', 'nos', 'suspiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E gozou-a, gozou-a loucamente, com delírio, com \n",
      "verdadeira satisfação de animal no cio'\n",
      "Tokens gerados: ['e', 'gozou-a', ',', 'gozou-a', 'loucamente', ',', 'com', 'delirio', ',', 'com', 'verdadeira', 'satisfacao', 'de', 'animal', 'no', 'cio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E ela também, ela também gozou, estimulada por aquela circunstância picante do ressentimento que os desunia; \n",
      "gozou a desonestidade daquele ato que a ambos acanalhava aos olhos um do outro; estorceu-se toda, rangendo os \n",
      "dentes, grunhindo, debaixo daquele seu inimigo odiado, achando-o também agora, como homem, melhor que nunca, \n",
      "sufocando-o nos seus braços nus, metendo-lhe pela boca a língua úmida e em brasa'\n",
      "Tokens gerados: ['e', 'ela', 'tambem', ',', 'ela', 'tambem', 'gozou', ',', 'estimulada', 'por', 'aquela', 'circunstancia', 'picante', 'do', 'ressentimento', 'que', 'os', 'desunia', 'gozou', 'a', 'desonestidade', 'daquele', 'ato', 'que', 'a', 'ambos', 'acanalhava', 'aos', 'olhos', 'um', 'do', 'outro', 'estorceu-se', 'toda', ',', 'rangendo', 'os', 'dentes', ',', 'grunhindo', ',', 'debaixo', 'daquele', 'seu', 'inimigo', 'odiado', ',', 'achando-o', 'tambem', 'agora', ',', 'como', 'homem', ',', 'melhor', 'que', 'nunca', ',', 'sufocando-o', 'nos', 'seus', 'bracos', 'nus', ',', 'metendo-lhe', 'pela', 'boca', 'a', 'lingua', 'umida', 'e', 'em', 'brasa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois, um arranco de corpo \n",
      "inteiro, com um soluço gutural e estrangulado, arquejante e convulsa, estatelou-se num abandono de pernas e braços \n",
      "abertos, a cabeça para o lado, os olhos moribundos e chorosos, toda ela agonizante, como se a tivessem crucificado na \n",
      "cama'\n",
      "Tokens gerados: ['depois', ',', 'um', 'arranco', 'de', 'corpo', 'inteiro', ',', 'com', 'um', 'soluco', 'gutural', 'e', 'estrangulado', ',', 'arquejante', 'e', 'convulsa', ',', 'estatelou-se', 'num', 'abandono', 'de', 'pernas', 'e', 'bracos', 'abertos', ',', 'a', 'cabeca', 'para', 'o', 'lado', ',', 'os', 'olhos', 'moribundos', 'e', 'chorosos', ',', 'toda', 'ela', 'agonizante', ',', 'como', 'se', 'a', 'tivessem', 'crucificado', 'na', 'cama']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A partir dessa noite, da qual só pela manhã o Miranda se retirou do quarto da mulher, estabeleceu-se entre eles o \n",
      "hábito de uma felicidade sexual, tão completa como ainda não a tinham desfrutado, posto que no intimo de cada um \n",
      "persistisse contra o outro a mesma repugnância moral em nada enfraquecida'\n",
      "Tokens gerados: ['a', 'partir', 'dessa', 'noite', ',', 'da', 'qual', 'so', 'pela', 'manha', 'o', 'miranda', 'se', 'retirou', 'do', 'quarto', 'da', 'mulher', ',', 'estabeleceu-se', 'entre', 'eles', 'o', 'habito', 'de', 'uma', 'felicidade', 'sexual', ',', 'tao', 'completa', 'como', 'ainda', 'nao', 'a', 'tinham', 'desfrutado', ',', 'posto', 'que', 'no', 'intimo', 'de', 'cada', 'um', 'persistisse', 'contra', 'o', 'outro', 'a', 'mesma', 'repugnancia', 'moral', 'em', 'nada', 'enfraquecida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Durante dez anos viveram muito bem casados; agora, porém, tanto tempo depois da primeira infidelidade \n",
      "conjugal, e agora que o negociante já não era acometido tão freqüentemente por aquelas crises que o arrojavam fora de \n",
      "horas ao dormitório de Dona Estela; agora, eis que a leviana parecia disposta a reincidir na culpa, dando corda aos \n",
      "caixeiros do marido, na ocasião em que estes subiam para almoçar ou jantar'\n",
      "Tokens gerados: ['durante', 'dez', 'anos', 'viveram', 'muito', 'bem', 'casados', 'agora', ',', 'porem', ',', 'tanto', 'tempo', 'depois', 'da', 'primeira', 'infidelidade', 'conjugal', ',', 'e', 'agora', 'que', 'o', 'negociante', 'ja', 'nao', 'era', 'acometido', 'tao', 'frequentemente', 'por', 'aquelas', 'crises', 'que', 'o', 'arrojavam', 'fora', 'de', 'horas', 'ao', 'dormitorio', 'de', 'dona', 'estela', 'agora', ',', 'eis', 'que', 'a', 'leviana', 'parecia', 'disposta', 'a', 'reincidir', 'na', 'culpa', ',', 'dando', 'corda', 'aos', 'caixeiros', 'do', 'marido', ',', 'na', 'ocasiao', 'em', 'que', 'estes', 'subiam', 'para', 'almocar', 'ou', 'jantar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi por isso que o Miranda comprou o prédio vizinho a João Romão'\n",
      "Tokens gerados: ['foi', 'por', 'isso', 'que', 'o', 'miranda', 'comprou', 'o', 'predio', 'vizinho', 'a', 'joao', 'romao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A casa era boa; seu único defeito estava na escassez do quintal; mas para isso havia remédio: com muito pouco \n",
      "compravam-se umas dez braças daquele terreno do fundo que ia até à pedreira, e mais uns dez ou quinze palmos do lado \n",
      "em que ficava a venda'\n",
      "Tokens gerados: ['a', 'casa', 'era', 'boa', 'seu', 'unico', 'defeito', 'estava', 'na', 'escassez', 'do', 'quintal', 'mas', 'para', 'isso', 'havia', 'remedio', 'com', 'muito', 'pouco', 'compravam-se', 'umas', 'dez', 'bracas', 'daquele', 'terreno', 'do', 'fundo', 'que', 'ia', 'ate', 'a', 'pedreira', ',', 'e', 'mais', 'uns', 'dez', 'ou', 'quinze', 'palmos', 'do', 'lado', 'em', 'que', 'ficava', 'a', 'venda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Miranda foi logo entender-se com o Romão e propôs-lhe negócio'\n",
      "Tokens gerados: ['miranda', 'foi', 'logo', 'entender-se', 'com', 'o', 'romao', 'e', 'propos-lhe', 'negocio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O taverneiro recusou formalmente'\n",
      "Tokens gerados: ['o', 'taverneiro', 'recusou', 'formalmente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Miranda insistiu'\n",
      "Tokens gerados: ['miranda', 'insistiu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— O senhor perde seu tempo e seu latim! retrucou o amigo de Bertoleza'\n",
      "Tokens gerados: ['—', 'o', 'senhor', 'perde', 'seu', 'tempo', 'e', 'seu', 'latim', 'retrucou', 'o', 'amigo', 'de', 'bertoleza']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nem só não cedo uma polegada do meu \n",
      "terreno, como ainda lhe compro, se mo quiser vender, aquele pedaço que lhe fica ao fundo da casa! \n",
      "— O quintal? \n",
      "— É exato'\n",
      "Tokens gerados: ['nem', 'so', 'nao', 'cedo', 'uma', 'polegada', 'do', 'meu', 'terreno', ',', 'como', 'ainda', 'lhe', 'compro', ',', 'se', 'mo', 'quiser', 'vender', ',', 'aquele', 'pedaco', 'que', 'lhe', 'fica', 'ao', 'fundo', 'da', 'casa', '—', 'o', 'quintal', '?', '—', 'e', 'exato']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pois você quer que eu fique sem chácara, sem jardim, sem nada? \n",
      "— Para mim era de vantagem'\n",
      "Tokens gerados: ['—', 'pois', 'voce', 'quer', 'que', 'eu', 'fique', 'sem', 'chacara', ',', 'sem', 'jardim', ',', 'sem', 'nada', '?', '—', 'para', 'mim', 'era', 'de', 'vantagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora, deixe-se disso, homem, e diga lá quanto quer pelo que lhe propus'\n",
      "Tokens gerados: ['—', 'ora', ',', 'deixe-se', 'disso', ',', 'homem', ',', 'e', 'diga', 'la', 'quanto', 'quer', 'pelo', 'que', 'lhe', 'propus']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Já disse o que tinha a dizer'\n",
      "Tokens gerados: ['—', 'ja', 'disse', 'o', 'que', 'tinha', 'a', 'dizer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ceda-me então ao menos as dez braças do fundo'\n",
      "Tokens gerados: ['—', 'ceda-me', 'entao', 'ao', 'menos', 'as', 'dez', 'bracas', 'do', 'fundo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Nem meio palmo! \n",
      "— Isso é maldade de sua parte, sabe? Eu, se faço tamanho empenho, é pela minha pequena, que precisa, coitada, \n",
      "de um pouco de espaço para alargar-se'\n",
      "Tokens gerados: ['—', 'nem', 'meio', 'palmo', '—', 'isso', 'e', 'maldade', 'de', 'sua', 'parte', ',', 'sabe', '?', 'eu', ',', 'se', 'faco', 'tamanho', 'empenho', ',', 'e', 'pela', 'minha', 'pequena', ',', 'que', 'precisa', ',', 'coitada', ',', 'de', 'um', 'pouco', 'de', 'espaco', 'para', 'alargar-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— E eu não cedo, porque preciso do meu terreno! \n",
      "— Ora qual! Que diabo pode lá você fazer ali? Uma porcaria de um pedaço de terreno quase grudado ao morro e \n",
      "aos fundos de minha casa! quando você, aliás, dispõe de tanto espaço ainda! \n",
      "— Hei de lhe mostrar se tenho ou não o que fazer ali! \n",
      "— É que você é teimoso! Olhe, se me cedesse as dez braças do fundo, a sua parte ficaria cortada em linha reta até \n",
      "à pedreira, e escusava eu de ficar com uma aba de terreno alheio a meter-se pelo meu'\n",
      "Tokens gerados: ['—', 'e', 'eu', 'nao', 'cedo', ',', 'porque', 'preciso', 'do', 'meu', 'terreno', '—', 'ora', 'qual', 'que', 'diabo', 'pode', 'la', 'voce', 'fazer', 'ali', '?', 'uma', 'porcaria', 'de', 'um', 'pedaco', 'de', 'terreno', 'quase', 'grudado', 'ao', 'morro', 'e', 'aos', 'fundos', 'de', 'minha', 'casa', 'quando', 'voce', ',', 'alias', ',', 'dispoe', 'de', 'tanto', 'espaco', 'ainda', '—', 'hei', 'de', 'lhe', 'mostrar', 'se', 'tenho', 'ou', 'nao', 'o', 'que', 'fazer', 'ali', '—', 'e', 'que', 'voce', 'e', 'teimoso', 'olhe', ',', 'se', 'me', 'cedesse', 'as', 'dez', 'bracas', 'do', 'fundo', ',', 'a', 'sua', 'parte', 'ficaria', 'cortada', 'em', 'linha', 'reta', 'ate', 'a', 'pedreira', ',', 'e', 'escusava', 'eu', 'de', 'ficar', 'com', 'uma', 'aba', 'de', 'terreno', 'alheio', 'a', 'meter-se', 'pelo', 'meu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quer saber? não amuro o quintal \n",
      "sem você decidir-se! \n",
      "— Então ficará com o quintal para sempre sem muro, porque o que tinha a dizer já disse! \n",
      "— Mas, homem de Deus, que diabo! pense um pouco! Você ali não pode construir nada! Ou pensará que lhe \n",
      "deixarei abrir janelas sobre o meu quintal!'\n",
      "Tokens gerados: ['quer', 'saber', '?', 'nao', 'amuro', 'o', 'quintal', 'sem', 'voce', 'decidir-se', '—', 'entao', 'ficara', 'com', 'o', 'quintal', 'para', 'sempre', 'sem', 'muro', ',', 'porque', 'o', 'que', 'tinha', 'a', 'dizer', 'ja', 'disse', '—', 'mas', ',', 'homem', 'de', 'deus', ',', 'que', 'diabo', 'pense', 'um', 'pouco', 'voce', 'ali', 'nao', 'pode', 'construir', 'nada', 'ou', 'pensara', 'que', 'lhe', 'deixarei', 'abrir', 'janelas', 'sobre', 'o', 'meu', 'quintal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não preciso abrir janelas sobre o quintal de ninguém! \n",
      "— Nem tampouco lhe deixarei levantar parede, tapando-me as janelas da esquerda! \n",
      "— Não preciso levantar parede desse lado'\n",
      "Tokens gerados: ['—', 'nao', 'preciso', 'abrir', 'janelas', 'sobre', 'o', 'quintal', 'de', 'ninguem', '—', 'nem', 'tampouco', 'lhe', 'deixarei', 'levantar', 'parede', ',', 'tapando-me', 'as', 'janelas', 'da', 'esquerda', '—', 'nao', 'preciso', 'levantar', 'parede', 'desse', 'lado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então que diabo vai você fazer de todo este terreno?'\n",
      "Tokens gerados: ['—', 'entao', 'que', 'diabo', 'vai', 'voce', 'fazer', 'de', 'todo', 'este', 'terreno', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah! isso agora é cá comigo!'\n",
      "Tokens gerados: ['—', 'ah', 'isso', 'agora', 'e', 'ca', 'comigo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O que for soará! \n",
      "— Pois creia que se arrepende de não me ceder o terreno!'\n",
      "Tokens gerados: ['o', 'que', 'for', 'soara', '—', 'pois', 'creia', 'que', 'se', 'arrepende', 'de', 'nao', 'me', 'ceder', 'o', 'terreno']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Se me arrepender, paciência! Só lhe digo é que muito mal se sairá quem quiser meter-se cá com a minha vida! \n",
      "— Passe bem! \n",
      "— Adeus! \n",
      "Travou-se então uma lata renhida e surda entre o português negociante de fazendas por atacado e o português \n",
      "negociante de secos e molhados'\n",
      "Tokens gerados: ['—', 'se', 'me', 'arrepender', ',', 'paciencia', 'so', 'lhe', 'digo', 'e', 'que', 'muito', 'mal', 'se', 'saira', 'quem', 'quiser', 'meter-se', 'ca', 'com', 'a', 'minha', 'vida', '—', 'passe', 'bem', '—', 'adeus', 'travou-se', 'entao', 'uma', 'lata', 'renhida', 'e', 'surda', 'entre', 'o', 'portugues', 'negociante', 'de', 'fazendas', 'por', 'atacado', 'e', 'o', 'portugues', 'negociante', 'de', 'secos', 'e', 'molhados']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aquele não se resolvia a fazer o muro do quintal, sem ter alcançado o pedaço de \n",
      "terreno que o separava do morro; e o outro, por seu lado, não perdia a esperança de apanhar-lhe ainda, pelo menos, duas \n",
      "ou três braças aos fundos da casa; parte esta que, conforme os seus cálculos, valeria ouro, uma vez realizado o grande \n",
      "projeto que ultimamente o trazia preocupado — a criação de uma estalagem em ponto enorme, uma estalagem monstro, \n",
      "sem exemplo, destinada a matar toda aquela miuçalha de cortiços que alastravam por Botafogo'\n",
      "Tokens gerados: ['aquele', 'nao', 'se', 'resolvia', 'a', 'fazer', 'o', 'muro', 'do', 'quintal', ',', 'sem', 'ter', 'alcancado', 'o', 'pedaco', 'de', 'terreno', 'que', 'o', 'separava', 'do', 'morro', 'e', 'o', 'outro', ',', 'por', 'seu', 'lado', ',', 'nao', 'perdia', 'a', 'esperanca', 'de', 'apanhar-lhe', 'ainda', ',', 'pelo', 'menos', ',', 'duas', 'ou', 'tres', 'bracas', 'aos', 'fundos', 'da', 'casa', 'parte', 'esta', 'que', ',', 'conforme', 'os', 'seus', 'calculos', ',', 'valeria', 'ouro', ',', 'uma', 'vez', 'realizado', 'o', 'grande', 'projeto', 'que', 'ultimamente', 'o', 'trazia', 'preocupado', '—', 'a', 'criacao', 'de', 'uma', 'estalagem', 'em', 'ponto', 'enorme', ',', 'uma', 'estalagem', 'monstro', ',', 'sem', 'exemplo', ',', 'destinada', 'a', 'matar', 'toda', 'aquela', 'miucalha', 'de', 'corticos', 'que', 'alastravam', 'por', 'botafogo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era este o seu ideal'\n",
      "Tokens gerados: ['era', 'este', 'o', 'seu', 'ideal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia muito que João Romão vivia exclusivamente para essa idéia; sonhava com ela todas as \n",
      "noites; comparecia a todos os leilões de materiais de construção; arrematava madeiramentos já servidos; comprava telha \n",
      "em segunda mão; fazia pechinchas de cal e tijolos; o que era tudo depositado no seu extenso chão vazio, cujo aspecto \n",
      "tomava em breve o caráter estranho de uma enorme barricada, tal era a variedade dos objetos que ali se apinhavam \n",
      "acumulados: tábuas e sarrafos, troncos de árvore, mastros de navio, caibros, restos de carroças, chaminés de barro e de \n",
      "ferro, fogões desmantelados, pilhas e pilhas de tijolos de todos os feitios, barricas de cimento, montes de areia e terra \n",
      "vermelha, aglomerações de telhas velhas, escadas partidas, depósitos de cal, o diabo enfim; ao que ele, que sabia \n",
      "perfeitamente como essas coisas se furtavam, resguardava, soltando à noite um formidável cão de fila'\n",
      "Tokens gerados: ['havia', 'muito', 'que', 'joao', 'romao', 'vivia', 'exclusivamente', 'para', 'essa', 'ideia', 'sonhava', 'com', 'ela', 'todas', 'as', 'noites', 'comparecia', 'a', 'todos', 'os', 'leiloes', 'de', 'materiais', 'de', 'construcao', 'arrematava', 'madeiramentos', 'ja', 'servidos', 'comprava', 'telha', 'em', 'segunda', 'mao', 'fazia', 'pechinchas', 'de', 'cal', 'e', 'tijolos', 'o', 'que', 'era', 'tudo', 'depositado', 'no', 'seu', 'extenso', 'chao', 'vazio', ',', 'cujo', 'aspecto', 'tomava', 'em', 'breve', 'o', 'carater', 'estranho', 'de', 'uma', 'enorme', 'barricada', ',', 'tal', 'era', 'a', 'variedade', 'dos', 'objetos', 'que', 'ali', 'se', 'apinhavam', 'acumulados', 'tabuas', 'e', 'sarrafos', ',', 'troncos', 'de', 'arvore', ',', 'mastros', 'de', 'navio', ',', 'caibros', ',', 'restos', 'de', 'carrocas', ',', 'chamines', 'de', 'barro', 'e', 'de', 'ferro', ',', 'fogoes', 'desmantelados', ',', 'pilhas', 'e', 'pilhas', 'de', 'tijolos', 'de', 'todos', 'os', 'feitios', ',', 'barricas', 'de', 'cimento', ',', 'montes', 'de', 'areia', 'e', 'terra', 'vermelha', ',', 'aglomeracoes', 'de', 'telhas', 'velhas', ',', 'escadas', 'partidas', ',', 'depositos', 'de', 'cal', ',', 'o', 'diabo', 'enfim', 'ao', 'que', 'ele', ',', 'que', 'sabia', 'perfeitamente', 'como', 'essas', 'coisas', 'se', 'furtavam', ',', 'resguardava', ',', 'soltando', 'a', 'noite', 'um', 'formidavel', 'cao', 'de', 'fila']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Este cão era pretexto de eternas resingas com a gente do Miranda, a cujo quintal ninguém de casa podia descer, \n",
      "depois das dez horas da noite, sem correr o risco de ser assaltado pela fera'\n",
      "Tokens gerados: ['este', 'cao', 'era', 'pretexto', 'de', 'eternas', 'resingas', 'com', 'a', 'gente', 'do', 'miranda', ',', 'a', 'cujo', 'quintal', 'ninguem', 'de', 'casa', 'podia', 'descer', ',', 'depois', 'das', 'dez', 'horas', 'da', 'noite', ',', 'sem', 'correr', 'o', 'risco', 'de', 'ser', 'assaltado', 'pela', 'fera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— É fazer o muro! dizia o João Romão, sacudindo os ombros'\n",
      "Tokens gerados: ['—', 'e', 'fazer', 'o', 'muro', 'dizia', 'o', 'joao', 'romao', ',', 'sacudindo', 'os', 'ombros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não faço! replicava o outro'\n",
      "Tokens gerados: ['—', 'nao', 'faco', 'replicava', 'o', 'outro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se ele é questão de capricho eu também tenho capricho! \n",
      "Em compensação, não caia no quintal do Miranda galinha ou frango, fugidos do cercado do vendeiro, que não \n",
      "levasse imediato sumiço'\n",
      "Tokens gerados: ['se', 'ele', 'e', 'questao', 'de', 'capricho', 'eu', 'tambem', 'tenho', 'capricho', 'em', 'compensacao', ',', 'nao', 'caia', 'no', 'quintal', 'do', 'miranda', 'galinha', 'ou', 'frango', ',', 'fugidos', 'do', 'cercado', 'do', 'vendeiro', ',', 'que', 'nao', 'levasse', 'imediato', 'sumico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão protestava contra o roubo em termos violentos, jurando vinganças terríveis, \n",
      "falando em dar tiros'\n",
      "Tokens gerados: ['joao', 'romao', 'protestava', 'contra', 'o', 'roubo', 'em', 'termos', 'violentos', ',', 'jurando', 'vingancas', 'terriveis', ',', 'falando', 'em', 'dar', 'tiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pois é fazer um muro no galinheiro! repontava o marido de Estela'\n",
      "Tokens gerados: ['—', 'pois', 'e', 'fazer', 'um', 'muro', 'no', 'galinheiro', 'repontava', 'o', 'marido', 'de', 'estela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Daí a alguns meses, João Romão, depois de tentar um derradeiro esforço para conseguir algumas braças do quintal \n",
      "do vizinho, resolveu principiar as obras da estalagem'\n",
      "Tokens gerados: ['dai', 'a', 'alguns', 'meses', ',', 'joao', 'romao', ',', 'depois', 'de', 'tentar', 'um', 'derradeiro', 'esforco', 'para', 'conseguir', 'algumas', 'bracas', 'do', 'quintal', 'do', 'vizinho', ',', 'resolveu', 'principiar', 'as', 'obras', 'da', 'estalagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Deixa estar, conversava ele na cama com a Bertoleza; deixa estar que ainda lhe hei de entrar pelos fundos da \n",
      "casa, se é que não lhe entre pela frente! Mais cedo ou mais tarde como-lhe, não duas braças, mas seis, oito, todo o \n",
      "quintal e até o próprio sobrado talvez! \n",
      "E dizia isto com uma convicção de quem tudo pode e tudo espera da sua perseverança, do seu esforço \n",
      "inquebrantável e da fecundidade prodigiosa do seu dinheiro, dinheiro que só lhe saia das unhas para voltar multiplicado'\n",
      "Tokens gerados: ['—', 'deixa', 'estar', ',', 'conversava', 'ele', 'na', 'cama', 'com', 'a', 'bertoleza', 'deixa', 'estar', 'que', 'ainda', 'lhe', 'hei', 'de', 'entrar', 'pelos', 'fundos', 'da', 'casa', ',', 'se', 'e', 'que', 'nao', 'lhe', 'entre', 'pela', 'frente', 'mais', 'cedo', 'ou', 'mais', 'tarde', 'como-lhe', ',', 'nao', 'duas', 'bracas', ',', 'mas', 'seis', ',', 'oito', ',', 'todo', 'o', 'quintal', 'e', 'ate', 'o', 'proprio', 'sobrado', 'talvez', 'e', 'dizia', 'isto', 'com', 'uma', 'conviccao', 'de', 'quem', 'tudo', 'pode', 'e', 'tudo', 'espera', 'da', 'sua', 'perseveranca', ',', 'do', 'seu', 'esforco', 'inquebrantavel', 'e', 'da', 'fecundidade', 'prodigiosa', 'do', 'seu', 'dinheiro', ',', 'dinheiro', 'que', 'so', 'lhe', 'saia', 'das', 'unhas', 'para', 'voltar', 'multiplicado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desde que a febre de possuir se apoderou dele totalmente, todos os seus atos, todos, fosse o mais simples, visavam \n",
      "um interesse pecuniário'\n",
      "Tokens gerados: ['desde', 'que', 'a', 'febre', 'de', 'possuir', 'se', 'apoderou', 'dele', 'totalmente', ',', 'todos', 'os', 'seus', 'atos', ',', 'todos', ',', 'fosse', 'o', 'mais', 'simples', ',', 'visavam', 'um', 'interesse', 'pecuniario']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Só tinha uma preocupação: aumentar os bens'\n",
      "Tokens gerados: ['so', 'tinha', 'uma', 'preocupacao', 'aumentar', 'os', 'bens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Das suas hortas recolhia para si e para a \n",
      "companheira os piores legumes, aqueles que, por maus, ninguém compraria; as suas galinhas produziam muito e ele não \n",
      "comia um ovo, do que no entanto gostava imenso; vendia-os todos e contentava-se com os restos da comida dos \n",
      "trabalhadores'\n",
      "Tokens gerados: ['das', 'suas', 'hortas', 'recolhia', 'para', 'si', 'e', 'para', 'a', 'companheira', 'os', 'piores', 'legumes', ',', 'aqueles', 'que', ',', 'por', 'maus', ',', 'ninguem', 'compraria', 'as', 'suas', 'galinhas', 'produziam', 'muito', 'e', 'ele', 'nao', 'comia', 'um', 'ovo', ',', 'do', 'que', 'no', 'entanto', 'gostava', 'imenso', 'vendia-os', 'todos', 'e', 'contentava-se', 'com', 'os', 'restos', 'da', 'comida', 'dos', 'trabalhadores']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aquilo já não era ambição, era uma moléstia nervosa, uma loucura, um desespero de acumular; de \n",
      "reduzir tudo a moeda'\n",
      "Tokens gerados: ['aquilo', 'ja', 'nao', 'era', 'ambicao', ',', 'era', 'uma', 'molestia', 'nervosa', ',', 'uma', 'loucura', ',', 'um', 'desespero', 'de', 'acumular', 'de', 'reduzir', 'tudo', 'a', 'moeda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E seu tipo baixote, socado, de cabelos à escovinha, a barba sempre por fazer, ia e vinha da \n",
      "pedreira para a venda, da venda às hortas e ao capinzal, sempre em mangas de camisa, de tamancos, sem meias, olhando \n",
      "para todos os lados, com o seu eterno ar de cobiça, apoderando-se, com os olhos, de tudo aquilo de que ele não podia \n",
      "apoderar-se logo com as unhas'\n",
      "Tokens gerados: ['e', 'seu', 'tipo', 'baixote', ',', 'socado', ',', 'de', 'cabelos', 'a', 'escovinha', ',', 'a', 'barba', 'sempre', 'por', 'fazer', ',', 'ia', 'e', 'vinha', 'da', 'pedreira', 'para', 'a', 'venda', ',', 'da', 'venda', 'as', 'hortas', 'e', 'ao', 'capinzal', ',', 'sempre', 'em', 'mangas', 'de', 'camisa', ',', 'de', 'tamancos', ',', 'sem', 'meias', ',', 'olhando', 'para', 'todos', 'os', 'lados', ',', 'com', 'o', 'seu', 'eterno', 'ar', 'de', 'cobica', ',', 'apoderando-se', ',', 'com', 'os', 'olhos', ',', 'de', 'tudo', 'aquilo', 'de', 'que', 'ele', 'nao', 'podia', 'apoderar-se', 'logo', 'com', 'as', 'unhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, a rua lá fora povoava-se de um modo admirável'\n",
      "Tokens gerados: ['entretanto', ',', 'a', 'rua', 'la', 'fora', 'povoava-se', 'de', 'um', 'modo', 'admiravel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Construía-se mal, porém muito; surgiam chalés e \n",
      "casinhas da noite para o dia; subiam os aluguéis; as propriedades dobravam de valor'\n",
      "Tokens gerados: ['construia-se', 'mal', ',', 'porem', 'muito', 'surgiam', 'chales', 'e', 'casinhas', 'da', 'noite', 'para', 'o', 'dia', 'subiam', 'os', 'alugueis', 'as', 'propriedades', 'dobravam', 'de', 'valor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Montara-se uma fábrica de massas \n",
      "italianas e outra de velas, e os trabalhadores passavam de manhã e às Ave-Marias, e a maior parte deles ia comer à casa \n",
      "de pasto que João Romão arranjara aos fundos da sua varanda'\n",
      "Tokens gerados: ['montara-se', 'uma', 'fabrica', 'de', 'massas', 'italianas', 'e', 'outra', 'de', 'velas', ',', 'e', 'os', 'trabalhadores', 'passavam', 'de', 'manha', 'e', 'as', 'ave-marias', ',', 'e', 'a', 'maior', 'parte', 'deles', 'ia', 'comer', 'a', 'casa', 'de', 'pasto', 'que', 'joao', 'romao', 'arranjara', 'aos', 'fundos', 'da', 'sua', 'varanda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Abriram-se novas tavernas; nenhuma, porém, conseguia \n",
      "ser tão afreguesada como a dele'\n",
      "Tokens gerados: ['abriram-se', 'novas', 'tavernas', 'nenhuma', ',', 'porem', ',', 'conseguia', 'ser', 'tao', 'afreguesada', 'como', 'a', 'dele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nunca o seu negocio fora tão bem, nunca o finório vendera tanto; vendia mais agora, \n",
      "muito mais, que nos anos anteriores'\n",
      "Tokens gerados: ['nunca', 'o', 'seu', 'negocio', 'fora', 'tao', 'bem', ',', 'nunca', 'o', 'finorio', 'vendera', 'tanto', 'vendia', 'mais', 'agora', ',', 'muito', 'mais', ',', 'que', 'nos', 'anos', 'anteriores']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Teve até de admitir caixeiros'\n",
      "Tokens gerados: ['teve', 'ate', 'de', 'admitir', 'caixeiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As mercadorias não lhe paravam nas prateleiras; o \n",
      "balcão estava cada vez mais lustroso, mais gasto'\n",
      "Tokens gerados: ['as', 'mercadorias', 'nao', 'lhe', 'paravam', 'nas', 'prateleiras', 'o', 'balcao', 'estava', 'cada', 'vez', 'mais', 'lustroso', ',', 'mais', 'gasto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o dinheiro a pingar, vintém por vintém, dentro da gaveta, e a \n",
      "escorrer da gaveta para a barra, aos cinqüenta e aos cem mil-réis, e da burra para o banco, aos contos e aos contos'\n",
      "Tokens gerados: ['e', 'o', 'dinheiro', 'a', 'pingar', ',', 'vintem', 'por', 'vintem', ',', 'dentro', 'da', 'gaveta', ',', 'e', 'a', 'escorrer', 'da', 'gaveta', 'para', 'a', 'barra', ',', 'aos', 'cinquenta', 'e', 'aos', 'cem', 'mil-reis', ',', 'e', 'da', 'burra', 'para', 'o', 'banco', ',', 'aos', 'contos', 'e', 'aos', 'contos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Afinal, já lhe não bastava sortir o seu estabelecimento nos armazéns fornecedores; começou a receber alguns \n",
      "gêneros diretamente da Europa: o vinho, por exemplo, que ele dantes comprava aos quintos nas casas de atacado, \n",
      "vinha-lhe agora de Portugal às pipas, e de cada uma fazia três com água e cachaça; e despachava faturas de barris de \n",
      "manteiga, de caixas de conserva, caixões de fósforos, azeite, queijos, louça e muitas outras mercadorias'\n",
      "Tokens gerados: ['afinal', ',', 'ja', 'lhe', 'nao', 'bastava', 'sortir', 'o', 'seu', 'estabelecimento', 'nos', 'armazens', 'fornecedores', 'comecou', 'a', 'receber', 'alguns', 'generos', 'diretamente', 'da', 'europa', 'o', 'vinho', ',', 'por', 'exemplo', ',', 'que', 'ele', 'dantes', 'comprava', 'aos', 'quintos', 'nas', 'casas', 'de', 'atacado', ',', 'vinha-lhe', 'agora', 'de', 'portugal', 'as', 'pipas', ',', 'e', 'de', 'cada', 'uma', 'fazia', 'tres', 'com', 'agua', 'e', 'cachaca', 'e', 'despachava', 'faturas', 'de', 'barris', 'de', 'manteiga', ',', 'de', 'caixas', 'de', 'conserva', ',', 'caixoes', 'de', 'fosforos', ',', 'azeite', ',', 'queijos', ',', 'louca', 'e', 'muitas', 'outras', 'mercadorias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Criou armazéns para depósito, aboliu a quitanda e transferiu o dormitório, aproveitando o espaço para ampliar a \n",
      "venda, que dobrou de tamanho e ganhou mais duas portas'\n",
      "Tokens gerados: ['criou', 'armazens', 'para', 'deposito', ',', 'aboliu', 'a', 'quitanda', 'e', 'transferiu', 'o', 'dormitorio', ',', 'aproveitando', 'o', 'espaco', 'para', 'ampliar', 'a', 'venda', ',', 'que', 'dobrou', 'de', 'tamanho', 'e', 'ganhou', 'mais', 'duas', 'portas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Já não era uma simples taverna, era um bazar em que se encontrava de tudo, objetos de armarinho, ferragens, \n",
      "porcelanas, utensílios de escritório, roupa de riscado para os trabalhadores, fazenda para roupa de mulher, chapéus de \n",
      "palha próprios para o serviço ao sol, perfumarias baratas, pentes de chifre, lenços com versos de amor, e anéis e brincos \n",
      "de metal ordinário'\n",
      "Tokens gerados: ['ja', 'nao', 'era', 'uma', 'simples', 'taverna', ',', 'era', 'um', 'bazar', 'em', 'que', 'se', 'encontrava', 'de', 'tudo', ',', 'objetos', 'de', 'armarinho', ',', 'ferragens', ',', 'porcelanas', ',', 'utensilios', 'de', 'escritorio', ',', 'roupa', 'de', 'riscado', 'para', 'os', 'trabalhadores', ',', 'fazenda', 'para', 'roupa', 'de', 'mulher', ',', 'chapeus', 'de', 'palha', 'proprios', 'para', 'o', 'servico', 'ao', 'sol', ',', 'perfumarias', 'baratas', ',', 'pentes', 'de', 'chifre', ',', 'lencos', 'com', 'versos', 'de', 'amor', ',', 'e', 'aneis', 'e', 'brincos', 'de', 'metal', 'ordinario']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E toda a gentalha daquelas redondezas ia cair lá, ou então ali ao lado, na casa de pasto, onde os operários das \n",
      "fábricas e os trabalhadores da pedreira se reuniam depois do serviço, e ficavam bebendo e conversando até as dez horas \n",
      "da noite, entre o espesso fumo dos cachimbos, do peixe frito em azeite e dos lampiões de querosene'\n",
      "Tokens gerados: ['e', 'toda', 'a', 'gentalha', 'daquelas', 'redondezas', 'ia', 'cair', 'la', ',', 'ou', 'entao', 'ali', 'ao', 'lado', ',', 'na', 'casa', 'de', 'pasto', ',', 'onde', 'os', 'operarios', 'das', 'fabricas', 'e', 'os', 'trabalhadores', 'da', 'pedreira', 'se', 'reuniam', 'depois', 'do', 'servico', ',', 'e', 'ficavam', 'bebendo', 'e', 'conversando', 'ate', 'as', 'dez', 'horas', 'da', 'noite', ',', 'entre', 'o', 'espesso', 'fumo', 'dos', 'cachimbos', ',', 'do', 'peixe', 'frito', 'em', 'azeite', 'e', 'dos', 'lampioes', 'de', 'querosene']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era João Romão quem lhes fornecia tudo, tudo, até dinheiro adiantado, quando algum precisava'\n",
      "Tokens gerados: ['era', 'joao', 'romao', 'quem', 'lhes', 'fornecia', 'tudo', ',', 'tudo', ',', 'ate', 'dinheiro', 'adiantado', ',', 'quando', 'algum', 'precisava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por ali não se \n",
      "encontrava jornaleiro, cujo ordenado não fosse inteirinho parar às mãos do velhaco'\n",
      "Tokens gerados: ['por', 'ali', 'nao', 'se', 'encontrava', 'jornaleiro', ',', 'cujo', 'ordenado', 'nao', 'fosse', 'inteirinho', 'parar', 'as', 'maos', 'do', 'velhaco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E sobre este cobre, quase sempre \n",
      "emprestado aos tostões, cobrava juros de oito por cento ao mês, um pouco mais do que levava aos que garantiam a \n",
      "divida com penhores de ouro ou prata'\n",
      "Tokens gerados: ['e', 'sobre', 'este', 'cobre', ',', 'quase', 'sempre', 'emprestado', 'aos', 'tostoes', ',', 'cobrava', 'juros', 'de', 'oito', 'por', 'cento', 'ao', 'mes', ',', 'um', 'pouco', 'mais', 'do', 'que', 'levava', 'aos', 'que', 'garantiam', 'a', 'divida', 'com', 'penhores', 'de', 'ouro', 'ou', 'prata']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não obstante, as casinhas do cortiço, à proporção que se atamancavam, enchiam-se logo, sem mesmo dar tempo a \n",
      "que as tintas secassem'\n",
      "Tokens gerados: ['nao', 'obstante', ',', 'as', 'casinhas', 'do', 'cortico', ',', 'a', 'proporcao', 'que', 'se', 'atamancavam', ',', 'enchiam-se', 'logo', ',', 'sem', 'mesmo', 'dar', 'tempo', 'a', 'que', 'as', 'tintas', 'secassem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia grande avidez em alugá-las; aquele era o melhor ponto do bairro para a gente do trabalho'\n",
      "Tokens gerados: ['havia', 'grande', 'avidez', 'em', 'aluga-las', 'aquele', 'era', 'o', 'melhor', 'ponto', 'do', 'bairro', 'para', 'a', 'gente', 'do', 'trabalho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os empregados da pedreira preferiam todos morar lá, porque ficavam a dois passos da obrigação'\n",
      "Tokens gerados: ['os', 'empregados', 'da', 'pedreira', 'preferiam', 'todos', 'morar', 'la', ',', 'porque', 'ficavam', 'a', 'dois', 'passos', 'da', 'obrigacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Miranda rebentava de raiva'\n",
      "Tokens gerados: ['o', 'miranda', 'rebentava', 'de', 'raiva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Um cortiço! exclamava ele, possesso'\n",
      "Tokens gerados: ['—', 'um', 'cortico', 'exclamava', 'ele', ',', 'possesso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um cortiço! Maldito seja aquele vendeiro de todos os diabos! Fazer-me \n",
      "um cortiço debaixo das janelas!'\n",
      "Tokens gerados: ['um', 'cortico', 'maldito', 'seja', 'aquele', 'vendeiro', 'de', 'todos', 'os', 'diabos', 'fazer-me', 'um', 'cortico', 'debaixo', 'das', 'janelas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estragou-me a casa, o malvado! \n",
      "E vomitava pragas, jurando que havia de vingar-se, e protestando aos berros contra o pó que lhe invadia em ondas \n",
      "as salas, e contra o infernal baralho dos pedreiros e carpinteiros que levavam a martelar de sol a sol'\n",
      "Tokens gerados: ['estragou-me', 'a', 'casa', ',', 'o', 'malvado', 'e', 'vomitava', 'pragas', ',', 'jurando', 'que', 'havia', 'de', 'vingar-se', ',', 'e', 'protestando', 'aos', 'berros', 'contra', 'o', 'po', 'que', 'lhe', 'invadia', 'em', 'ondas', 'as', 'salas', ',', 'e', 'contra', 'o', 'infernal', 'baralho', 'dos', 'pedreiros', 'e', 'carpinteiros', 'que', 'levavam', 'a', 'martelar', 'de', 'sol', 'a', 'sol']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O que aliás não impediu que as casinhas continuassem a surgir, uma após outra, e fossem logo se enchendo, a \n",
      "estenderem-se unidas por ali a fora, desde a venda até quase ao morro, e depois dobrassem para o lado do Miranda e \n",
      "avançassem sobre o quintal deste, que parecia ameaçado por aquela serpente de pedra e cal'\n",
      "Tokens gerados: ['o', 'que', 'alias', 'nao', 'impediu', 'que', 'as', 'casinhas', 'continuassem', 'a', 'surgir', ',', 'uma', 'apos', 'outra', ',', 'e', 'fossem', 'logo', 'se', 'enchendo', ',', 'a', 'estenderem-se', 'unidas', 'por', 'ali', 'a', 'fora', ',', 'desde', 'a', 'venda', 'ate', 'quase', 'ao', 'morro', ',', 'e', 'depois', 'dobrassem', 'para', 'o', 'lado', 'do', 'miranda', 'e', 'avancassem', 'sobre', 'o', 'quintal', 'deste', ',', 'que', 'parecia', 'ameacado', 'por', 'aquela', 'serpente', 'de', 'pedra', 'e', 'cal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Miranda mandou logo levantar o muro'\n",
      "Tokens gerados: ['o', 'miranda', 'mandou', 'logo', 'levantar', 'o', 'muro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nada! aquele demônio era capaz de invadir-lhe a casa até a sala de visitas! \n",
      "E os quartos do cortiço pararam enfim de encontro ao muro do negociante, formando com a continuação da casa \n",
      "deste um grande quadrilongo, espécie de pátio de quartel, onde podia formar um batalhão'\n",
      "Tokens gerados: ['nada', 'aquele', 'demonio', 'era', 'capaz', 'de', 'invadir-lhe', 'a', 'casa', 'ate', 'a', 'sala', 'de', 'visitas', 'e', 'os', 'quartos', 'do', 'cortico', 'pararam', 'enfim', 'de', 'encontro', 'ao', 'muro', 'do', 'negociante', ',', 'formando', 'com', 'a', 'continuacao', 'da', 'casa', 'deste', 'um', 'grande', 'quadrilongo', ',', 'especie', 'de', 'patio', 'de', 'quartel', ',', 'onde', 'podia', 'formar', 'um', 'batalhao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Noventa e cinco casinhas comportou a imensa estalagem'\n",
      "Tokens gerados: ['noventa', 'e', 'cinco', 'casinhas', 'comportou', 'a', 'imensa', 'estalagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prontas, João Romão mandou levantar na frente, nas vinte braças que separavam a venda do sobrado do Miranda, \n",
      "um grosso muro de dez palmos de altura, coroado de cacos de vidro e fundos de garrafa, e com um grande portão no \n",
      "centro, onde se dependurou uma lanterna de vidraças vermelhas, por cima de uma tabuleta amarela, em que se lia o \n",
      "seguinte, escrito a tinta encarnada e sem ortografia: \n",
      "“Estalagem de São Romão'\n",
      "Tokens gerados: ['prontas', ',', 'joao', 'romao', 'mandou', 'levantar', 'na', 'frente', ',', 'nas', 'vinte', 'bracas', 'que', 'separavam', 'a', 'venda', 'do', 'sobrado', 'do', 'miranda', ',', 'um', 'grosso', 'muro', 'de', 'dez', 'palmos', 'de', 'altura', ',', 'coroado', 'de', 'cacos', 'de', 'vidro', 'e', 'fundos', 'de', 'garrafa', ',', 'e', 'com', 'um', 'grande', 'portao', 'no', 'centro', ',', 'onde', 'se', 'dependurou', 'uma', 'lanterna', 'de', 'vidracas', 'vermelhas', ',', 'por', 'cima', 'de', 'uma', 'tabuleta', 'amarela', ',', 'em', 'que', 'se', 'lia', 'o', 'seguinte', ',', 'escrito', 'a', 'tinta', 'encarnada', 'e', 'sem', 'ortografia', '“', 'estalagem', 'de', 'sao', 'romao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Alugam-se casinhas e tinas para lavadeiras”'\n",
      "Tokens gerados: ['alugam-se', 'casinhas', 'e', 'tinas', 'para', 'lavadeiras', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As casinhas eram alugadas por mês e as tinas por dia; tudo pago adiantado'\n",
      "Tokens gerados: ['as', 'casinhas', 'eram', 'alugadas', 'por', 'mes', 'e', 'as', 'tinas', 'por', 'dia', 'tudo', 'pago', 'adiantado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O preço de cada tina, metendo a água, \n",
      "quinhentos réis; sabão à parte'\n",
      "Tokens gerados: ['o', 'preco', 'de', 'cada', 'tina', ',', 'metendo', 'a', 'agua', ',', 'quinhentos', 'reis', 'sabao', 'a', 'parte']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As moradoras do cortiço tinham preferência e não pagavam nada para lavar'\n",
      "Tokens gerados: ['as', 'moradoras', 'do', 'cortico', 'tinham', 'preferencia', 'e', 'nao', 'pagavam', 'nada', 'para', 'lavar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Graças à abundância da água que lá havia, como em nenhuma outra parte, e graças ao muito espaço de que se \n",
      "dispunha no cortiço para estender a roupa, a concorrência às tinas não se fez esperar; acudiram lavadeiras de todos os \n",
      "pontos da cidade, entre elas algumas vindas de bem longe'\n",
      "Tokens gerados: ['gracas', 'a', 'abundancia', 'da', 'agua', 'que', 'la', 'havia', ',', 'como', 'em', 'nenhuma', 'outra', 'parte', ',', 'e', 'gracas', 'ao', 'muito', 'espaco', 'de', 'que', 'se', 'dispunha', 'no', 'cortico', 'para', 'estender', 'a', 'roupa', ',', 'a', 'concorrencia', 'as', 'tinas', 'nao', 'se', 'fez', 'esperar', 'acudiram', 'lavadeiras', 'de', 'todos', 'os', 'pontos', 'da', 'cidade', ',', 'entre', 'elas', 'algumas', 'vindas', 'de', 'bem', 'longe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, mal vagava uma das casinhas, ou um quarto, um canto \n",
      "onde coubesse um colchão, surgia uma nuvem de pretendentes a disputá-los'\n",
      "Tokens gerados: ['e', ',', 'mal', 'vagava', 'uma', 'das', 'casinhas', ',', 'ou', 'um', 'quarto', ',', 'um', 'canto', 'onde', 'coubesse', 'um', 'colchao', ',', 'surgia', 'uma', 'nuvem', 'de', 'pretendentes', 'a', 'disputa-los']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E aquilo se foi constituindo numa grande lavanderia, agitada e barulhenta, com as suas cercas de varas, as suas \n",
      "hortaliças verdejantes e os seus jardinzinhos de três e quatro palmos, que apareciam como manchas alegres por entre a \n",
      "negrura das limosas tinas transbordantes e o revérbero das claras barracas de algodão cru, armadas sobre os lustrosos \n",
      "bancos de lavar'\n",
      "Tokens gerados: ['e', 'aquilo', 'se', 'foi', 'constituindo', 'numa', 'grande', 'lavanderia', ',', 'agitada', 'e', 'barulhenta', ',', 'com', 'as', 'suas', 'cercas', 'de', 'varas', ',', 'as', 'suas', 'hortalicas', 'verdejantes', 'e', 'os', 'seus', 'jardinzinhos', 'de', 'tres', 'e', 'quatro', 'palmos', ',', 'que', 'apareciam', 'como', 'manchas', 'alegres', 'por', 'entre', 'a', 'negrura', 'das', 'limosas', 'tinas', 'transbordantes', 'e', 'o', 'reverbero', 'das', 'claras', 'barracas', 'de', 'algodao', 'cru', ',', 'armadas', 'sobre', 'os', 'lustrosos', 'bancos', 'de', 'lavar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E os gotejantes jiraus, cobertos de roupa molhada, cintilavam ao sol, que nem lagos de metal branco'\n",
      "Tokens gerados: ['e', 'os', 'gotejantes', 'jiraus', ',', 'cobertos', 'de', 'roupa', 'molhada', ',', 'cintilavam', 'ao', 'sol', ',', 'que', 'nem', 'lagos', 'de', 'metal', 'branco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E naquela terra encharcada e fumegante, naquela umidade quente e lodosa, começou a minhocar, a esfervilhar, a \n",
      "crescer, um mundo, uma coisa viva, uma geração, que parecia brotar espontânea, ali mesmo, daquele lameiro, e \n",
      "multiplicar-se como larvas no esterco'\n",
      "Tokens gerados: ['e', 'naquela', 'terra', 'encharcada', 'e', 'fumegante', ',', 'naquela', 'umidade', 'quente', 'e', 'lodosa', ',', 'comecou', 'a', 'minhocar', ',', 'a', 'esfervilhar', ',', 'a', 'crescer', ',', 'um', 'mundo', ',', 'uma', 'coisa', 'viva', ',', 'uma', 'geracao', ',', 'que', 'parecia', 'brotar', 'espontanea', ',', 'ali', 'mesmo', ',', 'daquele', 'lameiro', ',', 'e', 'multiplicar-se', 'como', 'larvas', 'no', 'esterco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_o_cortico_aluisio_azevedo_cap_1.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Durante dois anos o cortiço prosperou de dia para dia, ganhando forças, socando-se de gente'\n",
      "Tokens gerados: ['durante', 'dois', 'anos', 'o', 'cortico', 'prosperou', 'de', 'dia', 'para', 'dia', ',', 'ganhando', 'forcas', ',', 'socando-se', 'de', 'gente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E ao lado o Miranda \n",
      "assustava-se, inquieto com aquela exuberância brutal de vida, aterrado defronte daquela floresta implacável que lhe \n",
      "crescia junto da casa, por debaixo das janelas, e cujas raízes, piores e mais grossas do que serpentes, minavam por toda \n",
      "a parte, ameaçando rebentar o chão em torno dela, rachando o solo e abalando tudo'\n",
      "Tokens gerados: ['e', 'ao', 'lado', 'o', 'miranda', 'assustava-se', ',', 'inquieto', 'com', 'aquela', 'exuberancia', 'brutal', 'de', 'vida', ',', 'aterrado', 'defronte', 'daquela', 'floresta', 'implacavel', 'que', 'lhe', 'crescia', 'junto', 'da', 'casa', ',', 'por', 'debaixo', 'das', 'janelas', ',', 'e', 'cujas', 'raizes', ',', 'piores', 'e', 'mais', 'grossas', 'do', 'que', 'serpentes', ',', 'minavam', 'por', 'toda', 'a', 'parte', ',', 'ameacando', 'rebentar', 'o', 'chao', 'em', 'torno', 'dela', ',', 'rachando', 'o', 'solo', 'e', 'abalando', 'tudo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Posto que lá na Rua do Hospício os seus negócios não corressem mal, custava-lhe a sofrer a escandalosa fortuna \n",
      "do vendeiro “aquele tipo! um miserável, um sujo, que não pusera nunca um paletó, e que vivia de cama e mesa com \n",
      "uma negra!” \n",
      "À noite e aos domingos ainda mais recrudescia o seu azedume, quando ele, recolhendo-se fatigado do serviço, \n",
      "deixava-se ficar estendido numa preguiçosa, junto à mesa da sala de jantar, e ouvia, a contragosto, o grosseiro rumor \n",
      "que vinha da estalagem numa exalação forte de animais cansados'\n",
      "Tokens gerados: ['posto', 'que', 'la', 'na', 'rua', 'do', 'hospicio', 'os', 'seus', 'negocios', 'nao', 'corressem', 'mal', ',', 'custava-lhe', 'a', 'sofrer', 'a', 'escandalosa', 'fortuna', 'do', 'vendeiro', '“', 'aquele', 'tipo', 'um', 'miseravel', ',', 'um', 'sujo', ',', 'que', 'nao', 'pusera', 'nunca', 'um', 'paleto', ',', 'e', 'que', 'vivia', 'de', 'cama', 'e', 'mesa', 'com', 'uma', 'negra', '”', 'a', 'noite', 'e', 'aos', 'domingos', 'ainda', 'mais', 'recrudescia', 'o', 'seu', 'azedume', ',', 'quando', 'ele', ',', 'recolhendo-se', 'fatigado', 'do', 'servico', ',', 'deixava-se', 'ficar', 'estendido', 'numa', 'preguicosa', ',', 'junto', 'a', 'mesa', 'da', 'sala', 'de', 'jantar', ',', 'e', 'ouvia', ',', 'a', 'contragosto', ',', 'o', 'grosseiro', 'rumor', 'que', 'vinha', 'da', 'estalagem', 'numa', 'exalacao', 'forte', 'de', 'animais', 'cansados']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não podia chegar à janela sem receber no rosto \n",
      "aquele bafo, quente e sensual, que o embebedava com o seu fartum de bestas no coito'\n",
      "Tokens gerados: ['nao', 'podia', 'chegar', 'a', 'janela', 'sem', 'receber', 'no', 'rosto', 'aquele', 'bafo', ',', 'quente', 'e', 'sensual', ',', 'que', 'o', 'embebedava', 'com', 'o', 'seu', 'fartum', 'de', 'bestas', 'no', 'coito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E depois, fechado no quarto de dormir, indiferente e habituado às torpezas carnais da mulher, isento já dos \n",
      "primitivos sobressaltos que lhe faziam, a ele, ferver o sangue e perder a tramontana, era ainda a prosperidade do vizinho \n",
      "o que lhe obsedava o espírito, enegrecendo-lhe a alma com um feio ressentimento de despeito'\n",
      "Tokens gerados: ['e', 'depois', ',', 'fechado', 'no', 'quarto', 'de', 'dormir', ',', 'indiferente', 'e', 'habituado', 'as', 'torpezas', 'carnais', 'da', 'mulher', ',', 'isento', 'ja', 'dos', 'primitivos', 'sobressaltos', 'que', 'lhe', 'faziam', ',', 'a', 'ele', ',', 'ferver', 'o', 'sangue', 'e', 'perder', 'a', 'tramontana', ',', 'era', 'ainda', 'a', 'prosperidade', 'do', 'vizinho', 'o', 'que', 'lhe', 'obsedava', 'o', 'espirito', ',', 'enegrecendo-lhe', 'a', 'alma', 'com', 'um', 'feio', 'ressentimento', 'de', 'despeito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha inveja do outro, daquele outro português que fizera fortuna, sem precisar roer nenhum chifre; daquele outro \n",
      "que, para ser mais rico três vezes do que ele, não teve de casar com a filha do patrão ou com a bastarda de algum \n",
      "fazendeiro freguês da casa! \n",
      "Mas então, ele Miranda, que se supunha a última expressão da ladinagem e da esperteza; ele, que, logo depois do \n",
      "seu casamento, respondendo para Portugal a um ex-colega que o felicitava, dissera que o Brasil era uma cavalgadura \n",
      "carregada de dinheiro, cujas rédeas um homem fino empolgava facilmente; ele, que se tinha na conta de invencível \n",
      "matreiro, não passava afinal de um pedaço de asno comparado com o seu vizinho! Pensara fazer-se senhor do Brasil e \n",
      "fizera-se escravo de uma brasileira mal-educada e sem escrúpulos de virtude! Imaginara-se talhado para grandes \n",
      "conquistas, e não passava de uma vitima ridícula e sofredora!'\n",
      "Tokens gerados: ['tinha', 'inveja', 'do', 'outro', ',', 'daquele', 'outro', 'portugues', 'que', 'fizera', 'fortuna', ',', 'sem', 'precisar', 'roer', 'nenhum', 'chifre', 'daquele', 'outro', 'que', ',', 'para', 'ser', 'mais', 'rico', 'tres', 'vezes', 'do', 'que', 'ele', ',', 'nao', 'teve', 'de', 'casar', 'com', 'a', 'filha', 'do', 'patrao', 'ou', 'com', 'a', 'bastarda', 'de', 'algum', 'fazendeiro', 'fregues', 'da', 'casa', 'mas', 'entao', ',', 'ele', 'miranda', ',', 'que', 'se', 'supunha', 'a', 'ultima', 'expressao', 'da', 'ladinagem', 'e', 'da', 'esperteza', 'ele', ',', 'que', ',', 'logo', 'depois', 'do', 'seu', 'casamento', ',', 'respondendo', 'para', 'portugal', 'a', 'um', 'ex-colega', 'que', 'o', 'felicitava', ',', 'dissera', 'que', 'o', 'brasil', 'era', 'uma', 'cavalgadura', 'carregada', 'de', 'dinheiro', ',', 'cujas', 'redeas', 'um', 'homem', 'fino', 'empolgava', 'facilmente', 'ele', ',', 'que', 'se', 'tinha', 'na', 'conta', 'de', 'invencivel', 'matreiro', ',', 'nao', 'passava', 'afinal', 'de', 'um', 'pedaco', 'de', 'asno', 'comparado', 'com', 'o', 'seu', 'vizinho', 'pensara', 'fazer-se', 'senhor', 'do', 'brasil', 'e', 'fizera-se', 'escravo', 'de', 'uma', 'brasileira', 'mal-educada', 'e', 'sem', 'escrupulos', 'de', 'virtude', 'imaginara-se', 'talhado', 'para', 'grandes', 'conquistas', ',', 'e', 'nao', 'passava', 'de', 'uma', 'vitima', 'ridicula', 'e', 'sofredora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sim! no fim de contas qual fora a sua África?'\n",
      "Tokens gerados: ['sim', 'no', 'fim', 'de', 'contas', 'qual', 'fora', 'a', 'sua', 'africa', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Enriquecera um pouco, é verdade, mas como? a que preço? hipotecando-se a um diabo, que lhe trouxera oitenta contos \n",
      "de réis, mas incalculáveis milhões de desgostos e vergonhas! Arranjara a vida, sim, mas teve de aturar eternamente uma \n",
      "mulher que ele odiava! E do que afinal lhe aproveitar tudo isso? Qual era afinal a sua grande existência? Do inferno da \n",
      "casa para o purgatório do trabalho e vice-versa! Invejável sorte, não havia dúvida! \n",
      "Na dolorosa incerteza de que Zulmira fosse sua filha, o desgraçado nem sequer gozava o prazer de ser pai'\n",
      "Tokens gerados: ['enriquecera', 'um', 'pouco', ',', 'e', 'verdade', ',', 'mas', 'como', '?', 'a', 'que', 'preco', '?', 'hipotecando-se', 'a', 'um', 'diabo', ',', 'que', 'lhe', 'trouxera', 'oitenta', 'contos', 'de', 'reis', ',', 'mas', 'incalculaveis', 'milhoes', 'de', 'desgostos', 'e', 'vergonhas', 'arranjara', 'a', 'vida', ',', 'sim', ',', 'mas', 'teve', 'de', 'aturar', 'eternamente', 'uma', 'mulher', 'que', 'ele', 'odiava', 'e', 'do', 'que', 'afinal', 'lhe', 'aproveitar', 'tudo', 'isso', '?', 'qual', 'era', 'afinal', 'a', 'sua', 'grande', 'existencia', '?', 'do', 'inferno', 'da', 'casa', 'para', 'o', 'purgatorio', 'do', 'trabalho', 'e', 'vice-versa', 'invejavel', 'sorte', ',', 'nao', 'havia', 'duvida', 'na', 'dolorosa', 'incerteza', 'de', 'que', 'zulmira', 'fosse', 'sua', 'filha', ',', 'o', 'desgracado', 'nem', 'sequer', 'gozava', 'o', 'prazer', 'de', 'ser', 'pai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se ela, \n",
      "em vez de nascer de Estela, fora uma enjeitadinha recolhida por ele, é natural que a amasse e então a vida lhe correria \n",
      "de outro modo; mas naquelas condições, a pobre criança nada mais representava que o documento vivo do ludibrio \n",
      "materno, e o Miranda estendia até à inocentezinha d'África o ódio que sustentava contra a esposa'\n",
      "Tokens gerados: ['se', 'ela', ',', 'em', 'vez', 'de', 'nascer', 'de', 'estela', ',', 'fora', 'uma', 'enjeitadinha', 'recolhida', 'por', 'ele', ',', 'e', 'natural', 'que', 'a', 'amasse', 'e', 'entao', 'a', 'vida', 'lhe', 'correria', 'de', 'outro', 'modo', 'mas', 'naquelas', 'condicoes', ',', 'a', 'pobre', 'crianca', 'nada', 'mais', 'representava', 'que', 'o', 'documento', 'vivo', 'do', 'ludibrio', 'materno', ',', 'e', 'o', 'miranda', 'estendia', 'ate', 'a', 'inocentezinha', \"d'africa\", 'o', 'odio', 'que', 'sustentava', 'contra', 'a', 'esposa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma espiga a tal da sua vida! \n",
      "— Fui uma besta! resumiu ele, em voz alta, apeando-se da cama, onde se havia recolhido inutilmente'\n",
      "Tokens gerados: ['uma', 'espiga', 'a', 'tal', 'da', 'sua', 'vida', '—', 'fui', 'uma', 'besta', 'resumiu', 'ele', ',', 'em', 'voz', 'alta', ',', 'apeando-se', 'da', 'cama', ',', 'onde', 'se', 'havia', 'recolhido', 'inutilmente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E pôs-se a passear no quarto sem vontade de dormir, sentindo que a febre daquela inveja lhe estorricava os \n",
      "miolos'\n",
      "Tokens gerados: ['e', 'pos-se', 'a', 'passear', 'no', 'quarto', 'sem', 'vontade', 'de', 'dormir', ',', 'sentindo', 'que', 'a', 'febre', 'daquela', 'inveja', 'lhe', 'estorricava', 'os', 'miolos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Feliz e esperto era o João Romão! esse, sim, senhor! Para esse é que havia de ser a vida!'\n",
      "Tokens gerados: ['feliz', 'e', 'esperto', 'era', 'o', 'joao', 'romao', 'esse', ',', 'sim', ',', 'senhor', 'para', 'esse', 'e', 'que', 'havia', 'de', 'ser', 'a', 'vida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Filho da mãe, que \n",
      "estava hoje tão livre e desembaraçado como no dia em que chegou da terra sem um vintém de seu! esse, sim, que era \n",
      "moço e podia ainda gozar muito, porque quando mesmo viesse a casar e a mulher lhe saísse uma outra Estela era só \n",
      "mandá-la para o diabo com um pontapé! Podia fazê-lo! Para esse é que era o Brasil! \n",
      "— Fui uma besta! repisava ele sem conseguir conformar-se com a felicidade do vendeiro'\n",
      "Tokens gerados: ['filho', 'da', 'mae', ',', 'que', 'estava', 'hoje', 'tao', 'livre', 'e', 'desembaracado', 'como', 'no', 'dia', 'em', 'que', 'chegou', 'da', 'terra', 'sem', 'um', 'vintem', 'de', 'seu', 'esse', ',', 'sim', ',', 'que', 'era', 'moco', 'e', 'podia', 'ainda', 'gozar', 'muito', ',', 'porque', 'quando', 'mesmo', 'viesse', 'a', 'casar', 'e', 'a', 'mulher', 'lhe', 'saisse', 'uma', 'outra', 'estela', 'era', 'so', 'manda-la', 'para', 'o', 'diabo', 'com', 'um', 'pontape', 'podia', 'faze-lo', 'para', 'esse', 'e', 'que', 'era', 'o', 'brasil', '—', 'fui', 'uma', 'besta', 'repisava', 'ele', 'sem', 'conseguir', 'conformar-se', 'com', 'a', 'felicidade', 'do', 'vendeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma grandíssima! No \n",
      "fim de contas que diabo possuo eu?'\n",
      "Tokens gerados: ['uma', 'grandissima', 'no', 'fim', 'de', 'contas', 'que', 'diabo', 'possuo', 'eu', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma casa de negócio, da qual não posso separar-me sem comprometer o que lá \n",
      "está enterrado! um capital metido numa rede de transações que não se liquidam nunca, e cada vez mais se complicam e \n",
      "mais me grudam ao estupor desta terra, onde deixarei a casca! Que tenho de meu, se a alma do meu crédito é o dote, que \n",
      "me trouxe aquela sem-vergonha e que a ela me prende como a peste da casa comercial me prende a esta Costa d’África? \n",
      "Foi da supuração fétida destas idéias que se formou no coração vazio do Miranda um novo ideal — o título'\n",
      "Tokens gerados: ['uma', 'casa', 'de', 'negocio', ',', 'da', 'qual', 'nao', 'posso', 'separar-me', 'sem', 'comprometer', 'o', 'que', 'la', 'esta', 'enterrado', 'um', 'capital', 'metido', 'numa', 'rede', 'de', 'transacoes', 'que', 'nao', 'se', 'liquidam', 'nunca', ',', 'e', 'cada', 'vez', 'mais', 'se', 'complicam', 'e', 'mais', 'me', 'grudam', 'ao', 'estupor', 'desta', 'terra', ',', 'onde', 'deixarei', 'a', 'casca', 'que', 'tenho', 'de', 'meu', ',', 'se', 'a', 'alma', 'do', 'meu', 'credito', 'e', 'o', 'dote', ',', 'que', 'me', 'trouxe', 'aquela', 'sem-vergonha', 'e', 'que', 'a', 'ela', 'me', 'prende', 'como', 'a', 'peste', 'da', 'casa', 'comercial', 'me', 'prende', 'a', 'esta', 'costa', 'd', '’', 'africa', '?', 'foi', 'da', 'supuracao', 'fetida', 'destas', 'ideias', 'que', 'se', 'formou', 'no', 'coracao', 'vazio', 'do', 'miranda', 'um', 'novo', 'ideal', '—', 'o', 'titulo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Faltando-lhe temperamento próprio para os vícios fortes que enchem a vida de um homem; sem família, a quem amar e \n",
      "sem imaginação para poder gozar com as prostitutas, o náufrago agarrou-se àquela tábua, como um agonizante, \n",
      "consciente da morte, que se apega à esperança de uma vida futura'\n",
      "Tokens gerados: ['faltando-lhe', 'temperamento', 'proprio', 'para', 'os', 'vicios', 'fortes', 'que', 'enchem', 'a', 'vida', 'de', 'um', 'homem', 'sem', 'familia', ',', 'a', 'quem', 'amar', 'e', 'sem', 'imaginacao', 'para', 'poder', 'gozar', 'com', 'as', 'prostitutas', ',', 'o', 'naufrago', 'agarrou-se', 'aquela', 'tabua', ',', 'como', 'um', 'agonizante', ',', 'consciente', 'da', 'morte', ',', 'que', 'se', 'apega', 'a', 'esperanca', 'de', 'uma', 'vida', 'futura']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A vaidade de Estela, que a principio lhe tirava dos \n",
      "lábios incrédulos sorrisos de mofa, agora lhe comprazia à farta'\n",
      "Tokens gerados: ['a', 'vaidade', 'de', 'estela', ',', 'que', 'a', 'principio', 'lhe', 'tirava', 'dos', 'labios', 'incredulos', 'sorrisos', 'de', 'mofa', ',', 'agora', 'lhe', 'comprazia', 'a', 'farta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Procurou capacitar-se de que ela com efeito herdara \n",
      "sangue nobre, que ele, por sua vez, se não o tinha herdado, trouxera-o por natureza própria, o que devia valer mais \n",
      "ainda; e desde então principiou a sonhar com um baronato, fazendo disso o objeto querido da sua existência, muito \n",
      "satisfeito no intimo por ter afinal descoberto uma coisa em que podia empregar dinheiro, sem ter, nunca mais, de \n",
      "restituí-lo à mulher, nem ter de deixá-lo a pessoa alguma'\n",
      "Tokens gerados: ['procurou', 'capacitar-se', 'de', 'que', 'ela', 'com', 'efeito', 'herdara', 'sangue', 'nobre', ',', 'que', 'ele', ',', 'por', 'sua', 'vez', ',', 'se', 'nao', 'o', 'tinha', 'herdado', ',', 'trouxera-o', 'por', 'natureza', 'propria', ',', 'o', 'que', 'devia', 'valer', 'mais', 'ainda', 'e', 'desde', 'entao', 'principiou', 'a', 'sonhar', 'com', 'um', 'baronato', ',', 'fazendo', 'disso', 'o', 'objeto', 'querido', 'da', 'sua', 'existencia', ',', 'muito', 'satisfeito', 'no', 'intimo', 'por', 'ter', 'afinal', 'descoberto', 'uma', 'coisa', 'em', 'que', 'podia', 'empregar', 'dinheiro', ',', 'sem', 'ter', ',', 'nunca', 'mais', ',', 'de', 'restitui-lo', 'a', 'mulher', ',', 'nem', 'ter', 'de', 'deixa-lo', 'a', 'pessoa', 'alguma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Semelhante preocupação modificou-o em extremo'\n",
      "Tokens gerados: ['semelhante', 'preocupacao', 'modificou-o', 'em', 'extremo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deu logo para fingir-se escravo das conveniências, afetando \n",
      "escrúpulos sociais, empertigando-se quanto podia e disfarçando a sua inveja pelo vizinho com um desdenhoso ar de \n",
      "superioridade condescendente'\n",
      "Tokens gerados: ['deu', 'logo', 'para', 'fingir-se', 'escravo', 'das', 'conveniencias', ',', 'afetando', 'escrupulos', 'sociais', ',', 'empertigando-se', 'quanto', 'podia', 'e', 'disfarcando', 'a', 'sua', 'inveja', 'pelo', 'vizinho', 'com', 'um', 'desdenhoso', 'ar', 'de', 'superioridade', 'condescendente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao passar-lhe todos os dias pela venda, cumprimentava-o com proteção, sorrindo sem rir \n",
      "e fechando logo a cara em seguida, muito sério'\n",
      "Tokens gerados: ['ao', 'passar-lhe', 'todos', 'os', 'dias', 'pela', 'venda', ',', 'cumprimentava-o', 'com', 'protecao', ',', 'sorrindo', 'sem', 'rir', 'e', 'fechando', 'logo', 'a', 'cara', 'em', 'seguida', ',', 'muito', 'serio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dados os primeiros passos para a compra do titulo abriu a casa e deu festas'\n",
      "Tokens gerados: ['dados', 'os', 'primeiros', 'passos', 'para', 'a', 'compra', 'do', 'titulo', 'abriu', 'a', 'casa', 'e', 'deu', 'festas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mulher, posto que lhe apontassem \n",
      "já os cabelos brancos, rejubilou com isso'\n",
      "Tokens gerados: ['a', 'mulher', ',', 'posto', 'que', 'lhe', 'apontassem', 'ja', 'os', 'cabelos', 'brancos', ',', 'rejubilou', 'com', 'isso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Zulmira tinha então doze para treze anos e era o tipo acabado da fluminense; pálida, magrinha, com pequeninas \n",
      "manchas roxas nas mucosas do nariz, das pálpebras e dos lábios, faces levemente pintalgadas de sardas'\n",
      "Tokens gerados: ['zulmira', 'tinha', 'entao', 'doze', 'para', 'treze', 'anos', 'e', 'era', 'o', 'tipo', 'acabado', 'da', 'fluminense', 'palida', ',', 'magrinha', ',', 'com', 'pequeninas', 'manchas', 'roxas', 'nas', 'mucosas', 'do', 'nariz', ',', 'das', 'palpebras', 'e', 'dos', 'labios', ',', 'faces', 'levemente', 'pintalgadas', 'de', 'sardas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Respirava o \n",
      "tom úmido das flores noturnas, uma brancura fria de magnólia; cabelos castanho-claros, mãos quase transparentes, \n",
      "unhas moles e curtas, como as da mãe, dentes pouco mais claros do que a cútis do rosto, pés pequeninos, quadril estreito \n",
      "mas os olhos grandes, negros, vivos e maliciosos'\n",
      "Tokens gerados: ['respirava', 'o', 'tom', 'umido', 'das', 'flores', 'noturnas', ',', 'uma', 'brancura', 'fria', 'de', 'magnolia', 'cabelos', 'castanho-claros', ',', 'maos', 'quase', 'transparentes', ',', 'unhas', 'moles', 'e', 'curtas', ',', 'como', 'as', 'da', 'mae', ',', 'dentes', 'pouco', 'mais', 'claros', 'do', 'que', 'a', 'cutis', 'do', 'rosto', ',', 'pes', 'pequeninos', ',', 'quadril', 'estreito', 'mas', 'os', 'olhos', 'grandes', ',', 'negros', ',', 'vivos', 'e', 'maliciosos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por essa época, justamente, chegava de Minas, recomendado ao pai dela, o filho de um fazendeiro \n",
      "importantíssimo que dava belos lucros à casa comercial de Miranda e que era talvez o melhor freguês que este possuía \n",
      "no interior'\n",
      "Tokens gerados: ['por', 'essa', 'epoca', ',', 'justamente', ',', 'chegava', 'de', 'minas', ',', 'recomendado', 'ao', 'pai', 'dela', ',', 'o', 'filho', 'de', 'um', 'fazendeiro', 'importantissimo', 'que', 'dava', 'belos', 'lucros', 'a', 'casa', 'comercial', 'de', 'miranda', 'e', 'que', 'era', 'talvez', 'o', 'melhor', 'fregues', 'que', 'este', 'possuia', 'no', 'interior']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O rapaz chamava-se Henrique, tinha quinze anos e vinha terminar na corte alguns preparatórios que lhe faltavam \n",
      "para entrar na Academia de Medicina'\n",
      "Tokens gerados: ['o', 'rapaz', 'chamava-se', 'henrique', ',', 'tinha', 'quinze', 'anos', 'e', 'vinha', 'terminar', 'na', 'corte', 'alguns', 'preparatorios', 'que', 'lhe', 'faltavam', 'para', 'entrar', 'na', 'academia', 'de', 'medicina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Miranda hospedou-o no seu sobrado da Rua do Hospício mas o estudante \n",
      "queixou-se, no fim de alguns dias, de que ai ficava mal acomodado, e o negociante, a quem não convinha \n",
      "desagradar-lhe, carregou com ele para a sua residência particular de Botafogo'\n",
      "Tokens gerados: ['miranda', 'hospedou-o', 'no', 'seu', 'sobrado', 'da', 'rua', 'do', 'hospicio', 'mas', 'o', 'estudante', 'queixou-se', ',', 'no', 'fim', 'de', 'alguns', 'dias', ',', 'de', 'que', 'ai', 'ficava', 'mal', 'acomodado', ',', 'e', 'o', 'negociante', ',', 'a', 'quem', 'nao', 'convinha', 'desagradar-lhe', ',', 'carregou', 'com', 'ele', 'para', 'a', 'sua', 'residencia', 'particular', 'de', 'botafogo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Henrique era bonitinho, cheio de acanhamentos, com umas delicadezas de menina'\n",
      "Tokens gerados: ['henrique', 'era', 'bonitinho', ',', 'cheio', 'de', 'acanhamentos', ',', 'com', 'umas', 'delicadezas', 'de', 'menina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Parecia muito cuidadoso dos \n",
      "seus estudos e tão pouco extravagante e gastador, que não despendia um vintém fora das necessidade de primeira \n",
      "urgência'\n",
      "Tokens gerados: ['parecia', 'muito', 'cuidadoso', 'dos', 'seus', 'estudos', 'e', 'tao', 'pouco', 'extravagante', 'e', 'gastador', ',', 'que', 'nao', 'despendia', 'um', 'vintem', 'fora', 'das', 'necessidade', 'de', 'primeira', 'urgencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De resto, a não ser de manhã para as aulas, que ia sempre com o Miranda, não arredava pé de casa senão em \n",
      "companhia da família, deste'\n",
      "Tokens gerados: ['de', 'resto', ',', 'a', 'nao', 'ser', 'de', 'manha', 'para', 'as', 'aulas', ',', 'que', 'ia', 'sempre', 'com', 'o', 'miranda', ',', 'nao', 'arredava', 'pe', 'de', 'casa', 'senao', 'em', 'companhia', 'da', 'familia', ',', 'deste']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dona Estela, no cabo de pouco tempo, mostrou por ele estima quase maternal e \n",
      "encarregou-se de tomar conta da sua mesada, mesada posta pelo negociante, visto que o Henriquinho tinha ordem \n",
      "franca do pai'\n",
      "Tokens gerados: ['dona', 'estela', ',', 'no', 'cabo', 'de', 'pouco', 'tempo', ',', 'mostrou', 'por', 'ele', 'estima', 'quase', 'maternal', 'e', 'encarregou-se', 'de', 'tomar', 'conta', 'da', 'sua', 'mesada', ',', 'mesada', 'posta', 'pelo', 'negociante', ',', 'visto', 'que', 'o', 'henriquinho', 'tinha', 'ordem', 'franca', 'do', 'pai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nunca pedia dinheiro; quando precisava de qualquer coisa, reclamava-a de Dona Estela, que por sua vez \n",
      "encarregava o marido de comprá-la, sendo o objeto lançado na conta do fazendeiro com uma comissão de usurário'\n",
      "Tokens gerados: ['nunca', 'pedia', 'dinheiro', 'quando', 'precisava', 'de', 'qualquer', 'coisa', ',', 'reclamava-a', 'de', 'dona', 'estela', ',', 'que', 'por', 'sua', 'vez', 'encarregava', 'o', 'marido', 'de', 'compra-la', ',', 'sendo', 'o', 'objeto', 'lancado', 'na', 'conta', 'do', 'fazendeiro', 'com', 'uma', 'comissao', 'de', 'usurario']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sua \n",
      "hospedagem custava duzentos e cinqüenta mil-réis por mês, do que ele todavia não tinha conhecimento, nem queria ter'\n",
      "Tokens gerados: ['sua', 'hospedagem', 'custava', 'duzentos', 'e', 'cinquenta', 'mil-reis', 'por', 'mes', ',', 'do', 'que', 'ele', 'todavia', 'nao', 'tinha', 'conhecimento', ',', 'nem', 'queria', 'ter']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nada lhe faltava, e os criados da casa o respeitavam como a um filho do próprio senhor'\n",
      "Tokens gerados: ['nada', 'lhe', 'faltava', ',', 'e', 'os', 'criados', 'da', 'casa', 'o', 'respeitavam', 'como', 'a', 'um', 'filho', 'do', 'proprio', 'senhor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'À noite, às vezes, quando o tempo estava bom, Dona Estela saia com ele, a filha e um moleque, o Valentim, a \n",
      "darem uma volta ate à praia e, em tendo convite para qualquer festa em casa das amigas, levava-o em sua companhia'\n",
      "Tokens gerados: ['a', 'noite', ',', 'as', 'vezes', ',', 'quando', 'o', 'tempo', 'estava', 'bom', ',', 'dona', 'estela', 'saia', 'com', 'ele', ',', 'a', 'filha', 'e', 'um', 'moleque', ',', 'o', 'valentim', ',', 'a', 'darem', 'uma', 'volta', 'ate', 'a', 'praia', 'e', ',', 'em', 'tendo', 'convite', 'para', 'qualquer', 'festa', 'em', 'casa', 'das', 'amigas', ',', 'levava-o', 'em', 'sua', 'companhia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A criadagem da família, do Miranda compunha-se de Isaura, mulata ainda moça, moleirona e tola, que gastava \n",
      "todo o vintenzinho que pilhava em comprar capilé na venda de João Romão; uma negrinha virgem, chamada Leonor, \n",
      "muito ligeira e viva, lisa e seca como um moleque, conhecendo de orelha, sem lhe faltar um termo, a vasta tecnologia da \n",
      "obscenidade, e dizendo, sempre que os caixeiros ou os fregueses da taverna, só para mexer com ela, lhe davam \n",
      "atracações: “Óia, que eu me queixo ao juiz de orfe!”, e finalmente o tal Valentim, filho de uma escrava que foi de Dona \n",
      "Estela e a quem esta havia alforriado'\n",
      "Tokens gerados: ['a', 'criadagem', 'da', 'familia', ',', 'do', 'miranda', 'compunha-se', 'de', 'isaura', ',', 'mulata', 'ainda', 'moca', ',', 'moleirona', 'e', 'tola', ',', 'que', 'gastava', 'todo', 'o', 'vintenzinho', 'que', 'pilhava', 'em', 'comprar', 'capile', 'na', 'venda', 'de', 'joao', 'romao', 'uma', 'negrinha', 'virgem', ',', 'chamada', 'leonor', ',', 'muito', 'ligeira', 'e', 'viva', ',', 'lisa', 'e', 'seca', 'como', 'um', 'moleque', ',', 'conhecendo', 'de', 'orelha', ',', 'sem', 'lhe', 'faltar', 'um', 'termo', ',', 'a', 'vasta', 'tecnologia', 'da', 'obscenidade', ',', 'e', 'dizendo', ',', 'sempre', 'que', 'os', 'caixeiros', 'ou', 'os', 'fregueses', 'da', 'taverna', ',', 'so', 'para', 'mexer', 'com', 'ela', ',', 'lhe', 'davam', 'atracacoes', '“', 'oia', ',', 'que', 'eu', 'me', 'queixo', 'ao', 'juiz', 'de', 'orfe', '”', ',', 'e', 'finalmente', 'o', 'tal', 'valentim', ',', 'filho', 'de', 'uma', 'escrava', 'que', 'foi', 'de', 'dona', 'estela', 'e', 'a', 'quem', 'esta', 'havia', 'alforriado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mulher do Miranda tinha por este moleque uma afeição sem limites: dava-lhe toda a liberdade, dinheiro, \n",
      "presentes, levava-o consigo a passeio, trazia-o bem vestido e muita vez chegou a fazer ciúmes à filha, de tão solicita que \n",
      "se mostrava com ele'\n",
      "Tokens gerados: ['a', 'mulher', 'do', 'miranda', 'tinha', 'por', 'este', 'moleque', 'uma', 'afeicao', 'sem', 'limites', 'dava-lhe', 'toda', 'a', 'liberdade', ',', 'dinheiro', ',', 'presentes', ',', 'levava-o', 'consigo', 'a', 'passeio', ',', 'trazia-o', 'bem', 'vestido', 'e', 'muita', 'vez', 'chegou', 'a', 'fazer', 'ciumes', 'a', 'filha', ',', 'de', 'tao', 'solicita', 'que', 'se', 'mostrava', 'com', 'ele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois se a caprichosa senhora ralhava com Zulmira por causa do negrinho! Pois, se quando se \n",
      "queixavam os dois, um contra o outro, ela nunca dava razão à filha! Pois se o que havia de melhor na casa era para o \n",
      "Valentim! Pois, se quando foi este atacado de bexigas e o Miranda, apesar das súplicas e dos protestos da esposa, \n",
      "mandou-o para um hospital, Dona Estela chorava todos os dias e durante a ausência dele não tocou piano, nem cantou, \n",
      "nem mostrou os dentes a ninguém? E o pobre Miranda, se não queria sofrer impertinências da mulher e ouvir \n",
      "sensaborias defronte dos criados, tinha de dar ao moleque toda a consideração e fazer-lhe humildemente todas as \n",
      "vontades'\n",
      "Tokens gerados: ['pois', 'se', 'a', 'caprichosa', 'senhora', 'ralhava', 'com', 'zulmira', 'por', 'causa', 'do', 'negrinho', 'pois', ',', 'se', 'quando', 'se', 'queixavam', 'os', 'dois', ',', 'um', 'contra', 'o', 'outro', ',', 'ela', 'nunca', 'dava', 'razao', 'a', 'filha', 'pois', 'se', 'o', 'que', 'havia', 'de', 'melhor', 'na', 'casa', 'era', 'para', 'o', 'valentim', 'pois', ',', 'se', 'quando', 'foi', 'este', 'atacado', 'de', 'bexigas', 'e', 'o', 'miranda', ',', 'apesar', 'das', 'suplicas', 'e', 'dos', 'protestos', 'da', 'esposa', ',', 'mandou-o', 'para', 'um', 'hospital', ',', 'dona', 'estela', 'chorava', 'todos', 'os', 'dias', 'e', 'durante', 'a', 'ausencia', 'dele', 'nao', 'tocou', 'piano', ',', 'nem', 'cantou', ',', 'nem', 'mostrou', 'os', 'dentes', 'a', 'ninguem', '?', 'e', 'o', 'pobre', 'miranda', ',', 'se', 'nao', 'queria', 'sofrer', 'impertinencias', 'da', 'mulher', 'e', 'ouvir', 'sensaborias', 'defronte', 'dos', 'criados', ',', 'tinha', 'de', 'dar', 'ao', 'moleque', 'toda', 'a', 'consideracao', 'e', 'fazer-lhe', 'humildemente', 'todas', 'as', 'vontades']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia ainda, sob as telhas do negociante, um outro hóspede além do Henrique, o velho Botelho'\n",
      "Tokens gerados: ['havia', 'ainda', ',', 'sob', 'as', 'telhas', 'do', 'negociante', ',', 'um', 'outro', 'hospede', 'alem', 'do', 'henrique', ',', 'o', 'velho', 'botelho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Este, porém, na \n",
      "qualidade de parasita'\n",
      "Tokens gerados: ['este', ',', 'porem', ',', 'na', 'qualidade', 'de', 'parasita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era um pobre-diabo caminhando para os setenta anos, antipático, cabelo branco, curto e duro, como escova, barba \n",
      "e bigode do mesmo teor; muito macilento, com uns óculos redondos que lhe aumentavam o tamanho da pupila e \n",
      "davam-lhe à cara uma expressão de abutre, perfeitamente de acordo com o seu nariz adunco e com a sua boca sem \n",
      "lábios: viam-se-lhe ainda todos os dentes, mas, tão gastos, que pareciam limados até ao meio'\n",
      "Tokens gerados: ['era', 'um', 'pobre-diabo', 'caminhando', 'para', 'os', 'setenta', 'anos', ',', 'antipatico', ',', 'cabelo', 'branco', ',', 'curto', 'e', 'duro', ',', 'como', 'escova', ',', 'barba', 'e', 'bigode', 'do', 'mesmo', 'teor', 'muito', 'macilento', ',', 'com', 'uns', 'oculos', 'redondos', 'que', 'lhe', 'aumentavam', 'o', 'tamanho', 'da', 'pupila', 'e', 'davam-lhe', 'a', 'cara', 'uma', 'expressao', 'de', 'abutre', ',', 'perfeitamente', 'de', 'acordo', 'com', 'o', 'seu', 'nariz', 'adunco', 'e', 'com', 'a', 'sua', 'boca', 'sem', 'labios', 'viam-se-lhe', 'ainda', 'todos', 'os', 'dentes', ',', 'mas', ',', 'tao', 'gastos', ',', 'que', 'pareciam', 'limados', 'ate', 'ao', 'meio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Andava sempre de preto, \n",
      "com um guarda-chuva debaixo do braço e um chapéu de Braga enterrado nas orelhas'\n",
      "Tokens gerados: ['andava', 'sempre', 'de', 'preto', ',', 'com', 'um', 'guarda-chuva', 'debaixo', 'do', 'braco', 'e', 'um', 'chapeu', 'de', 'braga', 'enterrado', 'nas', 'orelhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fora em seu tempo empregado do \n",
      "comércio, depois corretor de escravos; contava mesmo que estivera mais de uma vez na África negociando negros por \n",
      "sua conta'\n",
      "Tokens gerados: ['fora', 'em', 'seu', 'tempo', 'empregado', 'do', 'comercio', ',', 'depois', 'corretor', 'de', 'escravos', 'contava', 'mesmo', 'que', 'estivera', 'mais', 'de', 'uma', 'vez', 'na', 'africa', 'negociando', 'negros', 'por', 'sua', 'conta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Atirou-se muito às especulações; durante a guerra do Paraguai ainda ganhara forte, chegando a ser bem rico; \n",
      "mas a roda desandou e, de malogro em malogro, foi-lhe escapando tudo por entre as suas garras de ave de rapina'\n",
      "Tokens gerados: ['atirou-se', 'muito', 'as', 'especulacoes', 'durante', 'a', 'guerra', 'do', 'paraguai', 'ainda', 'ganhara', 'forte', ',', 'chegando', 'a', 'ser', 'bem', 'rico', 'mas', 'a', 'roda', 'desandou', 'e', ',', 'de', 'malogro', 'em', 'malogro', ',', 'foi-lhe', 'escapando', 'tudo', 'por', 'entre', 'as', 'suas', 'garras', 'de', 'ave', 'de', 'rapina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E \n",
      "agora, coitado, já velho, comido de desilusões, cheio de hemorróidas, via-se totalmente sem recursos e vegetava à \n",
      "sombra do Mirada, com quem por muitos anos trabalhou em rapaz, sob as ordens do mesmo patrão, e de quem se \n",
      "conservara amigo, a principio por acaso e mais tarde por necessidade'\n",
      "Tokens gerados: ['e', 'agora', ',', 'coitado', ',', 'ja', 'velho', ',', 'comido', 'de', 'desilusoes', ',', 'cheio', 'de', 'hemorroidas', ',', 'via-se', 'totalmente', 'sem', 'recursos', 'e', 'vegetava', 'a', 'sombra', 'do', 'mirada', ',', 'com', 'quem', 'por', 'muitos', 'anos', 'trabalhou', 'em', 'rapaz', ',', 'sob', 'as', 'ordens', 'do', 'mesmo', 'patrao', ',', 'e', 'de', 'quem', 'se', 'conservara', 'amigo', ',', 'a', 'principio', 'por', 'acaso', 'e', 'mais', 'tarde', 'por', 'necessidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Devorava-o, noite e dia, uma implacável amargura, uma surda tristeza de vencido, um desespero impotente, \n",
      "contra tudo e contra todos, por não lhe ter sido possível empolgar o mundo com as suas mãos hoje inúteis e trêmulas'\n",
      "Tokens gerados: ['devorava-o', ',', 'noite', 'e', 'dia', ',', 'uma', 'implacavel', 'amargura', ',', 'uma', 'surda', 'tristeza', 'de', 'vencido', ',', 'um', 'desespero', 'impotente', ',', 'contra', 'tudo', 'e', 'contra', 'todos', ',', 'por', 'nao', 'lhe', 'ter', 'sido', 'possivel', 'empolgar', 'o', 'mundo', 'com', 'as', 'suas', 'maos', 'hoje', 'inuteis', 'e', 'tremulas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, \n",
      "como o seu atual estado de miséria não lhe permitia abrir contra ninguém o bico, desabafava vituperando as idéias da \n",
      "época'\n",
      "Tokens gerados: ['e', ',', 'como', 'o', 'seu', 'atual', 'estado', 'de', 'miseria', 'nao', 'lhe', 'permitia', 'abrir', 'contra', 'ninguem', 'o', 'bico', ',', 'desabafava', 'vituperando', 'as', 'ideias', 'da', 'epoca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Assim, eram às vezes muito quentes as sobremesas do Miranda, quando, entre outros assuntos palpitantes, vinha à \n",
      "discussão o movimento abolicionista que principiava a formar-se em torno da lei Rio Branco'\n",
      "Tokens gerados: ['assim', ',', 'eram', 'as', 'vezes', 'muito', 'quentes', 'as', 'sobremesas', 'do', 'miranda', ',', 'quando', ',', 'entre', 'outros', 'assuntos', 'palpitantes', ',', 'vinha', 'a', 'discussao', 'o', 'movimento', 'abolicionista', 'que', 'principiava', 'a', 'formar-se', 'em', 'torno', 'da', 'lei', 'rio', 'branco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Então o Botelho ficava \n",
      "possesso e vomitava frases terríveis, para a direita e para a esquerda, como quem dispara tiros sem fazer alvo, e \n",
      "vociferava imprecações, aproveitando aquela válvula para desafogar o velho ódio acumulado dentro dele'\n",
      "Tokens gerados: ['entao', 'o', 'botelho', 'ficava', 'possesso', 'e', 'vomitava', 'frases', 'terriveis', ',', 'para', 'a', 'direita', 'e', 'para', 'a', 'esquerda', ',', 'como', 'quem', 'dispara', 'tiros', 'sem', 'fazer', 'alvo', ',', 'e', 'vociferava', 'imprecacoes', ',', 'aproveitando', 'aquela', 'valvula', 'para', 'desafogar', 'o', 'velho', 'odio', 'acumulado', 'dentro', 'dele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Bandidos! berrava apoplético'\n",
      "Tokens gerados: ['—', 'bandidos', 'berrava', 'apopletico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cáfila de salteadores! \n",
      "E o seu rancor irradiava-lhe dos olhos em setas envenenadas, procurando cravar-se em todas as brancuras e em \n",
      "todas as claridades'\n",
      "Tokens gerados: ['cafila', 'de', 'salteadores', 'e', 'o', 'seu', 'rancor', 'irradiava-lhe', 'dos', 'olhos', 'em', 'setas', 'envenenadas', ',', 'procurando', 'cravar-se', 'em', 'todas', 'as', 'brancuras', 'e', 'em', 'todas', 'as', 'claridades']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A virtude, a beleza, o talento, a mocidade, a força, a saúde, e principalmente a fortuna, eis o que ele \n",
      "não perdoava a ninguém, amaldiçoando todo aquele que conseguia o que ele não obtivera; que gozava o que ele não \n",
      "desfrutara; que sabia o que ele não aprendera'\n",
      "Tokens gerados: ['a', 'virtude', ',', 'a', 'beleza', ',', 'o', 'talento', ',', 'a', 'mocidade', ',', 'a', 'forca', ',', 'a', 'saude', ',', 'e', 'principalmente', 'a', 'fortuna', ',', 'eis', 'o', 'que', 'ele', 'nao', 'perdoava', 'a', 'ninguem', ',', 'amaldicoando', 'todo', 'aquele', 'que', 'conseguia', 'o', 'que', 'ele', 'nao', 'obtivera', 'que', 'gozava', 'o', 'que', 'ele', 'nao', 'desfrutara', 'que', 'sabia', 'o', 'que', 'ele', 'nao', 'aprendera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, para individualizar o objeto do seu ódio, voltava-se contra o Brasil, \n",
      "essa terra que, na sua opinião, só tinha uma serventia: enriquecer os portugueses, e que, no entanto, o deixara, a ele, na \n",
      "penúria'\n",
      "Tokens gerados: ['e', ',', 'para', 'individualizar', 'o', 'objeto', 'do', 'seu', 'odio', ',', 'voltava-se', 'contra', 'o', 'brasil', ',', 'essa', 'terra', 'que', ',', 'na', 'sua', 'opiniao', ',', 'so', 'tinha', 'uma', 'serventia', 'enriquecer', 'os', 'portugueses', ',', 'e', 'que', ',', 'no', 'entanto', ',', 'o', 'deixara', ',', 'a', 'ele', ',', 'na', 'penuria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seus dias eram consumidos do seguinte modo: acordava às oito da manhã, lavava-se mesmo no quarto com uma \n",
      "toalha molhada em espírito de vinho; depois ia ler os jornais para a sala de jantar, à espera do almoço; almoçava e sala, \n",
      "tomava o bonde e ia direitinho para uma charutaria da Rua do Ouvidor, onde costumava ficar assentado até às horas do \n",
      "jantar, entretido a dizer mal das pessoas que passavam lá fora, defronte dele'\n",
      "Tokens gerados: ['seus', 'dias', 'eram', 'consumidos', 'do', 'seguinte', 'modo', 'acordava', 'as', 'oito', 'da', 'manha', ',', 'lavava-se', 'mesmo', 'no', 'quarto', 'com', 'uma', 'toalha', 'molhada', 'em', 'espirito', 'de', 'vinho', 'depois', 'ia', 'ler', 'os', 'jornais', 'para', 'a', 'sala', 'de', 'jantar', ',', 'a', 'espera', 'do', 'almoco', 'almocava', 'e', 'sala', ',', 'tomava', 'o', 'bonde', 'e', 'ia', 'direitinho', 'para', 'uma', 'charutaria', 'da', 'rua', 'do', 'ouvidor', ',', 'onde', 'costumava', 'ficar', 'assentado', 'ate', 'as', 'horas', 'do', 'jantar', ',', 'entretido', 'a', 'dizer', 'mal', 'das', 'pessoas', 'que', 'passavam', 'la', 'fora', ',', 'defronte', 'dele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha a pretensão de conhecer todo o Rio \n",
      "de Janeiro e os podres de cada um em particular'\n",
      "Tokens gerados: ['tinha', 'a', 'pretensao', 'de', 'conhecer', 'todo', 'o', 'rio', 'de', 'janeiro', 'e', 'os', 'podres', 'de', 'cada', 'um', 'em', 'particular']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Às vezes, poucas, Dona Estela encarregava-o de fazer pequenas \n",
      "compras de armarinho, o que o Botelho desempenhava melhor que ninguém? Mas a sua grande paixão, o seu fraco, era \n",
      "a farda, adorava tudo que dissesse respeito a militarismo, posto que tivera sempre invencível medo às armas de qualquer \n",
      "espécie, mormente às de fogo'\n",
      "Tokens gerados: ['as', 'vezes', ',', 'poucas', ',', 'dona', 'estela', 'encarregava-o', 'de', 'fazer', 'pequenas', 'compras', 'de', 'armarinho', ',', 'o', 'que', 'o', 'botelho', 'desempenhava', 'melhor', 'que', 'ninguem', '?', 'mas', 'a', 'sua', 'grande', 'paixao', ',', 'o', 'seu', 'fraco', ',', 'era', 'a', 'farda', ',', 'adorava', 'tudo', 'que', 'dissesse', 'respeito', 'a', 'militarismo', ',', 'posto', 'que', 'tivera', 'sempre', 'invencivel', 'medo', 'as', 'armas', 'de', 'qualquer', 'especie', ',', 'mormente', 'as', 'de', 'fogo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não podia ouvir disparar perto de si uma espingarda, entusiasmava-se porém com tudo \n",
      "que cheirasse a guerra; a presença de um oficial em grande uniforme tirava-lhe lágrimas de comoção; conhecia na ponta \n",
      "da língua o que se referia à vida de quartel; distinguia ao primeiro lance de olhos o posto e o corpo a que pertencia \n",
      "qualquer soldado e, apesar dos seus achaques, era ouvir tocar na rua a corneta ou o tambor conduzindo o batalhão, \n",
      "ficava logo no ar, e, muita vez, quando dava por si, fazia parte dos que acompanhavam a tropa'\n",
      "Tokens gerados: ['nao', 'podia', 'ouvir', 'disparar', 'perto', 'de', 'si', 'uma', 'espingarda', ',', 'entusiasmava-se', 'porem', 'com', 'tudo', 'que', 'cheirasse', 'a', 'guerra', 'a', 'presenca', 'de', 'um', 'oficial', 'em', 'grande', 'uniforme', 'tirava-lhe', 'lagrimas', 'de', 'comocao', 'conhecia', 'na', 'ponta', 'da', 'lingua', 'o', 'que', 'se', 'referia', 'a', 'vida', 'de', 'quartel', 'distinguia', 'ao', 'primeiro', 'lance', 'de', 'olhos', 'o', 'posto', 'e', 'o', 'corpo', 'a', 'que', 'pertencia', 'qualquer', 'soldado', 'e', ',', 'apesar', 'dos', 'seus', 'achaques', ',', 'era', 'ouvir', 'tocar', 'na', 'rua', 'a', 'corneta', 'ou', 'o', 'tambor', 'conduzindo', 'o', 'batalhao', ',', 'ficava', 'logo', 'no', 'ar', ',', 'e', ',', 'muita', 'vez', ',', 'quando', 'dava', 'por', 'si', ',', 'fazia', 'parte', 'dos', 'que', 'acompanhavam', 'a', 'tropa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Então, não tornava para \n",
      "casa enquanto os militares neo se recolhessem'\n",
      "Tokens gerados: ['entao', ',', 'nao', 'tornava', 'para', 'casa', 'enquanto', 'os', 'militares', 'neo', 'se', 'recolhessem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quase sempre voltava dessa loucura às seis da tarde, moído a fazer dó, \n",
      "sem poder ter-se nas pernas, estrompado de marchar horas e horas ao som da música de pancadaria'\n",
      "Tokens gerados: ['quase', 'sempre', 'voltava', 'dessa', 'loucura', 'as', 'seis', 'da', 'tarde', ',', 'moido', 'a', 'fazer', 'do', ',', 'sem', 'poder', 'ter-se', 'nas', 'pernas', ',', 'estrompado', 'de', 'marchar', 'horas', 'e', 'horas', 'ao', 'som', 'da', 'musica', 'de', 'pancadaria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o mais \n",
      "interessante é que ele, ao vir-lhe a reação, revoltava-se furioso contra o maldito comandante que o obrigava àquela \n",
      "estopada, levando o batalhão por uma infinidade de ruas e fazendo de propósito o caminho mais longo'\n",
      "Tokens gerados: ['e', 'o', 'mais', 'interessante', 'e', 'que', 'ele', ',', 'ao', 'vir-lhe', 'a', 'reacao', ',', 'revoltava-se', 'furioso', 'contra', 'o', 'maldito', 'comandante', 'que', 'o', 'obrigava', 'aquela', 'estopada', ',', 'levando', 'o', 'batalhao', 'por', 'uma', 'infinidade', 'de', 'ruas', 'e', 'fazendo', 'de', 'proposito', 'o', 'caminho', 'mais', 'longo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Só parece, lamentava-se ele, que a intenção daquele malvado era dar-me cabo da pele! Ora vejam! Três horas \n",
      "de marche-marche por uma soalheira de todos os diabos! \n",
      "Uma das birras mais cômicas do Botelho era o seu ódio pelo Valentim'\n",
      "Tokens gerados: ['—', 'so', 'parece', ',', 'lamentava-se', 'ele', ',', 'que', 'a', 'intencao', 'daquele', 'malvado', 'era', 'dar-me', 'cabo', 'da', 'pele', 'ora', 'vejam', 'tres', 'horas', 'de', 'marche-marche', 'por', 'uma', 'soalheira', 'de', 'todos', 'os', 'diabos', 'uma', 'das', 'birras', 'mais', 'comicas', 'do', 'botelho', 'era', 'o', 'seu', 'odio', 'pelo', 'valentim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O moleque causava-lhe febre com as suas \n",
      "petulâncias de mimalho, e, velhaco, percebendo quanto elas o irritavam, ainda mais abusava, seguro na proteção de \n",
      "Dona Estela'\n",
      "Tokens gerados: ['o', 'moleque', 'causava-lhe', 'febre', 'com', 'as', 'suas', 'petulancias', 'de', 'mimalho', ',', 'e', ',', 'velhaco', ',', 'percebendo', 'quanto', 'elas', 'o', 'irritavam', ',', 'ainda', 'mais', 'abusava', ',', 'seguro', 'na', 'protecao', 'de', 'dona', 'estela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O parasita de muito que o teria estrangulado, se não fora a necessidade de agradar à dona da casa'\n",
      "Tokens gerados: ['o', 'parasita', 'de', 'muito', 'que', 'o', 'teria', 'estrangulado', ',', 'se', 'nao', 'fora', 'a', 'necessidade', 'de', 'agradar', 'a', 'dona', 'da', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Botelho conhecia as faltas de Estela como as palmas da própria mão'\n",
      "Tokens gerados: ['botelho', 'conhecia', 'as', 'faltas', 'de', 'estela', 'como', 'as', 'palmas', 'da', 'propria', 'mao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Miranda mesmo, que o via em conta de \n",
      "amigo fiel, muitas e muitas vezes lhas confiara em ocasiões desesperadas de desabafo, declarando francamente o quanto \n",
      "no intimo a desprezava e a razão por que não a punha na rua aos pontapés'\n",
      "Tokens gerados: ['o', 'miranda', 'mesmo', ',', 'que', 'o', 'via', 'em', 'conta', 'de', 'amigo', 'fiel', ',', 'muitas', 'e', 'muitas', 'vezes', 'lhas', 'confiara', 'em', 'ocasioes', 'desesperadas', 'de', 'desabafo', ',', 'declarando', 'francamente', 'o', 'quanto', 'no', 'intimo', 'a', 'desprezava', 'e', 'a', 'razao', 'por', 'que', 'nao', 'a', 'punha', 'na', 'rua', 'aos', 'pontapes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o Botelho dava-lhe toda a razão; entendia \n",
      "também que os sérios interesses comerciais estavam acima de tudo'\n",
      "Tokens gerados: ['e', 'o', 'botelho', 'dava-lhe', 'toda', 'a', 'razao', 'entendia', 'tambem', 'que', 'os', 'serios', 'interesses', 'comerciais', 'estavam', 'acima', 'de', 'tudo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Uma mulher naquelas condições, dizia ele convicto, representa nada menos que o capital, e um capital em caso \n",
      "nenhum a gente despreza! Agora, você o que devia era nunca chegar-se para ela'\n",
      "Tokens gerados: ['—', 'uma', 'mulher', 'naquelas', 'condicoes', ',', 'dizia', 'ele', 'convicto', ',', 'representa', 'nada', 'menos', 'que', 'o', 'capital', ',', 'e', 'um', 'capital', 'em', 'caso', 'nenhum', 'a', 'gente', 'despreza', 'agora', ',', 'voce', 'o', 'que', 'devia', 'era', 'nunca', 'chegar-se', 'para', 'ela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora! explicava o marido'\n",
      "Tokens gerados: ['—', 'ora', 'explicava', 'o', 'marido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eu me sirvo dela como quem se serve de uma escarradeira! \n",
      "O parasita, feliz por ver quanto o amigo aviltava a mulher, concordava em tudo plenamente, dando-lhe um \n",
      "carinhoso abraço de admiração'\n",
      "Tokens gerados: ['eu', 'me', 'sirvo', 'dela', 'como', 'quem', 'se', 'serve', 'de', 'uma', 'escarradeira', 'o', 'parasita', ',', 'feliz', 'por', 'ver', 'quanto', 'o', 'amigo', 'aviltava', 'a', 'mulher', ',', 'concordava', 'em', 'tudo', 'plenamente', ',', 'dando-lhe', 'um', 'carinhoso', 'abraco', 'de', 'admiracao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas por outro lado, quando ouvia Estela falar do marido, com infinito desdém e até \n",
      "com asco, ainda mais resplandecia de contente'\n",
      "Tokens gerados: ['mas', 'por', 'outro', 'lado', ',', 'quando', 'ouvia', 'estela', 'falar', 'do', 'marido', ',', 'com', 'infinito', 'desdem', 'e', 'ate', 'com', 'asco', ',', 'ainda', 'mais', 'resplandecia', 'de', 'contente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Você quer saber? afirmava ela, eu bem percebo quanto aquele traste do senhor meu marido me detesta, mas \n",
      "isso tanto se me dá como a primeira camisa que vesti! Desgraçadamente para nós, mulheres de sociedade, não podemos \n",
      "viver sem esposo, quando somos casadas; de forma que tenho de aturar o que me caiu em sorte, quer goste dele quer \n",
      "não goste! Juro-lhe, porém, que, se consinto que o Miranda se chegue às vezes para mim, é porque entendo que paga \n",
      "mais à pena ceder do que puxar discussão com uma besta daquela ordem! \n",
      "O Botelho, com a sua encanecida experiência do mundo, nunca transmitia a nenhum dos dois o que cada qual lhe \n",
      "dizia contra o outro; tanto assim que, certa ocasião, recolhendo-se à casa incomodado, em hora que não era do seu \n",
      "costume, ouviu, ao passar pelo quintal, sussurros de vozes abafadas que pareciam vir de um canto afogado de verdura, \n",
      "onde em geral não ia ninguém'\n",
      "Tokens gerados: ['—', 'voce', 'quer', 'saber', '?', 'afirmava', 'ela', ',', 'eu', 'bem', 'percebo', 'quanto', 'aquele', 'traste', 'do', 'senhor', 'meu', 'marido', 'me', 'detesta', ',', 'mas', 'isso', 'tanto', 'se', 'me', 'da', 'como', 'a', 'primeira', 'camisa', 'que', 'vesti', 'desgracadamente', 'para', 'nos', ',', 'mulheres', 'de', 'sociedade', ',', 'nao', 'podemos', 'viver', 'sem', 'esposo', ',', 'quando', 'somos', 'casadas', 'de', 'forma', 'que', 'tenho', 'de', 'aturar', 'o', 'que', 'me', 'caiu', 'em', 'sorte', ',', 'quer', 'goste', 'dele', 'quer', 'nao', 'goste', 'juro-lhe', ',', 'porem', ',', 'que', ',', 'se', 'consinto', 'que', 'o', 'miranda', 'se', 'chegue', 'as', 'vezes', 'para', 'mim', ',', 'e', 'porque', 'entendo', 'que', 'paga', 'mais', 'a', 'pena', 'ceder', 'do', 'que', 'puxar', 'discussao', 'com', 'uma', 'besta', 'daquela', 'ordem', 'o', 'botelho', ',', 'com', 'a', 'sua', 'encanecida', 'experiencia', 'do', 'mundo', ',', 'nunca', 'transmitia', 'a', 'nenhum', 'dos', 'dois', 'o', 'que', 'cada', 'qual', 'lhe', 'dizia', 'contra', 'o', 'outro', 'tanto', 'assim', 'que', ',', 'certa', 'ocasiao', ',', 'recolhendo-se', 'a', 'casa', 'incomodado', ',', 'em', 'hora', 'que', 'nao', 'era', 'do', 'seu', 'costume', ',', 'ouviu', ',', 'ao', 'passar', 'pelo', 'quintal', ',', 'sussurros', 'de', 'vozes', 'abafadas', 'que', 'pareciam', 'vir', 'de', 'um', 'canto', 'afogado', 'de', 'verdura', ',', 'onde', 'em', 'geral', 'nao', 'ia', 'ninguem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Encaminhou-se para lá em bicos de pés e, sem ser percebido, descobriu Estela entalada entre o muro e o \n",
      "Henrique'\n",
      "Tokens gerados: ['encaminhou-se', 'para', 'la', 'em', 'bicos', 'de', 'pes', 'e', ',', 'sem', 'ser', 'percebido', ',', 'descobriu', 'estela', 'entalada', 'entre', 'o', 'muro', 'e', 'o', 'henrique']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deixou-se ficar espiando, sem tugir nem mugir, e, só quando os dois se separaram, foi que ele se mostrou'\n",
      "Tokens gerados: ['deixou-se', 'ficar', 'espiando', ',', 'sem', 'tugir', 'nem', 'mugir', ',', 'e', ',', 'so', 'quando', 'os', 'dois', 'se', 'separaram', ',', 'foi', 'que', 'ele', 'se', 'mostrou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A senhora soltou um pequeno grito, e o rapaz, de vermelho que estava, fez-se cor de cera; mas o Botelho procurou \n",
      "tranqüilizá-los, dizendo em voz amiga e misteriosa: \n",
      "— Isso é uma imprudência o que vocês estão fazendo!'\n",
      "Tokens gerados: ['a', 'senhora', 'soltou', 'um', 'pequeno', 'grito', ',', 'e', 'o', 'rapaz', ',', 'de', 'vermelho', 'que', 'estava', ',', 'fez-se', 'cor', 'de', 'cera', 'mas', 'o', 'botelho', 'procurou', 'tranquiliza-los', ',', 'dizendo', 'em', 'voz', 'amiga', 'e', 'misteriosa', '—', 'isso', 'e', 'uma', 'imprudencia', 'o', 'que', 'voces', 'estao', 'fazendo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estas coisas não é deste modo que se arranjam! Assim \n",
      "como fui eu, podia ser outra pessoa'\n",
      "Tokens gerados: ['estas', 'coisas', 'nao', 'e', 'deste', 'modo', 'que', 'se', 'arranjam', 'assim', 'como', 'fui', 'eu', ',', 'podia', 'ser', 'outra', 'pessoa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois numa casa em que há tantos quartos, é lá preciso vir meterem-se neste canto \n",
      "do quintal?'\n",
      "Tokens gerados: ['pois', 'numa', 'casa', 'em', 'que', 'ha', 'tantos', 'quartos', ',', 'e', 'la', 'preciso', 'vir', 'meterem-se', 'neste', 'canto', 'do', 'quintal', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Nós não estávamos fazendo nada! disse Estela, recuperando o sangue-frio'\n",
      "Tokens gerados: ['—', 'nos', 'nao', 'estavamos', 'fazendo', 'nada', 'disse', 'estela', ',', 'recuperando', 'o', 'sangue-frio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah! tornou o velho, aparentando sumo respeito: então desculpe, pensei que estivessem'\n",
      "Tokens gerados: ['—', 'ah', 'tornou', 'o', 'velho', ',', 'aparentando', 'sumo', 'respeito', 'entao', 'desculpe', ',', 'pensei', 'que', 'estivessem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E olhe que, se assim \n",
      "fosse, para mim seria o mesmo, porque acho isso a coisa mais natural do mundo e entendo que desta vida a gente só \n",
      "leva o que come!'\n",
      "Tokens gerados: ['e', 'olhe', 'que', ',', 'se', 'assim', 'fosse', ',', 'para', 'mim', 'seria', 'o', 'mesmo', ',', 'porque', 'acho', 'isso', 'a', 'coisa', 'mais', 'natural', 'do', 'mundo', 'e', 'entendo', 'que', 'desta', 'vida', 'a', 'gente', 'so', 'leva', 'o', 'que', 'come']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se vi, creia, foi como se nada visse, porque nada tenho a cheirar com a vida de cada um!'\n",
      "Tokens gerados: ['se', 'vi', ',', 'creia', ',', 'foi', 'como', 'se', 'nada', 'visse', ',', 'porque', 'nada', 'tenho', 'a', 'cheirar', 'com', 'a', 'vida', 'de', 'cada', 'um']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A \n",
      "senhora está moça, está na força dos anos; seu marido não a satisfaz, é justo que o substitua por outro! Ah! isto é o \n",
      "mundo, e, se é torto, não fomos nós que o fizemos torto!'\n",
      "Tokens gerados: ['a', 'senhora', 'esta', 'moca', ',', 'esta', 'na', 'forca', 'dos', 'anos', 'seu', 'marido', 'nao', 'a', 'satisfaz', ',', 'e', 'justo', 'que', 'o', 'substitua', 'por', 'outro', 'ah', 'isto', 'e', 'o', 'mundo', ',', 'e', ',', 'se', 'e', 'torto', ',', 'nao', 'fomos', 'nos', 'que', 'o', 'fizemos', 'torto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Até certa idade todos temos dentro um bichinho-carpinteiro, \n",
      "que é preciso matar, antes que ele nos mate! Não lhes doam as mãos!'\n",
      "Tokens gerados: ['ate', 'certa', 'idade', 'todos', 'temos', 'dentro', 'um', 'bichinho-carpinteiro', ',', 'que', 'e', 'preciso', 'matar', ',', 'antes', 'que', 'ele', 'nos', 'mate', 'nao', 'lhes', 'doam', 'as', 'maos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'apenas acho que, para outra vez, devem ter um \n",
      "pouquinho mais de cuidado e'\n",
      "Tokens gerados: ['apenas', 'acho', 'que', ',', 'para', 'outra', 'vez', ',', 'devem', 'ter', 'um', 'pouquinho', 'mais', 'de', 'cuidado', 'e']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Está bom! basta! ordenou Estela'\n",
      "Tokens gerados: ['—', 'esta', 'bom', 'basta', 'ordenou', 'estela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Perdão! eu, se digo isto, é para deixá-los bem tranqüilos a meu respeito'\n",
      "Tokens gerados: ['—', 'perdao', 'eu', ',', 'se', 'digo', 'isto', ',', 'e', 'para', 'deixa-los', 'bem', 'tranquilos', 'a', 'meu', 'respeito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não quero, nem por sombra, que se \n",
      "persuadam de que'\n",
      "Tokens gerados: ['nao', 'quero', ',', 'nem', 'por', 'sombra', ',', 'que', 'se', 'persuadam', 'de', 'que']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Henrique atalhou, com a voz ainda comovida: \n",
      "— Mas, acredite, seu Botelho, que'\n",
      "Tokens gerados: ['o', 'henrique', 'atalhou', ',', 'com', 'a', 'voz', 'ainda', 'comovida', '—', 'mas', ',', 'acredite', ',', 'seu', 'botelho', ',', 'que']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O velho interrompeu-o também por sua vez, passando-lhe a mão no ombro e afastando-o consigo: \n",
      "— Não tenha receio, que não o comprometerei, menino! \n",
      "E, como já estivessem distantes de Estela, segredou-lhe em tom protetor: \n",
      "— Não torne a fazer isto assim, que você se estraga'\n",
      "Tokens gerados: ['o', 'velho', 'interrompeu-o', 'tambem', 'por', 'sua', 'vez', ',', 'passando-lhe', 'a', 'mao', 'no', 'ombro', 'e', 'afastando-o', 'consigo', '—', 'nao', 'tenha', 'receio', ',', 'que', 'nao', 'o', 'comprometerei', ',', 'menino', 'e', ',', 'como', 'ja', 'estivessem', 'distantes', 'de', 'estela', ',', 'segredou-lhe', 'em', 'tom', 'protetor', '—', 'nao', 'torne', 'a', 'fazer', 'isto', 'assim', ',', 'que', 'voce', 'se', 'estraga']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Olhe como lhe tremem as pernas! \n",
      "Dona Estela acompanhou-os a distancia, vagarosamente, afetando preocupação em compor um ramalhete, cujas \n",
      "flores ela ia colhendo com muita graça, ora toda vergada sobre as plantas rasteiras, ora pondo-se na pontinha dos pés \n",
      "para alcançar os heliotrópios e os manacás'\n",
      "Tokens gerados: ['olhe', 'como', 'lhe', 'tremem', 'as', 'pernas', 'dona', 'estela', 'acompanhou-os', 'a', 'distancia', ',', 'vagarosamente', ',', 'afetando', 'preocupacao', 'em', 'compor', 'um', 'ramalhete', ',', 'cujas', 'flores', 'ela', 'ia', 'colhendo', 'com', 'muita', 'graca', ',', 'ora', 'toda', 'vergada', 'sobre', 'as', 'plantas', 'rasteiras', ',', 'ora', 'pondo-se', 'na', 'pontinha', 'dos', 'pes', 'para', 'alcancar', 'os', 'heliotropios', 'e', 'os', 'manacas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Henrique seguiu o Botelho até ao quarto deste, conversando sem mudar de assunto'\n",
      "Tokens gerados: ['henrique', 'seguiu', 'o', 'botelho', 'ate', 'ao', 'quarto', 'deste', ',', 'conversando', 'sem', 'mudar', 'de', 'assunto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Você então não fala nisto, hein? Jura? perguntou-lhe'\n",
      "Tokens gerados: ['—', 'voce', 'entao', 'nao', 'fala', 'nisto', ',', 'hein', '?', 'jura', '?', 'perguntou-lhe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O velho tinha já declarado, a rir, que os pilhara em flagrante e que ficara bom tempo à espreita'\n",
      "Tokens gerados: ['o', 'velho', 'tinha', 'ja', 'declarado', ',', 'a', 'rir', ',', 'que', 'os', 'pilhara', 'em', 'flagrante', 'e', 'que', 'ficara', 'bom', 'tempo', 'a', 'espreita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Falar o quê, seu tolo?'\n",
      "Tokens gerados: ['—', 'falar', 'o', 'que', ',', 'seu', 'tolo', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois então quem pensa você que eu sou?'\n",
      "Tokens gerados: ['pois', 'entao', 'quem', 'pensa', 'voce', 'que', 'eu', 'sou', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Só abrirei o bico se você me der motivo \n",
      "para isso, mas estou convencido que não dará'\n",
      "Tokens gerados: ['so', 'abrirei', 'o', 'bico', 'se', 'voce', 'me', 'der', 'motivo', 'para', 'isso', ',', 'mas', 'estou', 'convencido', 'que', 'nao', 'dara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quer saber? eu até simpatizo muito com você, Henrique! Acho que \n",
      "você é um excelente menino, uma flor! E digo-lhe mais: hei de proteger os seus negócios com Dona Estela'\n",
      "Tokens gerados: ['quer', 'saber', '?', 'eu', 'ate', 'simpatizo', 'muito', 'com', 'voce', ',', 'henrique', 'acho', 'que', 'voce', 'e', 'um', 'excelente', 'menino', ',', 'uma', 'flor', 'e', 'digo-lhe', 'mais', 'hei', 'de', 'proteger', 'os', 'seus', 'negocios', 'com', 'dona', 'estela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Falando assim, tinha-lhe tomado as mãos e afagava-as'\n",
      "Tokens gerados: ['falando', 'assim', ',', 'tinha-lhe', 'tomado', 'as', 'maos', 'e', 'afagava-as']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Olhe, continuou, acariciando-o sempre; não se meta com donzelas, entende?'\n",
      "Tokens gerados: ['—', 'olhe', ',', 'continuou', ',', 'acariciando-o', 'sempre', 'nao', 'se', 'meta', 'com', 'donzelas', ',', 'entende', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'São o diabo! Por dá cá aquela \n",
      "palha fica um homem em apuros! agora quanto às outras, papo com elas! Não mande nenhuma ao vigário, nem lhe doa \n",
      "a cabeça, porque, no fim de contas, nas circunstâncias de Dona Estela, é até um grande serviço que você lhe faz! Meu \n",
      "rico amiguinho, quando uma mulher já passou dos trinta e pilha a jeito um rapazito da sua idade, é como se descobrisse \n",
      "ouro em pó! sabe-lhe a gaitas! Fique então sabendo de que não é só a ela que você faz o obséquio, mas também ao \n",
      "marido: quanto mais escovar-lhe você a mulher, melhor ela ficará de gênio, e por conseguinte melhor será para o pobre \n",
      "homem, coitado! que tem já bastante com que se aborrecer lá por baixo, com os seus negócios, e precisa de um pouco \n",
      "de descanso quando volta do serviço e mete-se em casa! Escove-a, escove-a! que a porá macia que nem veludo! O que é \n",
      "preciso é muito juizinho, percebe? Não faça outra criançada como a de hoje e continue para diante, não só com ela, mas \n",
      "com todas as que lhe caírem debaixo da asa! Vá passando! menos as de casa aberta, que isso é perigoso por causa das \n",
      "moléstias; nem tampouco donzelas! Não se meta com a Zulmira! E creia que lhe falo assim, porque sou seu amigo, \n",
      "porque o acho simpático, porque o acho bonito! \n",
      "E acarinhou-o tão vivamente dessa vez, que o estudante, fugindo-lhe das mãos, afastou-se com um gesto de \n",
      "repugnância e desprezo, enquanto o velho lhe dizia em voz comprimida: \n",
      "— Olha! Espera! Vem cá! Você é desconfiado!'\n",
      "Tokens gerados: ['sao', 'o', 'diabo', 'por', 'da', 'ca', 'aquela', 'palha', 'fica', 'um', 'homem', 'em', 'apuros', 'agora', 'quanto', 'as', 'outras', ',', 'papo', 'com', 'elas', 'nao', 'mande', 'nenhuma', 'ao', 'vigario', ',', 'nem', 'lhe', 'doa', 'a', 'cabeca', ',', 'porque', ',', 'no', 'fim', 'de', 'contas', ',', 'nas', 'circunstancias', 'de', 'dona', 'estela', ',', 'e', 'ate', 'um', 'grande', 'servico', 'que', 'voce', 'lhe', 'faz', 'meu', 'rico', 'amiguinho', ',', 'quando', 'uma', 'mulher', 'ja', 'passou', 'dos', 'trinta', 'e', 'pilha', 'a', 'jeito', 'um', 'rapazito', 'da', 'sua', 'idade', ',', 'e', 'como', 'se', 'descobrisse', 'ouro', 'em', 'po', 'sabe-lhe', 'a', 'gaitas', 'fique', 'entao', 'sabendo', 'de', 'que', 'nao', 'e', 'so', 'a', 'ela', 'que', 'voce', 'faz', 'o', 'obsequio', ',', 'mas', 'tambem', 'ao', 'marido', 'quanto', 'mais', 'escovar-lhe', 'voce', 'a', 'mulher', ',', 'melhor', 'ela', 'ficara', 'de', 'genio', ',', 'e', 'por', 'conseguinte', 'melhor', 'sera', 'para', 'o', 'pobre', 'homem', ',', 'coitado', 'que', 'tem', 'ja', 'bastante', 'com', 'que', 'se', 'aborrecer', 'la', 'por', 'baixo', ',', 'com', 'os', 'seus', 'negocios', ',', 'e', 'precisa', 'de', 'um', 'pouco', 'de', 'descanso', 'quando', 'volta', 'do', 'servico', 'e', 'mete-se', 'em', 'casa', 'escove-a', ',', 'escove-a', 'que', 'a', 'pora', 'macia', 'que', 'nem', 'veludo', 'o', 'que', 'e', 'preciso', 'e', 'muito', 'juizinho', ',', 'percebe', '?', 'nao', 'faca', 'outra', 'criancada', 'como', 'a', 'de', 'hoje', 'e', 'continue', 'para', 'diante', ',', 'nao', 'so', 'com', 'ela', ',', 'mas', 'com', 'todas', 'as', 'que', 'lhe', 'cairem', 'debaixo', 'da', 'asa', 'va', 'passando', 'menos', 'as', 'de', 'casa', 'aberta', ',', 'que', 'isso', 'e', 'perigoso', 'por', 'causa', 'das', 'molestias', 'nem', 'tampouco', 'donzelas', 'nao', 'se', 'meta', 'com', 'a', 'zulmira', 'e', 'creia', 'que', 'lhe', 'falo', 'assim', ',', 'porque', 'sou', 'seu', 'amigo', ',', 'porque', 'o', 'acho', 'simpatico', ',', 'porque', 'o', 'acho', 'bonito', 'e', 'acarinhou-o', 'tao', 'vivamente', 'dessa', 'vez', ',', 'que', 'o', 'estudante', ',', 'fugindo-lhe', 'das', 'maos', ',', 'afastou-se', 'com', 'um', 'gesto', 'de', 'repugnancia', 'e', 'desprezo', ',', 'enquanto', 'o', 'velho', 'lhe', 'dizia', 'em', 'voz', 'comprimida', '—', 'olha', 'espera', 'vem', 'ca', 'voce', 'e', 'desconfiado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_o_cortico_aluisio_azevedo_cap_2.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Eram cinco horas da manhã e o cortiço acordava, abrindo, não os olhos, mas a sua infinidade de portas e janelas \n",
      "alinhadas'\n",
      "Tokens gerados: ['eram', 'cinco', 'horas', 'da', 'manha', 'e', 'o', 'cortico', 'acordava', ',', 'abrindo', ',', 'nao', 'os', 'olhos', ',', 'mas', 'a', 'sua', 'infinidade', 'de', 'portas', 'e', 'janelas', 'alinhadas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um acordar alegre e farto de quem dormiu de uma assentada sete horas de chumbo'\n",
      "Tokens gerados: ['um', 'acordar', 'alegre', 'e', 'farto', 'de', 'quem', 'dormiu', 'de', 'uma', 'assentada', 'sete', 'horas', 'de', 'chumbo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Como que se sentiam ainda \n",
      "na indolência de neblina as derradeiras notas da ultima guitarra da noite antecedente, dissolvendo-se à luz loura e tenra \n",
      "da aurora, que nem um suspiro de saudade perdido em terra alheia'\n",
      "Tokens gerados: ['como', 'que', 'se', 'sentiam', 'ainda', 'na', 'indolencia', 'de', 'neblina', 'as', 'derradeiras', 'notas', 'da', 'ultima', 'guitarra', 'da', 'noite', 'antecedente', ',', 'dissolvendo-se', 'a', 'luz', 'loura', 'e', 'tenra', 'da', 'aurora', ',', 'que', 'nem', 'um', 'suspiro', 'de', 'saudade', 'perdido', 'em', 'terra', 'alheia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A roupa lavada, que ficara de véspera nos coradouros, umedecia o ar e punha-lhe um farto acre de sabão \n",
      "ordinário'\n",
      "Tokens gerados: ['a', 'roupa', 'lavada', ',', 'que', 'ficara', 'de', 'vespera', 'nos', 'coradouros', ',', 'umedecia', 'o', 'ar', 'e', 'punha-lhe', 'um', 'farto', 'acre', 'de', 'sabao', 'ordinario']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As pedras do chão, esbranquiçadas no lugar da lavagem e em alguns pontos azuladas pelo anil, mostravam \n",
      "uma palidez grisalha e triste, feita de acumulações de espumas secas'\n",
      "Tokens gerados: ['as', 'pedras', 'do', 'chao', ',', 'esbranquicadas', 'no', 'lugar', 'da', 'lavagem', 'e', 'em', 'alguns', 'pontos', 'azuladas', 'pelo', 'anil', ',', 'mostravam', 'uma', 'palidez', 'grisalha', 'e', 'triste', ',', 'feita', 'de', 'acumulacoes', 'de', 'espumas', 'secas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, das portas surgiam cabeças congestionadas de sono; ouviam-se amplos bocejos, fortes como o \n",
      "marulhar das ondas; pigarreava-se grosso por toda a parte; começavam as xícaras a tilintar; o cheiro quente do café \n",
      "aquecia, suplantando todos os outros; trocavam-se de janela para janela as primeiras palavras, os bons-dias; reatavam-se \n",
      "conversas interrompidas à noite; a pequenada cá fora traquinava já, e lá dentro das casas vinham choros abafados de \n",
      "crianças que ainda não andam'\n",
      "Tokens gerados: ['entretanto', ',', 'das', 'portas', 'surgiam', 'cabecas', 'congestionadas', 'de', 'sono', 'ouviam-se', 'amplos', 'bocejos', ',', 'fortes', 'como', 'o', 'marulhar', 'das', 'ondas', 'pigarreava-se', 'grosso', 'por', 'toda', 'a', 'parte', 'comecavam', 'as', 'xicaras', 'a', 'tilintar', 'o', 'cheiro', 'quente', 'do', 'cafe', 'aquecia', ',', 'suplantando', 'todos', 'os', 'outros', 'trocavam-se', 'de', 'janela', 'para', 'janela', 'as', 'primeiras', 'palavras', ',', 'os', 'bons-dias', 'reatavam-se', 'conversas', 'interrompidas', 'a', 'noite', 'a', 'pequenada', 'ca', 'fora', 'traquinava', 'ja', ',', 'e', 'la', 'dentro', 'das', 'casas', 'vinham', 'choros', 'abafados', 'de', 'criancas', 'que', 'ainda', 'nao', 'andam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No confuso rumor que se formava, destacavam-se risos, sons de vozes que altercavam, \n",
      "sem se saber onde, grasnar de marrecos, cantar de galos, cacarejar de galinhas'\n",
      "Tokens gerados: ['no', 'confuso', 'rumor', 'que', 'se', 'formava', ',', 'destacavam-se', 'risos', ',', 'sons', 'de', 'vozes', 'que', 'altercavam', ',', 'sem', 'se', 'saber', 'onde', ',', 'grasnar', 'de', 'marrecos', ',', 'cantar', 'de', 'galos', ',', 'cacarejar', 'de', 'galinhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De alguns quartos saiam mulheres que \n",
      "vinham pendurar cá fora, na parede, a gaiola do papagaio, e os louros, à semelhança dos donos, cumprimentavam-se \n",
      "ruidosamente, espanejando-se à luz nova do dia'\n",
      "Tokens gerados: ['de', 'alguns', 'quartos', 'saiam', 'mulheres', 'que', 'vinham', 'pendurar', 'ca', 'fora', ',', 'na', 'parede', ',', 'a', 'gaiola', 'do', 'papagaio', ',', 'e', 'os', 'louros', ',', 'a', 'semelhanca', 'dos', 'donos', ',', 'cumprimentavam-se', 'ruidosamente', ',', 'espanejando-se', 'a', 'luz', 'nova', 'do', 'dia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Daí a pouco, em volta das bicas era um zunzum crescente; uma aglomeração tumultuosa de machos e fêmeas'\n",
      "Tokens gerados: ['dai', 'a', 'pouco', ',', 'em', 'volta', 'das', 'bicas', 'era', 'um', 'zunzum', 'crescente', 'uma', 'aglomeracao', 'tumultuosa', 'de', 'machos', 'e', 'femeas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uns, após outros, lavavam a cara, incomodamente, debaixo do fio de água que escorria da altura de uns cinco palmos'\n",
      "Tokens gerados: ['uns', ',', 'apos', 'outros', ',', 'lavavam', 'a', 'cara', ',', 'incomodamente', ',', 'debaixo', 'do', 'fio', 'de', 'agua', 'que', 'escorria', 'da', 'altura', 'de', 'uns', 'cinco', 'palmos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O \n",
      "chão inundava-se'\n",
      "Tokens gerados: ['o', 'chao', 'inundava-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As mulheres precisavam já prender as saias entre as coxas para não as molhar; via-se-lhes a tostada \n",
      "nudez dos braços e do pescoço, que elas despiam, suspendendo o cabelo todo para o alto do casco; os homens, esses não \n",
      "se preocupavam em não molhar o pêlo, ao contrário metiam a cabeça bem debaixo da água e esfregavam com força as \n",
      "ventas e as barbas, fossando e fungando contra as palmas da mão'\n",
      "Tokens gerados: ['as', 'mulheres', 'precisavam', 'ja', 'prender', 'as', 'saias', 'entre', 'as', 'coxas', 'para', 'nao', 'as', 'molhar', 'via-se-lhes', 'a', 'tostada', 'nudez', 'dos', 'bracos', 'e', 'do', 'pescoco', ',', 'que', 'elas', 'despiam', ',', 'suspendendo', 'o', 'cabelo', 'todo', 'para', 'o', 'alto', 'do', 'casco', 'os', 'homens', ',', 'esses', 'nao', 'se', 'preocupavam', 'em', 'nao', 'molhar', 'o', 'pelo', ',', 'ao', 'contrario', 'metiam', 'a', 'cabeca', 'bem', 'debaixo', 'da', 'agua', 'e', 'esfregavam', 'com', 'forca', 'as', 'ventas', 'e', 'as', 'barbas', ',', 'fossando', 'e', 'fungando', 'contra', 'as', 'palmas', 'da', 'mao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As portas das latrinas não descansavam, era um abrir \n",
      "e fechar de cada instante, um entrar e sair sem tréguas'\n",
      "Tokens gerados: ['as', 'portas', 'das', 'latrinas', 'nao', 'descansavam', ',', 'era', 'um', 'abrir', 'e', 'fechar', 'de', 'cada', 'instante', ',', 'um', 'entrar', 'e', 'sair', 'sem', 'treguas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não se demoravam lá dentro e vinham ainda amarrando as \n",
      "calças ou as saias; as crianças não se davam ao trabalho de lá ir, despachavam-se ali mesmo, no capinzal dos fundos, \n",
      "por detrás da estalagem ou no recanto das hortas'\n",
      "Tokens gerados: ['nao', 'se', 'demoravam', 'la', 'dentro', 'e', 'vinham', 'ainda', 'amarrando', 'as', 'calcas', 'ou', 'as', 'saias', 'as', 'criancas', 'nao', 'se', 'davam', 'ao', 'trabalho', 'de', 'la', 'ir', ',', 'despachavam-se', 'ali', 'mesmo', ',', 'no', 'capinzal', 'dos', 'fundos', ',', 'por', 'detras', 'da', 'estalagem', 'ou', 'no', 'recanto', 'das', 'hortas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O rumor crescia, condensando-se; o zunzum de todos os dias acentuava-se; já se não destacavam vozes dispersas, \n",
      "mas um só ruído compacto que enchia todo o cortiço'\n",
      "Tokens gerados: ['o', 'rumor', 'crescia', ',', 'condensando-se', 'o', 'zunzum', 'de', 'todos', 'os', 'dias', 'acentuava-se', 'ja', 'se', 'nao', 'destacavam', 'vozes', 'dispersas', ',', 'mas', 'um', 'so', 'ruido', 'compacto', 'que', 'enchia', 'todo', 'o', 'cortico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Começavam a fazer compras na venda; ensarilhavam-se \n",
      "discussões e resingas; ouviam-se gargalhadas e pragas; já se não falava, gritava-se'\n",
      "Tokens gerados: ['comecavam', 'a', 'fazer', 'compras', 'na', 'venda', 'ensarilhavam-se', 'discussoes', 'e', 'resingas', 'ouviam-se', 'gargalhadas', 'e', 'pragas', 'ja', 'se', 'nao', 'falava', ',', 'gritava-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sentia-se naquela fermentação \n",
      "sangüínea, naquela gula viçosa de plantas rasteiras que mergulham os pés vigorosos na lama preta e nutriente da vida, o \n",
      "prazer animal de existir, a triunfante satisfação de respirar sobre a terra'\n",
      "Tokens gerados: ['sentia-se', 'naquela', 'fermentacao', 'sanguinea', ',', 'naquela', 'gula', 'vicosa', 'de', 'plantas', 'rasteiras', 'que', 'mergulham', 'os', 'pes', 'vigorosos', 'na', 'lama', 'preta', 'e', 'nutriente', 'da', 'vida', ',', 'o', 'prazer', 'animal', 'de', 'existir', ',', 'a', 'triunfante', 'satisfacao', 'de', 'respirar', 'sobre', 'a', 'terra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Da porta da venda que dava para o cortiço iam e vinham como formigas; fazendo compras'\n",
      "Tokens gerados: ['da', 'porta', 'da', 'venda', 'que', 'dava', 'para', 'o', 'cortico', 'iam', 'e', 'vinham', 'como', 'formigas', 'fazendo', 'compras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Duas janelas do Miranda abriram-se'\n",
      "Tokens gerados: ['duas', 'janelas', 'do', 'miranda', 'abriram-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Apareceu numa a Isaura, que se dispunha a começar a limpeza da casa'\n",
      "Tokens gerados: ['apareceu', 'numa', 'a', 'isaura', ',', 'que', 'se', 'dispunha', 'a', 'comecar', 'a', 'limpeza', 'da', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Nhá Dunga! gritou ela para baixo, a sacudir um pano de mesa; se você tem cuscuz de milho hoje, bata na \n",
      "porta, ouviu? \n",
      "A Leonor surgiu logo também, enfiando curiosa a carapinha por entre o pescoço e o ombro da mulata'\n",
      "Tokens gerados: ['—', 'nha', 'dunga', 'gritou', 'ela', 'para', 'baixo', ',', 'a', 'sacudir', 'um', 'pano', 'de', 'mesa', 'se', 'voce', 'tem', 'cuscuz', 'de', 'milho', 'hoje', ',', 'bata', 'na', 'porta', ',', 'ouviu', '?', 'a', 'leonor', 'surgiu', 'logo', 'tambem', ',', 'enfiando', 'curiosa', 'a', 'carapinha', 'por', 'entre', 'o', 'pescoco', 'e', 'o', 'ombro', 'da', 'mulata']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O padeiro entrou na estalagem, com a sua grande cesta à cabeça e o seu banco de pau fechado debaixo do braço, e \n",
      "foi estacionar em meio do pátio, à espera dos fregueses, pousando a canastra sobre o cavalete que ele armou \n",
      "prontamente'\n",
      "Tokens gerados: ['o', 'padeiro', 'entrou', 'na', 'estalagem', ',', 'com', 'a', 'sua', 'grande', 'cesta', 'a', 'cabeca', 'e', 'o', 'seu', 'banco', 'de', 'pau', 'fechado', 'debaixo', 'do', 'braco', ',', 'e', 'foi', 'estacionar', 'em', 'meio', 'do', 'patio', ',', 'a', 'espera', 'dos', 'fregueses', ',', 'pousando', 'a', 'canastra', 'sobre', 'o', 'cavalete', 'que', 'ele', 'armou', 'prontamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em breve estava cercado por uma nuvem de gente'\n",
      "Tokens gerados: ['em', 'breve', 'estava', 'cercado', 'por', 'uma', 'nuvem', 'de', 'gente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As crianças adulavam-no, e, à proporção que cada \n",
      "mulher ou cada homem recebia o pão, disparava para casa com este abraçado contra o peito'\n",
      "Tokens gerados: ['as', 'criancas', 'adulavam-no', ',', 'e', ',', 'a', 'proporcao', 'que', 'cada', 'mulher', 'ou', 'cada', 'homem', 'recebia', 'o', 'pao', ',', 'disparava', 'para', 'casa', 'com', 'este', 'abracado', 'contra', 'o', 'peito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma vaca, seguida por um \n",
      "bezerro amordaçado, ia, tilintando tristemente o seu chocalho, de porta em porta, guiada por um homem carregado de \n",
      "vasilhame de folha'\n",
      "Tokens gerados: ['uma', 'vaca', ',', 'seguida', 'por', 'um', 'bezerro', 'amordacado', ',', 'ia', ',', 'tilintando', 'tristemente', 'o', 'seu', 'chocalho', ',', 'de', 'porta', 'em', 'porta', ',', 'guiada', 'por', 'um', 'homem', 'carregado', 'de', 'vasilhame', 'de', 'folha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O zunzum chegava ao seu apogeu'\n",
      "Tokens gerados: ['o', 'zunzum', 'chegava', 'ao', 'seu', 'apogeu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A fábrica de massas italianas, ali mesmo da vizinhança, começou a trabalhar, \n",
      "engrossando o barulho com o seu arfar monótono de máquina a vapor'\n",
      "Tokens gerados: ['a', 'fabrica', 'de', 'massas', 'italianas', ',', 'ali', 'mesmo', 'da', 'vizinhanca', ',', 'comecou', 'a', 'trabalhar', ',', 'engrossando', 'o', 'barulho', 'com', 'o', 'seu', 'arfar', 'monotono', 'de', 'maquina', 'a', 'vapor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As corridas até à venda reproduziam-se, \n",
      "transformando-se num verminar constante de formigueiro assanhado'\n",
      "Tokens gerados: ['as', 'corridas', 'ate', 'a', 'venda', 'reproduziam-se', ',', 'transformando-se', 'num', 'verminar', 'constante', 'de', 'formigueiro', 'assanhado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Agora, no lugar das bicas apinhavam-se latas de \n",
      "todos os feitios, sobressaindo as de querosene com um braço de madeira em cima; sentia-se o trapejar da água caindo na \n",
      "folha'\n",
      "Tokens gerados: ['agora', ',', 'no', 'lugar', 'das', 'bicas', 'apinhavam-se', 'latas', 'de', 'todos', 'os', 'feitios', ',', 'sobressaindo', 'as', 'de', 'querosene', 'com', 'um', 'braco', 'de', 'madeira', 'em', 'cima', 'sentia-se', 'o', 'trapejar', 'da', 'agua', 'caindo', 'na', 'folha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Algumas lavadeiras enchiam já as suas tinas; outras estendiam nos coradouros a roupa que ficara de molho'\n",
      "Tokens gerados: ['algumas', 'lavadeiras', 'enchiam', 'ja', 'as', 'suas', 'tinas', 'outras', 'estendiam', 'nos', 'coradouros', 'a', 'roupa', 'que', 'ficara', 'de', 'molho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Principiava o trabalho'\n",
      "Tokens gerados: ['principiava', 'o', 'trabalho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Rompiam das gargantas os fados portugueses e as modinhas brasileiras'\n",
      "Tokens gerados: ['rompiam', 'das', 'gargantas', 'os', 'fados', 'portugueses', 'e', 'as', 'modinhas', 'brasileiras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um carroção de lixo \n",
      "entrou com grande barulho de rodas na pedra, seguido de uma algazarra medonha algaraviada pelo carroceiro contra o \n",
      "burro'\n",
      "Tokens gerados: ['um', 'carrocao', 'de', 'lixo', 'entrou', 'com', 'grande', 'barulho', 'de', 'rodas', 'na', 'pedra', ',', 'seguido', 'de', 'uma', 'algazarra', 'medonha', 'algaraviada', 'pelo', 'carroceiro', 'contra', 'o', 'burro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, durante muito tempo, fez-se um vaivém de mercadores'\n",
      "Tokens gerados: ['e', ',', 'durante', 'muito', 'tempo', ',', 'fez-se', 'um', 'vaivem', 'de', 'mercadores']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Apareceram os tabuleiros de carne fresca e outros de \n",
      "tripas e fatos de boi; só não vinham hortaliças, porque havia muitas hortas no cortiço'\n",
      "Tokens gerados: ['apareceram', 'os', 'tabuleiros', 'de', 'carne', 'fresca', 'e', 'outros', 'de', 'tripas', 'e', 'fatos', 'de', 'boi', 'so', 'nao', 'vinham', 'hortalicas', ',', 'porque', 'havia', 'muitas', 'hortas', 'no', 'cortico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vieram os ruidosos mascates, \n",
      "com as suas latas de quinquilharia, com as suas caixas de candeeiros e objetos de vidro e com o seu fornecimento de \n",
      "caçarolas e chocolateiras, de folha-de-flandres'\n",
      "Tokens gerados: ['vieram', 'os', 'ruidosos', 'mascates', ',', 'com', 'as', 'suas', 'latas', 'de', 'quinquilharia', ',', 'com', 'as', 'suas', 'caixas', 'de', 'candeeiros', 'e', 'objetos', 'de', 'vidro', 'e', 'com', 'o', 'seu', 'fornecimento', 'de', 'cacarolas', 'e', 'chocolateiras', ',', 'de', 'folha-de-flandres']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cada vendedor tinha o seu modo especial de apregoar, destacando-se o \n",
      "homem das sardinhas, com as cestas do peixe dependuradas, à moda de balança, de um pau que ele trazia ao ombro'\n",
      "Tokens gerados: ['cada', 'vendedor', 'tinha', 'o', 'seu', 'modo', 'especial', 'de', 'apregoar', ',', 'destacando-se', 'o', 'homem', 'das', 'sardinhas', ',', 'com', 'as', 'cestas', 'do', 'peixe', 'dependuradas', ',', 'a', 'moda', 'de', 'balanca', ',', 'de', 'um', 'pau', 'que', 'ele', 'trazia', 'ao', 'ombro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nada mais foi preciso do que o seu primeiro guincho estridente e gutural para surgirem logo, como por encanto, uma \n",
      "enorme variedade de gatos, que vieram correndo acercar-se dele com grande familiaridade, roçando-se-lhe nas pernas \n",
      "arregaçadas e miando suplicantemente'\n",
      "Tokens gerados: ['nada', 'mais', 'foi', 'preciso', 'do', 'que', 'o', 'seu', 'primeiro', 'guincho', 'estridente', 'e', 'gutural', 'para', 'surgirem', 'logo', ',', 'como', 'por', 'encanto', ',', 'uma', 'enorme', 'variedade', 'de', 'gatos', ',', 'que', 'vieram', 'correndo', 'acercar-se', 'dele', 'com', 'grande', 'familiaridade', ',', 'rocando-se-lhe', 'nas', 'pernas', 'arregacadas', 'e', 'miando', 'suplicantemente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O sardinheiro os afastava com o pé, enquanto vendia o seu peixe à porta das \n",
      "casinhas, mas os bichanos não desistiam e continuavam a implorar, arranhando os cestos que o homem cuidadosamente \n",
      "tapava mal servia ao freguês'\n",
      "Tokens gerados: ['o', 'sardinheiro', 'os', 'afastava', 'com', 'o', 'pe', ',', 'enquanto', 'vendia', 'o', 'seu', 'peixe', 'a', 'porta', 'das', 'casinhas', ',', 'mas', 'os', 'bichanos', 'nao', 'desistiam', 'e', 'continuavam', 'a', 'implorar', ',', 'arranhando', 'os', 'cestos', 'que', 'o', 'homem', 'cuidadosamente', 'tapava', 'mal', 'servia', 'ao', 'fregues']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Para ver-se livre por um instante dos importunos era necessário atirar para bem longe um \n",
      "punhado de sardinhas, sobre o qual se precipitava logo, aos pulos, o grupo dos pedinchões'\n",
      "Tokens gerados: ['para', 'ver-se', 'livre', 'por', 'um', 'instante', 'dos', 'importunos', 'era', 'necessario', 'atirar', 'para', 'bem', 'longe', 'um', 'punhado', 'de', 'sardinhas', ',', 'sobre', 'o', 'qual', 'se', 'precipitava', 'logo', ',', 'aos', 'pulos', ',', 'o', 'grupo', 'dos', 'pedinchoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A primeira que se pôs a lavar foi a Leandra, por alcunha a “Machona”, portuguesa feroz, berradora, pulsos \n",
      "cabeludos e grossos, anca de animal do campo'\n",
      "Tokens gerados: ['a', 'primeira', 'que', 'se', 'pos', 'a', 'lavar', 'foi', 'a', 'leandra', ',', 'por', 'alcunha', 'a', '“', 'machona', '”', ',', 'portuguesa', 'feroz', ',', 'berradora', ',', 'pulsos', 'cabeludos', 'e', 'grossos', ',', 'anca', 'de', 'animal', 'do', 'campo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha duas filhas, uma casada e separada do marido, Ana das Dores, a \n",
      "quem só chamavam a “das Dores” e outra donzela ainda, a Nenen, e mais um filho, o Agostinho, menino levado dos \n",
      "diabos, que gritava tanto ou melhor que a mãe'\n",
      "Tokens gerados: ['tinha', 'duas', 'filhas', ',', 'uma', 'casada', 'e', 'separada', 'do', 'marido', ',', 'ana', 'das', 'dores', ',', 'a', 'quem', 'so', 'chamavam', 'a', '“', 'das', 'dores', '”', 'e', 'outra', 'donzela', 'ainda', ',', 'a', 'nenen', ',', 'e', 'mais', 'um', 'filho', ',', 'o', 'agostinho', ',', 'menino', 'levado', 'dos', 'diabos', ',', 'que', 'gritava', 'tanto', 'ou', 'melhor', 'que', 'a', 'mae']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A das Dores morava em sua casinha à parte, mas toda a família habitava \n",
      "no cortiço'\n",
      "Tokens gerados: ['a', 'das', 'dores', 'morava', 'em', 'sua', 'casinha', 'a', 'parte', ',', 'mas', 'toda', 'a', 'familia', 'habitava', 'no', 'cortico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ninguém ali sabia ao certo se a Machona era viúva ou desquitada; os filhos não se pareciam uns com os outros'\n",
      "Tokens gerados: ['ninguem', 'ali', 'sabia', 'ao', 'certo', 'se', 'a', 'machona', 'era', 'viuva', 'ou', 'desquitada', 'os', 'filhos', 'nao', 'se', 'pareciam', 'uns', 'com', 'os', 'outros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A \n",
      "das Dores, sim, afirmavam que fora casada e que largara o marido para meter-se com um homem do comércio; e que \n",
      "este, retirando-se para a terra e não querendo soltá-la ao desamparo, deixara o sócio em seu lugar'\n",
      "Tokens gerados: ['a', 'das', 'dores', ',', 'sim', ',', 'afirmavam', 'que', 'fora', 'casada', 'e', 'que', 'largara', 'o', 'marido', 'para', 'meter-se', 'com', 'um', 'homem', 'do', 'comercio', 'e', 'que', 'este', ',', 'retirando-se', 'para', 'a', 'terra', 'e', 'nao', 'querendo', 'solta-la', 'ao', 'desamparo', ',', 'deixara', 'o', 'socio', 'em', 'seu', 'lugar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Teria vinte e cinco \n",
      "anos'\n",
      "Tokens gerados: ['teria', 'vinte', 'e', 'cinco', 'anos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nenen dezessete'\n",
      "Tokens gerados: ['nenen', 'dezessete']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Espigada, franzina e forte, com uma proazinha de orgulho da sua virgindade, escapando como \n",
      "enguia por entre os dedos dos rapazes que a queriam sem ser para casar'\n",
      "Tokens gerados: ['espigada', ',', 'franzina', 'e', 'forte', ',', 'com', 'uma', 'proazinha', 'de', 'orgulho', 'da', 'sua', 'virgindade', ',', 'escapando', 'como', 'enguia', 'por', 'entre', 'os', 'dedos', 'dos', 'rapazes', 'que', 'a', 'queriam', 'sem', 'ser', 'para', 'casar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Engomava bem e sabia fazer roupa branca de \n",
      "homem com muita perfeição'\n",
      "Tokens gerados: ['engomava', 'bem', 'e', 'sabia', 'fazer', 'roupa', 'branca', 'de', 'homem', 'com', 'muita', 'perfeicao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao lado da Leandra foi colocar-se à sua tina a Augusta Carne-Mole, brasileira, branca, mulher de Alexandre, um \n",
      "mulato de quarenta anos, soldado de policia, pernóstico, de grande bigode preto, queixo sempre escanhoado e um luxo \n",
      "de calças brancas engomadas e botões limpos na farda, quando estava de serviço'\n",
      "Tokens gerados: ['ao', 'lado', 'da', 'leandra', 'foi', 'colocar-se', 'a', 'sua', 'tina', 'a', 'augusta', 'carne-mole', ',', 'brasileira', ',', 'branca', ',', 'mulher', 'de', 'alexandre', ',', 'um', 'mulato', 'de', 'quarenta', 'anos', ',', 'soldado', 'de', 'policia', ',', 'pernostico', ',', 'de', 'grande', 'bigode', 'preto', ',', 'queixo', 'sempre', 'escanhoado', 'e', 'um', 'luxo', 'de', 'calcas', 'brancas', 'engomadas', 'e', 'botoes', 'limpos', 'na', 'farda', ',', 'quando', 'estava', 'de', 'servico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Também tinham filhos, mas ainda \n",
      "pequenos, um dos quais, a Juju, vivia na cidade com a madrinha que se encarregava dela'\n",
      "Tokens gerados: ['tambem', 'tinham', 'filhos', ',', 'mas', 'ainda', 'pequenos', ',', 'um', 'dos', 'quais', ',', 'a', 'juju', ',', 'vivia', 'na', 'cidade', 'com', 'a', 'madrinha', 'que', 'se', 'encarregava', 'dela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Esta madrinha era uma cocote \n",
      "de trinta mil-réis para cima, a Léonie, com sobrado na cidade'\n",
      "Tokens gerados: ['esta', 'madrinha', 'era', 'uma', 'cocote', 'de', 'trinta', 'mil-reis', 'para', 'cima', ',', 'a', 'leonie', ',', 'com', 'sobrado', 'na', 'cidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Procedência francesa'\n",
      "Tokens gerados: ['procedencia', 'francesa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Alexandre, em casa, à hora de descanso, nos seus chinelos e na sua camisa desabotoada, era muito chão com os \n",
      "companheiros de estalagem, conversava, ria e brincava, mas envergando o uniforme, encerando o bigode e empunhando \n",
      "a sua chibata, com que tinha o costume de fustigar as calças de brim, ninguém mais lhe via os dentes e então a todos \n",
      "falava teso e por cima do ombro'\n",
      "Tokens gerados: ['alexandre', ',', 'em', 'casa', ',', 'a', 'hora', 'de', 'descanso', ',', 'nos', 'seus', 'chinelos', 'e', 'na', 'sua', 'camisa', 'desabotoada', ',', 'era', 'muito', 'chao', 'com', 'os', 'companheiros', 'de', 'estalagem', ',', 'conversava', ',', 'ria', 'e', 'brincava', ',', 'mas', 'envergando', 'o', 'uniforme', ',', 'encerando', 'o', 'bigode', 'e', 'empunhando', 'a', 'sua', 'chibata', ',', 'com', 'que', 'tinha', 'o', 'costume', 'de', 'fustigar', 'as', 'calcas', 'de', 'brim', ',', 'ninguem', 'mais', 'lhe', 'via', 'os', 'dentes', 'e', 'entao', 'a', 'todos', 'falava', 'teso', 'e', 'por', 'cima', 'do', 'ombro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mulher, a quem ele só dava tu quando não estava fardado, era de uma honestidade \n",
      "proverbial no cortiço, honestidade sem mérito, porque vinha da indolência do seu temperamento e não do arbítrio do \n",
      "seu caráter'\n",
      "Tokens gerados: ['a', 'mulher', ',', 'a', 'quem', 'ele', 'so', 'dava', 'tu', 'quando', 'nao', 'estava', 'fardado', ',', 'era', 'de', 'uma', 'honestidade', 'proverbial', 'no', 'cortico', ',', 'honestidade', 'sem', 'merito', ',', 'porque', 'vinha', 'da', 'indolencia', 'do', 'seu', 'temperamento', 'e', 'nao', 'do', 'arbitrio', 'do', 'seu', 'carater']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Junto dela pôs-se a trabalhar a Leocádia, mulher de um ferreiro chamado Bruno, portuguesa pequena e socada, de \n",
      "carnes duras, com uma fama terrível de leviana entre as suas vizinhas'\n",
      "Tokens gerados: ['junto', 'dela', 'pos-se', 'a', 'trabalhar', 'a', 'leocadia', ',', 'mulher', 'de', 'um', 'ferreiro', 'chamado', 'bruno', ',', 'portuguesa', 'pequena', 'e', 'socada', ',', 'de', 'carnes', 'duras', ',', 'com', 'uma', 'fama', 'terrivel', 'de', 'leviana', 'entre', 'as', 'suas', 'vizinhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seguia-se a Paula, uma cabocla velha, meio idiota, a quem respeitavam todos pelas virtudes de que só ela \n",
      "dispunha para benzer erisipelas e cortar febres por meio de rezas e feitiçarias'\n",
      "Tokens gerados: ['seguia-se', 'a', 'paula', ',', 'uma', 'cabocla', 'velha', ',', 'meio', 'idiota', ',', 'a', 'quem', 'respeitavam', 'todos', 'pelas', 'virtudes', 'de', 'que', 'so', 'ela', 'dispunha', 'para', 'benzer', 'erisipelas', 'e', 'cortar', 'febres', 'por', 'meio', 'de', 'rezas', 'e', 'feiticarias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era extremamente feia, grossa, triste, com \n",
      "olhos desvairados, dentes cortados à navalha, formando ponta, como dentes de cão, cabelos lisos, escorridos e ainda \n",
      "retintos apesar da idade'\n",
      "Tokens gerados: ['era', 'extremamente', 'feia', ',', 'grossa', ',', 'triste', ',', 'com', 'olhos', 'desvairados', ',', 'dentes', 'cortados', 'a', 'navalha', ',', 'formando', 'ponta', ',', 'como', 'dentes', 'de', 'cao', ',', 'cabelos', 'lisos', ',', 'escorridos', 'e', 'ainda', 'retintos', 'apesar', 'da', 'idade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Chamavam-lhe “Bruxa”'\n",
      "Tokens gerados: ['chamavam-lhe', '“', 'bruxa', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois seguiam-se a Marciana e mais a sua filha Florinda'\n",
      "Tokens gerados: ['depois', 'seguiam-se', 'a', 'marciana', 'e', 'mais', 'a', 'sua', 'filha', 'florinda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A primeira, mulata antiga, muito seria e asseada em \n",
      "exagero: a sua casa estava sempre úmida das consecutivas lavagens'\n",
      "Tokens gerados: ['a', 'primeira', ',', 'mulata', 'antiga', ',', 'muito', 'seria', 'e', 'asseada', 'em', 'exagero', 'a', 'sua', 'casa', 'estava', 'sempre', 'umida', 'das', 'consecutivas', 'lavagens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em lhe apanhando o mau humor punha-se logo a \n",
      "espanar, a varrer febrilmente, e, quando a raiva era grande, corria a buscar um balde de água e descarregava-o com fúria \n",
      "pelo chão da sala'\n",
      "Tokens gerados: ['em', 'lhe', 'apanhando', 'o', 'mau', 'humor', 'punha-se', 'logo', 'a', 'espanar', ',', 'a', 'varrer', 'febrilmente', ',', 'e', ',', 'quando', 'a', 'raiva', 'era', 'grande', ',', 'corria', 'a', 'buscar', 'um', 'balde', 'de', 'agua', 'e', 'descarregava-o', 'com', 'furia', 'pelo', 'chao', 'da', 'sala']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A filha tinha quinze anos, a pele de um moreno quente, beiços sensuais, bonitos dentes, olhos \n",
      "luxuriosos de macaca'\n",
      "Tokens gerados: ['a', 'filha', 'tinha', 'quinze', 'anos', ',', 'a', 'pele', 'de', 'um', 'moreno', 'quente', ',', 'beicos', 'sensuais', ',', 'bonitos', 'dentes', ',', 'olhos', 'luxuriosos', 'de', 'macaca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Toda ela estava a pedir homem, mas sustentava ainda a sua virgindade e não cedia, nem à mão de \n",
      "Deus Padre, aos rogos de João Romão, que a desejava apanhar a troco de pequenas concessões na medida e no peso das \n",
      "compras que Florinda fazia diariamente à venda'\n",
      "Tokens gerados: ['toda', 'ela', 'estava', 'a', 'pedir', 'homem', ',', 'mas', 'sustentava', 'ainda', 'a', 'sua', 'virgindade', 'e', 'nao', 'cedia', ',', 'nem', 'a', 'mao', 'de', 'deus', 'padre', ',', 'aos', 'rogos', 'de', 'joao', 'romao', ',', 'que', 'a', 'desejava', 'apanhar', 'a', 'troco', 'de', 'pequenas', 'concessoes', 'na', 'medida', 'e', 'no', 'peso', 'das', 'compras', 'que', 'florinda', 'fazia', 'diariamente', 'a', 'venda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois via-se a velha Isabel, isto é, Dona Isabel, porque ali na estalagem lhes dispensavam todos certa \n",
      "consideração, privilegiada pelas suas maneiras graves de pessoa que já teve tratamento: uma pobre mulher comida de \n",
      "desgostos'\n",
      "Tokens gerados: ['depois', 'via-se', 'a', 'velha', 'isabel', ',', 'isto', 'e', ',', 'dona', 'isabel', ',', 'porque', 'ali', 'na', 'estalagem', 'lhes', 'dispensavam', 'todos', 'certa', 'consideracao', ',', 'privilegiada', 'pelas', 'suas', 'maneiras', 'graves', 'de', 'pessoa', 'que', 'ja', 'teve', 'tratamento', 'uma', 'pobre', 'mulher', 'comida', 'de', 'desgostos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fora casada com o dono de uma casa de chapéus, que quebrou e suicidou-se, deixando-lhe uma filha muito \n",
      "doentinha e fraca, a quem Isabel sacrificou tudo para educar, dando-lhe mestre até de francês'\n",
      "Tokens gerados: ['fora', 'casada', 'com', 'o', 'dono', 'de', 'uma', 'casa', 'de', 'chapeus', ',', 'que', 'quebrou', 'e', 'suicidou-se', ',', 'deixando-lhe', 'uma', 'filha', 'muito', 'doentinha', 'e', 'fraca', ',', 'a', 'quem', 'isabel', 'sacrificou', 'tudo', 'para', 'educar', ',', 'dando-lhe', 'mestre', 'ate', 'de', 'frances']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha uma cara macilenta \n",
      "de velha portuguesa devota, que já foi gorda, bochechas moles de pelancas rechupadas, que lhe pendiam dos cantos da \n",
      "boca como saquinhos vazios; fios negros no queixo, olhos castanhos, sempre chorosos engolidos pelas pálpebras'\n",
      "Tokens gerados: ['tinha', 'uma', 'cara', 'macilenta', 'de', 'velha', 'portuguesa', 'devota', ',', 'que', 'ja', 'foi', 'gorda', ',', 'bochechas', 'moles', 'de', 'pelancas', 'rechupadas', ',', 'que', 'lhe', 'pendiam', 'dos', 'cantos', 'da', 'boca', 'como', 'saquinhos', 'vazios', 'fios', 'negros', 'no', 'queixo', ',', 'olhos', 'castanhos', ',', 'sempre', 'chorosos', 'engolidos', 'pelas', 'palpebras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Puxava em bandos sobre as fontes o escasso cabelo grisalho untado de óleo de amêndoas doces'\n",
      "Tokens gerados: ['puxava', 'em', 'bandos', 'sobre', 'as', 'fontes', 'o', 'escasso', 'cabelo', 'grisalho', 'untado', 'de', 'oleo', 'de', 'amendoas', 'doces']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando saia à rua \n",
      "punha um eterno vestido de seda preta, achamalotada, cuja saia não fazia rugas, e um xale encarnado que lhe dava a \n",
      "todo o corpo um feitio piramidal'\n",
      "Tokens gerados: ['quando', 'saia', 'a', 'rua', 'punha', 'um', 'eterno', 'vestido', 'de', 'seda', 'preta', ',', 'achamalotada', ',', 'cuja', 'saia', 'nao', 'fazia', 'rugas', ',', 'e', 'um', 'xale', 'encarnado', 'que', 'lhe', 'dava', 'a', 'todo', 'o', 'corpo', 'um', 'feitio', 'piramidal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Da sua passada grandeza só lhe ficara uma caixa de rapé de ouro, na qual a \n",
      "inconsolável senhora pitadeava  agora, suspirando a cada pitada'\n",
      "Tokens gerados: ['da', 'sua', 'passada', 'grandeza', 'so', 'lhe', 'ficara', 'uma', 'caixa', 'de', 'rape', 'de', 'ouro', ',', 'na', 'qual', 'a', 'inconsolavel', 'senhora', 'pitadeava', 'agora', ',', 'suspirando', 'a', 'cada', 'pitada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A filha era a flor do cortiço'\n",
      "Tokens gerados: ['a', 'filha', 'era', 'a', 'flor', 'do', 'cortico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Chamavam-lhe Pombinha'\n",
      "Tokens gerados: ['chamavam-lhe', 'pombinha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bonita, posto que enfermiça e nervosa ao último ponto; \n",
      "loura, muito pálida, com uns modos de menina de boa família'\n",
      "Tokens gerados: ['bonita', ',', 'posto', 'que', 'enfermica', 'e', 'nervosa', 'ao', 'ultimo', 'ponto', 'loura', ',', 'muito', 'palida', ',', 'com', 'uns', 'modos', 'de', 'menina', 'de', 'boa', 'familia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mãe não lhe permitia lavar, nem engomar, mesmo \n",
      "porque o médico a proibira expressamente'\n",
      "Tokens gerados: ['a', 'mae', 'nao', 'lhe', 'permitia', 'lavar', ',', 'nem', 'engomar', ',', 'mesmo', 'porque', 'o', 'medico', 'a', 'proibira', 'expressamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha o seu noivo, o João da Costa, moço do comércio, estimado do patrão e dos colegas, com muito futuro, e que \n",
      "a adorava e conhecia desde pequenita; mas Dona Isabel não queria que o casamento se fizesse já'\n",
      "Tokens gerados: ['tinha', 'o', 'seu', 'noivo', ',', 'o', 'joao', 'da', 'costa', ',', 'moco', 'do', 'comercio', ',', 'estimado', 'do', 'patrao', 'e', 'dos', 'colegas', ',', 'com', 'muito', 'futuro', ',', 'e', 'que', 'a', 'adorava', 'e', 'conhecia', 'desde', 'pequenita', 'mas', 'dona', 'isabel', 'nao', 'queria', 'que', 'o', 'casamento', 'se', 'fizesse', 'ja']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É que Pombinha, \n",
      "orçando aliás pelos dezoito anos, não tinha ainda pago à natureza o cruento tributo da puberdade, apesar do zelo da \n",
      "velha e dos sacrifícios que esta fazia para cumprir à risca as prescrições do médico e não faltar à filha o menor desvelo'\n",
      "Tokens gerados: ['e', 'que', 'pombinha', ',', 'orcando', 'alias', 'pelos', 'dezoito', 'anos', ',', 'nao', 'tinha', 'ainda', 'pago', 'a', 'natureza', 'o', 'cruento', 'tributo', 'da', 'puberdade', ',', 'apesar', 'do', 'zelo', 'da', 'velha', 'e', 'dos', 'sacrificios', 'que', 'esta', 'fazia', 'para', 'cumprir', 'a', 'risca', 'as', 'prescricoes', 'do', 'medico', 'e', 'nao', 'faltar', 'a', 'filha', 'o', 'menor', 'desvelo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No entanto, coitadas! daquele casamento dependia a felicidade de ambas, porque o Costa, bem empregado como se \n",
      "achava em casa de um tio seu, de quem mais tarde havia de ser sócio, tencionava, logo que mudasse de estado, \n",
      "restituí-las ao seu primitivo circulo social'\n",
      "Tokens gerados: ['no', 'entanto', ',', 'coitadas', 'daquele', 'casamento', 'dependia', 'a', 'felicidade', 'de', 'ambas', ',', 'porque', 'o', 'costa', ',', 'bem', 'empregado', 'como', 'se', 'achava', 'em', 'casa', 'de', 'um', 'tio', 'seu', ',', 'de', 'quem', 'mais', 'tarde', 'havia', 'de', 'ser', 'socio', ',', 'tencionava', ',', 'logo', 'que', 'mudasse', 'de', 'estado', ',', 'restitui-las', 'ao', 'seu', 'primitivo', 'circulo', 'social']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A pobre velha desesperava-se com o fato e pedia a Deus, todas as noites, \n",
      "antes de dormir, que as protegesse e conferisse à filha uma graça tão simples que ele fazia, sem distinção de \n",
      "merecimento, a quantas raparigas havia pelo mundo; mas, a despeito de tamanho empenho, por coisa nenhuma desta \n",
      "vida consentiria que a sua pequena casasse antes de “ser mulher”, como dizia ela'\n",
      "Tokens gerados: ['a', 'pobre', 'velha', 'desesperava-se', 'com', 'o', 'fato', 'e', 'pedia', 'a', 'deus', ',', 'todas', 'as', 'noites', ',', 'antes', 'de', 'dormir', ',', 'que', 'as', 'protegesse', 'e', 'conferisse', 'a', 'filha', 'uma', 'graca', 'tao', 'simples', 'que', 'ele', 'fazia', ',', 'sem', 'distincao', 'de', 'merecimento', ',', 'a', 'quantas', 'raparigas', 'havia', 'pelo', 'mundo', 'mas', ',', 'a', 'despeito', 'de', 'tamanho', 'empenho', ',', 'por', 'coisa', 'nenhuma', 'desta', 'vida', 'consentiria', 'que', 'a', 'sua', 'pequena', 'casasse', 'antes', 'de', '“', 'ser', 'mulher', '”', ',', 'como', 'dizia', 'ela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E “que deixassem lá falar o doutor, \n",
      "entendia que não era decente, nem tinha jeito, dar homem a uma moça que ainda não fora visitada pelas regras! Não! \n",
      "Antes vê-la solteira toda a vida e ficarem ambas curtindo para sempre aquele inferno da estalagem!” \n",
      "Lá no cortiço estavam todos a par desta história; não era segredo para ninguém'\n",
      "Tokens gerados: ['e', '“', 'que', 'deixassem', 'la', 'falar', 'o', 'doutor', ',', 'entendia', 'que', 'nao', 'era', 'decente', ',', 'nem', 'tinha', 'jeito', ',', 'dar', 'homem', 'a', 'uma', 'moca', 'que', 'ainda', 'nao', 'fora', 'visitada', 'pelas', 'regras', 'nao', 'antes', 've-la', 'solteira', 'toda', 'a', 'vida', 'e', 'ficarem', 'ambas', 'curtindo', 'para', 'sempre', 'aquele', 'inferno', 'da', 'estalagem', '”', 'la', 'no', 'cortico', 'estavam', 'todos', 'a', 'par', 'desta', 'historia', 'nao', 'era', 'segredo', 'para', 'ninguem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E não se passava um dia que não \n",
      "interrogassem duas e três vezes a velha com estas frases: \n",
      "— Então? Já veio? \n",
      "— Por que não tenta os banhos de mar? \n",
      "— Por que não chama outro médico? \n",
      "— Eu, se fosse a senhora, casava-os assim mesmo! \n",
      "A velha respondia dizendo que a felicidade não se fizera para ela'\n",
      "Tokens gerados: ['e', 'nao', 'se', 'passava', 'um', 'dia', 'que', 'nao', 'interrogassem', 'duas', 'e', 'tres', 'vezes', 'a', 'velha', 'com', 'estas', 'frases', '—', 'entao', '?', 'ja', 'veio', '?', '—', 'por', 'que', 'nao', 'tenta', 'os', 'banhos', 'de', 'mar', '?', '—', 'por', 'que', 'nao', 'chama', 'outro', 'medico', '?', '—', 'eu', ',', 'se', 'fosse', 'a', 'senhora', ',', 'casava-os', 'assim', 'mesmo', 'a', 'velha', 'respondia', 'dizendo', 'que', 'a', 'felicidade', 'nao', 'se', 'fizera', 'para', 'ela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E suspirava resignada'\n",
      "Tokens gerados: ['e', 'suspirava', 'resignada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando o Costa aparecia depois da sua obrigação para visitar a noiva, os moradores da estalagem \n",
      "cumprimentavam-no em silêncio com um respeitoso ar de lástima e piedade, empenhados tacitamente por aquele \n",
      "caiporismo, contra o qual não valiam nem mesmo as virtudes da Bruxa'\n",
      "Tokens gerados: ['quando', 'o', 'costa', 'aparecia', 'depois', 'da', 'sua', 'obrigacao', 'para', 'visitar', 'a', 'noiva', ',', 'os', 'moradores', 'da', 'estalagem', 'cumprimentavam-no', 'em', 'silencio', 'com', 'um', 'respeitoso', 'ar', 'de', 'lastima', 'e', 'piedade', ',', 'empenhados', 'tacitamente', 'por', 'aquele', 'caiporismo', ',', 'contra', 'o', 'qual', 'nao', 'valiam', 'nem', 'mesmo', 'as', 'virtudes', 'da', 'bruxa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pombinha era muito querida por toda aquela gente'\n",
      "Tokens gerados: ['pombinha', 'era', 'muito', 'querida', 'por', 'toda', 'aquela', 'gente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era quem lhe escrevia as cartas; quem em geral fazia o rol \n",
      "para as lavadeiras; quem tirava as contas; quem lia o jornal para os que quisessem ouvir'\n",
      "Tokens gerados: ['era', 'quem', 'lhe', 'escrevia', 'as', 'cartas', 'quem', 'em', 'geral', 'fazia', 'o', 'rol', 'para', 'as', 'lavadeiras', 'quem', 'tirava', 'as', 'contas', 'quem', 'lia', 'o', 'jornal', 'para', 'os', 'que', 'quisessem', 'ouvir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prezavam-na com muito \n",
      "respeito e davam-lhe presentes, o que lhe permitia certo luxo relativo'\n",
      "Tokens gerados: ['prezavam-na', 'com', 'muito', 'respeito', 'e', 'davam-lhe', 'presentes', ',', 'o', 'que', 'lhe', 'permitia', 'certo', 'luxo', 'relativo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Andava sempre de botinhas ou sapatinhos com \n",
      "meias de cor, seu vestido de chita engomado; tinha as suas joiazinhas para sair à rua, e, aos domingos, quem a \n",
      "encontrasse à missa na igreja de São João Batista, não seria capaz de desconfiar que ela morava em cortiço'\n",
      "Tokens gerados: ['andava', 'sempre', 'de', 'botinhas', 'ou', 'sapatinhos', 'com', 'meias', 'de', 'cor', ',', 'seu', 'vestido', 'de', 'chita', 'engomado', 'tinha', 'as', 'suas', 'joiazinhas', 'para', 'sair', 'a', 'rua', ',', 'e', ',', 'aos', 'domingos', ',', 'quem', 'a', 'encontrasse', 'a', 'missa', 'na', 'igreja', 'de', 'sao', 'joao', 'batista', ',', 'nao', 'seria', 'capaz', 'de', 'desconfiar', 'que', 'ela', 'morava', 'em', 'cortico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fechava a fila das primeiras lavadeiras, o Albino, um sujeito afeminado, fraco, cor de espargo cozido e com um \n",
      "cabelinho castanho, deslavado e pobre, que lhe caia, numa só linha, até ao pescocinho mole e fino'\n",
      "Tokens gerados: ['fechava', 'a', 'fila', 'das', 'primeiras', 'lavadeiras', ',', 'o', 'albino', ',', 'um', 'sujeito', 'afeminado', ',', 'fraco', ',', 'cor', 'de', 'espargo', 'cozido', 'e', 'com', 'um', 'cabelinho', 'castanho', ',', 'deslavado', 'e', 'pobre', ',', 'que', 'lhe', 'caia', ',', 'numa', 'so', 'linha', ',', 'ate', 'ao', 'pescocinho', 'mole', 'e', 'fino']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era lavadeiro e vivia \n",
      "sempre entre as mulheres, com quem já estava tão familiarizado que elas o tratavam como a uma pessoa do mesmo \n",
      "sexo; em presença dele falavam de coisas que não exporiam em presença de outro homem; faziam-no até confidente dos \n",
      "seus amores e das suas infidelidades, com uma franqueza que o não revoltava, nem comovia'\n",
      "Tokens gerados: ['era', 'lavadeiro', 'e', 'vivia', 'sempre', 'entre', 'as', 'mulheres', ',', 'com', 'quem', 'ja', 'estava', 'tao', 'familiarizado', 'que', 'elas', 'o', 'tratavam', 'como', 'a', 'uma', 'pessoa', 'do', 'mesmo', 'sexo', 'em', 'presenca', 'dele', 'falavam', 'de', 'coisas', 'que', 'nao', 'exporiam', 'em', 'presenca', 'de', 'outro', 'homem', 'faziam-no', 'ate', 'confidente', 'dos', 'seus', 'amores', 'e', 'das', 'suas', 'infidelidades', ',', 'com', 'uma', 'franqueza', 'que', 'o', 'nao', 'revoltava', ',', 'nem', 'comovia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando um casal brigava \n",
      "ou duas amigas se disputavam, era sempre Albino quem tratava de reconciliá-los, exortando as mulheres à concórdia'\n",
      "Tokens gerados: ['quando', 'um', 'casal', 'brigava', 'ou', 'duas', 'amigas', 'se', 'disputavam', ',', 'era', 'sempre', 'albino', 'quem', 'tratava', 'de', 'reconcilia-los', ',', 'exortando', 'as', 'mulheres', 'a', 'concordia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dantes encarregava-se de cobrar o rol das colegas, por amabilidade; mas uma vez, indo a uma república de estudantes, \n",
      "deram-lhe lá, ninguém sabia por quê, uma dúzia de bolos, e o pobre-diabo jurou então, entre lágrimas e soluços, que \n",
      "nunca mais se incumbiria de receber os róis'\n",
      "Tokens gerados: ['dantes', 'encarregava-se', 'de', 'cobrar', 'o', 'rol', 'das', 'colegas', ',', 'por', 'amabilidade', 'mas', 'uma', 'vez', ',', 'indo', 'a', 'uma', 'republica', 'de', 'estudantes', ',', 'deram-lhe', 'la', ',', 'ninguem', 'sabia', 'por', 'que', ',', 'uma', 'duzia', 'de', 'bolos', ',', 'e', 'o', 'pobre-diabo', 'jurou', 'entao', ',', 'entre', 'lagrimas', 'e', 'solucos', ',', 'que', 'nunca', 'mais', 'se', 'incumbiria', 'de', 'receber', 'os', 'rois']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E daí em diante, com efeito, não arredava os pezinhos do cortiço, a não ser nos dias de carnaval, em que ia, \n",
      "vestido de dançarina, passear à tarde pelas ruas e à noite dançar nos bailes dos teatros'\n",
      "Tokens gerados: ['e', 'dai', 'em', 'diante', ',', 'com', 'efeito', ',', 'nao', 'arredava', 'os', 'pezinhos', 'do', 'cortico', ',', 'a', 'nao', 'ser', 'nos', 'dias', 'de', 'carnaval', ',', 'em', 'que', 'ia', ',', 'vestido', 'de', 'dancarina', ',', 'passear', 'a', 'tarde', 'pelas', 'ruas', 'e', 'a', 'noite', 'dancar', 'nos', 'bailes', 'dos', 'teatros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha verdadeira paixão por esse \n",
      "divertimento; ajuntava dinheiro durante o ano para gastar todo com a mascarada'\n",
      "Tokens gerados: ['tinha', 'verdadeira', 'paixao', 'por', 'esse', 'divertimento', 'ajuntava', 'dinheiro', 'durante', 'o', 'ano', 'para', 'gastar', 'todo', 'com', 'a', 'mascarada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E ninguém o encontrava, domingo ou \n",
      "dia de semana, lavando ou descansando, que não estivesse com a sua calça branca engomada, a sua camisa limpa, um \n",
      "lenço ao pescoço, e, amarrado à cinta, um avental que lhe caia sobre as pernas como uma saia'\n",
      "Tokens gerados: ['e', 'ninguem', 'o', 'encontrava', ',', 'domingo', 'ou', 'dia', 'de', 'semana', ',', 'lavando', 'ou', 'descansando', ',', 'que', 'nao', 'estivesse', 'com', 'a', 'sua', 'calca', 'branca', 'engomada', ',', 'a', 'sua', 'camisa', 'limpa', ',', 'um', 'lenco', 'ao', 'pescoco', ',', 'e', ',', 'amarrado', 'a', 'cinta', ',', 'um', 'avental', 'que', 'lhe', 'caia', 'sobre', 'as', 'pernas', 'como', 'uma', 'saia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não fumava, não bebia \n",
      "espíritos e trazia sempre as mãos geladas e úmidas'\n",
      "Tokens gerados: ['nao', 'fumava', ',', 'nao', 'bebia', 'espiritos', 'e', 'trazia', 'sempre', 'as', 'maos', 'geladas', 'e', 'umidas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Naquela manhã levantara-se ainda um pouco mais lânguido que do costume, porque passara mal a noite'\n",
      "Tokens gerados: ['naquela', 'manha', 'levantara-se', 'ainda', 'um', 'pouco', 'mais', 'languido', 'que', 'do', 'costume', ',', 'porque', 'passara', 'mal', 'a', 'noite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A velha \n",
      "Isabel, que lhe ficava ao lado esquerdo, ouvindo-o suspirar com insistência, perguntou-lhe o que tinha'\n",
      "Tokens gerados: ['a', 'velha', 'isabel', ',', 'que', 'lhe', 'ficava', 'ao', 'lado', 'esquerdo', ',', 'ouvindo-o', 'suspirar', 'com', 'insistencia', ',', 'perguntou-lhe', 'o', 'que', 'tinha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah! muita moleza de corpo e uma pontada do vazio que o não deixava! \n",
      "A velha receitou diversos remédios, e ficaram os dois, no meio de toda aquela vida, a falar tristemente sobre \n",
      "moléstias'\n",
      "Tokens gerados: ['ah', 'muita', 'moleza', 'de', 'corpo', 'e', 'uma', 'pontada', 'do', 'vazio', 'que', 'o', 'nao', 'deixava', 'a', 'velha', 'receitou', 'diversos', 'remedios', ',', 'e', 'ficaram', 'os', 'dois', ',', 'no', 'meio', 'de', 'toda', 'aquela', 'vida', ',', 'a', 'falar', 'tristemente', 'sobre', 'molestias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, enquanto, no resto da fileira, a Machona, a Augusta, a Leocádia, a Bruxa, a Marciana e sua filha conversavam \n",
      "de tina a tina, berrando e quase sem se ouvirem, a voz um tanto cansada já pelo serviço, defronte delas, separado pelos \n",
      "jiraus, formava-se um novo renque de lavadeiras, que acudiam de fora, carregadas de trouxas, e iam ruidosamente \n",
      "tomando lagar ao lado umas das outras, entre uma agitação sem tréguas, onde se não distinguia o que era galhofa e o \n",
      "que era briga'\n",
      "Tokens gerados: ['e', ',', 'enquanto', ',', 'no', 'resto', 'da', 'fileira', ',', 'a', 'machona', ',', 'a', 'augusta', ',', 'a', 'leocadia', ',', 'a', 'bruxa', ',', 'a', 'marciana', 'e', 'sua', 'filha', 'conversavam', 'de', 'tina', 'a', 'tina', ',', 'berrando', 'e', 'quase', 'sem', 'se', 'ouvirem', ',', 'a', 'voz', 'um', 'tanto', 'cansada', 'ja', 'pelo', 'servico', ',', 'defronte', 'delas', ',', 'separado', 'pelos', 'jiraus', ',', 'formava-se', 'um', 'novo', 'renque', 'de', 'lavadeiras', ',', 'que', 'acudiam', 'de', 'fora', ',', 'carregadas', 'de', 'trouxas', ',', 'e', 'iam', 'ruidosamente', 'tomando', 'lagar', 'ao', 'lado', 'umas', 'das', 'outras', ',', 'entre', 'uma', 'agitacao', 'sem', 'treguas', ',', 'onde', 'se', 'nao', 'distinguia', 'o', 'que', 'era', 'galhofa', 'e', 'o', 'que', 'era', 'briga']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma a uma ocupavam-se todas as tinas'\n",
      "Tokens gerados: ['uma', 'a', 'uma', 'ocupavam-se', 'todas', 'as', 'tinas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E de todos os casulos do cortiço saiam homens para as suas \n",
      "obrigações'\n",
      "Tokens gerados: ['e', 'de', 'todos', 'os', 'casulos', 'do', 'cortico', 'saiam', 'homens', 'para', 'as', 'suas', 'obrigacoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por uma porta que havia ao fundo da estalagem desapareciam os trabalhadores da pedreira, donde vinha \n",
      "agora o retinir dos alviões e das picaretas'\n",
      "Tokens gerados: ['por', 'uma', 'porta', 'que', 'havia', 'ao', 'fundo', 'da', 'estalagem', 'desapareciam', 'os', 'trabalhadores', 'da', 'pedreira', ',', 'donde', 'vinha', 'agora', 'o', 'retinir', 'dos', 'alvioes', 'e', 'das', 'picaretas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Miranda, de calças de brim, chapéu alto e sobrecasaca preta, passou lá fora, \n",
      "em caminho para o armazém, acompanhado pelo Henrique que ia para as aulas'\n",
      "Tokens gerados: ['o', 'miranda', ',', 'de', 'calcas', 'de', 'brim', ',', 'chapeu', 'alto', 'e', 'sobrecasaca', 'preta', ',', 'passou', 'la', 'fora', ',', 'em', 'caminho', 'para', 'o', 'armazem', ',', 'acompanhado', 'pelo', 'henrique', 'que', 'ia', 'para', 'as', 'aulas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Alexandre, que estivera de serviço \n",
      "essa madrugada, entrou solene, atravessou o pátio, sem falar a ninguém, nem mesmo à mulher, e recolheu-se à casa, \n",
      "para dormir'\n",
      "Tokens gerados: ['o', 'alexandre', ',', 'que', 'estivera', 'de', 'servico', 'essa', 'madrugada', ',', 'entrou', 'solene', ',', 'atravessou', 'o', 'patio', ',', 'sem', 'falar', 'a', 'ninguem', ',', 'nem', 'mesmo', 'a', 'mulher', ',', 'e', 'recolheu-se', 'a', 'casa', ',', 'para', 'dormir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um grupo de mascates, o Delporto, o Pompeo, o Francesco e o Andréa, armado cada qual com a sua \n",
      "grande caixa de bugigangas, saiu para a peregrinação de todos os dias, altercando e praguejando em italiano'\n",
      "Tokens gerados: ['um', 'grupo', 'de', 'mascates', ',', 'o', 'delporto', ',', 'o', 'pompeo', ',', 'o', 'francesco', 'e', 'o', 'andrea', ',', 'armado', 'cada', 'qual', 'com', 'a', 'sua', 'grande', 'caixa', 'de', 'bugigangas', ',', 'saiu', 'para', 'a', 'peregrinacao', 'de', 'todos', 'os', 'dias', ',', 'altercando', 'e', 'praguejando', 'em', 'italiano']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um rapazito de paletó entrou da rua e foi perguntar à Machona pela Nhá Rita'\n",
      "Tokens gerados: ['um', 'rapazito', 'de', 'paleto', 'entrou', 'da', 'rua', 'e', 'foi', 'perguntar', 'a', 'machona', 'pela', 'nha', 'rita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— A Rita Baiana? Sei cá! Faz amanhã oito dias que ela arribou! \n",
      "A Leocádia explicou logo que a mulata estava com certeza de pândega com o Firmo'\n",
      "Tokens gerados: ['—', 'a', 'rita', 'baiana', '?', 'sei', 'ca', 'faz', 'amanha', 'oito', 'dias', 'que', 'ela', 'arribou', 'a', 'leocadia', 'explicou', 'logo', 'que', 'a', 'mulata', 'estava', 'com', 'certeza', 'de', 'pandega', 'com', 'o', 'firmo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que Firmo? interrogou Augusta'\n",
      "Tokens gerados: ['—', 'que', 'firmo', '?', 'interrogou', 'augusta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Aquele cabravasco que se metia às vezes ai com ela'\n",
      "Tokens gerados: ['—', 'aquele', 'cabravasco', 'que', 'se', 'metia', 'as', 'vezes', 'ai', 'com', 'ela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Diz que é torneiro'\n",
      "Tokens gerados: ['diz', 'que', 'e', 'torneiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ela mudou-se? perguntou o pequeno'\n",
      "Tokens gerados: ['—', 'ela', 'mudou-se', '?', 'perguntou', 'o', 'pequeno']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não, disse a Machona; o quarto está fechado, mas a mulata tem coisas lá'\n",
      "Tokens gerados: ['—', 'nao', ',', 'disse', 'a', 'machona', 'o', 'quarto', 'esta', 'fechado', ',', 'mas', 'a', 'mulata', 'tem', 'coisas', 'la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Você o que queria? \n",
      "— Vinha buscar uma roupa que está com ela'\n",
      "Tokens gerados: ['voce', 'o', 'que', 'queria', '?', '—', 'vinha', 'buscar', 'uma', 'roupa', 'que', 'esta', 'com', 'ela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não sei, filho, pergunta na venda ao João Romão, que talvez te possa dizer alguma coisa'\n",
      "Tokens gerados: ['—', 'nao', 'sei', ',', 'filho', ',', 'pergunta', 'na', 'venda', 'ao', 'joao', 'romao', ',', 'que', 'talvez', 'te', 'possa', 'dizer', 'alguma', 'coisa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ali? \n",
      "— Sim, pequeno, naquela porta, onde a preta do tabuleiro está vendendo! Ó diabo! olha que pisas a boneca de \n",
      "anil! Já se viu que sorte? Parece que não vê onde pisa este raio de criança! \n",
      "E, notando que o filho, o Agostinho, se aproximava para tomar o lugar do outro que já se ia: \n",
      "— Sai daí, tu também, peste! Já principias na reinação de todos os dias? Vem para cá, que levas! Mas, é verdade, \n",
      "que fazes tu que não vais regar a horta do Comendador? \n",
      "— Ele disse ontem que eu agora fosse à tarde, que era melhor'\n",
      "Tokens gerados: ['—', 'ali', '?', '—', 'sim', ',', 'pequeno', ',', 'naquela', 'porta', ',', 'onde', 'a', 'preta', 'do', 'tabuleiro', 'esta', 'vendendo', 'o', 'diabo', 'olha', 'que', 'pisas', 'a', 'boneca', 'de', 'anil', 'ja', 'se', 'viu', 'que', 'sorte', '?', 'parece', 'que', 'nao', 've', 'onde', 'pisa', 'este', 'raio', 'de', 'crianca', 'e', ',', 'notando', 'que', 'o', 'filho', ',', 'o', 'agostinho', ',', 'se', 'aproximava', 'para', 'tomar', 'o', 'lugar', 'do', 'outro', 'que', 'ja', 'se', 'ia', '—', 'sai', 'dai', ',', 'tu', 'tambem', ',', 'peste', 'ja', 'principias', 'na', 'reinacao', 'de', 'todos', 'os', 'dias', '?', 'vem', 'para', 'ca', ',', 'que', 'levas', 'mas', ',', 'e', 'verdade', ',', 'que', 'fazes', 'tu', 'que', 'nao', 'vais', 'regar', 'a', 'horta', 'do', 'comendador', '?', '—', 'ele', 'disse', 'ontem', 'que', 'eu', 'agora', 'fosse', 'a', 'tarde', ',', 'que', 'era', 'melhor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah! E amanhã, não te esqueças, recebe os dois mil-réis, que é fim do mês'\n",
      "Tokens gerados: ['—', 'ah', 'e', 'amanha', ',', 'nao', 'te', 'esquecas', ',', 'recebe', 'os', 'dois', 'mil-reis', ',', 'que', 'e', 'fim', 'do', 'mes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Olha! Vai lá dentro e diz a Nenen \n",
      "que te entregue a roupa que veio ontem à noite'\n",
      "Tokens gerados: ['olha', 'vai', 'la', 'dentro', 'e', 'diz', 'a', 'nenen', 'que', 'te', 'entregue', 'a', 'roupa', 'que', 'veio', 'ontem', 'a', 'noite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O pequeno afastou-se de carreira, e ela lhe gritou na pista: \n",
      "— E que não ponha o refogado no fogo sem eu ter lá ido! \n",
      "Uma conversa cerrada travara-se no resto da fila de lavadeiras a respeito da Rita Baiana'\n",
      "Tokens gerados: ['o', 'pequeno', 'afastou-se', 'de', 'carreira', ',', 'e', 'ela', 'lhe', 'gritou', 'na', 'pista', '—', 'e', 'que', 'nao', 'ponha', 'o', 'refogado', 'no', 'fogo', 'sem', 'eu', 'ter', 'la', 'ido', 'uma', 'conversa', 'cerrada', 'travara-se', 'no', 'resto', 'da', 'fila', 'de', 'lavadeiras', 'a', 'respeito', 'da', 'rita', 'baiana']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— É doida mesmo!'\n",
      "Tokens gerados: ['—', 'e', 'doida', 'mesmo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'censurava Augusta'\n",
      "Tokens gerados: ['censurava', 'augusta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Meter-se na pândega sem dar conta da roupa que lhe entregaram'\n",
      "Tokens gerados: ['meter-se', 'na', 'pandega', 'sem', 'dar', 'conta', 'da', 'roupa', 'que', 'lhe', 'entregaram']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Assim há de ficar sem um freguês'\n",
      "Tokens gerados: ['assim', 'ha', 'de', 'ficar', 'sem', 'um', 'fregues']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Aquela não endireita mais!'\n",
      "Tokens gerados: ['—', 'aquela', 'nao', 'endireita', 'mais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cada vez fica até mais assanhada!'\n",
      "Tokens gerados: ['cada', 'vez', 'fica', 'ate', 'mais', 'assanhada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Parece que tem fogo no rabo! Pode haver o \n",
      "serviço que houver, aparecendo pagode, vai tudo pro lado! Olha o que saiu o ano passado com a festa da Penha!'\n",
      "Tokens gerados: ['parece', 'que', 'tem', 'fogo', 'no', 'rabo', 'pode', 'haver', 'o', 'servico', 'que', 'houver', ',', 'aparecendo', 'pagode', ',', 'vai', 'tudo', 'pro', 'lado', 'olha', 'o', 'que', 'saiu', 'o', 'ano', 'passado', 'com', 'a', 'festa', 'da', 'penha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então agora, com este mulato, o Firmo, é uma pouca-vergonha! Est’ro dia, pois você não viu? levaram ai \n",
      "numa bebedeira, a dançar e cantar à viola, que nem sei o que parecia! Deus te livre! \n",
      "— Para tudo há horas e há dias!'\n",
      "Tokens gerados: ['—', 'entao', 'agora', ',', 'com', 'este', 'mulato', ',', 'o', 'firmo', ',', 'e', 'uma', 'pouca-vergonha', 'est', '’', 'ro', 'dia', ',', 'pois', 'voce', 'nao', 'viu', '?', 'levaram', 'ai', 'numa', 'bebedeira', ',', 'a', 'dancar', 'e', 'cantar', 'a', 'viola', ',', 'que', 'nem', 'sei', 'o', 'que', 'parecia', 'deus', 'te', 'livre', '—', 'para', 'tudo', 'ha', 'horas', 'e', 'ha', 'dias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Para a Rita todos os dias são dias santos! A questão é aparecer quem puxe por ela! \n",
      "— Ainda assim não e má criatura'\n",
      "Tokens gerados: ['—', 'para', 'a', 'rita', 'todos', 'os', 'dias', 'sao', 'dias', 'santos', 'a', 'questao', 'e', 'aparecer', 'quem', 'puxe', 'por', 'ela', '—', 'ainda', 'assim', 'nao', 'e', 'ma', 'criatura']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tirante o defeito da vadiagem'\n",
      "Tokens gerados: ['tirante', 'o', 'defeito', 'da', 'vadiagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Bom coração tem ela, até demais, que não guarda um vintém pro dia de amanhã'\n",
      "Tokens gerados: ['—', 'bom', 'coracao', 'tem', 'ela', ',', 'ate', 'demais', ',', 'que', 'nao', 'guarda', 'um', 'vintem', 'pro', 'dia', 'de', 'amanha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Parece que o dinheiro lhe faz \n",
      "comichão no corpo! \n",
      "— Depois é que são elas!'\n",
      "Tokens gerados: ['parece', 'que', 'o', 'dinheiro', 'lhe', 'faz', 'comichao', 'no', 'corpo', '—', 'depois', 'e', 'que', 'sao', 'elas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O João Romão já lhe não fia! \n",
      "— Pois olhe que a Rita lhe tem enchido bem as mãos; quando ela tem dinheiro é porque o gasta mesmo! \n",
      "E as lavadeiras não se calavam, sempre a esfregar, e a bater, e a torcer camisas e ceroulas, esfogueadas já pelo \n",
      "exercício'\n",
      "Tokens gerados: ['o', 'joao', 'romao', 'ja', 'lhe', 'nao', 'fia', '—', 'pois', 'olhe', 'que', 'a', 'rita', 'lhe', 'tem', 'enchido', 'bem', 'as', 'maos', 'quando', 'ela', 'tem', 'dinheiro', 'e', 'porque', 'o', 'gasta', 'mesmo', 'e', 'as', 'lavadeiras', 'nao', 'se', 'calavam', ',', 'sempre', 'a', 'esfregar', ',', 'e', 'a', 'bater', ',', 'e', 'a', 'torcer', 'camisas', 'e', 'ceroulas', ',', 'esfogueadas', 'ja', 'pelo', 'exercicio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao passo que, em torno da sua tagarelice, o cortiço se embandeirava todo de roupa molhada, de onde o sol \n",
      "tirava cintilações de prata'\n",
      "Tokens gerados: ['ao', 'passo', 'que', ',', 'em', 'torno', 'da', 'sua', 'tagarelice', ',', 'o', 'cortico', 'se', 'embandeirava', 'todo', 'de', 'roupa', 'molhada', ',', 'de', 'onde', 'o', 'sol', 'tirava', 'cintilacoes', 'de', 'prata']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estavam em dezembro e o dia era ardente'\n",
      "Tokens gerados: ['estavam', 'em', 'dezembro', 'e', 'o', 'dia', 'era', 'ardente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A grama dos coradouros tinha reflexos esmeraldinos; as paredes que \n",
      "davam frente ao Nascente, caiadinhas de novo, reverberavam iluminadas, ofuscando a vista'\n",
      "Tokens gerados: ['a', 'grama', 'dos', 'coradouros', 'tinha', 'reflexos', 'esmeraldinos', 'as', 'paredes', 'que', 'davam', 'frente', 'ao', 'nascente', ',', 'caiadinhas', 'de', 'novo', ',', 'reverberavam', 'iluminadas', ',', 'ofuscando', 'a', 'vista']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em uma das janelas da sala \n",
      "de jantar do Miranda, Dona Estela e Zulmira, ambas vestidas de claro e ambas a limarem as unhas, conversavam em \n",
      "voz surda, indiferentes à agitação que ia lá embaixo, muito esquecidas na sua tranqüilidade de entes felizes'\n",
      "Tokens gerados: ['em', 'uma', 'das', 'janelas', 'da', 'sala', 'de', 'jantar', 'do', 'miranda', ',', 'dona', 'estela', 'e', 'zulmira', ',', 'ambas', 'vestidas', 'de', 'claro', 'e', 'ambas', 'a', 'limarem', 'as', 'unhas', ',', 'conversavam', 'em', 'voz', 'surda', ',', 'indiferentes', 'a', 'agitacao', 'que', 'ia', 'la', 'embaixo', ',', 'muito', 'esquecidas', 'na', 'sua', 'tranquilidade', 'de', 'entes', 'felizes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, agora o maior movimento era na venda à entrada da estalagem'\n",
      "Tokens gerados: ['entretanto', ',', 'agora', 'o', 'maior', 'movimento', 'era', 'na', 'venda', 'a', 'entrada', 'da', 'estalagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Davam nove horas e os operários das \n",
      "fábricas chegavam-se para o almoço'\n",
      "Tokens gerados: ['davam', 'nove', 'horas', 'e', 'os', 'operarios', 'das', 'fabricas', 'chegavam-se', 'para', 'o', 'almoco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao balcão o Domingos e o Manuel não tinham mãos a medir com a criadagem da \n",
      "vizinhança; os embrulhos de papel amarelo sucediam-se, e o dinheiro pingava sem intermitência dentro da gaveta'\n",
      "Tokens gerados: ['ao', 'balcao', 'o', 'domingos', 'e', 'o', 'manuel', 'nao', 'tinham', 'maos', 'a', 'medir', 'com', 'a', 'criadagem', 'da', 'vizinhanca', 'os', 'embrulhos', 'de', 'papel', 'amarelo', 'sucediam-se', ',', 'e', 'o', 'dinheiro', 'pingava', 'sem', 'intermitencia', 'dentro', 'da', 'gaveta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Meio quilo de arroz! \n",
      "— Um tostão de açúcar! \n",
      "— Uma garrafa de vinagre! \n",
      "— Dois martelos de vinho! \n",
      "— Dois vinténs de fumo! \n",
      "— Quatro de sabão! \n",
      "E os gritos confundiam-se numa mistura de vozes de todos os tons'\n",
      "Tokens gerados: ['—', 'meio', 'quilo', 'de', 'arroz', '—', 'um', 'tostao', 'de', 'acucar', '—', 'uma', 'garrafa', 'de', 'vinagre', '—', 'dois', 'martelos', 'de', 'vinho', '—', 'dois', 'vintens', 'de', 'fumo', '—', 'quatro', 'de', 'sabao', 'e', 'os', 'gritos', 'confundiam-se', 'numa', 'mistura', 'de', 'vozes', 'de', 'todos', 'os', 'tons']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ouviam-se protestos entre os compradores: \n",
      "— Me avie, seu Domingos! Eu deixei a comida no fogo! \n",
      "— Ó peste! dá cá as batatas, que eu tenho mais o que fazer! \n",
      "— Seu Manuel, não me demore essa manteiga! \n",
      "Ao lado, na casinha de pasto, a Bertoleza, de saias arrepanhadas no quadril, o cachaço grosso e negro, reluzindo \n",
      "de suor, ia e vinha de uma panela à outra, fazendo pratos, que João Romão levava de carreira aos trabalhadores \n",
      "assentados num compartimento junto'\n",
      "Tokens gerados: ['ouviam-se', 'protestos', 'entre', 'os', 'compradores', '—', 'me', 'avie', ',', 'seu', 'domingos', 'eu', 'deixei', 'a', 'comida', 'no', 'fogo', '—', 'o', 'peste', 'da', 'ca', 'as', 'batatas', ',', 'que', 'eu', 'tenho', 'mais', 'o', 'que', 'fazer', '—', 'seu', 'manuel', ',', 'nao', 'me', 'demore', 'essa', 'manteiga', 'ao', 'lado', ',', 'na', 'casinha', 'de', 'pasto', ',', 'a', 'bertoleza', ',', 'de', 'saias', 'arrepanhadas', 'no', 'quadril', ',', 'o', 'cachaco', 'grosso', 'e', 'negro', ',', 'reluzindo', 'de', 'suor', ',', 'ia', 'e', 'vinha', 'de', 'uma', 'panela', 'a', 'outra', ',', 'fazendo', 'pratos', ',', 'que', 'joao', 'romao', 'levava', 'de', 'carreira', 'aos', 'trabalhadores', 'assentados', 'num', 'compartimento', 'junto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Admitira-se um novo caixeiro, só para o frege, e o rapaz, a cada comensal que ia \n",
      "chegando, recitava, em tom cantado e estridente, a sua interminável lista das comidas que havia'\n",
      "Tokens gerados: ['admitira-se', 'um', 'novo', 'caixeiro', ',', 'so', 'para', 'o', 'frege', ',', 'e', 'o', 'rapaz', ',', 'a', 'cada', 'comensal', 'que', 'ia', 'chegando', ',', 'recitava', ',', 'em', 'tom', 'cantado', 'e', 'estridente', ',', 'a', 'sua', 'interminavel', 'lista', 'das', 'comidas', 'que', 'havia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um cheiro forte de \n",
      "azeite frito predominava'\n",
      "Tokens gerados: ['um', 'cheiro', 'forte', 'de', 'azeite', 'frito', 'predominava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O parati circulava por todas as mesas, e cada caneca de café, de louça espessa, erguia um \n",
      "vulcão de fumo tresandando a milho queimado'\n",
      "Tokens gerados: ['o', 'parati', 'circulava', 'por', 'todas', 'as', 'mesas', ',', 'e', 'cada', 'caneca', 'de', 'cafe', ',', 'de', 'louca', 'espessa', ',', 'erguia', 'um', 'vulcao', 'de', 'fumo', 'tresandando', 'a', 'milho', 'queimado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma algazarra medonha, em que ninguém se entendia! Cruzavam-se \n",
      "conversas em todas as direções, discutia-se a berros, com valentes punhadas sobre as mesas'\n",
      "Tokens gerados: ['uma', 'algazarra', 'medonha', ',', 'em', 'que', 'ninguem', 'se', 'entendia', 'cruzavam-se', 'conversas', 'em', 'todas', 'as', 'direcoes', ',', 'discutia-se', 'a', 'berros', ',', 'com', 'valentes', 'punhadas', 'sobre', 'as', 'mesas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E sempre a sair, e sempre a \n",
      "entrar gente, e os que saiam, depois daquela comezaina grossa, iam radiantes de contentamento, com a barriga bem \n",
      "cheia, a arrotar'\n",
      "Tokens gerados: ['e', 'sempre', 'a', 'sair', ',', 'e', 'sempre', 'a', 'entrar', 'gente', ',', 'e', 'os', 'que', 'saiam', ',', 'depois', 'daquela', 'comezaina', 'grossa', ',', 'iam', 'radiantes', 'de', 'contentamento', ',', 'com', 'a', 'barriga', 'bem', 'cheia', ',', 'a', 'arrotar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Num banco de pau tosco, que existia do lado de fora, junto à parede e perto da venda, um homem, de calça e \n",
      "camisa de zuarte, chinelos de couro cru, esperava, havia já uma boa hora, para falar com o vendeiro'\n",
      "Tokens gerados: ['num', 'banco', 'de', 'pau', 'tosco', ',', 'que', 'existia', 'do', 'lado', 'de', 'fora', ',', 'junto', 'a', 'parede', 'e', 'perto', 'da', 'venda', ',', 'um', 'homem', ',', 'de', 'calca', 'e', 'camisa', 'de', 'zuarte', ',', 'chinelos', 'de', 'couro', 'cru', ',', 'esperava', ',', 'havia', 'ja', 'uma', 'boa', 'hora', ',', 'para', 'falar', 'com', 'o', 'vendeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era um português de seus trinta e cinco a quarenta anos, alto, espadaúdo, barbas ásperas, cabelos pretos e \n",
      "maltratados caindo-lhe sobre a testa, por debaixo de um chapéu de feltro ordinário: pescoço de touro e cara de Hércules, \n",
      "na qual os olhos todavia, humildes como os olhos de um boi de canga, exprimiam tranqüila bondade'\n",
      "Tokens gerados: ['era', 'um', 'portugues', 'de', 'seus', 'trinta', 'e', 'cinco', 'a', 'quarenta', 'anos', ',', 'alto', ',', 'espadaudo', ',', 'barbas', 'asperas', ',', 'cabelos', 'pretos', 'e', 'maltratados', 'caindo-lhe', 'sobre', 'a', 'testa', ',', 'por', 'debaixo', 'de', 'um', 'chapeu', 'de', 'feltro', 'ordinario', 'pescoco', 'de', 'touro', 'e', 'cara', 'de', 'hercules', ',', 'na', 'qual', 'os', 'olhos', 'todavia', ',', 'humildes', 'como', 'os', 'olhos', 'de', 'um', 'boi', 'de', 'canga', ',', 'exprimiam', 'tranquila', 'bondade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então ainda não se pode falar ao homem? perguntou ele, indo ao balcão entender-se com o Domingos'\n",
      "Tokens gerados: ['—', 'entao', 'ainda', 'nao', 'se', 'pode', 'falar', 'ao', 'homem', '?', 'perguntou', 'ele', ',', 'indo', 'ao', 'balcao', 'entender-se', 'com', 'o', 'domingos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— O patrão está agora muito ocupado'\n",
      "Tokens gerados: ['—', 'o', 'patrao', 'esta', 'agora', 'muito', 'ocupado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Espere! \n",
      "— Mas são quase dez horas e estou com um gole de café no estômago! \n",
      "— Volte logo! \n",
      "— Moro na cidade nova'\n",
      "Tokens gerados: ['espere', '—', 'mas', 'sao', 'quase', 'dez', 'horas', 'e', 'estou', 'com', 'um', 'gole', 'de', 'cafe', 'no', 'estomago', '—', 'volte', 'logo', '—', 'moro', 'na', 'cidade', 'nova']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É um estirão daqui! \n",
      "O caixeiro gritou então para a cozinha, sem interromper o que fazia: \n",
      "— O homem que ai está, seu João, diz que se vai embora! \n",
      "— Ele que espere um pouco, que já lhe falo! respondeu o vendeiro no meio de uma carreira'\n",
      "Tokens gerados: ['e', 'um', 'estirao', 'daqui', 'o', 'caixeiro', 'gritou', 'entao', 'para', 'a', 'cozinha', ',', 'sem', 'interromper', 'o', 'que', 'fazia', '—', 'o', 'homem', 'que', 'ai', 'esta', ',', 'seu', 'joao', ',', 'diz', 'que', 'se', 'vai', 'embora', '—', 'ele', 'que', 'espere', 'um', 'pouco', ',', 'que', 'ja', 'lhe', 'falo', 'respondeu', 'o', 'vendeiro', 'no', 'meio', 'de', 'uma', 'carreira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Diga-lhe que não vá! \n",
      "— Mas é que ainda não almocei e estou aqui a tinir!'\n",
      "Tokens gerados: ['diga-lhe', 'que', 'nao', 'va', '—', 'mas', 'e', 'que', 'ainda', 'nao', 'almocei', 'e', 'estou', 'aqui', 'a', 'tinir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'observou o Hércules com a sua voz grossa e sonora'\n",
      "Tokens gerados: ['observou', 'o', 'hercules', 'com', 'a', 'sua', 'voz', 'grossa', 'e', 'sonora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ó filho, almoce ai mesmo! Aqui o que não falta é de comer'\n",
      "Tokens gerados: ['—', 'o', 'filho', ',', 'almoce', 'ai', 'mesmo', 'aqui', 'o', 'que', 'nao', 'falta', 'e', 'de', 'comer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Já podia estar aviado! \n",
      "— Pois vá lá! resolveu o homenzarrão, saindo da venda para entrar na casa de pasto, onde os que lá se achavam o \n",
      "receberam com ar curioso, medindo-o da cabeça aos pés, como faziam sempre com todos os que ai se apresentavam \n",
      "pela primeira vez'\n",
      "Tokens gerados: ['ja', 'podia', 'estar', 'aviado', '—', 'pois', 'va', 'la', 'resolveu', 'o', 'homenzarrao', ',', 'saindo', 'da', 'venda', 'para', 'entrar', 'na', 'casa', 'de', 'pasto', ',', 'onde', 'os', 'que', 'la', 'se', 'achavam', 'o', 'receberam', 'com', 'ar', 'curioso', ',', 'medindo-o', 'da', 'cabeca', 'aos', 'pes', ',', 'como', 'faziam', 'sempre', 'com', 'todos', 'os', 'que', 'ai', 'se', 'apresentavam', 'pela', 'primeira', 'vez']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E assentou-se a uma das mesinhas, vindo logo o caixeiro cantar-lhe a lista dos pratos'\n",
      "Tokens gerados: ['e', 'assentou-se', 'a', 'uma', 'das', 'mesinhas', ',', 'vindo', 'logo', 'o', 'caixeiro', 'cantar-lhe', 'a', 'lista', 'dos', 'pratos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Traga lá o pescado com batatas e veja um martelo de vinho'\n",
      "Tokens gerados: ['—', 'traga', 'la', 'o', 'pescado', 'com', 'batatas', 'e', 'veja', 'um', 'martelo', 'de', 'vinho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Quer verde ou virgem? \n",
      "— Venha o verde; mas anda com isso, filho, que já não vem sem tempo!'\n",
      "Tokens gerados: ['—', 'quer', 'verde', 'ou', 'virgem', '?', '—', 'venha', 'o', 'verde', 'mas', 'anda', 'com', 'isso', ',', 'filho', ',', 'que', 'ja', 'nao', 'vem', 'sem', 'tempo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_o_cortico_aluisio_azevedo_cap_3.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Meia hora depois, quando João Romão se viu menos ocupado, foi ter com o sujeito que o procurava e assentou-se \n",
      "defronte dele, caindo de fadiga, mas sem se queixar, nem se lhe trair a fisionomia o menor sintoma de cansaço'\n",
      "Tokens gerados: ['meia', 'hora', 'depois', ',', 'quando', 'joao', 'romao', 'se', 'viu', 'menos', 'ocupado', ',', 'foi', 'ter', 'com', 'o', 'sujeito', 'que', 'o', 'procurava', 'e', 'assentou-se', 'defronte', 'dele', ',', 'caindo', 'de', 'fadiga', ',', 'mas', 'sem', 'se', 'queixar', ',', 'nem', 'se', 'lhe', 'trair', 'a', 'fisionomia', 'o', 'menor', 'sintoma', 'de', 'cansaco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Você vem da parte do Machucas? perguntou-lhe'\n",
      "Tokens gerados: ['—', 'voce', 'vem', 'da', 'parte', 'do', 'machucas', '?', 'perguntou-lhe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ele falou-me de um homem que sabe calçar pedra, lascar \n",
      "fogo e fazer lajedo'\n",
      "Tokens gerados: ['ele', 'falou-me', 'de', 'um', 'homem', 'que', 'sabe', 'calcar', 'pedra', ',', 'lascar', 'fogo', 'e', 'fazer', 'lajedo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Sou eu'\n",
      "Tokens gerados: ['—', 'sou', 'eu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Estava empregado em outra pedreira? \n",
      "— Estava e estou'\n",
      "Tokens gerados: ['—', 'estava', 'empregado', 'em', 'outra', 'pedreira', '?', '—', 'estava', 'e', 'estou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na de São Diogo, mas desgostei-me dela e quero passar adiante'\n",
      "Tokens gerados: ['na', 'de', 'sao', 'diogo', ',', 'mas', 'desgostei-me', 'dela', 'e', 'quero', 'passar', 'adiante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Quanto lhe dão lá? \n",
      "— Setenta mil-réis'\n",
      "Tokens gerados: ['—', 'quanto', 'lhe', 'dao', 'la', '?', '—', 'setenta', 'mil-reis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Oh! Isso é um disparate! \n",
      "— Não trabalho por menos'\n",
      "Tokens gerados: ['—', 'oh', 'isso', 'e', 'um', 'disparate', '—', 'nao', 'trabalho', 'por', 'menos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Eu, o maior ordenado que faço é de cinqüenta'\n",
      "Tokens gerados: ['—', 'eu', ',', 'o', 'maior', 'ordenado', 'que', 'faco', 'e', 'de', 'cinquenta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Cinqüenta ganha um macaqueiro'\n",
      "Tokens gerados: ['—', 'cinquenta', 'ganha', 'um', 'macaqueiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora! tenho aí muitos trabalhadores de lajedo por esse preço! \n",
      "— Duvido que prestem! Aposto a mão direita em como o senhor não encontra por cinqüenta mil-réis quem dirija \n",
      "a broca, pese a pólvora e lasque fogo, sem lhe estragar a pedra e sem fazer desastres! \n",
      "— Sim, mas setenta mil-réis é um ordenado impossível! \n",
      "— Nesse caso vou como vim'\n",
      "Tokens gerados: ['—', 'ora', 'tenho', 'ai', 'muitos', 'trabalhadores', 'de', 'lajedo', 'por', 'esse', 'preco', '—', 'duvido', 'que', 'prestem', 'aposto', 'a', 'mao', 'direita', 'em', 'como', 'o', 'senhor', 'nao', 'encontra', 'por', 'cinquenta', 'mil-reis', 'quem', 'dirija', 'a', 'broca', ',', 'pese', 'a', 'polvora', 'e', 'lasque', 'fogo', ',', 'sem', 'lhe', 'estragar', 'a', 'pedra', 'e', 'sem', 'fazer', 'desastres', '—', 'sim', ',', 'mas', 'setenta', 'mil-reis', 'e', 'um', 'ordenado', 'impossivel', '—', 'nesse', 'caso', 'vou', 'como', 'vim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fica o dito por não dito! \n",
      "— Setenta mil-réis é muito dinheiro!'\n",
      "Tokens gerados: ['fica', 'o', 'dito', 'por', 'nao', 'dito', '—', 'setenta', 'mil-reis', 'e', 'muito', 'dinheiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Cá por mim, entendo que vale a pena pagar mais um pouco a um trabalhador bom, do que estar a sofrer \n",
      "desastres, como o que sofreu sua pedreira a semana passada! Não falando na vida do pobre de Cristo que ficou debaixo \n",
      "da pedra! \n",
      "— Ah! O Machucas falou-lhe no desastre? \n",
      "— Contou-mo, sim senhor, e o desastre não aconteceria se o homem soubesse fazer o serviço! \n",
      "— Mas setenta mil-réis é impossível'\n",
      "Tokens gerados: ['—', 'ca', 'por', 'mim', ',', 'entendo', 'que', 'vale', 'a', 'pena', 'pagar', 'mais', 'um', 'pouco', 'a', 'um', 'trabalhador', 'bom', ',', 'do', 'que', 'estar', 'a', 'sofrer', 'desastres', ',', 'como', 'o', 'que', 'sofreu', 'sua', 'pedreira', 'a', 'semana', 'passada', 'nao', 'falando', 'na', 'vida', 'do', 'pobre', 'de', 'cristo', 'que', 'ficou', 'debaixo', 'da', 'pedra', '—', 'ah', 'o', 'machucas', 'falou-lhe', 'no', 'desastre', '?', '—', 'contou-mo', ',', 'sim', 'senhor', ',', 'e', 'o', 'desastre', 'nao', 'aconteceria', 'se', 'o', 'homem', 'soubesse', 'fazer', 'o', 'servico', '—', 'mas', 'setenta', 'mil-reis', 'e', 'impossivel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desça um pouco! \n",
      "— Por menos não me serve'\n",
      "Tokens gerados: ['desca', 'um', 'pouco', '—', 'por', 'menos', 'nao', 'me', 'serve']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E escusamos de gastar palavras! \n",
      "— Você conhece a pedreira? \n",
      "— Nunca a vi de perto, mas quis me parecer que é boa'\n",
      "Tokens gerados: ['e', 'escusamos', 'de', 'gastar', 'palavras', '—', 'voce', 'conhece', 'a', 'pedreira', '?', '—', 'nunca', 'a', 'vi', 'de', 'perto', ',', 'mas', 'quis', 'me', 'parecer', 'que', 'e', 'boa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De longe cheirou-me a granito'\n",
      "Tokens gerados: ['de', 'longe', 'cheirou-me', 'a', 'granito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Espere um instante'\n",
      "Tokens gerados: ['—', 'espere', 'um', 'instante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão deu um pulo à venda, deixou algumas ordens, enterrou um chapéu na cabeça e voltou a ter com o \n",
      "outro'\n",
      "Tokens gerados: ['joao', 'romao', 'deu', 'um', 'pulo', 'a', 'venda', ',', 'deixou', 'algumas', 'ordens', ',', 'enterrou', 'um', 'chapeu', 'na', 'cabeca', 'e', 'voltou', 'a', 'ter', 'com', 'o', 'outro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ande a ver! gritou-lhe da porta do frege, que a pouco e pouco se esvaziara de todo'\n",
      "Tokens gerados: ['—', 'ande', 'a', 'ver', 'gritou-lhe', 'da', 'porta', 'do', 'frege', ',', 'que', 'a', 'pouco', 'e', 'pouco', 'se', 'esvaziara', 'de', 'todo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cavouqueiro pagou doze vinténs pelo seu almoço e acompanhou-o em silêncio'\n",
      "Tokens gerados: ['o', 'cavouqueiro', 'pagou', 'doze', 'vintens', 'pelo', 'seu', 'almoco', 'e', 'acompanhou-o', 'em', 'silencio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Atravessaram o cortiço'\n",
      "Tokens gerados: ['atravessaram', 'o', 'cortico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A labutação continuava'\n",
      "Tokens gerados: ['a', 'labutacao', 'continuava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As lavadeiras tinham já ido almoçar e tinham voltado de novo para o trabalho'\n",
      "Tokens gerados: ['as', 'lavadeiras', 'tinham', 'ja', 'ido', 'almocar', 'e', 'tinham', 'voltado', 'de', 'novo', 'para', 'o', 'trabalho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Agora \n",
      "estavam todas de chapéu de palha, apesar das toldas que se armaram'\n",
      "Tokens gerados: ['agora', 'estavam', 'todas', 'de', 'chapeu', 'de', 'palha', ',', 'apesar', 'das', 'toldas', 'que', 'se', 'armaram']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um calor de cáustico mordia-lhes os toutiços em \n",
      "brasa e cintilantes de suor'\n",
      "Tokens gerados: ['um', 'calor', 'de', 'caustico', 'mordia-lhes', 'os', 'touticos', 'em', 'brasa', 'e', 'cintilantes', 'de', 'suor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um estado febril apoderava-se delas naquele rescaldo; aquela digestão feita ao sol \n",
      "fermentava-lhes o sangue'\n",
      "Tokens gerados: ['um', 'estado', 'febril', 'apoderava-se', 'delas', 'naquele', 'rescaldo', 'aquela', 'digestao', 'feita', 'ao', 'sol', 'fermentava-lhes', 'o', 'sangue']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A Machona altercava com uma preta que fora reclamar um par de meias e destrocar uma \n",
      "camisa; a Augusta, muito mole sobre a sua tábua de lavar, parecia derreter-se como sebo; a Leocádia largava de vez em \n",
      "quando a roupa e o sabão para coçar as comichões do quadril e das virilhas, assanhadas pelo mormaço; a Bruxa \n",
      "monologava, resmungando numa insistência de idiota, ao lado da Marciana que, com o seu tipo de mulata velha, um \n",
      "cachimbo ao canto da boca, cantava toadas monótonas do sertão: \n",
      " \n",
      "“Maricas tá marimbando, \n",
      "Maricas tá marimbando, \n",
      "Na passage do riacho \n",
      "Maricas tá marimbando'\n",
      "Tokens gerados: ['a', 'machona', 'altercava', 'com', 'uma', 'preta', 'que', 'fora', 'reclamar', 'um', 'par', 'de', 'meias', 'e', 'destrocar', 'uma', 'camisa', 'a', 'augusta', ',', 'muito', 'mole', 'sobre', 'a', 'sua', 'tabua', 'de', 'lavar', ',', 'parecia', 'derreter-se', 'como', 'sebo', 'a', 'leocadia', 'largava', 'de', 'vez', 'em', 'quando', 'a', 'roupa', 'e', 'o', 'sabao', 'para', 'cocar', 'as', 'comichoes', 'do', 'quadril', 'e', 'das', 'virilhas', ',', 'assanhadas', 'pelo', 'mormaco', 'a', 'bruxa', 'monologava', ',', 'resmungando', 'numa', 'insistencia', 'de', 'idiota', ',', 'ao', 'lado', 'da', 'marciana', 'que', ',', 'com', 'o', 'seu', 'tipo', 'de', 'mulata', 'velha', ',', 'um', 'cachimbo', 'ao', 'canto', 'da', 'boca', ',', 'cantava', 'toadas', 'monotonas', 'do', 'sertao', '“', 'maricas', 'ta', 'marimbando', ',', 'maricas', 'ta', 'marimbando', ',', 'na', 'passage', 'do', 'riacho', 'maricas', 'ta', 'marimbando']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” \n",
      " \n",
      "A Florinda, alegre, perfeitamente bem com o rigor do sol, a rebolar sem fadigas, assoviava os chorados e lundus \n",
      "que se tocavam na estalagem, e junto dela, a melancólica senhora Dona Isabel suspirava, esfregando a sua roupa dentro \n",
      "da tina, automaticamente, como um condenado a trabalhar no presídio; ao passo que o Albino, saracoteando os seus \n",
      "quadris pobres de homem linfático, batia na tábua um par de calças, no ritmo cadenciado e miúdo de um cozinheiro a \n",
      "bater bifes'\n",
      "Tokens gerados: ['”', 'a', 'florinda', ',', 'alegre', ',', 'perfeitamente', 'bem', 'com', 'o', 'rigor', 'do', 'sol', ',', 'a', 'rebolar', 'sem', 'fadigas', ',', 'assoviava', 'os', 'chorados', 'e', 'lundus', 'que', 'se', 'tocavam', 'na', 'estalagem', ',', 'e', 'junto', 'dela', ',', 'a', 'melancolica', 'senhora', 'dona', 'isabel', 'suspirava', ',', 'esfregando', 'a', 'sua', 'roupa', 'dentro', 'da', 'tina', ',', 'automaticamente', ',', 'como', 'um', 'condenado', 'a', 'trabalhar', 'no', 'presidio', 'ao', 'passo', 'que', 'o', 'albino', ',', 'saracoteando', 'os', 'seus', 'quadris', 'pobres', 'de', 'homem', 'linfatico', ',', 'batia', 'na', 'tabua', 'um', 'par', 'de', 'calcas', ',', 'no', 'ritmo', 'cadenciado', 'e', 'miudo', 'de', 'um', 'cozinheiro', 'a', 'bater', 'bifes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O corpo tremia-lhe todo, e ele, de vez em quando, suspendia o lenço do pescoço para enxugar a fronte, e \n",
      "então um gemido suspirado subia-lhe aos lábios'\n",
      "Tokens gerados: ['o', 'corpo', 'tremia-lhe', 'todo', ',', 'e', 'ele', ',', 'de', 'vez', 'em', 'quando', ',', 'suspendia', 'o', 'lenco', 'do', 'pescoco', 'para', 'enxugar', 'a', 'fronte', ',', 'e', 'entao', 'um', 'gemido', 'suspirado', 'subia-lhe', 'aos', 'labios']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Da casinha número 8 vinha um falsete agudo, mas afinado'\n",
      "Tokens gerados: ['da', 'casinha', 'numero', '8', 'vinha', 'um', 'falsete', 'agudo', ',', 'mas', 'afinado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era a das Dores que principiava o seu serviço; não \n",
      "sabia engomar sem cantar'\n",
      "Tokens gerados: ['era', 'a', 'das', 'dores', 'que', 'principiava', 'o', 'seu', 'servico', 'nao', 'sabia', 'engomar', 'sem', 'cantar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No número 7 Nenen cantarolava em tom muito mais baixo; e de um dos quartos do fundo da \n",
      "estalagem saia de espaço a espaço uma nota áspera de trombone'\n",
      "Tokens gerados: ['no', 'numero', '7', 'nenen', 'cantarolava', 'em', 'tom', 'muito', 'mais', 'baixo', 'e', 'de', 'um', 'dos', 'quartos', 'do', 'fundo', 'da', 'estalagem', 'saia', 'de', 'espaco', 'a', 'espaco', 'uma', 'nota', 'aspera', 'de', 'trombone']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O vendeiro, ao passar por detrás de Florinda, que no momento apanhava roupa do chão, ferrou-lhe uma palmada \n",
      "na parte do corpo então mais em evidência'\n",
      "Tokens gerados: ['o', 'vendeiro', ',', 'ao', 'passar', 'por', 'detras', 'de', 'florinda', ',', 'que', 'no', 'momento', 'apanhava', 'roupa', 'do', 'chao', ',', 'ferrou-lhe', 'uma', 'palmada', 'na', 'parte', 'do', 'corpo', 'entao', 'mais', 'em', 'evidencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não bula, hein?!'\n",
      "Tokens gerados: ['—', 'nao', 'bula', ',', 'hein', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'gritou ela, rápido, erguendo-se tesa'\n",
      "Tokens gerados: ['gritou', 'ela', ',', 'rapido', ',', 'erguendo-se', 'tesa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, dando com João Romão: \n",
      "— Eu logo vi'\n",
      "Tokens gerados: ['e', ',', 'dando', 'com', 'joao', 'romao', '—', 'eu', 'logo', 'vi']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Leva implicando aqui com a gente e depois, vai-se comprar na venda, o safado rouba no peso! \n",
      "Diabo do galego  Eu não te quero, sabe? \n",
      "O vendeiro soltou-lhe nova palmada com mais força e fugiu, porque ela se armara com um regador cheio de água'\n",
      "Tokens gerados: ['leva', 'implicando', 'aqui', 'com', 'a', 'gente', 'e', 'depois', ',', 'vai-se', 'comprar', 'na', 'venda', ',', 'o', 'safado', 'rouba', 'no', 'peso', 'diabo', 'do', 'galego', 'eu', 'nao', 'te', 'quero', ',', 'sabe', '?', 'o', 'vendeiro', 'soltou-lhe', 'nova', 'palmada', 'com', 'mais', 'forca', 'e', 'fugiu', ',', 'porque', 'ela', 'se', 'armara', 'com', 'um', 'regador', 'cheio', 'de', 'agua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Vem pra cá, se és capaz! Diabo da peste! \n",
      "João Romão já se havia afastado com o cavouqueiro'\n",
      "Tokens gerados: ['—', 'vem', 'pra', 'ca', ',', 'se', 'es', 'capaz', 'diabo', 'da', 'peste', 'joao', 'romao', 'ja', 'se', 'havia', 'afastado', 'com', 'o', 'cavouqueiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— O senhor tem aqui muita gente!'\n",
      "Tokens gerados: ['—', 'o', 'senhor', 'tem', 'aqui', 'muita', 'gente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'observou-lhe este'\n",
      "Tokens gerados: ['observou-lhe', 'este']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Oh! fez o outro, sacudindo os ombros, e disse depois com empáfia: — Houvesse mais cem quartos que \n",
      "estariam cheios! Mas é tudo gente séria! Não há chinfrins nesta estalagem; se aparece uma rusga, eu chego, e tudo \n",
      "acaba logo! Nunca nos entrou cá a policia, nem nunca a deixaremos entrar! E olhe que se divertem bem com as suas \n",
      "violas! Tudo gente muita boa! \n",
      "Tinham chegado ao fim do pátio do cortiço e, depois de transporem uma porta que se fechava com um peso \n",
      "amarrado a uma corda, acharam-se no capinzal que havia antes da pedreira'\n",
      "Tokens gerados: ['—', 'oh', 'fez', 'o', 'outro', ',', 'sacudindo', 'os', 'ombros', ',', 'e', 'disse', 'depois', 'com', 'empafia', '—', 'houvesse', 'mais', 'cem', 'quartos', 'que', 'estariam', 'cheios', 'mas', 'e', 'tudo', 'gente', 'seria', 'nao', 'ha', 'chinfrins', 'nesta', 'estalagem', 'se', 'aparece', 'uma', 'rusga', ',', 'eu', 'chego', ',', 'e', 'tudo', 'acaba', 'logo', 'nunca', 'nos', 'entrou', 'ca', 'a', 'policia', ',', 'nem', 'nunca', 'a', 'deixaremos', 'entrar', 'e', 'olhe', 'que', 'se', 'divertem', 'bem', 'com', 'as', 'suas', 'violas', 'tudo', 'gente', 'muita', 'boa', 'tinham', 'chegado', 'ao', 'fim', 'do', 'patio', 'do', 'cortico', 'e', ',', 'depois', 'de', 'transporem', 'uma', 'porta', 'que', 'se', 'fechava', 'com', 'um', 'peso', 'amarrado', 'a', 'uma', 'corda', ',', 'acharam-se', 'no', 'capinzal', 'que', 'havia', 'antes', 'da', 'pedreira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Vamos por aqui mesmo que é mais perto, aconselhou o vendeiro'\n",
      "Tokens gerados: ['—', 'vamos', 'por', 'aqui', 'mesmo', 'que', 'e', 'mais', 'perto', ',', 'aconselhou', 'o', 'vendeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E os dois, em vez de procurarem a estrada, atravessaram o capim quente e trescalante'\n",
      "Tokens gerados: ['e', 'os', 'dois', ',', 'em', 'vez', 'de', 'procurarem', 'a', 'estrada', ',', 'atravessaram', 'o', 'capim', 'quente', 'e', 'trescalante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Meio-dia em ponto'\n",
      "Tokens gerados: ['meio-dia', 'em', 'ponto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O sol estava a pino; tudo reverberava a luz irreconciliável de dezembro, num dia sem nuvens'\n",
      "Tokens gerados: ['o', 'sol', 'estava', 'a', 'pino', 'tudo', 'reverberava', 'a', 'luz', 'irreconciliavel', 'de', 'dezembro', ',', 'num', 'dia', 'sem', 'nuvens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A pedreira, em que ela batia de chapa em cima, cegava olhada de frente'\n",
      "Tokens gerados: ['a', 'pedreira', ',', 'em', 'que', 'ela', 'batia', 'de', 'chapa', 'em', 'cima', ',', 'cegava', 'olhada', 'de', 'frente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era preciso martirizar a vista para descobrir as \n",
      "nuanças da pedra; nada mais que uma grande mancha branca e luminosa, terminando pela parte de baixo no chão \n",
      "coberto de cascalho miúdo, que ao longe produzia o efeito de um betume cinzento, e pela parte de cima na espessura \n",
      "compacta do arvoredo, onde se não distinguiam outros tons mais do que nódoas negras, bem negras, sobre o \n",
      "verde-escuro'\n",
      "Tokens gerados: ['era', 'preciso', 'martirizar', 'a', 'vista', 'para', 'descobrir', 'as', 'nuancas', 'da', 'pedra', 'nada', 'mais', 'que', 'uma', 'grande', 'mancha', 'branca', 'e', 'luminosa', ',', 'terminando', 'pela', 'parte', 'de', 'baixo', 'no', 'chao', 'coberto', 'de', 'cascalho', 'miudo', ',', 'que', 'ao', 'longe', 'produzia', 'o', 'efeito', 'de', 'um', 'betume', 'cinzento', ',', 'e', 'pela', 'parte', 'de', 'cima', 'na', 'espessura', 'compacta', 'do', 'arvoredo', ',', 'onde', 'se', 'nao', 'distinguiam', 'outros', 'tons', 'mais', 'do', 'que', 'nodoas', 'negras', ',', 'bem', 'negras', ',', 'sobre', 'o', 'verde-escuro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'À proporção que os dois se aproximavam da imponente pedreira, o terreno ia-se tornando mais e mais cascalhudo; \n",
      "os sapatos enfarinhavam-se de uma poeira clara'\n",
      "Tokens gerados: ['a', 'proporcao', 'que', 'os', 'dois', 'se', 'aproximavam', 'da', 'imponente', 'pedreira', ',', 'o', 'terreno', 'ia-se', 'tornando', 'mais', 'e', 'mais', 'cascalhudo', 'os', 'sapatos', 'enfarinhavam-se', 'de', 'uma', 'poeira', 'clara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mais adiante, por aqui e por ali, havia muitas carroças, algumas em \n",
      "movimento, puxadas a burro e cheias de calhaus partidos; outras já prontas para seguir, à espera do animal, e outras \n",
      "enfim com os braços para o ar, como se acabassem de ser despejadas naquele instante'\n",
      "Tokens gerados: ['mais', 'adiante', ',', 'por', 'aqui', 'e', 'por', 'ali', ',', 'havia', 'muitas', 'carrocas', ',', 'algumas', 'em', 'movimento', ',', 'puxadas', 'a', 'burro', 'e', 'cheias', 'de', 'calhaus', 'partidos', 'outras', 'ja', 'prontas', 'para', 'seguir', ',', 'a', 'espera', 'do', 'animal', ',', 'e', 'outras', 'enfim', 'com', 'os', 'bracos', 'para', 'o', 'ar', ',', 'como', 'se', 'acabassem', 'de', 'ser', 'despejadas', 'naquele', 'instante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Homens labutavam'\n",
      "Tokens gerados: ['homens', 'labutavam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'À esquerda, por cima de um vestígio de rio, que parecia ter sido bebido de um trago por aquele sol sedento, havia \n",
      "uma ponte de tábuas, onde três pequenos, quase nus, conversavam assentados, sem fazer sombra, iluminados a prumo \n",
      "pelo sol do meio-dia'\n",
      "Tokens gerados: ['a', 'esquerda', ',', 'por', 'cima', 'de', 'um', 'vestigio', 'de', 'rio', ',', 'que', 'parecia', 'ter', 'sido', 'bebido', 'de', 'um', 'trago', 'por', 'aquele', 'sol', 'sedento', ',', 'havia', 'uma', 'ponte', 'de', 'tabuas', ',', 'onde', 'tres', 'pequenos', ',', 'quase', 'nus', ',', 'conversavam', 'assentados', ',', 'sem', 'fazer', 'sombra', ',', 'iluminados', 'a', 'prumo', 'pelo', 'sol', 'do', 'meio-dia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Para adiante, na mesma direção, corria um vasto telheiro, velho e sujo, firmado sobre colunas de \n",
      "pedra tosca; ai muitos portugueses trabalhavam de canteiro, ao barulho metálico do picão que feria o granito'\n",
      "Tokens gerados: ['para', 'adiante', ',', 'na', 'mesma', 'direcao', ',', 'corria', 'um', 'vasto', 'telheiro', ',', 'velho', 'e', 'sujo', ',', 'firmado', 'sobre', 'colunas', 'de', 'pedra', 'tosca', 'ai', 'muitos', 'portugueses', 'trabalhavam', 'de', 'canteiro', ',', 'ao', 'barulho', 'metalico', 'do', 'picao', 'que', 'feria', 'o', 'granito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Logo em \n",
      "seguida, surgia uma oficina de ferreiro, toda atravancada de destroços e objetos quebrados, entre os quais avultavam \n",
      "rodas de carro; em volta da bigorna dois homens, de corpo nu, banhados de suor e alumiados de vermelho como dois \n",
      "diabos, martelavam cadenciosamente sobre um pedaço de ferro em brasa; e ali mesmo, perto deles, a forja escancarava \n",
      "uma goela infernal, de onde saiam pequenas línguas de fogo, irrequietas e gulosas'\n",
      "Tokens gerados: ['logo', 'em', 'seguida', ',', 'surgia', 'uma', 'oficina', 'de', 'ferreiro', ',', 'toda', 'atravancada', 'de', 'destrocos', 'e', 'objetos', 'quebrados', ',', 'entre', 'os', 'quais', 'avultavam', 'rodas', 'de', 'carro', 'em', 'volta', 'da', 'bigorna', 'dois', 'homens', ',', 'de', 'corpo', 'nu', ',', 'banhados', 'de', 'suor', 'e', 'alumiados', 'de', 'vermelho', 'como', 'dois', 'diabos', ',', 'martelavam', 'cadenciosamente', 'sobre', 'um', 'pedaco', 'de', 'ferro', 'em', 'brasa', 'e', 'ali', 'mesmo', ',', 'perto', 'deles', ',', 'a', 'forja', 'escancarava', 'uma', 'goela', 'infernal', ',', 'de', 'onde', 'saiam', 'pequenas', 'linguas', 'de', 'fogo', ',', 'irrequietas', 'e', 'gulosas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão parou à entrada da oficina e gritou para um dos ferreiros: \n",
      "— O Bruno! Não se esqueça do varal da lanterna do portão! \n",
      "Os dois homens suspenderam por um instante o trabalho'\n",
      "Tokens gerados: ['joao', 'romao', 'parou', 'a', 'entrada', 'da', 'oficina', 'e', 'gritou', 'para', 'um', 'dos', 'ferreiros', '—', 'o', 'bruno', 'nao', 'se', 'esqueca', 'do', 'varal', 'da', 'lanterna', 'do', 'portao', 'os', 'dois', 'homens', 'suspenderam', 'por', 'um', 'instante', 'o', 'trabalho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Já lá fui ver, respondeu o Bruno'\n",
      "Tokens gerados: ['—', 'ja', 'la', 'fui', 'ver', ',', 'respondeu', 'o', 'bruno']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não vale a pena consertá-lo; está todo comido de ferragem! Faz-se-lhe um \n",
      "novo, que é melhor! \n",
      "— Pois veja lá isso, que a lanterna está a cair! \n",
      "E o vendeiro seguiu adiante com o outro, enquanto atrás recomeçava o martelar sobre a bigorna'\n",
      "Tokens gerados: ['nao', 'vale', 'a', 'pena', 'conserta-lo', 'esta', 'todo', 'comido', 'de', 'ferragem', 'faz-se-lhe', 'um', 'novo', ',', 'que', 'e', 'melhor', '—', 'pois', 'veja', 'la', 'isso', ',', 'que', 'a', 'lanterna', 'esta', 'a', 'cair', 'e', 'o', 'vendeiro', 'seguiu', 'adiante', 'com', 'o', 'outro', ',', 'enquanto', 'atras', 'recomecava', 'o', 'martelar', 'sobre', 'a', 'bigorna']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em seguida via-se uma miserável estrebaria, cheia de capim seco e excremento de bestas, com lugar para meia \n",
      "dúzia de animais'\n",
      "Tokens gerados: ['em', 'seguida', 'via-se', 'uma', 'miseravel', 'estrebaria', ',', 'cheia', 'de', 'capim', 'seco', 'e', 'excremento', 'de', 'bestas', ',', 'com', 'lugar', 'para', 'meia', 'duzia', 'de', 'animais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estava deserta, mas, no vivo fartum exalado de lá, sentia-se que fora habitada ainda aquela noite'\n",
      "Tokens gerados: ['estava', 'deserta', ',', 'mas', ',', 'no', 'vivo', 'fartum', 'exalado', 'de', 'la', ',', 'sentia-se', 'que', 'fora', 'habitada', 'ainda', 'aquela', 'noite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia depois um depósito de madeiras, servindo ao mesmo tempo de oficina de carpinteiro, tendo à porta troncos de \n",
      "arvore, alguns já serrados, muitas tábuas empilhadas, restos de cavernas e mastros de navio'\n",
      "Tokens gerados: ['havia', 'depois', 'um', 'deposito', 'de', 'madeiras', ',', 'servindo', 'ao', 'mesmo', 'tempo', 'de', 'oficina', 'de', 'carpinteiro', ',', 'tendo', 'a', 'porta', 'troncos', 'de', 'arvore', ',', 'alguns', 'ja', 'serrados', ',', 'muitas', 'tabuas', 'empilhadas', ',', 'restos', 'de', 'cavernas', 'e', 'mastros', 'de', 'navio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Daí à pedreira restavam apenas uns cinqüenta passos e o chão era já todo coberto por uma farinha de pedra moída \n",
      "que sujava como a cal'\n",
      "Tokens gerados: ['dai', 'a', 'pedreira', 'restavam', 'apenas', 'uns', 'cinquenta', 'passos', 'e', 'o', 'chao', 'era', 'ja', 'todo', 'coberto', 'por', 'uma', 'farinha', 'de', 'pedra', 'moida', 'que', 'sujava', 'como', 'a', 'cal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aqui, ali, por toda a parte, encontravam-se trabalhadores, uns ao sol, outros debaixo de pequenas barracas feitas \n",
      "de lona ou de folhas de palmeira'\n",
      "Tokens gerados: ['aqui', ',', 'ali', ',', 'por', 'toda', 'a', 'parte', ',', 'encontravam-se', 'trabalhadores', ',', 'uns', 'ao', 'sol', ',', 'outros', 'debaixo', 'de', 'pequenas', 'barracas', 'feitas', 'de', 'lona', 'ou', 'de', 'folhas', 'de', 'palmeira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De um lado cunhavam pedra cantando; de outro a quebravam a picareta; de outro \n",
      "afeiçoavam lajedos a ponta de picão; mais adiante faziam paralelepípedos a escopro e macete'\n",
      "Tokens gerados: ['de', 'um', 'lado', 'cunhavam', 'pedra', 'cantando', 'de', 'outro', 'a', 'quebravam', 'a', 'picareta', 'de', 'outro', 'afeicoavam', 'lajedos', 'a', 'ponta', 'de', 'picao', 'mais', 'adiante', 'faziam', 'paralelepipedos', 'a', 'escopro', 'e', 'macete']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E todo aquele retintim de \n",
      "ferramentas, e o martelar da forja, e o coro dos que lá em cima brocavam a rocha para lançar-lhe fogo, e a surda zoada \n",
      "ao longe, que vinha do cortiço, como de uma aldeia alarmada; tudo dava a idéia de uma atividade feroz, de uma luta de \n",
      "vingança e de ódio'\n",
      "Tokens gerados: ['e', 'todo', 'aquele', 'retintim', 'de', 'ferramentas', ',', 'e', 'o', 'martelar', 'da', 'forja', ',', 'e', 'o', 'coro', 'dos', 'que', 'la', 'em', 'cima', 'brocavam', 'a', 'rocha', 'para', 'lancar-lhe', 'fogo', ',', 'e', 'a', 'surda', 'zoada', 'ao', 'longe', ',', 'que', 'vinha', 'do', 'cortico', ',', 'como', 'de', 'uma', 'aldeia', 'alarmada', 'tudo', 'dava', 'a', 'ideia', 'de', 'uma', 'atividade', 'feroz', ',', 'de', 'uma', 'luta', 'de', 'vinganca', 'e', 'de', 'odio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aqueles homens gotejantes de suor, bêbados de calor, desvairados de insolação, a quebrarem, a \n",
      "espicaçarem, a torturarem a pedra, pareciam um punhado de demônios revoltados na sua impotência contra o impassível \n",
      "gigante que os contemplava com desprezo, imperturbável a todos os golpes e a todos os tiros que lhe desfechavam no \n",
      "dorso, deixando sem um gemido que lhe abrissem as entranhas de granito'\n",
      "Tokens gerados: ['aqueles', 'homens', 'gotejantes', 'de', 'suor', ',', 'bebados', 'de', 'calor', ',', 'desvairados', 'de', 'insolacao', ',', 'a', 'quebrarem', ',', 'a', 'espicacarem', ',', 'a', 'torturarem', 'a', 'pedra', ',', 'pareciam', 'um', 'punhado', 'de', 'demonios', 'revoltados', 'na', 'sua', 'impotencia', 'contra', 'o', 'impassivel', 'gigante', 'que', 'os', 'contemplava', 'com', 'desprezo', ',', 'imperturbavel', 'a', 'todos', 'os', 'golpes', 'e', 'a', 'todos', 'os', 'tiros', 'que', 'lhe', 'desfechavam', 'no', 'dorso', ',', 'deixando', 'sem', 'um', 'gemido', 'que', 'lhe', 'abrissem', 'as', 'entranhas', 'de', 'granito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O membrudo cavouqueiro havia chegado a \n",
      "fralda do orgulhoso monstro de pedra; tinha-o cara a cara, mediu-o de alto a baixo, arrogante, num desafio surdo'\n",
      "Tokens gerados: ['o', 'membrudo', 'cavouqueiro', 'havia', 'chegado', 'a', 'fralda', 'do', 'orgulhoso', 'monstro', 'de', 'pedra', 'tinha-o', 'cara', 'a', 'cara', ',', 'mediu-o', 'de', 'alto', 'a', 'baixo', ',', 'arrogante', ',', 'num', 'desafio', 'surdo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A pedreira mostrava nesse ponto de vista o seu lado mais imponente'\n",
      "Tokens gerados: ['a', 'pedreira', 'mostrava', 'nesse', 'ponto', 'de', 'vista', 'o', 'seu', 'lado', 'mais', 'imponente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Descomposta, com o escalavrado flanco \n",
      "exposto ao sol, erguia-se altaneira e desassombrada, afrontando o céu, muito íngreme, lisa, escaldante e cheia de cordas \n",
      "que mesquinhamente lhe escorriam pela ciclópica nudez com um efeito de teias de aranha'\n",
      "Tokens gerados: ['descomposta', ',', 'com', 'o', 'escalavrado', 'flanco', 'exposto', 'ao', 'sol', ',', 'erguia-se', 'altaneira', 'e', 'desassombrada', ',', 'afrontando', 'o', 'ceu', ',', 'muito', 'ingreme', ',', 'lisa', ',', 'escaldante', 'e', 'cheia', 'de', 'cordas', 'que', 'mesquinhamente', 'lhe', 'escorriam', 'pela', 'ciclopica', 'nudez', 'com', 'um', 'efeito', 'de', 'teias', 'de', 'aranha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em certos lugares, muito \n",
      "alto do chão, lhe haviam espetado alfinetes de ferro, amparando, sobre um precipício, miseráveis tábuas que, vistas cá \n",
      "de baixo, pareciam palitos, mas em cima das quais uns atrevidos pigmeus de forma humana equilibravam-se, \n",
      "desfechando golpes de picareta contra o gigante'\n",
      "Tokens gerados: ['em', 'certos', 'lugares', ',', 'muito', 'alto', 'do', 'chao', ',', 'lhe', 'haviam', 'espetado', 'alfinetes', 'de', 'ferro', ',', 'amparando', ',', 'sobre', 'um', 'precipicio', ',', 'miseraveis', 'tabuas', 'que', ',', 'vistas', 'ca', 'de', 'baixo', ',', 'pareciam', 'palitos', ',', 'mas', 'em', 'cima', 'das', 'quais', 'uns', 'atrevidos', 'pigmeus', 'de', 'forma', 'humana', 'equilibravam-se', ',', 'desfechando', 'golpes', 'de', 'picareta', 'contra', 'o', 'gigante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cavouqueiro meneou a cabeça com ar de lástima'\n",
      "Tokens gerados: ['o', 'cavouqueiro', 'meneou', 'a', 'cabeca', 'com', 'ar', 'de', 'lastima']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O seu gesto desaprovava todo aquele serviço'\n",
      "Tokens gerados: ['o', 'seu', 'gesto', 'desaprovava', 'todo', 'aquele', 'servico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Veja lá! disse ele, apontando para certo ponto da rocha'\n",
      "Tokens gerados: ['—', 'veja', 'la', 'disse', 'ele', ',', 'apontando', 'para', 'certo', 'ponto', 'da', 'rocha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Olhe para aquilo! Sua gente tem ido às cegas no \n",
      "trabalho desta pedreira'\n",
      "Tokens gerados: ['olhe', 'para', 'aquilo', 'sua', 'gente', 'tem', 'ido', 'as', 'cegas', 'no', 'trabalho', 'desta', 'pedreira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deviam atacá-la justamente por aquele outro lado, para não contrariar os veios da pedra'\n",
      "Tokens gerados: ['deviam', 'ataca-la', 'justamente', 'por', 'aquele', 'outro', 'lado', ',', 'para', 'nao', 'contrariar', 'os', 'veios', 'da', 'pedra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Esta \n",
      "parte aqui é toda granito, é a melhor! Pois olhe só o que eles têm tirado de lá — umas lascas, uns calhaus que não \n",
      "servem para nada! É uma dor de coração ver estragar assim uma peça tão boa! Agora o que hão de fazer dessa \n",
      "cascalhada que ai está senão macacos? E brada aos céus, creia! ter pedra desta ordem para empregá-la em macacos! \n",
      "O vendeiro escutava-o em silêncio, apertando os beiços, aborrecido com a idéia daquele prejuízo'\n",
      "Tokens gerados: ['esta', 'parte', 'aqui', 'e', 'toda', 'granito', ',', 'e', 'a', 'melhor', 'pois', 'olhe', 'so', 'o', 'que', 'eles', 'tem', 'tirado', 'de', 'la', '—', 'umas', 'lascas', ',', 'uns', 'calhaus', 'que', 'nao', 'servem', 'para', 'nada', 'e', 'uma', 'dor', 'de', 'coracao', 'ver', 'estragar', 'assim', 'uma', 'peca', 'tao', 'boa', 'agora', 'o', 'que', 'hao', 'de', 'fazer', 'dessa', 'cascalhada', 'que', 'ai', 'esta', 'senao', 'macacos', '?', 'e', 'brada', 'aos', 'ceus', ',', 'creia', 'ter', 'pedra', 'desta', 'ordem', 'para', 'emprega-la', 'em', 'macacos', 'o', 'vendeiro', 'escutava-o', 'em', 'silencio', ',', 'apertando', 'os', 'beicos', ',', 'aborrecido', 'com', 'a', 'ideia', 'daquele', 'prejuizo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Uma porcaria de serviço! continuou o outro'\n",
      "Tokens gerados: ['—', 'uma', 'porcaria', 'de', 'servico', 'continuou', 'o', 'outro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ali onde está aquele homem é que deviam ter feito a broca, \n",
      "porque a explosão punha abaixo toda esta aba que é separada por um veio'\n",
      "Tokens gerados: ['ali', 'onde', 'esta', 'aquele', 'homem', 'e', 'que', 'deviam', 'ter', 'feito', 'a', 'broca', ',', 'porque', 'a', 'explosao', 'punha', 'abaixo', 'toda', 'esta', 'aba', 'que', 'e', 'separada', 'por', 'um', 'veio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas quem tem ai o senhor capaz de fazer \n",
      "isso? Ninguém; porque é preciso um empregado que saiba o que faz; que, se a pólvora não for muito bem medida, nem \n",
      "só não se abre o veio, como ainda sucede ao trabalhador o mesmo que sucedeu ao outro! É preciso conhecer muito bem \n",
      "o trabalho para se poder tirar partido vantajoso desta pedreira! Boa é ela, mas não nas mãos em que está! É muito \n",
      "perigosa nas explosões; é muito em pé! Quem lhe lascar fogo não pode fugir senão para cima pela corda, e se o sujeito \n",
      "não for fino leva-o o demo! Sou eu quem o diz! \n",
      "E depois de uma pausa, acrescentou, tomando na sua mão, grossa como o próprio cascalho, um paralelepípedo \n",
      "que estava no chão: \n",
      "— Que digo eu?! Cá está! Macacos de granito! Isto até é uma coisa que estes burros deviam esconder por \n",
      "vergonha! \n",
      "Acompanhando a pedreira pelo lado direito e seguindo-a na volta que ela dava depois, formando um ângulo \n",
      "obtuso, é que se via quanto era grande'\n",
      "Tokens gerados: ['mas', 'quem', 'tem', 'ai', 'o', 'senhor', 'capaz', 'de', 'fazer', 'isso', '?', 'ninguem', 'porque', 'e', 'preciso', 'um', 'empregado', 'que', 'saiba', 'o', 'que', 'faz', 'que', ',', 'se', 'a', 'polvora', 'nao', 'for', 'muito', 'bem', 'medida', ',', 'nem', 'so', 'nao', 'se', 'abre', 'o', 'veio', ',', 'como', 'ainda', 'sucede', 'ao', 'trabalhador', 'o', 'mesmo', 'que', 'sucedeu', 'ao', 'outro', 'e', 'preciso', 'conhecer', 'muito', 'bem', 'o', 'trabalho', 'para', 'se', 'poder', 'tirar', 'partido', 'vantajoso', 'desta', 'pedreira', 'boa', 'e', 'ela', ',', 'mas', 'nao', 'nas', 'maos', 'em', 'que', 'esta', 'e', 'muito', 'perigosa', 'nas', 'explosoes', 'e', 'muito', 'em', 'pe', 'quem', 'lhe', 'lascar', 'fogo', 'nao', 'pode', 'fugir', 'senao', 'para', 'cima', 'pela', 'corda', ',', 'e', 'se', 'o', 'sujeito', 'nao', 'for', 'fino', 'leva-o', 'o', 'demo', 'sou', 'eu', 'quem', 'o', 'diz', 'e', 'depois', 'de', 'uma', 'pausa', ',', 'acrescentou', ',', 'tomando', 'na', 'sua', 'mao', ',', 'grossa', 'como', 'o', 'proprio', 'cascalho', ',', 'um', 'paralelepipedo', 'que', 'estava', 'no', 'chao', '—', 'que', 'digo', 'eu', '?', 'ca', 'esta', 'macacos', 'de', 'granito', 'isto', 'ate', 'e', 'uma', 'coisa', 'que', 'estes', 'burros', 'deviam', 'esconder', 'por', 'vergonha', 'acompanhando', 'a', 'pedreira', 'pelo', 'lado', 'direito', 'e', 'seguindo-a', 'na', 'volta', 'que', 'ela', 'dava', 'depois', ',', 'formando', 'um', 'angulo', 'obtuso', ',', 'e', 'que', 'se', 'via', 'quanto', 'era', 'grande']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Suava-se bem antes de chegar ao seu limite com a mata'\n",
      "Tokens gerados: ['suava-se', 'bem', 'antes', 'de', 'chegar', 'ao', 'seu', 'limite', 'com', 'a', 'mata']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que mina de dinheiro!'\n",
      "Tokens gerados: ['—', 'que', 'mina', 'de', 'dinheiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'dizia o homenzarrão, parando entusiasmado defronte do novo pano de rocha viva que \n",
      "se desdobrava na presença dele'\n",
      "Tokens gerados: ['dizia', 'o', 'homenzarrao', ',', 'parando', 'entusiasmado', 'defronte', 'do', 'novo', 'pano', 'de', 'rocha', 'viva', 'que', 'se', 'desdobrava', 'na', 'presenca', 'dele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Toda esta parte que se segue agora, declarou João Romão, ainda não é minha'\n",
      "Tokens gerados: ['—', 'toda', 'esta', 'parte', 'que', 'se', 'segue', 'agora', ',', 'declarou', 'joao', 'romao', ',', 'ainda', 'nao', 'e', 'minha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E continuaram a andar para diante'\n",
      "Tokens gerados: ['e', 'continuaram', 'a', 'andar', 'para', 'diante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deste lado multiplicavam-se as barraquinhas; os macaqueiros trabalhavam à sombra delas, indiferentes àqueles \n",
      "dois'\n",
      "Tokens gerados: ['deste', 'lado', 'multiplicavam-se', 'as', 'barraquinhas', 'os', 'macaqueiros', 'trabalhavam', 'a', 'sombra', 'delas', ',', 'indiferentes', 'aqueles', 'dois']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Viam-se panelas ao fogo, sobre quatro pedras, ao ar livre, e rapazitos tratando do jantar dos pais'\n",
      "Tokens gerados: ['viam-se', 'panelas', 'ao', 'fogo', ',', 'sobre', 'quatro', 'pedras', ',', 'ao', 'ar', 'livre', ',', 'e', 'rapazitos', 'tratando', 'do', 'jantar', 'dos', 'pais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De mulher nem \n",
      "sinal'\n",
      "Tokens gerados: ['de', 'mulher', 'nem', 'sinal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De vez em quando, na penumbra de um ensombro de lona, dava-se com um grupo de homens, comendo de \n",
      "cócoras defronte uns dos outros, uma sardinha na mão esquerda, um pão na direita, ao lado de uma garrafa de água'\n",
      "Tokens gerados: ['de', 'vez', 'em', 'quando', ',', 'na', 'penumbra', 'de', 'um', 'ensombro', 'de', 'lona', ',', 'dava-se', 'com', 'um', 'grupo', 'de', 'homens', ',', 'comendo', 'de', 'cocoras', 'defronte', 'uns', 'dos', 'outros', ',', 'uma', 'sardinha', 'na', 'mao', 'esquerda', ',', 'um', 'pao', 'na', 'direita', ',', 'ao', 'lado', 'de', 'uma', 'garrafa', 'de', 'agua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Sempre o mesmo serviço malfeito e mal dirigido!'\n",
      "Tokens gerados: ['—', 'sempre', 'o', 'mesmo', 'servico', 'malfeito', 'e', 'mal', 'dirigido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'resmungou o cavouqueiro'\n",
      "Tokens gerados: ['resmungou', 'o', 'cavouqueiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, a mesma atividade parecia reinar por toda a parte'\n",
      "Tokens gerados: ['entretanto', ',', 'a', 'mesma', 'atividade', 'parecia', 'reinar', 'por', 'toda', 'a', 'parte']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, lá no fim, debaixo dos bambus que marcavam \n",
      "o limite da pedreira, alguns trabalhadores dormiam à sombra, de papo para o ar, a barba espetando para o alto, o \n",
      "pescoço intumescido de cordoveias grossas como enxárcias de navio, a boca aberta, a respiração forte e tranqüila de \n",
      "animal sadio, num feliz e pletórico resfolgar de besta cansada'\n",
      "Tokens gerados: ['mas', ',', 'la', 'no', 'fim', ',', 'debaixo', 'dos', 'bambus', 'que', 'marcavam', 'o', 'limite', 'da', 'pedreira', ',', 'alguns', 'trabalhadores', 'dormiam', 'a', 'sombra', ',', 'de', 'papo', 'para', 'o', 'ar', ',', 'a', 'barba', 'espetando', 'para', 'o', 'alto', ',', 'o', 'pescoco', 'intumescido', 'de', 'cordoveias', 'grossas', 'como', 'enxarcias', 'de', 'navio', ',', 'a', 'boca', 'aberta', ',', 'a', 'respiracao', 'forte', 'e', 'tranquila', 'de', 'animal', 'sadio', ',', 'num', 'feliz', 'e', 'pletorico', 'resfolgar', 'de', 'besta', 'cansada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que relaxamento! resmungou de novo o cavouqueiro'\n",
      "Tokens gerados: ['—', 'que', 'relaxamento', 'resmungou', 'de', 'novo', 'o', 'cavouqueiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tudo isto está a reclamar um homem teso que olhe a \n",
      "sério para o serviço! \n",
      "— Eu nada tenho que ver com este lado! observou Romão'\n",
      "Tokens gerados: ['tudo', 'isto', 'esta', 'a', 'reclamar', 'um', 'homem', 'teso', 'que', 'olhe', 'a', 'serio', 'para', 'o', 'servico', '—', 'eu', 'nada', 'tenho', 'que', 'ver', 'com', 'este', 'lado', 'observou', 'romao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Mas lá da sua banda hão de fazer o mesmo! Olará! \n",
      "— Abusam, porque tenho de olhar pelo negócio lá fora'\n",
      "Tokens gerados: ['—', 'mas', 'la', 'da', 'sua', 'banda', 'hao', 'de', 'fazer', 'o', 'mesmo', 'olara', '—', 'abusam', ',', 'porque', 'tenho', 'de', 'olhar', 'pelo', 'negocio', 'la', 'fora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Comigo aqui é que eles não fariam cera'\n",
      "Tokens gerados: ['—', 'comigo', 'aqui', 'e', 'que', 'eles', 'nao', 'fariam', 'cera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'isso juro eu! Entendo que o empregado deve ser bem pago, ter para a \n",
      "sua comida à farta, o seu gole de vinho, mas que deve fazer serviço que se veja, ou, então, rua! Rua, que não falta por ai \n",
      "quem queira ganhar dinheiro! Autorize-me a olhar por eles e verá! \n",
      "— O diabo é que você quer setenta mil-réis'\n",
      "Tokens gerados: ['isso', 'juro', 'eu', 'entendo', 'que', 'o', 'empregado', 'deve', 'ser', 'bem', 'pago', ',', 'ter', 'para', 'a', 'sua', 'comida', 'a', 'farta', ',', 'o', 'seu', 'gole', 'de', 'vinho', ',', 'mas', 'que', 'deve', 'fazer', 'servico', 'que', 'se', 'veja', ',', 'ou', ',', 'entao', ',', 'rua', 'rua', ',', 'que', 'nao', 'falta', 'por', 'ai', 'quem', 'queira', 'ganhar', 'dinheiro', 'autorize-me', 'a', 'olhar', 'por', 'eles', 'e', 'vera', '—', 'o', 'diabo', 'e', 'que', 'voce', 'quer', 'setenta', 'mil-reis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'suspirou João Romão'\n",
      "Tokens gerados: ['suspirou', 'joao', 'romao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah! nem menos um real!'\n",
      "Tokens gerados: ['—', 'ah', 'nem', 'menos', 'um', 'real']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas comigo aqui há de ver o que lhe faço entrar para algibeira! Temos cá muita \n",
      "gente que não precisa estar'\n",
      "Tokens gerados: ['mas', 'comigo', 'aqui', 'ha', 'de', 'ver', 'o', 'que', 'lhe', 'faco', 'entrar', 'para', 'algibeira', 'temos', 'ca', 'muita', 'gente', 'que', 'nao', 'precisa', 'estar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Para que tanto macaqueiro, por exemplo? Aquilo é serviço para descanso; é serviço de \n",
      "criança! Em vez de todas aquelas lesmas, pagas talvez a trinta mil-réis'\n",
      "Tokens gerados: ['para', 'que', 'tanto', 'macaqueiro', ',', 'por', 'exemplo', '?', 'aquilo', 'e', 'servico', 'para', 'descanso', 'e', 'servico', 'de', 'crianca', 'em', 'vez', 'de', 'todas', 'aquelas', 'lesmas', ',', 'pagas', 'talvez', 'a', 'trinta', 'mil-reis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— É justamente quanto lhes dou'\n",
      "Tokens gerados: ['—', 'e', 'justamente', 'quanto', 'lhes', 'dou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '—'\n",
      "Tokens gerados: ['—']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'melhor seria tomar dois bons trabalhadores de cinqüenta, que fazem o dobro do que fazem aqueles monos e \n",
      "que podem servir para outras coisas! Parece que nunca trabalharam! Olhe, é já a terceira vez que aquele que ali está \n",
      "deixa cair o escopro! Com efeito! \n",
      " \n",
      "João Romão ficou calado, a cismar, enquanto voltavam'\n",
      "Tokens gerados: ['melhor', 'seria', 'tomar', 'dois', 'bons', 'trabalhadores', 'de', 'cinquenta', ',', 'que', 'fazem', 'o', 'dobro', 'do', 'que', 'fazem', 'aqueles', 'monos', 'e', 'que', 'podem', 'servir', 'para', 'outras', 'coisas', 'parece', 'que', 'nunca', 'trabalharam', 'olhe', ',', 'e', 'ja', 'a', 'terceira', 'vez', 'que', 'aquele', 'que', 'ali', 'esta', 'deixa', 'cair', 'o', 'escopro', 'com', 'efeito', 'joao', 'romao', 'ficou', 'calado', ',', 'a', 'cismar', ',', 'enquanto', 'voltavam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vinham ambos pensativos'\n",
      "Tokens gerados: ['vinham', 'ambos', 'pensativos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— E você, se eu o tomar, disse depois o vendeiro, muda-se cá para a estalagem?'\n",
      "Tokens gerados: ['—', 'e', 'voce', ',', 'se', 'eu', 'o', 'tomar', ',', 'disse', 'depois', 'o', 'vendeiro', ',', 'muda-se', 'ca', 'para', 'a', 'estalagem', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Naturalmente! não hei de ficar lá na cidade nova, tendo o serviço aqui!'\n",
      "Tokens gerados: ['—', 'naturalmente', 'nao', 'hei', 'de', 'ficar', 'la', 'na', 'cidade', 'nova', ',', 'tendo', 'o', 'servico', 'aqui']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— E a comida, forneço-a eu?'\n",
      "Tokens gerados: ['—', 'e', 'a', 'comida', ',', 'forneco-a', 'eu', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Isso é que a mulher é quem a faz; mas as compras saem-lhe da venda'\n",
      "Tokens gerados: ['—', 'isso', 'e', 'que', 'a', 'mulher', 'e', 'quem', 'a', 'faz', 'mas', 'as', 'compras', 'saem-lhe', 'da', 'venda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pois está fechado o negócio! deliberou João Romão, convencido de que não podia, por economia, dispensar \n",
      "um homem daqueles'\n",
      "Tokens gerados: ['—', 'pois', 'esta', 'fechado', 'o', 'negocio', 'deliberou', 'joao', 'romao', ',', 'convencido', 'de', 'que', 'nao', 'podia', ',', 'por', 'economia', ',', 'dispensar', 'um', 'homem', 'daqueles']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E pensou lá de si para si: “Os meus setenta mil-réis voltar-me-ão à gaveta'\n",
      "Tokens gerados: ['e', 'pensou', 'la', 'de', 'si', 'para', 'si', '“', 'os', 'meus', 'setenta', 'mil-reis', 'voltar-me-ao', 'a', 'gaveta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tudo me fica em \n",
      "casa!” \n",
      "— Então estamos entendidos?'\n",
      "Tokens gerados: ['tudo', 'me', 'fica', 'em', 'casa', '”', '—', 'entao', 'estamos', 'entendidos', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Estamos entendidos! \n",
      "— Posso amanhã fazer a mudança? \n",
      "— Hoje mesmo, se quiser; tenho um cômodo que lhe há de calhar'\n",
      "Tokens gerados: ['—', 'estamos', 'entendidos', '—', 'posso', 'amanha', 'fazer', 'a', 'mudanca', '?', '—', 'hoje', 'mesmo', ',', 'se', 'quiser', 'tenho', 'um', 'comodo', 'que', 'lhe', 'ha', 'de', 'calhar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É o número 35'\n",
      "Tokens gerados: ['e', 'o', 'numero', '35']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vou mostrar-lho'\n",
      "Tokens gerados: ['vou', 'mostrar-lho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E aligeirando o passo, penetraram na estrada do capinzal com direção ao fundo do cortiço'\n",
      "Tokens gerados: ['e', 'aligeirando', 'o', 'passo', ',', 'penetraram', 'na', 'estrada', 'do', 'capinzal', 'com', 'direcao', 'ao', 'fundo', 'do', 'cortico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah! é verdade! como você se chama? \n",
      "— Jerônimo, para o servir'\n",
      "Tokens gerados: ['—', 'ah', 'e', 'verdade', 'como', 'voce', 'se', 'chama', '?', '—', 'jeronimo', ',', 'para', 'o', 'servir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Servir a Deus'\n",
      "Tokens gerados: ['—', 'servir', 'a', 'deus']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sua mulher lava? \n",
      "— É lavadeira, sim senhor'\n",
      "Tokens gerados: ['sua', 'mulher', 'lava', '?', '—', 'e', 'lavadeira', ',', 'sim', 'senhor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Bem, precisamos ver-lhe uma tina'\n",
      "Tokens gerados: ['—', 'bem', ',', 'precisamos', 'ver-lhe', 'uma', 'tina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o vendeiro empurrou a porta do fundo da estalagem, de onde escapou, como de uma panela fervendo que se \n",
      "destapa, uma baforada quente, vozeria tresandante à fermentação de suores e roupa ensaboada secando ao sol'\n",
      "Tokens gerados: ['e', 'o', 'vendeiro', 'empurrou', 'a', 'porta', 'do', 'fundo', 'da', 'estalagem', ',', 'de', 'onde', 'escapou', ',', 'como', 'de', 'uma', 'panela', 'fervendo', 'que', 'se', 'destapa', ',', 'uma', 'baforada', 'quente', ',', 'vozeria', 'tresandante', 'a', 'fermentacao', 'de', 'suores', 'e', 'roupa', 'ensaboada', 'secando', 'ao', 'sol']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_o_cortico_aluisio_azevedo_cap_4.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Era um dia abafadiço e aborrecido'\n",
      "Tokens gerados: ['era', 'um', 'dia', 'abafadico', 'e', 'aborrecido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A pobre cidade de São Luís do Maranhão parecia entorpecida\n",
      "pelo calor'\n",
      "Tokens gerados: ['a', 'pobre', 'cidade', 'de', 'sao', 'luis', 'do', 'maranhao', 'parecia', 'entorpecidapelo', 'calor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quase que se não podia sair à rua: as pedras escaldavam; as vidraças e os lampiões faiscavam\n",
      "ao sol como enormes diamantes, as paredes tinham reverberações de prata polida; as folhas das árvores\n",
      "nem se mexiam; as carroças d’água passavam ruidosamente a todo o instante, abalando os prédios; e os\n",
      "aguadeiros, em mangas de camisa e pernas arregaçadas, invadiam sem-cerimônia as casas para encher\n",
      "as banheiras e os potes'\n",
      "Tokens gerados: ['quase', 'que', 'se', 'nao', 'podia', 'sair', 'a', 'rua', 'as', 'pedras', 'escaldavam', 'as', 'vidracas', 'e', 'os', 'lampioes', 'faiscavamao', 'sol', 'como', 'enormes', 'diamantes', ',', 'as', 'paredes', 'tinham', 'reverberacoes', 'de', 'prata', 'polida', 'as', 'folhas', 'das', 'arvoresnem', 'se', 'mexiam', 'as', 'carrocas', 'd', '’', 'agua', 'passavam', 'ruidosamente', 'a', 'todo', 'o', 'instante', ',', 'abalando', 'os', 'predios', 'e', 'osaguadeiros', ',', 'em', 'mangas', 'de', 'camisa', 'e', 'pernas', 'arregacadas', ',', 'invadiam', 'sem-cerimonia', 'as', 'casas', 'para', 'encheras', 'banheiras', 'e', 'os', 'potes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em certos pontos não se encontrava viva alma na rua; tudo estava concentrado,\n",
      "adormecido; só os pretos faziam as compras para o jantar ou andavam no ganho'\n",
      "Tokens gerados: ['em', 'certos', 'pontos', 'nao', 'se', 'encontrava', 'viva', 'alma', 'na', 'rua', 'tudo', 'estava', 'concentrado', ',', 'adormecido', 'so', 'os', 'pretos', 'faziam', 'as', 'compras', 'para', 'o', 'jantar', 'ou', 'andavam', 'no', 'ganho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A Praça da Alegria apresentava um ar fúnebre'\n",
      "Tokens gerados: ['a', 'praca', 'da', 'alegria', 'apresentava', 'um', 'ar', 'funebre']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De um casebre miserável, de porta e janela,\n",
      "ouviam-se gemer os armadores enferrujados de uma rede e uma voz tísica e aflautada, de mulher,\n",
      "cantar em falsete a “gentil Carolina era bela”; do outro lado da praça, uma preta velha, vergada por\n",
      "imenso tabuleiro de madeira, sujo, seboso, cheio de sangue e coberto por uma nuvem de moscas,\n",
      "apregoava em tom muito arrastado e melancólico: “Fígado, rins e coração!’’ Era uma vendedeira de\n",
      "fatos de boi'\n",
      "Tokens gerados: ['de', 'um', 'casebre', 'miseravel', ',', 'de', 'porta', 'e', 'janela', ',', 'ouviam-se', 'gemer', 'os', 'armadores', 'enferrujados', 'de', 'uma', 'rede', 'e', 'uma', 'voz', 'tisica', 'e', 'aflautada', ',', 'de', 'mulher', ',', 'cantar', 'em', 'falsete', 'a', '“', 'gentil', 'carolina', 'era', 'bela', '”', 'do', 'outro', 'lado', 'da', 'praca', ',', 'uma', 'preta', 'velha', ',', 'vergada', 'porimenso', 'tabuleiro', 'de', 'madeira', ',', 'sujo', ',', 'seboso', ',', 'cheio', 'de', 'sangue', 'e', 'coberto', 'por', 'uma', 'nuvem', 'de', 'moscas', ',', 'apregoava', 'em', 'tom', 'muito', 'arrastado', 'e', 'melancolico', '“', 'figado', ',', 'rins', 'e', 'coracao', '’', '’', 'era', 'uma', 'vendedeira', 'defatos', 'de', 'boi']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As crianças nuas, com as perninhas tortas pelo costume de cavalgar as ilhargas maternas,\n",
      "as cabeças avermelhadas pelo sol, a pele crestada os ventrezinhos amarelentos e crescidos, corriam e\n",
      "guinchavam, empinando papagaios de papel'\n",
      "Tokens gerados: ['as', 'criancas', 'nuas', ',', 'com', 'as', 'perninhas', 'tortas', 'pelo', 'costume', 'de', 'cavalgar', 'as', 'ilhargas', 'maternas', ',', 'as', 'cabecas', 'avermelhadas', 'pelo', 'sol', ',', 'a', 'pele', 'crestada', 'os', 'ventrezinhos', 'amarelentos', 'e', 'crescidos', ',', 'corriam', 'eguinchavam', ',', 'empinando', 'papagaios', 'de', 'papel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um ou outro branco, levado pela necessidade de sair,\n",
      "atravessava a rua, suado, vermelho, afogueado, à sombra de um enorme chapéu-de-sol'\n",
      "Tokens gerados: ['um', 'ou', 'outro', 'branco', ',', 'levado', 'pela', 'necessidade', 'de', 'sair', ',', 'atravessava', 'a', 'rua', ',', 'suado', ',', 'vermelho', ',', 'afogueado', ',', 'a', 'sombra', 'de', 'um', 'enorme', 'chapeu-de-sol']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os cães,\n",
      "estendidos pelas calçadas, tinham uivos que pareciam gemidos humanos, movimentos irascíveis,\n",
      "mordiam o ar querendo morder os mosquitos'\n",
      "Tokens gerados: ['os', 'caes', ',', 'estendidos', 'pelas', 'calcadas', ',', 'tinham', 'uivos', 'que', 'pareciam', 'gemidos', 'humanos', ',', 'movimentos', 'irasciveis', ',', 'mordiam', 'o', 'ar', 'querendo', 'morder', 'os', 'mosquitos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao longe, para as bandas de São Pantaleão, ouvia-se\n",
      "apregoar: “Arroz de Veneza! Mangas! Mocajubas!” Às esquinas, nas quitandas vazias, fermentava um\n",
      "cheiro acre de sabão da terra e aguardente'\n",
      "Tokens gerados: ['ao', 'longe', ',', 'para', 'as', 'bandas', 'de', 'sao', 'pantaleao', ',', 'ouvia-seapregoar', '“', 'arroz', 'de', 'veneza', 'mangas', 'mocajubas', '”', 'as', 'esquinas', ',', 'nas', 'quitandas', 'vazias', ',', 'fermentava', 'umcheiro', 'acre', 'de', 'sabao', 'da', 'terra', 'e', 'aguardente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O quitandeiro, assentado sobre o balcão, cochilava a sua\n",
      "preguiça morrinhenta, acariciando o seu imenso e espalmado pé descalço'\n",
      "Tokens gerados: ['o', 'quitandeiro', ',', 'assentado', 'sobre', 'o', 'balcao', ',', 'cochilava', 'a', 'suapreguica', 'morrinhenta', ',', 'acariciando', 'o', 'seu', 'imenso', 'e', 'espalmado', 'pe', 'descalco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Da Praia de Santo Antônio\n",
      "enchiam toda a cidade os sons invariáveis e monótonos de uma buzina, anunciando que os pescadores\n",
      "chegavam do mar; para lá convergiam, apressadas e cheias de interesse, as peixeiras, quase todas negras,\n",
      "muito gordas, o tabuleiro na cabeça, rebolando os grossos quadris trêmulos e as tetas opulentas'\n",
      "Tokens gerados: ['da', 'praia', 'de', 'santo', 'antonioenchiam', 'toda', 'a', 'cidade', 'os', 'sons', 'invariaveis', 'e', 'monotonos', 'de', 'uma', 'buzina', ',', 'anunciando', 'que', 'os', 'pescadoreschegavam', 'do', 'mar', 'para', 'la', 'convergiam', ',', 'apressadas', 'e', 'cheias', 'de', 'interesse', ',', 'as', 'peixeiras', ',', 'quase', 'todas', 'negras', ',', 'muito', 'gordas', ',', 'o', 'tabuleiro', 'na', 'cabeca', ',', 'rebolando', 'os', 'grossos', 'quadris', 'tremulos', 'e', 'as', 'tetas', 'opulentas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A Praia Grande e a Rua da Estrela contrastavam todavia com o resto da cidade, porque era\n",
      "aquela hora justamente a de maior movimento comercial'\n",
      "Tokens gerados: ['a', 'praia', 'grande', 'e', 'a', 'rua', 'da', 'estrela', 'contrastavam', 'todavia', 'com', 'o', 'resto', 'da', 'cidade', ',', 'porque', 'eraaquela', 'hora', 'justamente', 'a', 'de', 'maior', 'movimento', 'comercial']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em todas as direções cruzavam-se homens\n",
      "esbofados e rubros; cruzavam-se os negros no carreto e os caixeiros que estavam em serviço na rua;\n",
      "avultavam os paletós-sacos, de brim pardo, mosqueados nas espáduas e nos sovacos por grandes manchas\n",
      "de suor'\n",
      "Tokens gerados: ['em', 'todas', 'as', 'direcoes', 'cruzavam-se', 'homensesbofados', 'e', 'rubros', 'cruzavam-se', 'os', 'negros', 'no', 'carreto', 'e', 'os', 'caixeiros', 'que', 'estavam', 'em', 'servico', 'na', 'ruaavultavam', 'os', 'paletos-sacos', ',', 'de', 'brim', 'pardo', ',', 'mosqueados', 'nas', 'espaduas', 'e', 'nos', 'sovacos', 'por', 'grandes', 'manchasde', 'suor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os corretores de escravos examinavam, à plena luz do sol, os negros e moleques que ali\n",
      "estavam para ser vendidos; revistavam-lhes os dentes, os pés e as virilhas; faziam-lhes perguntas sobre\n",
      "perguntas, batiam-lhes com a biqueira do chapéu nos ombros e nas coxas, experimentando-lhes o vigor\n",
      "da musculatura, como se estivessem a comprar cavalos'\n",
      "Tokens gerados: ['os', 'corretores', 'de', 'escravos', 'examinavam', ',', 'a', 'plena', 'luz', 'do', 'sol', ',', 'os', 'negros', 'e', 'moleques', 'que', 'aliestavam', 'para', 'ser', 'vendidos', 'revistavam-lhes', 'os', 'dentes', ',', 'os', 'pes', 'e', 'as', 'virilhas', 'faziam-lhes', 'perguntas', 'sobreperguntas', ',', 'batiam-lhes', 'com', 'a', 'biqueira', 'do', 'chapeu', 'nos', 'ombros', 'e', 'nas', 'coxas', ',', 'experimentando-lhes', 'o', 'vigorda', 'musculatura', ',', 'como', 'se', 'estivessem', 'a', 'comprar', 'cavalos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na Casa da Praça, debaixo das amendoeiras,\n",
      "nas portadas dos armazéns, entre pilhas de caixões de cebolas e batatas portuguesas, discutiam-se o\n",
      "câmbio, o preço do algodão, a taxa do açúcar, a tarifa dos gêneros nacionais; volumosos comendadores\n",
      "resolviam negócios, faziam transações, perdiam, ganhavam, tratavam de embarrilar uns aos outros,\n",
      "com muita manha de gente de negócios, falando numa gíria só deles trocando chalaças pesadas, mas\n",
      "em plena confiança de amizade'\n",
      "Tokens gerados: ['na', 'casa', 'da', 'praca', ',', 'debaixo', 'das', 'amendoeiras', ',', 'nas', 'portadas', 'dos', 'armazens', ',', 'entre', 'pilhas', 'de', 'caixoes', 'de', 'cebolas', 'e', 'batatas', 'portuguesas', ',', 'discutiam-se', 'ocambio', ',', 'o', 'preco', 'do', 'algodao', ',', 'a', 'taxa', 'do', 'acucar', ',', 'a', 'tarifa', 'dos', 'generos', 'nacionais', 'volumosos', 'comendadoresresolviam', 'negocios', ',', 'faziam', 'transacoes', ',', 'perdiam', ',', 'ganhavam', ',', 'tratavam', 'de', 'embarrilar', 'uns', 'aos', 'outros', ',', 'com', 'muita', 'manha', 'de', 'gente', 'de', 'negocios', ',', 'falando', 'numa', 'giria', 'so', 'deles', 'trocando', 'chalacas', 'pesadas', ',', 'masem', 'plena', 'confianca', 'de', 'amizade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os leiloeiros cantavam em voz alta o preço das mercadorias, com um\n",
      "abrimento afetado de vogais; diziam: “Mal-rais“ em vez de mil-réis'\n",
      "Tokens gerados: ['os', 'leiloeiros', 'cantavam', 'em', 'voz', 'alta', 'o', 'preco', 'das', 'mercadorias', ',', 'com', 'umabrimento', 'afetado', 'de', 'vogais', 'diziam', '“', 'mal-rais', '“', 'em', 'vez', 'de', 'mil-reis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'À porta dos leilões\n",
      "aglomeravam-se os que queriam comprar e os simples curiosos'\n",
      "Tokens gerados: ['a', 'porta', 'dos', 'leiloesaglomeravam-se', 'os', 'que', 'queriam', 'comprar', 'e', 'os', 'simples', 'curiosos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Corria um quente e grosseiro zunzum\n",
      "de feira'\n",
      "Tokens gerados: ['corria', 'um', 'quente', 'e', 'grosseiro', 'zunzumde', 'feira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O leiloeiro tinha piscos de olhos significativos; de martelo em punho, entusiasmado, o ar trágico,\n",
      "mostrava com o braço erguido um cálice de cachaça, ou, comicamente acocorado, esbrocava com o\n",
      "furador os paneiros de farinha e de milho'\n",
      "Tokens gerados: ['o', 'leiloeiro', 'tinha', 'piscos', 'de', 'olhos', 'significativos', 'de', 'martelo', 'em', 'punho', ',', 'entusiasmado', ',', 'o', 'ar', 'tragico', ',', 'mostrava', 'com', 'o', 'braco', 'erguido', 'um', 'calice', 'de', 'cachaca', ',', 'ou', ',', 'comicamente', 'acocorado', ',', 'esbrocava', 'com', 'ofurador', 'os', 'paneiros', 'de', 'farinha', 'e', 'de', 'milho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, quando chegava a ocasião de ceder a fazenda, repetia o\n",
      "preço muitas vezes, gritando, e afinal batia o martelo com grande barulho, arrastando a voz em um tom\n",
      "cantado e estridente'\n",
      "Tokens gerados: ['e', ',', 'quando', 'chegava', 'a', 'ocasiao', 'de', 'ceder', 'a', 'fazenda', ',', 'repetia', 'opreco', 'muitas', 'vezes', ',', 'gritando', ',', 'e', 'afinal', 'batia', 'o', 'martelo', 'com', 'grande', 'barulho', ',', 'arrastando', 'a', 'voz', 'em', 'um', 'tomcantado', 'e', 'estridente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Viam-se deslizar pela praça os imponentes e monstruosos abdomens dos capitalistas; viam-se\n",
      "cabeças escarlates e descabeladas, gotejando suor por debaixo do chapéu de pêlo; risinhos de proteção,\n",
      "bocas sem bigode dilatadas pelo calor, perninhas espertas e suadas na calça de brim de Hamburgo'\n",
      "Tokens gerados: ['viam-se', 'deslizar', 'pela', 'praca', 'os', 'imponentes', 'e', 'monstruosos', 'abdomens', 'dos', 'capitalistas', 'viam-secabecas', 'escarlates', 'e', 'descabeladas', ',', 'gotejando', 'suor', 'por', 'debaixo', 'do', 'chapeu', 'de', 'pelo', 'risinhos', 'de', 'protecao', ',', 'bocas', 'sem', 'bigode', 'dilatadas', 'pelo', 'calor', ',', 'perninhas', 'espertas', 'e', 'suadas', 'na', 'calca', 'de', 'brim', 'de', 'hamburgo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E\n",
      "toda esta atividade, posto que um tanto fingida, era geral e comunicativa; até os ricos ociosos, que iam\n",
      "para ali encher o dia, e os caixeiros, que “faziam cera” e até os próprios vadios desempregados,\n",
      "aparentavam diligência e prontidão'\n",
      "Tokens gerados: ['etoda', 'esta', 'atividade', ',', 'posto', 'que', 'um', 'tanto', 'fingida', ',', 'era', 'geral', 'e', 'comunicativa', 'ate', 'os', 'ricos', 'ociosos', ',', 'que', 'iampara', 'ali', 'encher', 'o', 'dia', ',', 'e', 'os', 'caixeiros', ',', 'que', '“', 'faziam', 'cera', '”', 'e', 'ate', 'os', 'proprios', 'vadios', 'desempregados', ',', 'aparentavam', 'diligencia', 'e', 'prontidao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A varanda do sobrado de Manuel Pescada, uma varanda larga e sem forro no teto, deixando ver\n",
      "as ripas e os caibros que sustentavam as telhas, tinha um aspecto mais ou menos pitoresco com a sua\n",
      "bela vista sobre o rio Bacanga e as suas rótulas pintadas de verde-paris'\n",
      "Tokens gerados: ['a', 'varanda', 'do', 'sobrado', 'de', 'manuel', 'pescada', ',', 'uma', 'varanda', 'larga', 'e', 'sem', 'forro', 'no', 'teto', ',', 'deixando', 'veras', 'ripas', 'e', 'os', 'caibros', 'que', 'sustentavam', 'as', 'telhas', ',', 'tinha', 'um', 'aspecto', 'mais', 'ou', 'menos', 'pitoresco', 'com', 'a', 'suabela', 'vista', 'sobre', 'o', 'rio', 'bacanga', 'e', 'as', 'suas', 'rotulas', 'pintadas', 'de', 'verde-paris']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Toda ela abria para o quintal,\n",
      "estreito e longo, onde, à mingua de sol, se mirravam duas tristes pitangueiras e passeava solenemente\n",
      "um pavão da terra'\n",
      "Tokens gerados: ['toda', 'ela', 'abria', 'para', 'o', 'quintal', ',', 'estreito', 'e', 'longo', ',', 'onde', ',', 'a', 'mingua', 'de', 'sol', ',', 'se', 'mirravam', 'duas', 'tristes', 'pitangueiras', 'e', 'passeava', 'solenementeum', 'pavao', 'da', 'terra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As paredes, barradas de azulejos portugueses e, para o alto, cobertas de papel pintado, mostravam,\n",
      "nos seus desenhos repetidos de assuntos de caça, alguns lugares sem tinta, cujas manchas brancacentas\n",
      "traziam à idéia joelheiras de calças surradas'\n",
      "Tokens gerados: ['as', 'paredes', ',', 'barradas', 'de', 'azulejos', 'portugueses', 'e', ',', 'para', 'o', 'alto', ',', 'cobertas', 'de', 'papel', 'pintado', ',', 'mostravam', ',', 'nos', 'seus', 'desenhos', 'repetidos', 'de', 'assuntos', 'de', 'caca', ',', 'alguns', 'lugares', 'sem', 'tinta', ',', 'cujas', 'manchas', 'brancacentastraziam', 'a', 'ideia', 'joelheiras', 'de', 'calcas', 'surradas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao lado, dominando a mesa de jantar, aprumava-se um\n",
      "velho armário de jacarandá polido, muito bem tratado, com as vidraças bem limpas, expondo as pratas\n",
      "e as porcelanas de gosto moderno; a um canto dormia, esquecida na sua caixa de pinho envernizado,\n",
      "uma máquina de costura de Wilson, das primeiras que chegaram ao Maranhão; nos intervalos das\n",
      "portas simetrizavam-se quatro estudos de Julien, representando em litografia as estações do ano; defronte\n",
      "do guarda-louça um relógio de corrente embalava melancolicamente a sua pêndula do tamanho de um\n",
      "prato e apontava para as duas horas'\n",
      "Tokens gerados: ['ao', 'lado', ',', 'dominando', 'a', 'mesa', 'de', 'jantar', ',', 'aprumava-se', 'umvelho', 'armario', 'de', 'jacaranda', 'polido', ',', 'muito', 'bem', 'tratado', ',', 'com', 'as', 'vidracas', 'bem', 'limpas', ',', 'expondo', 'as', 'pratase', 'as', 'porcelanas', 'de', 'gosto', 'moderno', 'a', 'um', 'canto', 'dormia', ',', 'esquecida', 'na', 'sua', 'caixa', 'de', 'pinho', 'envernizado', ',', 'uma', 'maquina', 'de', 'costura', 'de', 'wilson', ',', 'das', 'primeiras', 'que', 'chegaram', 'ao', 'maranhao', 'nos', 'intervalos', 'dasportas', 'simetrizavam-se', 'quatro', 'estudos', 'de', 'julien', ',', 'representando', 'em', 'litografia', 'as', 'estacoes', 'do', 'ano', 'defrontedo', 'guarda-louca', 'um', 'relogio', 'de', 'corrente', 'embalava', 'melancolicamente', 'a', 'sua', 'pendula', 'do', 'tamanho', 'de', 'umprato', 'e', 'apontava', 'para', 'as', 'duas', 'horas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Duas horas da tarde'\n",
      "Tokens gerados: ['duas', 'horas', 'da', 'tarde']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não obstante, ainda permanecia sobre a mesa a louça que servira ao almoço'\n",
      "Tokens gerados: ['nao', 'obstante', ',', 'ainda', 'permanecia', 'sobre', 'a', 'mesa', 'a', 'louca', 'que', 'servira', 'ao', 'almoco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma garrafa\n",
      "branca, com uns restos de vinho de Lisboa cintilava à claridade reverberante que vinha do quintal'\n",
      "Tokens gerados: ['uma', 'garrafabranca', ',', 'com', 'uns', 'restos', 'de', 'vinho', 'de', 'lisboa', 'cintilava', 'a', 'claridade', 'reverberante', 'que', 'vinha', 'do', 'quintal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De\n",
      "uma gaiola, dependurada entre as janelas desse lado, chilreava um sabiá'\n",
      "Tokens gerados: ['deuma', 'gaiola', ',', 'dependurada', 'entre', 'as', 'janelas', 'desse', 'lado', ',', 'chilreava', 'um', 'sabia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fazia preguiça estar ali'\n",
      "Tokens gerados: ['fazia', 'preguica', 'estar', 'ali']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A viração do Bacanga refrescava o ar da varanda e dava ao ambiente um\n",
      "tom morno e aprazível'\n",
      "Tokens gerados: ['a', 'viracao', 'do', 'bacanga', 'refrescava', 'o', 'ar', 'da', 'varanda', 'e', 'dava', 'ao', 'ambiente', 'umtom', 'morno', 'e', 'aprazivel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia a quietação dos dias inúteis, uma vontade lassa de fechar os olhos e\n",
      "esticar as pernas'\n",
      "Tokens gerados: ['havia', 'a', 'quietacao', 'dos', 'dias', 'inuteis', ',', 'uma', 'vontade', 'lassa', 'de', 'fechar', 'os', 'olhos', 'eesticar', 'as', 'pernas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lá defronte, nas margens opostas do rio, a silenciosa vegetação do Anjo da Guarda\n",
      "estava a provocar boas sestas sobre o capim, debaixo das mangueiras; as árvores pareciam abrir de\n",
      "longe os braços, chamando a gente para a calma tepidez das suas sombras'\n",
      "Tokens gerados: ['la', 'defronte', ',', 'nas', 'margens', 'opostas', 'do', 'rio', ',', 'a', 'silenciosa', 'vegetacao', 'do', 'anjo', 'da', 'guardaestava', 'a', 'provocar', 'boas', 'sestas', 'sobre', 'o', 'capim', ',', 'debaixo', 'das', 'mangueiras', 'as', 'arvores', 'pareciam', 'abrir', 'delonge', 'os', 'bracos', ',', 'chamando', 'a', 'gente', 'para', 'a', 'calma', 'tepidez', 'das', 'suas', 'sombras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então, Ana Rosa, que me respondes?'\n",
      "Tokens gerados: ['—', 'entao', ',', 'ana', 'rosa', ',', 'que', 'me', 'respondes', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'disse Manuel, esticando-se mais na cadeira em que\n",
      "se achava assentado, à cabeceira da mesa, em frente da filha'\n",
      "Tokens gerados: ['disse', 'manuel', ',', 'esticando-se', 'mais', 'na', 'cadeira', 'em', 'quese', 'achava', 'assentado', ',', 'a', 'cabeceira', 'da', 'mesa', ',', 'em', 'frente', 'da', 'filha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bem sabes que te não contrario'\n",
      "Tokens gerados: ['bem', 'sabes', 'que', 'te', 'nao', 'contrario']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'desejo\n",
      "este casamento, desejo'\n",
      "Tokens gerados: ['desejoeste', 'casamento', ',', 'desejo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'mas, em primeiro lugar, convém saber se ele é do teu gosto'\n",
      "Tokens gerados: ['mas', ',', 'em', 'primeiro', 'lugar', ',', 'convem', 'saber', 'se', 'ele', 'e', 'do', 'teu', 'gosto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vamos'\n",
      "Tokens gerados: ['vamos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): ', fala!\n",
      "Ana Rosa não respondeu e continuou muito embebida, como estava, a rolar sob a ponta\n",
      "cor-de-rosa dos seus dedos as migalhas de pão que ia encontrando sobre a toalha'\n",
      "Tokens gerados: [',', 'falaana', 'rosa', 'nao', 'respondeu', 'e', 'continuou', 'muito', 'embebida', ',', 'como', 'estava', ',', 'a', 'rolar', 'sob', 'a', 'pontacor-de-rosa', 'dos', 'seus', 'dedos', 'as', 'migalhas', 'de', 'pao', 'que', 'ia', 'encontrando', 'sobre', 'a', 'toalha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel Pedro da Silva, mais conhecido por Manuel Pescada, era um português de uns cinqüenta anos, forte,\n",
      "vermelho e trabalhador'\n",
      "Tokens gerados: ['manuel', 'pedro', 'da', 'silva', ',', 'mais', 'conhecido', 'por', 'manuel', 'pescada', ',', 'era', 'um', 'portugues', 'de', 'uns', 'cinquenta', 'anos', ',', 'forte', ',', 'vermelho', 'e', 'trabalhador']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Diziam-no atilado para o comércio e amigo do Brasil'\n",
      "Tokens gerados: ['diziam-no', 'atilado', 'para', 'o', 'comercio', 'e', 'amigo', 'do', 'brasil']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Gostava da sua leitura nas horas de descanso,\n",
      "assinava respeitosamente os jornais sérios da província e recebia alguns de Lisboa'\n",
      "Tokens gerados: ['gostava', 'da', 'sua', 'leitura', 'nas', 'horas', 'de', 'descanso', ',', 'assinava', 'respeitosamente', 'os', 'jornais', 'serios', 'da', 'provincia', 'e', 'recebia', 'alguns', 'de', 'lisboa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em pequeno meteram-lhe na cabeça\n",
      "vários trechos do Camões e não lhe esconderam de todo o nome de outros poetas'\n",
      "Tokens gerados: ['em', 'pequeno', 'meteram-lhe', 'na', 'cabecavarios', 'trechos', 'do', 'camoes', 'e', 'nao', 'lhe', 'esconderam', 'de', 'todo', 'o', 'nome', 'de', 'outros', 'poetas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prezava com fanatismo o Marquês de\n",
      "Pombal, de quem sabia muitas anedotas e tinha uma assinatura no Gabinete Português, a qual lhe aproveitava menos a ele do\n",
      "que à filha, que era perdida pelo romance'\n",
      "Tokens gerados: ['prezava', 'com', 'fanatismo', 'o', 'marques', 'depombal', ',', 'de', 'quem', 'sabia', 'muitas', 'anedotas', 'e', 'tinha', 'uma', 'assinatura', 'no', 'gabinete', 'portugues', ',', 'a', 'qual', 'lhe', 'aproveitava', 'menos', 'a', 'ele', 'doque', 'a', 'filha', ',', 'que', 'era', 'perdida', 'pelo', 'romance']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel Pedro fora casado com uma senhora de Alcântara, chamada Mariana, muito virtuosa e,\n",
      "como a melhor parte das maranhenses, extremada em pontos de religião; quando morreu, deixou em\n",
      "legado seis escravos a Nossa Senhora do Carmo'\n",
      "Tokens gerados: ['manuel', 'pedro', 'fora', 'casado', 'com', 'uma', 'senhora', 'de', 'alcantara', ',', 'chamada', 'mariana', ',', 'muito', 'virtuosa', 'e', ',', 'como', 'a', 'melhor', 'parte', 'das', 'maranhenses', ',', 'extremada', 'em', 'pontos', 'de', 'religiao', 'quando', 'morreu', ',', 'deixou', 'emlegado', 'seis', 'escravos', 'a', 'nossa', 'senhora', 'do', 'carmo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bem triste foi essa época, tanto para o viúvo como para a filha, orfanada; coitadinha, justamente\n",
      "quando mais precisava do amparo maternal'\n",
      "Tokens gerados: ['bem', 'triste', 'foi', 'essa', 'epoca', ',', 'tanto', 'para', 'o', 'viuvo', 'como', 'para', 'a', 'filha', ',', 'orfanada', 'coitadinha', ',', 'justamentequando', 'mais', 'precisava', 'do', 'amparo', 'maternal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nesse tempo moravam no Caminho Grande, numa casinha\n",
      "térrea, para onde a moléstia de Mariana os levara em busca de ares mais benignos; Manuel, porém, que\n",
      "era já então negociante e tinha o seu armazém na Praia Grande, mudou-se logo com a pequena para o\n",
      "sobrado da Rua da Estrela, em cujas lojas prosperava, havia dez anos, no comércio de fazendas por\n",
      "atacado'\n",
      "Tokens gerados: ['nesse', 'tempo', 'moravam', 'no', 'caminho', 'grande', ',', 'numa', 'casinhaterrea', ',', 'para', 'onde', 'a', 'molestia', 'de', 'mariana', 'os', 'levara', 'em', 'busca', 'de', 'ares', 'mais', 'benignos', 'manuel', ',', 'porem', ',', 'queera', 'ja', 'entao', 'negociante', 'e', 'tinha', 'o', 'seu', 'armazem', 'na', 'praia', 'grande', ',', 'mudou-se', 'logo', 'com', 'a', 'pequena', 'para', 'osobrado', 'da', 'rua', 'da', 'estrela', ',', 'em', 'cujas', 'lojas', 'prosperava', ',', 'havia', 'dez', 'anos', ',', 'no', 'comercio', 'de', 'fazendas', 'poratacado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Para não ficar só com a filha “que se fazia uma mulher” convidou a sogra, D'\n",
      "Tokens gerados: ['para', 'nao', 'ficar', 'so', 'com', 'a', 'filha', '“', 'que', 'se', 'fazia', 'uma', 'mulher', '”', 'convidou', 'a', 'sogra', ',', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Maria Bárbara, a\n",
      "abandonar o sítio em que vivia e ir morar com ele e mais a neta'\n",
      "Tokens gerados: ['maria', 'barbara', ',', 'aabandonar', 'o', 'sitio', 'em', 'que', 'vivia', 'e', 'ir', 'morar', 'com', 'ele', 'e', 'mais', 'a', 'neta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“A menina precisava de alguém que a\n",
      "guiasse, que a conduzisse! Um homem nunca podia servir para essas coisas! E, se fosse a meter em casa\n",
      "uma preceptora — Meu bom Jesus! — que não diriam por ai?'\n",
      "Tokens gerados: ['“', 'a', 'menina', 'precisava', 'de', 'alguem', 'que', 'aguiasse', ',', 'que', 'a', 'conduzisse', 'um', 'homem', 'nunca', 'podia', 'servir', 'para', 'essas', 'coisas', 'e', ',', 'se', 'fosse', 'a', 'meter', 'em', 'casauma', 'preceptora', '—', 'meu', 'bom', 'jesus', '—', 'que', 'nao', 'diriam', 'por', 'ai', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No Maranhão falava-se de tudo! D'\n",
      "Tokens gerados: ['no', 'maranhao', 'falava-se', 'de', 'tudo', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Maria Bárbara que se decidisse a deixar o mato e fosse de muda para a Rua da Estrela! Não teria que se\n",
      "arrepender'\n",
      "Tokens gerados: ['maria', 'barbara', 'que', 'se', 'decidisse', 'a', 'deixar', 'o', 'mato', 'e', 'fosse', 'de', 'muda', 'para', 'a', 'rua', 'da', 'estrela', 'nao', 'teria', 'que', 'searrepender']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'havia de estar como em sua própria casa — bom quarto, boa mesa, e plena liberdade!”\n",
      "A velha aceitou e lá foi, arrastando os seus cinqüenta e tantos anos, alojar-se em casa do genro,\n",
      "com um batalhão de moleques, suas crias, e com os cacaréus ainda do tempo do defunto marido'\n",
      "Tokens gerados: ['havia', 'de', 'estar', 'como', 'em', 'sua', 'propria', 'casa', '—', 'bom', 'quarto', ',', 'boa', 'mesa', ',', 'e', 'plena', 'liberdade', '”', 'a', 'velha', 'aceitou', 'e', 'la', 'foi', ',', 'arrastando', 'os', 'seus', 'cinquenta', 'e', 'tantos', 'anos', ',', 'alojar-se', 'em', 'casa', 'do', 'genro', ',', 'com', 'um', 'batalhao', 'de', 'moleques', ',', 'suas', 'crias', ',', 'e', 'com', 'os', 'cacareus', 'ainda', 'do', 'tempo', 'do', 'defunto', 'marido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em\n",
      "breve, porém, o bom português estava arrependido do passo que dera: D'\n",
      "Tokens gerados: ['embreve', ',', 'porem', ',', 'o', 'bom', 'portugues', 'estava', 'arrependido', 'do', 'passo', 'que', 'dera', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Maria Bárbara, apesar de\n",
      "muito piedosa; apesar de não sair do quarto sem vir bem penteada, sem lhe faltar nenhum dos cachinhos\n",
      "de seda preta, com que ela emoldurava disparatadamente o rosto enrugado e macilento; apesar do seu\n",
      "grande fervor pela igreja e apesar das missas que papava por dia, D'\n",
      "Tokens gerados: ['maria', 'barbara', ',', 'apesar', 'demuito', 'piedosa', 'apesar', 'de', 'nao', 'sair', 'do', 'quarto', 'sem', 'vir', 'bem', 'penteada', ',', 'sem', 'lhe', 'faltar', 'nenhum', 'dos', 'cachinhosde', 'seda', 'preta', ',', 'com', 'que', 'ela', 'emoldurava', 'disparatadamente', 'o', 'rosto', 'enrugado', 'e', 'macilento', 'apesar', 'do', 'seugrande', 'fervor', 'pela', 'igreja', 'e', 'apesar', 'das', 'missas', 'que', 'papava', 'por', 'dia', ',', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Maria Bárbara, apesar de tudo isso,\n",
      "saíra-lhe “má dona de casa”'\n",
      "Tokens gerados: ['maria', 'barbara', ',', 'apesar', 'de', 'tudo', 'isso', ',', 'saira-lhe', '“', 'ma', 'dona', 'de', 'casa', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era uma fúria! Uma víbora! Dava nos escravos por hábito e por gosto; só falava a gritar e,\n",
      "quando se punha a ralhar — Deus nos acuda! —, incomodava toda a vizinhança! Insuportável!\n",
      "Maria Bárbara tinha o verdadeiro tipo das velhas maranhenses criadas na fazenda'\n",
      "Tokens gerados: ['era', 'uma', 'furia', 'uma', 'vibora', 'dava', 'nos', 'escravos', 'por', 'habito', 'e', 'por', 'gosto', 'so', 'falava', 'a', 'gritar', 'e', ',', 'quando', 'se', 'punha', 'a', 'ralhar', '—', 'deus', 'nos', 'acuda', '—', ',', 'incomodava', 'toda', 'a', 'vizinhanca', 'insuportavelmaria', 'barbara', 'tinha', 'o', 'verdadeiro', 'tipo', 'das', 'velhas', 'maranhenses', 'criadas', 'na', 'fazenda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tratava muito dos avós, quase\n",
      "todos portugueses; muito orgulhosa; muito cheia de escrúpulos de sangue'\n",
      "Tokens gerados: ['tratava', 'muito', 'dos', 'avos', ',', 'quasetodos', 'portugueses', 'muito', 'orgulhosa', 'muito', 'cheia', 'de', 'escrupulos', 'de', 'sangue']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando falava nos pretos, dizia “Os sujos” e,\n",
      "quando se referia a um mulato dizia “O cabra”'\n",
      "Tokens gerados: ['quando', 'falava', 'nos', 'pretos', ',', 'dizia', '“', 'os', 'sujos', '”', 'e', ',', 'quando', 'se', 'referia', 'a', 'um', 'mulato', 'dizia', '“', 'o', 'cabra', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sempre fora assim e, como devota, não havia outra: Em Alcântara, tivera\n",
      "uma capela de Santa Bárbara e obrigava a sua escravatura a rezar aí todas as noites, em coro, de braços abertos, às vezes\n",
      "algemados'\n",
      "Tokens gerados: ['sempre', 'fora', 'assim', 'e', ',', 'como', 'devota', ',', 'nao', 'havia', 'outra', 'em', 'alcantara', ',', 'tiverauma', 'capela', 'de', 'santa', 'barbara', 'e', 'obrigava', 'a', 'sua', 'escravatura', 'a', 'rezar', 'ai', 'todas', 'as', 'noites', ',', 'em', 'coro', ',', 'de', 'bracos', 'abertos', ',', 'as', 'vezesalgemados']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lembrava-se com grandes suspiros do marido “do seu João Hipólito” um português fino, de olhos azuis e\n",
      "cabelos louros'\n",
      "Tokens gerados: ['lembrava-se', 'com', 'grandes', 'suspiros', 'do', 'marido', '“', 'do', 'seu', 'joao', 'hipolito', '”', 'um', 'portugues', 'fino', ',', 'de', 'olhos', 'azuis', 'ecabelos', 'louros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Este João Hipólito foi brasileiro adotivo e chegou a fazer alguma posição oficial na secretaria\n",
      "do governo da província'\n",
      "Tokens gerados: ['este', 'joao', 'hipolito', 'foi', 'brasileiro', 'adotivo', 'e', 'chegou', 'a', 'fazer', 'alguma', 'posicao', 'oficial', 'na', 'secretariado', 'governo', 'da', 'provincia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Morreu com o posto de coronel'\n",
      "Tokens gerados: ['morreu', 'com', 'o', 'posto', 'de', 'coronel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Maria Bárbara tinha grande admiração pelos portugueses, dedicava-lhes um entusiasmo sem\n",
      "limites, preferia-os em tudo aos brasileiros'\n",
      "Tokens gerados: ['maria', 'barbara', 'tinha', 'grande', 'admiracao', 'pelos', 'portugueses', ',', 'dedicava-lhes', 'um', 'entusiasmo', 'semlimites', ',', 'preferia-os', 'em', 'tudo', 'aos', 'brasileiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando a filha foi pedida por Manuel Pedroso, então\n",
      "principiante no comércio da capital, ela dissera: “Bem! Ao menos tenho a certeza de que é branco!”\n",
      "Mas o Pescada não compreendeu a esposa, nem foi amado por ela; a virtude, ou talvez\n",
      "simplesmente a maternidade, apenas conseguiu fazer de Mariana uma companheira fiel; viveu\n",
      "exclusivamente para a filha'\n",
      "Tokens gerados: ['quando', 'a', 'filha', 'foi', 'pedida', 'por', 'manuel', 'pedroso', ',', 'entaoprincipiante', 'no', 'comercio', 'da', 'capital', ',', 'ela', 'dissera', '“', 'bem', 'ao', 'menos', 'tenho', 'a', 'certeza', 'de', 'que', 'e', 'branco', '”', 'mas', 'o', 'pescada', 'nao', 'compreendeu', 'a', 'esposa', ',', 'nem', 'foi', 'amado', 'por', 'ela', 'a', 'virtude', ',', 'ou', 'talvezsimplesmente', 'a', 'maternidade', ',', 'apenas', 'conseguiu', 'fazer', 'de', 'mariana', 'uma', 'companheira', 'fiel', 'viveuexclusivamente', 'para', 'a', 'filha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É que a desgraçada, desde os quinze anos, ainda no irresponsável\n",
      "arrebatamento do primeiro amor, havia eleito já o homem a quem sua alma teria de pertencer por toda\n",
      "a vida'\n",
      "Tokens gerados: ['e', 'que', 'a', 'desgracada', ',', 'desde', 'os', 'quinze', 'anos', ',', 'ainda', 'no', 'irresponsavelarrebatamento', 'do', 'primeiro', 'amor', ',', 'havia', 'eleito', 'ja', 'o', 'homem', 'a', 'quem', 'sua', 'alma', 'teria', 'de', 'pertencer', 'por', 'todaa', 'vida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Esse homem existe hoje na história do Maranhão, era o agitador José Cândido de Moraes e Silva\n",
      "conhecido popularmente pelo “Farol”'\n",
      "Tokens gerados: ['esse', 'homem', 'existe', 'hoje', 'na', 'historia', 'do', 'maranhao', ',', 'era', 'o', 'agitador', 'jose', 'candido', 'de', 'moraes', 'e', 'silvaconhecido', 'popularmente', 'pelo', '“', 'farol', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fez todo o possível para casar com ele, mas foram baldados os\n",
      "seus esforços, nem só em virtude das perseguições políticas que, tão cedo, atribularam a curta existência\n",
      "daquela fenomenal criatura, como também pela inflexível oposição que tal idéia encontrou na própria\n",
      "família da rapariga'\n",
      "Tokens gerados: ['fez', 'todo', 'o', 'possivel', 'para', 'casar', 'com', 'ele', ',', 'mas', 'foram', 'baldados', 'osseus', 'esforcos', ',', 'nem', 'so', 'em', 'virtude', 'das', 'perseguicoes', 'politicas', 'que', ',', 'tao', 'cedo', ',', 'atribularam', 'a', 'curta', 'existenciadaquela', 'fenomenal', 'criatura', ',', 'como', 'tambem', 'pela', 'inflexivel', 'oposicao', 'que', 'tal', 'ideia', 'encontrou', 'na', 'propriafamilia', 'da', 'rapariga']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, o destino dela se havia prendido à sorte do desventurado maranhense'\n",
      "Tokens gerados: ['entretanto', ',', 'o', 'destino', 'dela', 'se', 'havia', 'prendido', 'a', 'sorte', 'do', 'desventurado', 'maranhense']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quem diria\n",
      "que aquela pobre moça, nascida e criada nos sertões do Norte, sentiria, como qualquer filha das grandes\n",
      "capitais, a mágica influência que os homens superiores exercem sobre o espírito feminino? Amou-o,\n",
      "sem saber por quê'\n",
      "Tokens gerados: ['quem', 'diriaque', 'aquela', 'pobre', 'moca', ',', 'nascida', 'e', 'criada', 'nos', 'sertoes', 'do', 'norte', ',', 'sentiria', ',', 'como', 'qualquer', 'filha', 'das', 'grandescapitais', ',', 'a', 'magica', 'influencia', 'que', 'os', 'homens', 'superiores', 'exercem', 'sobre', 'o', 'espirito', 'feminino', '?', 'amou-o', ',', 'sem', 'saber', 'por', 'que']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sentira-lhe a força dominadora do olhar, os ímpetos revolucionários do seu caráter\n",
      "americano, o heroísmo patriótico da sua individualidade tão superior ao meio em que floresceu;\n",
      "decorara-lhe as frases apaixonadas e vibrantes de indignação, com que ele fulminava os exploradores\n",
      "da sua pátria estremecida e os inimigos da integridade nacional; e tudo isso, sem que ela soubesse\n",
      "explicar, arrebatou-a para o belo e destemido moço com todo o ardor do seu primeiro desejo de mulher'\n",
      "Tokens gerados: ['sentira-lhe', 'a', 'forca', 'dominadora', 'do', 'olhar', ',', 'os', 'impetos', 'revolucionarios', 'do', 'seu', 'carateramericano', ',', 'o', 'heroismo', 'patriotico', 'da', 'sua', 'individualidade', 'tao', 'superior', 'ao', 'meio', 'em', 'que', 'floresceudecorara-lhe', 'as', 'frases', 'apaixonadas', 'e', 'vibrantes', 'de', 'indignacao', ',', 'com', 'que', 'ele', 'fulminava', 'os', 'exploradoresda', 'sua', 'patria', 'estremecida', 'e', 'os', 'inimigos', 'da', 'integridade', 'nacional', 'e', 'tudo', 'isso', ',', 'sem', 'que', 'ela', 'soubesseexplicar', ',', 'arrebatou-a', 'para', 'o', 'belo', 'e', 'destemido', 'moco', 'com', 'todo', 'o', 'ardor', 'do', 'seu', 'primeiro', 'desejo', 'de', 'mulher']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando, na Rua dos Remédios, que nesse tempo era ainda um arrabalde, o desditoso herói,\n",
      "apenas com pouco mais de vinte e cinco anos de idade sucumbiu ao jugo do seu próprio talento e da sua\n",
      "honra política, oculto, foragido, cheio de miséria, odiado por uns como um assassino e adorado por\n",
      "outros como um deus, a pobre senhora deixou-se possuir de uma grande tristeza e foi enfraquecendo, e\n",
      "ficando doente, e ficando feia e cada vez mais triste, até morrer silenciosamente poucos anos depois do\n",
      "seu amado'\n",
      "Tokens gerados: ['quando', ',', 'na', 'rua', 'dos', 'remedios', ',', 'que', 'nesse', 'tempo', 'era', 'ainda', 'um', 'arrabalde', ',', 'o', 'desditoso', 'heroi', ',', 'apenas', 'com', 'pouco', 'mais', 'de', 'vinte', 'e', 'cinco', 'anos', 'de', 'idade', 'sucumbiu', 'ao', 'jugo', 'do', 'seu', 'proprio', 'talento', 'e', 'da', 'suahonra', 'politica', ',', 'oculto', ',', 'foragido', ',', 'cheio', 'de', 'miseria', ',', 'odiado', 'por', 'uns', 'como', 'um', 'assassino', 'e', 'adorado', 'poroutros', 'como', 'um', 'deus', ',', 'a', 'pobre', 'senhora', 'deixou-se', 'possuir', 'de', 'uma', 'grande', 'tristeza', 'e', 'foi', 'enfraquecendo', ',', 'eficando', 'doente', ',', 'e', 'ficando', 'feia', 'e', 'cada', 'vez', 'mais', 'triste', ',', 'ate', 'morrer', 'silenciosamente', 'poucos', 'anos', 'depois', 'doseu', 'amado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ana Rosa não chegou a conhecer o Farol; a mãe porém, muito em segredo, ensinara-lhe a\n",
      "compreender e respeitar a memória do talentoso revolucionário, cujo nome de guerra despertava ainda,\n",
      "entre os portugueses, a raiva antiga do motim de 7 de agosto de 1831'\n",
      "Tokens gerados: ['ana', 'rosa', 'nao', 'chegou', 'a', 'conhecer', 'o', 'farol', 'a', 'mae', 'porem', ',', 'muito', 'em', 'segredo', ',', 'ensinara-lhe', 'acompreender', 'e', 'respeitar', 'a', 'memoria', 'do', 'talentoso', 'revolucionario', ',', 'cujo', 'nome', 'de', 'guerra', 'despertava', 'ainda', ',', 'entre', 'os', 'portugueses', ',', 'a', 'raiva', 'antiga', 'do', 'motim', 'de', '7', 'de', 'agosto', 'de', '1831']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Minha filha, disse-lhe a infeliz\n",
      "já nas vésperas da morte, não consintas nunca que te casem, sem que ames deveras o homem a ti\n",
      "destinado para marido'\n",
      "Tokens gerados: ['“', 'minha', 'filha', ',', 'disse-lhe', 'a', 'infelizja', 'nas', 'vesperas', 'da', 'morte', ',', 'nao', 'consintas', 'nunca', 'que', 'te', 'casem', ',', 'sem', 'que', 'ames', 'deveras', 'o', 'homem', 'a', 'tidestinado', 'para', 'marido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não te cases no ar! Lembra-te que o casamento deve ser sempre a conseqüência\n",
      "de duas inclinações irresistíveis'\n",
      "Tokens gerados: ['nao', 'te', 'cases', 'no', 'ar', 'lembra-te', 'que', 'o', 'casamento', 'deve', 'ser', 'sempre', 'a', 'consequenciade', 'duas', 'inclinacoes', 'irresistiveis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A gente deve casar porque ama, e não ter de amar porque casou'\n",
      "Tokens gerados: ['a', 'gente', 'deve', 'casar', 'porque', 'ama', ',', 'e', 'nao', 'ter', 'de', 'amar', 'porque', 'casou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se\n",
      "fizeres o que te digo, serás feliz!” Concluiu pedindo-lhe que prometesse, caso algum dia viessem a\n",
      "constrangê-la a aceitar marido contra seu gosto, arrostar tudo, tudo, para evitar semelhante desgraça,\n",
      "principalmente se então Ana Rosa já gostasse de outro; e por este, sim, fosse quem fosse, cometesse os\n",
      "maiores sacrifícios, arriscasse a própria vida, porque era nisso que consistia a verdadeira honestidade\n",
      "de uma moça'\n",
      "Tokens gerados: ['sefizeres', 'o', 'que', 'te', 'digo', ',', 'seras', 'feliz', '”', 'concluiu', 'pedindo-lhe', 'que', 'prometesse', ',', 'caso', 'algum', 'dia', 'viessem', 'aconstrange-la', 'a', 'aceitar', 'marido', 'contra', 'seu', 'gosto', ',', 'arrostar', 'tudo', ',', 'tudo', ',', 'para', 'evitar', 'semelhante', 'desgraca', ',', 'principalmente', 'se', 'entao', 'ana', 'rosa', 'ja', 'gostasse', 'de', 'outro', 'e', 'por', 'este', ',', 'sim', ',', 'fosse', 'quem', 'fosse', ',', 'cometesse', 'osmaiores', 'sacrificios', ',', 'arriscasse', 'a', 'propria', 'vida', ',', 'porque', 'era', 'nisso', 'que', 'consistia', 'a', 'verdadeira', 'honestidadede', 'uma', 'moca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E mais não foram os conselhos que Mariana deu à filha'\n",
      "Tokens gerados: ['e', 'mais', 'nao', 'foram', 'os', 'conselhos', 'que', 'mariana', 'deu', 'a', 'filha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ana Rosa era criança, não os compreendeu\n",
      "logo, nem tão cedo procurou compreendê-los; mas, tão ligados estavam eles à morte da mãe, que a\n",
      "idéia desta não lhe acudia à memória sem as palavras da moribunda'\n",
      "Tokens gerados: ['ana', 'rosa', 'era', 'crianca', ',', 'nao', 'os', 'compreendeulogo', ',', 'nem', 'tao', 'cedo', 'procurou', 'compreende-los', 'mas', ',', 'tao', 'ligados', 'estavam', 'eles', 'a', 'morte', 'da', 'mae', ',', 'que', 'aideia', 'desta', 'nao', 'lhe', 'acudia', 'a', 'memoria', 'sem', 'as', 'palavras', 'da', 'moribunda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel Pedro, apesar de bom, era um desses homens mais que alheados às sutilezas do\n",
      "sentimento; para outra mulher daria talvez um excelente esposo, não para aquela, cuja sensibilidade\n",
      "romântica, longe de o comover, havia muita vez de importuná-lo'\n",
      "Tokens gerados: ['manuel', 'pedro', ',', 'apesar', 'de', 'bom', ',', 'era', 'um', 'desses', 'homens', 'mais', 'que', 'alheados', 'as', 'sutilezas', 'dosentimento', 'para', 'outra', 'mulher', 'daria', 'talvez', 'um', 'excelente', 'esposo', ',', 'nao', 'para', 'aquela', ',', 'cuja', 'sensibilidaderomantica', ',', 'longe', 'de', 'o', 'comover', ',', 'havia', 'muita', 'vez', 'de', 'importuna-lo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando se achou viúvo, não sentiu,\n",
      "a despeito da sua natural bondade, mais do que certo desgosto pela ausência de uma companheira com\n",
      "que já se tinha habituado; contudo, não pensou em tornar a casar, convencido de que o afeto da filha lhe\n",
      "chegaria de sobra para amenizar as canseiras do trabalho, e que o auxílio imediato da sogra bastaria\n",
      "para garantir a decência da sua casa e a boa regra das suas despesas domésticas'\n",
      "Tokens gerados: ['quando', 'se', 'achou', 'viuvo', ',', 'nao', 'sentiu', ',', 'a', 'despeito', 'da', 'sua', 'natural', 'bondade', ',', 'mais', 'do', 'que', 'certo', 'desgosto', 'pela', 'ausencia', 'de', 'uma', 'companheira', 'comque', 'ja', 'se', 'tinha', 'habituado', 'contudo', ',', 'nao', 'pensou', 'em', 'tornar', 'a', 'casar', ',', 'convencido', 'de', 'que', 'o', 'afeto', 'da', 'filha', 'lhechegaria', 'de', 'sobra', 'para', 'amenizar', 'as', 'canseiras', 'do', 'trabalho', ',', 'e', 'que', 'o', 'auxilio', 'imediato', 'da', 'sogra', 'bastariapara', 'garantir', 'a', 'decencia', 'da', 'sua', 'casa', 'e', 'a', 'boa', 'regra', 'das', 'suas', 'despesas', 'domesticas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ana Rosa cresceu pois, como se vê, entre os desvelos insuficientes do pai e o mau gênio da avó'\n",
      "Tokens gerados: ['ana', 'rosa', 'cresceu', 'pois', ',', 'como', 'se', 've', ',', 'entre', 'os', 'desvelos', 'insuficientes', 'do', 'pai', 'e', 'o', 'mau', 'genio', 'da', 'avo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ainda assim aprendera de cor a gramática do Sotero dos Reis; lera alguma coisa; sabia rudimentos de\n",
      "francês e tocava modinhas sentimentais ao violão e ao piano'\n",
      "Tokens gerados: ['ainda', 'assim', 'aprendera', 'de', 'cor', 'a', 'gramatica', 'do', 'sotero', 'dos', 'reis', 'lera', 'alguma', 'coisa', 'sabia', 'rudimentos', 'defrances', 'e', 'tocava', 'modinhas', 'sentimentais', 'ao', 'violao', 'e', 'ao', 'piano']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não era estúpida; tinha a intuição perfeita\n",
      "da virtude, um modo bonito, e por vezes lamentara não ser mais instruída'\n",
      "Tokens gerados: ['nao', 'era', 'estupida', 'tinha', 'a', 'intuicao', 'perfeitada', 'virtude', ',', 'um', 'modo', 'bonito', ',', 'e', 'por', 'vezes', 'lamentara', 'nao', 'ser', 'mais', 'instruida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Conhecia muitos trabalhos\n",
      "de agulha; bordava como poucas, e dispunha de uma gargantazinha de contralto que fazia gosto ouvir'\n",
      "Tokens gerados: ['conhecia', 'muitos', 'trabalhosde', 'agulha', 'bordava', 'como', 'poucas', ',', 'e', 'dispunha', 'de', 'uma', 'gargantazinha', 'de', 'contralto', 'que', 'fazia', 'gosto', 'ouvir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tanto assim que, em pequena, servira várias vezes de anjo da verônica nas procissões da quaresma'\n",
      "Tokens gerados: ['tanto', 'assim', 'que', ',', 'em', 'pequena', ',', 'servira', 'varias', 'vezes', 'de', 'anjo', 'da', 'veronica', 'nas', 'procissoes', 'da', 'quaresma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E os cônegos da Sé gabavam-lhe o metal da voz e davam-lhe grandes cartuchos de amêndoas de\n",
      "mendubim, muito enfeitados nas suas pinturas, toscas e características, feitas a goma-arábica e tintas de\n",
      "botica'\n",
      "Tokens gerados: ['e', 'os', 'conegos', 'da', 'se', 'gabavam-lhe', 'o', 'metal', 'da', 'voz', 'e', 'davam-lhe', 'grandes', 'cartuchos', 'de', 'amendoas', 'demendubim', ',', 'muito', 'enfeitados', 'nas', 'suas', 'pinturas', ',', 'toscas', 'e', 'caracteristicas', ',', 'feitas', 'a', 'goma-arabica', 'e', 'tintas', 'debotica']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nessas ocasiões ela sentia-se radiante, com as faces carminadas, a cabeça coberta de cachos\n",
      "artificiais, grande roda no vestido curto, a jeito de dançarina'\n",
      "Tokens gerados: ['nessas', 'ocasioes', 'ela', 'sentia-se', 'radiante', ',', 'com', 'as', 'faces', 'carminadas', ',', 'a', 'cabeca', 'coberta', 'de', 'cachosartificiais', ',', 'grande', 'roda', 'no', 'vestido', 'curto', ',', 'a', 'jeito', 'de', 'dancarina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, muito concha, ufana dos seus galões de\n",
      "prata e ouro e das suas trêmulas asas de papelão e escumillha, caminhava triunfante e feliz no meio do\n",
      "cordão das irmandades religiosas, segurando a extremidade de um lenço, do qual o pai segurava a\n",
      "outra'\n",
      "Tokens gerados: ['e', ',', 'muito', 'concha', ',', 'ufana', 'dos', 'seus', 'galoes', 'deprata', 'e', 'ouro', 'e', 'das', 'suas', 'tremulas', 'asas', 'de', 'papelao', 'e', 'escumillha', ',', 'caminhava', 'triunfante', 'e', 'feliz', 'no', 'meio', 'docordao', 'das', 'irmandades', 'religiosas', ',', 'segurando', 'a', 'extremidade', 'de', 'um', 'lenco', ',', 'do', 'qual', 'o', 'pai', 'segurava', 'aoutra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Isto eram promessas feitas pela mãe ou pela avó em dias de grande enfermidade na família'\n",
      "Tokens gerados: ['isto', 'eram', 'promessas', 'feitas', 'pela', 'mae', 'ou', 'pela', 'avo', 'em', 'dias', 'de', 'grande', 'enfermidade', 'na', 'familia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E crescera sempre bonita de formas'\n",
      "Tokens gerados: ['e', 'crescera', 'sempre', 'bonita', 'de', 'formas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha os olhos pretos e os cabelos castanhos de Mariana, e\n",
      "puxara ao pai as rijezas de corpo e os dentes fortes'\n",
      "Tokens gerados: ['tinha', 'os', 'olhos', 'pretos', 'e', 'os', 'cabelos', 'castanhos', 'de', 'mariana', ',', 'epuxara', 'ao', 'pai', 'as', 'rijezas', 'de', 'corpo', 'e', 'os', 'dentes', 'fortes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Com a aproximação da puberdade apareceram-lhe\n",
      "caprichos românticos e fantasias poéticas: gostava dos passeios ao luar, das serenatas; arranjou ao lado\n",
      "do seu quarto um gabinete de estudo, uma bibliotecazinha de poetas e romancistas; tinha um Paulo e\n",
      "Virgínia de biscuit sobre a estante e, escondido por detrás de um espelho, o retrato do Farol, que herdara\n",
      "de Mariana'\n",
      "Tokens gerados: ['com', 'a', 'aproximacao', 'da', 'puberdade', 'apareceram-lhecaprichos', 'romanticos', 'e', 'fantasias', 'poeticas', 'gostava', 'dos', 'passeios', 'ao', 'luar', ',', 'das', 'serenatas', 'arranjou', 'ao', 'ladodo', 'seu', 'quarto', 'um', 'gabinete', 'de', 'estudo', ',', 'uma', 'bibliotecazinha', 'de', 'poetas', 'e', 'romancistas', 'tinha', 'um', 'paulo', 'evirginia', 'de', 'biscuit', 'sobre', 'a', 'estante', 'e', ',', 'escondido', 'por', 'detras', 'de', 'um', 'espelho', ',', 'o', 'retrato', 'do', 'farol', ',', 'que', 'herdarade', 'mariana']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lera com entusiasmo a Graziela de Lamartine'\n",
      "Tokens gerados: ['lera', 'com', 'entusiasmo', 'a', 'graziela', 'de', 'lamartine']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Chorou muito com essa leitura e, desde aí, todas\n",
      "as noites, antes de adormecer, procurava instintivamente imitar o sorriso de inocência que a procitana\n",
      "oferecia ao seu amante'\n",
      "Tokens gerados: ['chorou', 'muito', 'com', 'essa', 'leitura', 'e', ',', 'desde', 'ai', ',', 'todasas', 'noites', ',', 'antes', 'de', 'adormecer', ',', 'procurava', 'instintivamente', 'imitar', 'o', 'sorriso', 'de', 'inocencia', 'que', 'a', 'procitanaoferecia', 'ao', 'seu', 'amante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Praticava bem com os pobres, adorava os passarinhos e não podia ver matar\n",
      "perto de si uma borboleta'\n",
      "Tokens gerados: ['praticava', 'bem', 'com', 'os', 'pobres', ',', 'adorava', 'os', 'passarinhos', 'e', 'nao', 'podia', 'ver', 'matarperto', 'de', 'si', 'uma', 'borboleta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era um bocadinho supersticiosa: não queria as chinelas emborcadas debaixo\n",
      "da rede e só aparava os cabelos durante o quarto crescente da lua'\n",
      "Tokens gerados: ['era', 'um', 'bocadinho', 'supersticiosa', 'nao', 'queria', 'as', 'chinelas', 'emborcadas', 'debaixoda', 'rede', 'e', 'so', 'aparava', 'os', 'cabelos', 'durante', 'o', 'quarto', 'crescente', 'da', 'lua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Não que acreditasse nessas coisas”,\n",
      "justificava-se ela, “mas fazia porque os outros faziam'\n",
      "Tokens gerados: ['“', 'nao', 'que', 'acreditasse', 'nessas', 'coisas', '”', ',', 'justificava-se', 'ela', ',', '“', 'mas', 'fazia', 'porque', 'os', 'outros', 'faziam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Sobre a cômoda, havia muito tempo, tinha uma\n",
      "estampa litográfica e colorida de Nossa Senhora dos Remédios e rezava-lhe todas as noites, antes de\n",
      "dormir'\n",
      "Tokens gerados: ['“', 'sobre', 'a', 'comoda', ',', 'havia', 'muito', 'tempo', ',', 'tinha', 'umaestampa', 'litografica', 'e', 'colorida', 'de', 'nossa', 'senhora', 'dos', 'remedios', 'e', 'rezava-lhe', 'todas', 'as', 'noites', ',', 'antes', 'dedormir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nada conhecia melhor e mais agradável do que um passeio ao Cutim, e, quando soube que se\n",
      "projetava uma linha de bondes até lá, teve uma satisfação violenta e nervosa'\n",
      "Tokens gerados: ['nada', 'conhecia', 'melhor', 'e', 'mais', 'agradavel', 'do', 'que', 'um', 'passeio', 'ao', 'cutim', ',', 'e', ',', 'quando', 'soube', 'que', 'seprojetava', 'uma', 'linha', 'de', 'bondes', 'ate', 'la', ',', 'teve', 'uma', 'satisfacao', 'violenta', 'e', 'nervosa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Feitos os quinze anos, ela começou pouco a pouco a descobrir em si estranhas mudanças;\n",
      "percebeu, sentiu que uma transformação importante se operava no seu espírito e no seu corpo:\n",
      "sobressaltavam-na terrores infundados; acometiam-na tristezas sem motivo justificável'\n",
      "Tokens gerados: ['feitos', 'os', 'quinze', 'anos', ',', 'ela', 'comecou', 'pouco', 'a', 'pouco', 'a', 'descobrir', 'em', 'si', 'estranhas', 'mudancaspercebeu', ',', 'sentiu', 'que', 'uma', 'transformacao', 'importante', 'se', 'operava', 'no', 'seu', 'espirito', 'e', 'no', 'seu', 'corposobressaltavam-na', 'terrores', 'infundados', 'acometiam-na', 'tristezas', 'sem', 'motivo', 'justificavel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um dia, afinal,\n",
      "acordou mais preocupada; assentou-se na rede, a cismar'\n",
      "Tokens gerados: ['um', 'dia', ',', 'afinal', ',', 'acordou', 'mais', 'preocupada', 'assentou-se', 'na', 'rede', ',', 'a', 'cismar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, com surpresa, reparou que seus membros\n",
      "ultimamente se tinham arredondado; notou que em todo seu corpo a linha curva suplantara a reta e que\n",
      "as suas formas eram já completamente de mulher'\n",
      "Tokens gerados: ['e', ',', 'com', 'surpresa', ',', 'reparou', 'que', 'seus', 'membrosultimamente', 'se', 'tinham', 'arredondado', 'notou', 'que', 'em', 'todo', 'seu', 'corpo', 'a', 'linha', 'curva', 'suplantara', 'a', 'reta', 'e', 'queas', 'suas', 'formas', 'eram', 'ja', 'completamente', 'de', 'mulher']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Veio-lhe então um sobressalto de contentamento mas logo depois caiu a entristecer: sentia-se\n",
      "muito só; não lhe bastava o amor do pai e da velha Barbara; queria uma afeição mais exclusiva, mais\n",
      "dela'\n",
      "Tokens gerados: ['veio-lhe', 'entao', 'um', 'sobressalto', 'de', 'contentamento', 'mas', 'logo', 'depois', 'caiu', 'a', 'entristecer', 'sentia-semuito', 'so', 'nao', 'lhe', 'bastava', 'o', 'amor', 'do', 'pai', 'e', 'da', 'velha', 'barbara', 'queria', 'uma', 'afeicao', 'mais', 'exclusiva', ',', 'maisdela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lembrou-se dos seus namoros'\n",
      "Tokens gerados: ['lembrou-se', 'dos', 'seus', 'namoros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Riu-se “coisas de criança!'\n",
      "Tokens gerados: ['riu-se', '“', 'coisas', 'de', 'crianca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Aos doze anos namorara um estudante do Liceu'\n",
      "Tokens gerados: ['”', 'aos', 'doze', 'anos', 'namorara', 'um', 'estudante', 'do', 'liceu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Haviam conversado três ou quatro vezes na\n",
      "sala do pai e supunham-se deveras apaixonados um pelo outro; o estudante seguiu para a Escola Central\n",
      "da Corte, e ela nunca mais pensou nele'\n",
      "Tokens gerados: ['haviam', 'conversado', 'tres', 'ou', 'quatro', 'vezes', 'nasala', 'do', 'pai', 'e', 'supunham-se', 'deveras', 'apaixonados', 'um', 'pelo', 'outro', 'o', 'estudante', 'seguiu', 'para', 'a', 'escola', 'centralda', 'corte', ',', 'e', 'ela', 'nunca', 'mais', 'pensou', 'nele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois foi um oficial de marinha; “Como lhe ficava bem a\n",
      "farda!'\n",
      "Tokens gerados: ['depois', 'foi', 'um', 'oficial', 'de', 'marinha', '“', 'como', 'lhe', 'ficava', 'bem', 'afarda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que moço engraçado! bonito! e como sabia vestir-se!'\n",
      "Tokens gerados: ['que', 'moco', 'engracado', 'bonito', 'e', 'como', 'sabia', 'vestir-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ana Rosa chegou a principiar a bordar\n",
      "um par de chinelas para lho oferecer; antes porém de terminado o primeiro pé, já o bandoleiro havia\n",
      "desaparecido com a corveta “Baiana”'\n",
      "Tokens gerados: ['ana', 'rosa', 'chegou', 'a', 'principiar', 'a', 'bordarum', 'par', 'de', 'chinelas', 'para', 'lho', 'oferecer', 'antes', 'porem', 'de', 'terminado', 'o', 'primeiro', 'pe', ',', 'ja', 'o', 'bandoleiro', 'haviadesaparecido', 'com', 'a', 'corveta', '“', 'baiana', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seguiu-se um empregado do comércio'\n",
      "Tokens gerados: ['seguiu-se', 'um', 'empregado', 'do', 'comercio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Muito bom rapaz! muito\n",
      "cuidadoso da roupa e das unhas!'\n",
      "Tokens gerados: ['“', 'muito', 'bom', 'rapaz', 'muitocuidadoso', 'da', 'roupa', 'e', 'das', 'unhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” Parecia-lhe que ainda estava a vê-lo, todo metódico, escolhendo\n",
      "palavras para lhe pedir “a subida honra de dançar com ela uma quadrilha”'\n",
      "Tokens gerados: ['”', 'parecia-lhe', 'que', 'ainda', 'estava', 'a', 've-lo', ',', 'todo', 'metodico', ',', 'escolhendopalavras', 'para', 'lhe', 'pedir', '“', 'a', 'subida', 'honra', 'de', 'dancar', 'com', 'ela', 'uma', 'quadrilha', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah tempos! tempos!'\n",
      "Tokens gerados: ['—', 'ah', 'tempos', 'tempos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E não queria pensar ainda em semelhantes tolices'\n",
      "Tokens gerados: ['e', 'nao', 'queria', 'pensar', 'ainda', 'em', 'semelhantes', 'tolices']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Coisas de criança! Coisas de criança!'\n",
      "Tokens gerados: ['“', 'coisas', 'de', 'crianca', 'coisas', 'de', 'crianca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Agora, só o que lhe convinha era um marido! “O seu”, o verdadeiro, o lega!! O homem da sua casa, o\n",
      "dono do seu corpo, a quem ela pudesse amar abertamente como amante e obedecer em segredo como\n",
      "escrava'\n",
      "Tokens gerados: ['”', 'agora', ',', 'so', 'o', 'que', 'lhe', 'convinha', 'era', 'um', 'marido', '“', 'o', 'seu', '”', ',', 'o', 'verdadeiro', ',', 'o', 'lega', 'o', 'homem', 'da', 'sua', 'casa', ',', 'odono', 'do', 'seu', 'corpo', ',', 'a', 'quem', 'ela', 'pudesse', 'amar', 'abertamente', 'como', 'amante', 'e', 'obedecer', 'em', 'segredo', 'comoescrava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Precisava de dar-se e dedicar-se a alguém; sentia absoluta necessidade de pôr em ação a\n",
      "competência, que ela em si reconhecia, para tomar conta de uma casa e educar muitos filhos'\n",
      "Tokens gerados: ['precisava', 'de', 'dar-se', 'e', 'dedicar-se', 'a', 'alguem', 'sentia', 'absoluta', 'necessidade', 'de', 'por', 'em', 'acao', 'acompetencia', ',', 'que', 'ela', 'em', 'si', 'reconhecia', ',', 'para', 'tomar', 'conta', 'de', 'uma', 'casa', 'e', 'educar', 'muitos', 'filhos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Com estes devaneios, acudia-lhe sempre um arrepiozinho de febre; ficava excitada, idealizando\n",
      "um homem forte, corajoso, com um bonito talento, e capaz de matar-se por ela'\n",
      "Tokens gerados: ['com', 'estes', 'devaneios', ',', 'acudia-lhe', 'sempre', 'um', 'arrepiozinho', 'de', 'febre', 'ficava', 'excitada', ',', 'idealizandoum', 'homem', 'forte', ',', 'corajoso', ',', 'com', 'um', 'bonito', 'talento', ',', 'e', 'capaz', 'de', 'matar-se', 'por', 'ela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, nos seus sonhos\n",
      "agitados, debuxava-se um vulto confuso, mas encantador, que galgava precipícios, para chegar onde\n",
      "ela estava e merecer-lhe a ventura de um sorriso, uma doce esperança de casamento'\n",
      "Tokens gerados: ['e', ',', 'nos', 'seus', 'sonhosagitados', ',', 'debuxava-se', 'um', 'vulto', 'confuso', ',', 'mas', 'encantador', ',', 'que', 'galgava', 'precipicios', ',', 'para', 'chegar', 'ondeela', 'estava', 'e', 'merecer-lhe', 'a', 'ventura', 'de', 'um', 'sorriso', ',', 'uma', 'doce', 'esperanca', 'de', 'casamento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E sonhava o\n",
      "noivado: um banquete esplêndido! e junto dela, ao alcance de seus lábios, um mancebo apaixonado e\n",
      "formoso, um conjunto de força, graça e ternura, que a seus pés ardia de impaciência e devorava-a com\n",
      "o olhar em fogo'\n",
      "Tokens gerados: ['e', 'sonhava', 'onoivado', 'um', 'banquete', 'esplendido', 'e', 'junto', 'dela', ',', 'ao', 'alcance', 'de', 'seus', 'labios', ',', 'um', 'mancebo', 'apaixonado', 'eformoso', ',', 'um', 'conjunto', 'de', 'forca', ',', 'graca', 'e', 'ternura', ',', 'que', 'a', 'seus', 'pes', 'ardia', 'de', 'impaciencia', 'e', 'devorava-a', 'como', 'olhar', 'em', 'fogo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois — via-se dona de casa; pensando muito nos filhos; sonhava-se feliz, muito dependente\n",
      "na prisão do ninho e no domínio carinhoso do marido'\n",
      "Tokens gerados: ['depois', '—', 'via-se', 'dona', 'de', 'casa', 'pensando', 'muito', 'nos', 'filhos', 'sonhava-se', 'feliz', ',', 'muito', 'dependentena', 'prisao', 'do', 'ninho', 'e', 'no', 'dominio', 'carinhoso', 'do', 'marido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E sonhava umas criancinhas louras, ternas,\n",
      "balbuciando tolices engraçadas e comovedoras, chamando-lhe “mamã!”\n",
      "— Oh! Como devia ser bom!'\n",
      "Tokens gerados: ['e', 'sonhava', 'umas', 'criancinhas', 'louras', ',', 'ternas', ',', 'balbuciando', 'tolices', 'engracadas', 'e', 'comovedoras', ',', 'chamando-lhe', '“', 'mama', '”', '—', 'oh', 'como', 'devia', 'ser', 'bom']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E pensar que havia por aí mulheres que eram contra o casamento!'\n",
      "Tokens gerados: ['e', 'pensar', 'que', 'havia', 'por', 'ai', 'mulheres', 'que', 'eram', 'contra', 'o', 'casamento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não! Ela não podia admitir o celibato, principalmente para a mulher!'\n",
      "Tokens gerados: ['nao', 'ela', 'nao', 'podia', 'admitir', 'o', 'celibato', ',', 'principalmente', 'para', 'a', 'mulher']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Para o homem — ainda\n",
      "passava'\n",
      "Tokens gerados: ['“', 'para', 'o', 'homem', '—', 'aindapassava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'viveria triste, só; mas em todo o caso — era um homem'\n",
      "Tokens gerados: ['viveria', 'triste', ',', 'so', 'mas', 'em', 'todo', 'o', 'caso', '—', 'era', 'um', 'homem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'teria outras distrações! Mas uma\n",
      "pobre mulher, que melhor futuro poderia ambicionar que o casamento?'\n",
      "Tokens gerados: ['teria', 'outras', 'distracoes', 'mas', 'umapobre', 'mulher', ',', 'que', 'melhor', 'futuro', 'poderia', 'ambicionar', 'que', 'o', 'casamento', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'que mais legítimo prazer do\n",
      "que a maternidade; que companhia mais alegre do que a dos filhos, esses diabinhos tão feiticeiros?'\n",
      "Tokens gerados: ['que', 'mais', 'legitimo', 'prazer', 'doque', 'a', 'maternidade', 'que', 'companhia', 'mais', 'alegre', 'do', 'que', 'a', 'dos', 'filhos', ',', 'esses', 'diabinhos', 'tao', 'feiticeiros', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Além de que, sempre gostara muito de crianças: muita vez pedira a quem as tinha que lhas mandasse a\n",
      "fazer-lhe companhia, e, enquanto as pilhava em casa, não consentia que mais ninguém se incomodasse\n",
      "com elas; queria ser a própria a dar-lhes a comida, a lavá-las, a vesti-las, e acalentá-las E estava\n",
      "constantemente a talhar camisinhas e fraldas, a fazer toucas e sapatinhos de lã, e tudo com muita\n",
      "paciência, com muito amor, justamente como, em pequenina, ela fazia com as suas bonecas'\n",
      "Tokens gerados: ['”', 'alem', 'de', 'que', ',', 'sempre', 'gostara', 'muito', 'de', 'criancas', 'muita', 'vez', 'pedira', 'a', 'quem', 'as', 'tinha', 'que', 'lhas', 'mandasse', 'afazer-lhe', 'companhia', ',', 'e', ',', 'enquanto', 'as', 'pilhava', 'em', 'casa', ',', 'nao', 'consentia', 'que', 'mais', 'ninguem', 'se', 'incomodassecom', 'elas', 'queria', 'ser', 'a', 'propria', 'a', 'dar-lhes', 'a', 'comida', ',', 'a', 'lava-las', ',', 'a', 'vesti-las', ',', 'e', 'acalenta-las', 'e', 'estavaconstantemente', 'a', 'talhar', 'camisinhas', 'e', 'fraldas', ',', 'a', 'fazer', 'toucas', 'e', 'sapatinhos', 'de', 'la', ',', 'e', 'tudo', 'com', 'muitapaciencia', ',', 'com', 'muito', 'amor', ',', 'justamente', 'como', ',', 'em', 'pequenina', ',', 'ela', 'fazia', 'com', 'as', 'suas', 'bonecas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando\n",
      "alguma de suas amigas se casava, Ana Rosa exigia dela sempre um cravo do ramalhete ou um botão das\n",
      "flores de laranjeira da grinalda; este ou aquele, pregava-os religiosamente no seio com um dos alfinetes\n",
      "dourados da noiva, e quedava-se a fitá-los, cismando, até que dos lábios lhe partia um suspiro longo,\n",
      "muito longo, como o do viajante que em meio do caminho já se sente cansado e ainda não avista o lar'\n",
      "Tokens gerados: ['quandoalguma', 'de', 'suas', 'amigas', 'se', 'casava', ',', 'ana', 'rosa', 'exigia', 'dela', 'sempre', 'um', 'cravo', 'do', 'ramalhete', 'ou', 'um', 'botao', 'dasflores', 'de', 'laranjeira', 'da', 'grinalda', 'este', 'ou', 'aquele', ',', 'pregava-os', 'religiosamente', 'no', 'seio', 'com', 'um', 'dos', 'alfinetesdourados', 'da', 'noiva', ',', 'e', 'quedava-se', 'a', 'fita-los', ',', 'cismando', ',', 'ate', 'que', 'dos', 'labios', 'lhe', 'partia', 'um', 'suspiro', 'longo', ',', 'muito', 'longo', ',', 'como', 'o', 'do', 'viajante', 'que', 'em', 'meio', 'do', 'caminho', 'ja', 'se', 'sente', 'cansado', 'e', 'ainda', 'nao', 'avista', 'o', 'lar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas o noivo por onde andava que não vinha? Esse belo mancebo, tão ardente e tão apaixonado,\n",
      "por que se não apresentava logo? Dos homens que Ana Rosa conhecia na província nenhum decerto\n",
      "podia ser!'\n",
      "Tokens gerados: ['mas', 'o', 'noivo', 'por', 'onde', 'andava', 'que', 'nao', 'vinha', '?', 'esse', 'belo', 'mancebo', ',', 'tao', 'ardente', 'e', 'tao', 'apaixonado', ',', 'por', 'que', 'se', 'nao', 'apresentava', 'logo', '?', 'dos', 'homens', 'que', 'ana', 'rosa', 'conhecia', 'na', 'provincia', 'nenhum', 'decertopodia', 'ser']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, no entanto, ela amava'\n",
      "Tokens gerados: ['e', ',', 'no', 'entanto', ',', 'ela', 'amava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A quem?\n",
      "Não sabia dizê-lo, mas amava'\n",
      "Tokens gerados: ['a', 'quem', '?', 'nao', 'sabia', 'dize-lo', ',', 'mas', 'amava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sim! Fosse a quem fosse, ela amava; porque sentia vibrar-lhe\n",
      "todo o corpo, fibra por fibra, pensando nesse — Alguém — íntimo e desconhecido para ela; esse —\n",
      "Alguém — que não vinha e não lhe saía do pensamento; esse — Alguém — cuja ausência a fazia infeliz\n",
      "e lhe enchia a existência de lágrimas'\n",
      "Tokens gerados: ['sim', 'fosse', 'a', 'quem', 'fosse', ',', 'ela', 'amava', 'porque', 'sentia', 'vibrar-lhetodo', 'o', 'corpo', ',', 'fibra', 'por', 'fibra', ',', 'pensando', 'nesse', '—', 'alguem', '—', 'intimo', 'e', 'desconhecido', 'para', 'ela', 'esse', '—alguem', '—', 'que', 'nao', 'vinha', 'e', 'nao', 'lhe', 'saia', 'do', 'pensamento', 'esse', '—', 'alguem', '—', 'cuja', 'ausencia', 'a', 'fazia', 'infelize', 'lhe', 'enchia', 'a', 'existencia', 'de', 'lagrimas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Passaram-se meses — nada! Correram três anos'\n",
      "Tokens gerados: ['passaram-se', 'meses', '—', 'nada', 'correram', 'tres', 'anos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ana Rosa principiou a emagrecer visivelmente'\n",
      "Tokens gerados: ['ana', 'rosa', 'principiou', 'a', 'emagrecer', 'visivelmente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Agora dormia menos; estava pálida; à mesa mal tocava nos pratos'\n",
      "Tokens gerados: ['agora', 'dormia', 'menos', 'estava', 'palida', 'a', 'mesa', 'mal', 'tocava', 'nos', 'pratos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ó pequena, tu tens alguma coisa! disse-lhe um dia o pai, já incomodado com aquele ar\n",
      "doentio da filha'\n",
      "Tokens gerados: ['—', 'o', 'pequena', ',', 'tu', 'tens', 'alguma', 'coisa', 'disse-lhe', 'um', 'dia', 'o', 'pai', ',', 'ja', 'incomodado', 'com', 'aquele', 'ardoentio', 'da', 'filha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não me pareces a mesma! Que é isso, Anica?\n",
      "Não era nada!'\n",
      "Tokens gerados: ['nao', 'me', 'pareces', 'a', 'mesma', 'que', 'e', 'isso', ',', 'anica', '?', 'nao', 'era', 'nada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E Ana Rosa sobressaltava-se, como se tivera cometido uma falta'\n",
      "Tokens gerados: ['e', 'ana', 'rosa', 'sobressaltava-se', ',', 'como', 'se', 'tivera', 'cometido', 'uma', 'falta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Cansaço! Nervos! Não era\n",
      "coisa que valesse a pena!'\n",
      "Tokens gerados: ['“', 'cansaco', 'nervos', 'nao', 'eracoisa', 'que', 'valesse', 'a', 'pena']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“\n",
      "Mas chorava'\n",
      "Tokens gerados: ['“', 'mas', 'chorava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Olha! Aí temos! Agora o choro! Nada! É preciso chamar o médico!\n",
      "— Chamar o médico?'\n",
      "Tokens gerados: ['—', 'olha', 'ai', 'temos', 'agora', 'o', 'choro', 'nada', 'e', 'preciso', 'chamar', 'o', 'medico—', 'chamar', 'o', 'medico', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ora papai, não vale a pena!'\n",
      "Tokens gerados: ['ora', 'papai', ',', 'nao', 'vale', 'a', 'pena']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E tossia'\n",
      "Tokens gerados: ['e', 'tossia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Que a deixassem em paz! Que não a estivessem apoquentando com perguntas!'\n",
      "Tokens gerados: ['“', 'que', 'a', 'deixassem', 'em', 'paz', 'que', 'nao', 'a', 'estivessem', 'apoquentando', 'com', 'perguntas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "E tossia mais, sufocada'\n",
      "Tokens gerados: ['”', 'e', 'tossia', 'mais', ',', 'sufocada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Vês?! Estás achacada! Levas nesse “Chrum, chrum! chrum chrum!” E é só: “Não vale a\n",
      "pena! Não precisa chamar o médico!'\n",
      "Tokens gerados: ['—', 'ves', '?', 'estas', 'achacada', 'levas', 'nesse', '“', 'chrum', ',', 'chrum', 'chrum', 'chrum', '”', 'e', 'e', 'so', '“', 'nao', 'vale', 'apena', 'nao', 'precisa', 'chamar', 'o', 'medico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” Não senhora! com moléstias não se brinca!\n",
      "O médico receitou banhos de mar na Ponta d’Areia'\n",
      "Tokens gerados: ['”', 'nao', 'senhora', 'com', 'molestias', 'nao', 'se', 'brincao', 'medico', 'receitou', 'banhos', 'de', 'mar', 'na', 'ponta', 'd', '’', 'areia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi um tempo delicioso para ela os três meses que aí passou'\n",
      "Tokens gerados: ['foi', 'um', 'tempo', 'delicioso', 'para', 'ela', 'os', 'tres', 'meses', 'que', 'ai', 'passou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os ares da costa, os banhos de\n",
      "choque, os longos passeios a pé, restituíram-lhe o apetite e enriqueceram-lhe o sangue'\n",
      "Tokens gerados: ['os', 'ares', 'da', 'costa', ',', 'os', 'banhos', 'dechoque', ',', 'os', 'longos', 'passeios', 'a', 'pe', ',', 'restituiram-lhe', 'o', 'apetite', 'e', 'enriqueceram-lhe', 'o', 'sangue']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ficou mais\n",
      "forte; chegou a engordar'\n",
      "Tokens gerados: ['ficou', 'maisforte', 'chegou', 'a', 'engordar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na Ponta d’Areia travara uma nova amizade — D'\n",
      "Tokens gerados: ['na', 'ponta', 'd', '’', 'areia', 'travara', 'uma', 'nova', 'amizade', '—', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eufrasinha'\n",
      "Tokens gerados: ['eufrasinha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Viúva de um oficial do quinto de\n",
      "infantaria, batalhão que morreu todo na Guerra do Paraguai'\n",
      "Tokens gerados: ['viuva', 'de', 'um', 'oficial', 'do', 'quinto', 'deinfantaria', ',', 'batalhao', 'que', 'morreu', 'todo', 'na', 'guerra', 'do', 'paraguai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Muito romântica: falava do marido\n",
      "requebrando-se, e poetizava-lhe a curta história: “Dez dias depois de casados, seguira ele para o campo\n",
      "de batalha e, no denodo da sua coragem, fora atravessado por uma bala de artilharia, morrendo logo a\n",
      "balbuciar com o lábio ensangüentado o nome da esposa estremecida'\n",
      "Tokens gerados: ['muito', 'romantica', 'falava', 'do', 'maridorequebrando-se', ',', 'e', 'poetizava-lhe', 'a', 'curta', 'historia', '“', 'dez', 'dias', 'depois', 'de', 'casados', ',', 'seguira', 'ele', 'para', 'o', 'campode', 'batalha', 'e', ',', 'no', 'denodo', 'da', 'sua', 'coragem', ',', 'fora', 'atravessado', 'por', 'uma', 'bala', 'de', 'artilharia', ',', 'morrendo', 'logo', 'abalbuciar', 'com', 'o', 'labio', 'ensanguentado', 'o', 'nome', 'da', 'esposa', 'estremecida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "E com um suspiro, feito de desejos mal satisfeitos, a viúva concluía pesarosa que “prazeres\n",
      "nesta vida, conhecera apenas dez dias e dez noites'\n",
      "Tokens gerados: ['”', 'e', 'com', 'um', 'suspiro', ',', 'feito', 'de', 'desejos', 'mal', 'satisfeitos', ',', 'a', 'viuva', 'concluia', 'pesarosa', 'que', '“', 'prazeresnesta', 'vida', ',', 'conhecera', 'apenas', 'dez', 'dias', 'e', 'dez', 'noites']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Ana Rosa compadecia-se da amiga e escutava-lhe de boa-fé as frioleiras'\n",
      "Tokens gerados: ['”', 'ana', 'rosa', 'compadecia-se', 'da', 'amiga', 'e', 'escutava-lhe', 'de', 'boa-fe', 'as', 'frioleiras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na sua ingênua e\n",
      "comovida sinceridade facilmente se identificava com a história singular daquele casamento tão infeliz\n",
      "e tão simpático'\n",
      "Tokens gerados: ['na', 'sua', 'ingenua', 'ecomovida', 'sinceridade', 'facilmente', 'se', 'identificava', 'com', 'a', 'historia', 'singular', 'daquele', 'casamento', 'tao', 'infelize', 'tao', 'simpatico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por mais de uma vez chegou a chorar pela morte do pobre moço oficial de infantaria'\n",
      "Tokens gerados: ['por', 'mais', 'de', 'uma', 'vez', 'chegou', 'a', 'chorar', 'pela', 'morte', 'do', 'pobre', 'moco', 'oficial', 'de', 'infantaria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'D'\n",
      "Tokens gerados: ['d']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eufrasinha instruiu a sua nova amiga em muitas coisas que esta mal sonhava; ensinou-lhe\n",
      "certos mistérios da vida conjugal; pode dizer-se que lhe deu lições de amor: falou muito nos “homens”,\n",
      "disse-lhe como a mulher esperta devia lidar com eles; quais eram as manhas e os fracos dos maridos ou\n",
      "dos namorados; quais eram os tipos preferíveis; o que significava ter “olhos mortos, beiços grossos,\n",
      "nariz comprido”'\n",
      "Tokens gerados: ['eufrasinha', 'instruiu', 'a', 'sua', 'nova', 'amiga', 'em', 'muitas', 'coisas', 'que', 'esta', 'mal', 'sonhava', 'ensinou-lhecertos', 'misterios', 'da', 'vida', 'conjugal', 'pode', 'dizer-se', 'que', 'lhe', 'deu', 'licoes', 'de', 'amor', 'falou', 'muito', 'nos', '“', 'homens', '”', ',', 'disse-lhe', 'como', 'a', 'mulher', 'esperta', 'devia', 'lidar', 'com', 'eles', 'quais', 'eram', 'as', 'manhas', 'e', 'os', 'fracos', 'dos', 'maridos', 'oudos', 'namorados', 'quais', 'eram', 'os', 'tipos', 'preferiveis', 'o', 'que', 'significava', 'ter', '“', 'olhos', 'mortos', ',', 'beicos', 'grossos', ',', 'nariz', 'comprido', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A outra ria-se'\n",
      "Tokens gerados: ['a', 'outra', 'ria-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Não tomava a sério aquelas bobagens da Eufrasinha!”\n",
      "Mas intimamente ia, sem dar por isso, reconstruindo o seu ideal pelas instruções da viúva'\n",
      "Tokens gerados: ['“', 'nao', 'tomava', 'a', 'serio', 'aquelas', 'bobagens', 'da', 'eufrasinha', '”', 'mas', 'intimamente', 'ia', ',', 'sem', 'dar', 'por', 'isso', ',', 'reconstruindo', 'o', 'seu', 'ideal', 'pelas', 'instrucoes', 'da', 'viuva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fê-lo\n",
      "menos espiritual, mais humano, mais verossímil, mais suscetível de ser descoberto; e, desde então, o\n",
      "tipo, apenas debuxado ao fundo dos seus sonhos, veio para a frente, acentuou-se como uma figura que\n",
      "recebesse os últimos toques do pintor; e, depois de vê-lo bem correto, bem emendado e pronto, amouo ainda mais, muito mais, tanto quanto o amaria se ele fora com efeito uma realidade'\n",
      "Tokens gerados: ['fe-lomenos', 'espiritual', ',', 'mais', 'humano', ',', 'mais', 'verossimil', ',', 'mais', 'suscetivel', 'de', 'ser', 'descoberto', 'e', ',', 'desde', 'entao', ',', 'otipo', ',', 'apenas', 'debuxado', 'ao', 'fundo', 'dos', 'seus', 'sonhos', ',', 'veio', 'para', 'a', 'frente', ',', 'acentuou-se', 'como', 'uma', 'figura', 'querecebesse', 'os', 'ultimos', 'toques', 'do', 'pintor', 'e', ',', 'depois', 'de', 've-lo', 'bem', 'correto', ',', 'bem', 'emendado', 'e', 'pronto', ',', 'amouo', 'ainda', 'mais', ',', 'muito', 'mais', ',', 'tanto', 'quanto', 'o', 'amaria', 'se', 'ele', 'fora', 'com', 'efeito', 'uma', 'realidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A partir daí, era esse ideal, correto e emendado, a base das suas deliberações a respeito de\n",
      "casamento; era a bitola, por onde ela aferia todo aquele que a requestasse'\n",
      "Tokens gerados: ['a', 'partir', 'dai', ',', 'era', 'esse', 'ideal', ',', 'correto', 'e', 'emendado', ',', 'a', 'base', 'das', 'suas', 'deliberacoes', 'a', 'respeito', 'decasamento', 'era', 'a', 'bitola', ',', 'por', 'onde', 'ela', 'aferia', 'todo', 'aquele', 'que', 'a', 'requestasse']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se o pretendente não tivesse\n",
      "o nariz, o olhar, o gesto, o conjunto enfim de que constava o padrão, podia, desde logo, perder a\n",
      "esperança de cair nas graças da filha de Manuel Pedro'\n",
      "Tokens gerados: ['se', 'o', 'pretendente', 'nao', 'tivesseo', 'nariz', ',', 'o', 'olhar', ',', 'o', 'gesto', ',', 'o', 'conjunto', 'enfim', 'de', 'que', 'constava', 'o', 'padrao', ',', 'podia', ',', 'desde', 'logo', ',', 'perder', 'aesperanca', 'de', 'cair', 'nas', 'gracas', 'da', 'filha', 'de', 'manuel', 'pedro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eufrasinha mudou-se para a cidade; Ana Rosa já lá estava'\n",
      "Tokens gerados: ['eufrasinha', 'mudou-se', 'para', 'a', 'cidade', 'ana', 'rosa', 'ja', 'la', 'estava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Visitaram-se'\n",
      "Tokens gerados: ['visitaram-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E estas visitas, que se tornaram muito íntimas e repetidas, serviram mutuamente de consolo, ao\n",
      "afincado celibato de uma e a precoce viuvez da outra'\n",
      "Tokens gerados: ['e', 'estas', 'visitas', ',', 'que', 'se', 'tornaram', 'muito', 'intimas', 'e', 'repetidas', ',', 'serviram', 'mutuamente', 'de', 'consolo', ',', 'aoafincado', 'celibato', 'de', 'uma', 'e', 'a', 'precoce', 'viuvez', 'da', 'outra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia, empregado no armazém do pai de Ana Rosa, um rapaz português, de nome Luís Dias;\n",
      "muito ativo, econômico, discreto, trabalhador, com uma bonita letra, e muito estimado na Praça'\n",
      "Tokens gerados: ['havia', ',', 'empregado', 'no', 'armazem', 'do', 'pai', 'de', 'ana', 'rosa', ',', 'um', 'rapaz', 'portugues', ',', 'de', 'nome', 'luis', 'diasmuito', 'ativo', ',', 'economico', ',', 'discreto', ',', 'trabalhador', ',', 'com', 'uma', 'bonita', 'letra', ',', 'e', 'muito', 'estimado', 'na', 'praca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Contavam a seu favor invejáveis partidas de tino comercial, e ninguém seria capaz de dizer mal de tão\n",
      "excelente moço'\n",
      "Tokens gerados: ['contavam', 'a', 'seu', 'favor', 'invejaveis', 'partidas', 'de', 'tino', 'comercial', ',', 'e', 'ninguem', 'seria', 'capaz', 'de', 'dizer', 'mal', 'de', 'taoexcelente', 'moco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao contrário, quase sempre que falavam dele, diziam “Coitado!” e este — coitado — era\n",
      "inteiramente sem razão de ser, porque ao Dias, graças a Deus, nada faltava: tinha casa, comida, roupa\n",
      "lavada e engomada, e, ainda por cima, os cobres do emprego'\n",
      "Tokens gerados: ['ao', 'contrario', ',', 'quase', 'sempre', 'que', 'falavam', 'dele', ',', 'diziam', '“', 'coitado', '”', 'e', 'este', '—', 'coitado', '—', 'erainteiramente', 'sem', 'razao', 'de', 'ser', ',', 'porque', 'ao', 'dias', ',', 'gracas', 'a', 'deus', ',', 'nada', 'faltava', 'tinha', 'casa', ',', 'comida', ',', 'roupalavada', 'e', 'engomada', ',', 'e', ',', 'ainda', 'por', 'cima', ',', 'os', 'cobres', 'do', 'emprego']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas a coisa era que o diabo do homem,\n",
      "apesar das suas prósperas circunstâncias, impunha certa lástima, impressionava com o seu eterno ar de\n",
      "piedade, de súplica, de resignação e humildade'\n",
      "Tokens gerados: ['mas', 'a', 'coisa', 'era', 'que', 'o', 'diabo', 'do', 'homem', ',', 'apesar', 'das', 'suas', 'prosperas', 'circunstancias', ',', 'impunha', 'certa', 'lastima', ',', 'impressionava', 'com', 'o', 'seu', 'eterno', 'ar', 'depiedade', ',', 'de', 'suplica', ',', 'de', 'resignacao', 'e', 'humildade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fazia pena, incutia dó em quem o visse, tão submisso,\n",
      "tão passivo, tão pobre rapaz — tão besta de carga'\n",
      "Tokens gerados: ['fazia', 'pena', ',', 'incutia', 'do', 'em', 'quem', 'o', 'visse', ',', 'tao', 'submisso', ',', 'tao', 'passivo', ',', 'tao', 'pobre', 'rapaz', '—', 'tao', 'besta', 'de', 'carga']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ninguém, em caso algum, levantaria a mão sobre ele,\n",
      "sem experimentar a repugnância da covardia'\n",
      "Tokens gerados: ['ninguem', ',', 'em', 'caso', 'algum', ',', 'levantaria', 'a', 'mao', 'sobre', 'ele', ',', 'sem', 'experimentar', 'a', 'repugnancia', 'da', 'covardia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Elogiavam-no entretanto: “Que não fossem atrás daquele ar modesto, porque ali estava um\n",
      "empregadão de truz!”\n",
      "Vários negociantes ofereceram-lhe boas vantagens para tomá-lo ao seu serviço; mas o Dias,\n",
      "sempre humilde e de cabeça baixa, resistia-lhes a pé firme'\n",
      "Tokens gerados: ['elogiavam-no', 'entretanto', '“', 'que', 'nao', 'fossem', 'atras', 'daquele', 'ar', 'modesto', ',', 'porque', 'ali', 'estava', 'umempregadao', 'de', 'truz', '”', 'varios', 'negociantes', 'ofereceram-lhe', 'boas', 'vantagens', 'para', 'toma-lo', 'ao', 'seu', 'servico', 'mas', 'o', 'dias', ',', 'sempre', 'humilde', 'e', 'de', 'cabeca', 'baixa', ',', 'resistia-lhes', 'a', 'pe', 'firme']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, tal constância opôs as repetidas propostas,\n",
      "que todo o comércio, dando como certo o seu casamento com a filha do patrão, elogiou a escolha de\n",
      "Manuel Pedro e profetizou aos nubentes “um futuro muito bonito e muito rico”'\n",
      "Tokens gerados: ['e', ',', 'tal', 'constancia', 'opos', 'as', 'repetidas', 'propostas', ',', 'que', 'todo', 'o', 'comercio', ',', 'dando', 'como', 'certo', 'o', 'seu', 'casamento', 'com', 'a', 'filha', 'do', 'patrao', ',', 'elogiou', 'a', 'escolha', 'demanuel', 'pedro', 'e', 'profetizou', 'aos', 'nubentes', '“', 'um', 'futuro', 'muito', 'bonito', 'e', 'muito', 'rico', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Foi acertado, foi! diziam com olhar fito'\n",
      "Tokens gerados: ['—', 'foi', 'acertado', ',', 'foi', 'diziam', 'com', 'olhar', 'fito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel Pedro via, com efeito, naquela criatura, trabalhadora e passiva como um boi de carga e\n",
      "econômico como um usurário, o homem mais no caso de fazer a felicidade da filha'\n",
      "Tokens gerados: ['manuel', 'pedro', 'via', ',', 'com', 'efeito', ',', 'naquela', 'criatura', ',', 'trabalhadora', 'e', 'passiva', 'como', 'um', 'boi', 'de', 'carga', 'eeconomico', 'como', 'um', 'usurario', ',', 'o', 'homem', 'mais', 'no', 'caso', 'de', 'fazer', 'a', 'felicidade', 'da', 'filha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Queria-o para\n",
      "genro e para sócio; dizia a todos os colegas que o “seu Dias” apenas retirava por ano, para as suas\n",
      "despesas, a quarta parte do ordenado'\n",
      "Tokens gerados: ['queria-o', 'paragenro', 'e', 'para', 'socio', 'dizia', 'a', 'todos', 'os', 'colegas', 'que', 'o', '“', 'seu', 'dias', '”', 'apenas', 'retirava', 'por', 'ano', ',', 'para', 'as', 'suasdespesas', ',', 'a', 'quarta', 'parte', 'do', 'ordenado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Tem já o seu pecúlio, tem! considerava ele'\n",
      "Tokens gerados: ['—', 'tem', 'ja', 'o', 'seu', 'peculio', ',', 'tem', 'considerava', 'ele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mulher que o quisesse, levava um bom marido!\n",
      "Aquele virá a possuir alguma coisa'\n",
      "Tokens gerados: ['a', 'mulher', 'que', 'o', 'quisesse', ',', 'levava', 'um', 'bom', 'maridoaquele', 'vira', 'a', 'possuir', 'alguma', 'coisa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'é moço de muito futuro!\n",
      "E, pouco a pouco foi-se habituando a julgá-lo já da família e a estimá-lo e distingüi-lo como tal;\n",
      "só faltava que a pequena se decidisse'\n",
      "Tokens gerados: ['e', 'moco', 'de', 'muito', 'futuroe', ',', 'pouco', 'a', 'pouco', 'foi-se', 'habituando', 'a', 'julga-lo', 'ja', 'da', 'familia', 'e', 'a', 'estima-lo', 'e', 'distingui-lo', 'como', 'talso', 'faltava', 'que', 'a', 'pequena', 'se', 'decidisse']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas qual! ela nem queria vê-lo! Tinha-lhe birra; não podia\n",
      "sofrer aquele cabelo à escovinha, aquele cavanhaque sem bigode, aqueles dentes sujos, aquela economia\n",
      "torpe e aqueles movimentos de homem sem vontade própria'\n",
      "Tokens gerados: ['mas', 'qual', 'ela', 'nem', 'queria', 've-lo', 'tinha-lhe', 'birra', 'nao', 'podiasofrer', 'aquele', 'cabelo', 'a', 'escovinha', ',', 'aquele', 'cavanhaque', 'sem', 'bigode', ',', 'aqueles', 'dentes', 'sujos', ',', 'aquela', 'economiatorpe', 'e', 'aqueles', 'movimentos', 'de', 'homem', 'sem', 'vontade', 'propria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Um somítico! classificava Ana Rosa, franzindo o nariz'\n",
      "Tokens gerados: ['—', 'um', 'somitico', 'classificava', 'ana', 'rosa', ',', 'franzindo', 'o', 'nariz']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma ocasião, o pai tocou-lhe no casamento'\n",
      "Tokens gerados: ['uma', 'ocasiao', ',', 'o', 'pai', 'tocou-lhe', 'no', 'casamento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Com o Dias?'\n",
      "Tokens gerados: ['—', 'com', 'o', 'dias', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'perguntou espantada'\n",
      "Tokens gerados: ['perguntou', 'espantada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Sim'\n",
      "Tokens gerados: ['—', 'sim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora, papai!\n",
      "E soltou uma risada'\n",
      "Tokens gerados: ['—', 'ora', ',', 'papaie', 'soltou', 'uma', 'risada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel não se animou a dizer mais palavra; à noite, porém, contou tudo em particular ao\n",
      "compadre, um amigo velho, íntimo da casa — o cônego Diogo'\n",
      "Tokens gerados: ['manuel', 'nao', 'se', 'animou', 'a', 'dizer', 'mais', 'palavra', 'a', 'noite', ',', 'porem', ',', 'contou', 'tudo', 'em', 'particular', 'aocompadre', ',', 'um', 'amigo', 'velho', ',', 'intimo', 'da', 'casa', '—', 'o', 'conego', 'diogo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Optima soepè despecta! sentenciou este'\n",
      "Tokens gerados: ['—', 'optima', 'soepe', 'despecta', 'sentenciou', 'este']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É preciso dar tempo ao tempo, seu compadre! A\n",
      "coisa há de ser'\n",
      "Tokens gerados: ['e', 'preciso', 'dar', 'tempo', 'ao', 'tempo', ',', 'seu', 'compadre', 'acoisa', 'ha', 'de', 'ser']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'deixe correr o barco!\n",
      "No entanto, o Dias não se alterara; esperava calado, pacificamente, sem erguer os olhos, cheio\n",
      "sempre de humildade e resignação'\n",
      "Tokens gerados: ['deixe', 'correr', 'o', 'barcono', 'entanto', ',', 'o', 'dias', 'nao', 'se', 'alterara', 'esperava', 'calado', ',', 'pacificamente', ',', 'sem', 'erguer', 'os', 'olhos', ',', 'cheiosempre', 'de', 'humildade', 'e', 'resignacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_o_mulato_aluisio_azevedo_cap_1.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Assim era, quando Manuel Pedro, na varanda de sua casa, pedia à filha uma resposta definitiva\n",
      "a respeito do casamento'\n",
      "Tokens gerados: ['assim', 'era', ',', 'quando', 'manuel', 'pedro', ',', 'na', 'varanda', 'de', 'sua', 'casa', ',', 'pedia', 'a', 'filha', 'uma', 'resposta', 'definitivaa', 'respeito', 'do', 'casamento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Já lá se iam três meses depois da estada na Ponta d’Areia'\n",
      "Tokens gerados: ['ja', 'la', 'se', 'iam', 'tres', 'meses', 'depois', 'da', 'estada', 'na', 'ponta', 'd', '’', 'areia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ana Rosa continuou muda no seu lugar, a fitar a toalha da mesa, como se procurasse aí uma\n",
      "resolução'\n",
      "Tokens gerados: ['ana', 'rosa', 'continuou', 'muda', 'no', 'seu', 'lugar', ',', 'a', 'fitar', 'a', 'toalha', 'da', 'mesa', ',', 'como', 'se', 'procurasse', 'ai', 'umaresolucao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O sabiá cantava na gaiola'\n",
      "Tokens gerados: ['o', 'sabia', 'cantava', 'na', 'gaiola']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então, minha filha, não dás sequer uma esperança?'\n",
      "Tokens gerados: ['—', 'entao', ',', 'minha', 'filha', ',', 'nao', 'das', 'sequer', 'uma', 'esperanca', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pode ser'\n",
      "Tokens gerados: ['—', 'pode', 'ser']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E ela ergueu-se'\n",
      "Tokens gerados: ['e', 'ela', 'ergueu-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Bom'\n",
      "Tokens gerados: ['—', 'bom']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Assim é que te quero ver'\n",
      "Tokens gerados: ['assim', 'e', 'que', 'te', 'quero', 'ver']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O negociante passou o braço em volta da cintura da rapariga, disposto a conversar ainda, mas\n",
      "foi interrompido por umas passadas no corredor'\n",
      "Tokens gerados: ['o', 'negociante', 'passou', 'o', 'braco', 'em', 'volta', 'da', 'cintura', 'da', 'rapariga', ',', 'disposto', 'a', 'conversar', 'ainda', ',', 'masfoi', 'interrompido', 'por', 'umas', 'passadas', 'no', 'corredor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Dá licença? disse o cônego, já na porta da varanda'\n",
      "Tokens gerados: ['—', 'da', 'licenca', '?', 'disse', 'o', 'conego', ',', 'ja', 'na', 'porta', 'da', 'varanda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Vá entrando, compadre!\n",
      "O cônego entrou, devagar, com o seu sorriso discreto e amável'\n",
      "Tokens gerados: ['—', 'va', 'entrando', ',', 'compadreo', 'conego', 'entrou', ',', 'devagar', ',', 'com', 'o', 'seu', 'sorriso', 'discreto', 'e', 'amavel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era um velho bonito; teria quando menos sessenta anos, porém estava ainda forte e bem\n",
      "conservado; o olhar vivo, o corpo teso, mas ungido de brandura santarrona'\n",
      "Tokens gerados: ['era', 'um', 'velho', 'bonito', 'teria', 'quando', 'menos', 'sessenta', 'anos', ',', 'porem', 'estava', 'ainda', 'forte', 'e', 'bemconservado', 'o', 'olhar', 'vivo', ',', 'o', 'corpo', 'teso', ',', 'mas', 'ungido', 'de', 'brandura', 'santarrona']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Calçava-se com esmero, de\n",
      "polimento; mandava buscar da Europa, para seu uso, meias e colarinhos especiais, e, quando ria, mostrava\n",
      "dentes limpos, todos chumbados a ouro'\n",
      "Tokens gerados: ['calcava-se', 'com', 'esmero', ',', 'depolimento', 'mandava', 'buscar', 'da', 'europa', ',', 'para', 'seu', 'uso', ',', 'meias', 'e', 'colarinhos', 'especiais', ',', 'e', ',', 'quando', 'ria', ',', 'mostravadentes', 'limpos', ',', 'todos', 'chumbados', 'a', 'ouro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha os movimentos distintos; mãos brancas e cabelos alvos\n",
      "que fazia gosto'\n",
      "Tokens gerados: ['tinha', 'os', 'movimentos', 'distintos', 'maos', 'brancas', 'e', 'cabelos', 'alvosque', 'fazia', 'gosto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Diogo era o confidente e o conselheiro do bom e pesado Manuel; este não dava um passo sem\n",
      "consultar o compadre'\n",
      "Tokens gerados: ['diogo', 'era', 'o', 'confidente', 'e', 'o', 'conselheiro', 'do', 'bom', 'e', 'pesado', 'manuel', 'este', 'nao', 'dava', 'um', 'passo', 'semconsultar', 'o', 'compadre']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Formara-se em Coimbra, donde contava maravilhas; um bocadinho rico, e não\n",
      "relaxava o seu passeio a Lisboa, de vez em quando, “para descarregar anos da costa'\n",
      "Tokens gerados: ['formara-se', 'em', 'coimbra', ',', 'donde', 'contava', 'maravilhas', 'um', 'bocadinho', 'rico', ',', 'e', 'naorelaxava', 'o', 'seu', 'passeio', 'a', 'lisboa', ',', 'de', 'vez', 'em', 'quando', ',', '“', 'para', 'descarregar', 'anos', 'da', 'costa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” explicava ele, a\n",
      "rir'\n",
      "Tokens gerados: ['”', 'explicava', 'ele', ',', 'arir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Logo que entrou, deu a beijar a Ana Rosa o seu grande e trabalhado anel de ametista, obra do\n",
      "Porto, feita de encomenda'\n",
      "Tokens gerados: ['logo', 'que', 'entrou', ',', 'deu', 'a', 'beijar', 'a', 'ana', 'rosa', 'o', 'seu', 'grande', 'e', 'trabalhado', 'anel', 'de', 'ametista', ',', 'obra', 'doporto', ',', 'feita', 'de', 'encomenda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E batendo-lhe na face com a mão fina e impregnada de sabonete inglês:\n",
      "— Então, minha afilhada, como vai essa bizarria?\n",
      "Ia bem, agradecida'\n",
      "Tokens gerados: ['e', 'batendo-lhe', 'na', 'face', 'com', 'a', 'mao', 'fina', 'e', 'impregnada', 'de', 'sabonete', 'ingles—', 'entao', ',', 'minha', 'afilhada', ',', 'como', 'vai', 'essa', 'bizarria', '?', 'ia', 'bem', ',', 'agradecida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sorriu'\n",
      "Tokens gerados: ['sorriu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Dindinho está bom?\n",
      "— Como sempre'\n",
      "Tokens gerados: ['—', 'dindinho', 'esta', 'bom', '?', '—', 'como', 'sempre']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que notícias de D'\n",
      "Tokens gerados: ['que', 'noticias', 'de', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Babita?\n",
      "Estava de passeio'\n",
      "Tokens gerados: ['babita', '?', 'estava', 'de', 'passeio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pois não vê a casa sossegada? interrogou Manuel'\n",
      "Tokens gerados: ['—', 'pois', 'nao', 've', 'a', 'casa', 'sossegada', '?', 'interrogou', 'manuel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi à missa e naturalmente almoçou por aí\n",
      "com alguma amiga'\n",
      "Tokens gerados: ['foi', 'a', 'missa', 'e', 'naturalmente', 'almocou', 'por', 'aicom', 'alguma', 'amiga']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deus a conserve por lá! Mas que milagre o trouxe a estas horas cá por casa, seu\n",
      "compadre?\n",
      "— Um negócio que lhe quero comunicar; particular, um bocado particular'\n",
      "Tokens gerados: ['deus', 'a', 'conserve', 'por', 'la', 'mas', 'que', 'milagre', 'o', 'trouxe', 'a', 'estas', 'horas', 'ca', 'por', 'casa', ',', 'seucompadre', '?', '—', 'um', 'negocio', 'que', 'lhe', 'quero', 'comunicar', 'particular', ',', 'um', 'bocado', 'particular']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ana Rosa fez logo menção de afastar-se'\n",
      "Tokens gerados: ['ana', 'rosa', 'fez', 'logo', 'mencao', 'de', 'afastar-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Deixa-te ficar, disse-lhe o pai'\n",
      "Tokens gerados: ['—', 'deixa-te', 'ficar', ',', 'disse-lhe', 'o', 'pai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nós vamos aqui para o escritório'\n",
      "Tokens gerados: ['nos', 'vamos', 'aqui', 'para', 'o', 'escritorio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E os dois compadres, conversando em voz baixa, encaminharam-se para uma saleta que havia\n",
      "na frente da casa'\n",
      "Tokens gerados: ['e', 'os', 'dois', 'compadres', ',', 'conversando', 'em', 'voz', 'baixa', ',', 'encaminharam-se', 'para', 'uma', 'saleta', 'que', 'haviana', 'frente', 'da', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A saleta era pequenina, com duas janelas para a Rua da Estrela'\n",
      "Tokens gerados: ['a', 'saleta', 'era', 'pequenina', ',', 'com', 'duas', 'janelas', 'para', 'a', 'rua', 'da', 'estrela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Chão esteirado paredes forradas\n",
      "de papel e o teto de travessinhas de paparaúba pintadas de branco'\n",
      "Tokens gerados: ['chao', 'esteirado', 'paredes', 'forradasde', 'papel', 'e', 'o', 'teto', 'de', 'travessinhas', 'de', 'paparauba', 'pintadas', 'de', 'branco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia uma carteira de escrita, muito\n",
      "alta, com o seu mocho inclinado, um cofre de ferro, uma pilha de livros de escrituração mercantil, uma\n",
      "prensa, o copiador ao lado e mais um copo sujo de pó, em cujas bordas descansava um pincel chato de\n",
      "cabo largo; uma cadeira de palhinha, um caixão de papéis inúteis, um bico de gás e duas escarradeiras'\n",
      "Tokens gerados: ['havia', 'uma', 'carteira', 'de', 'escrita', ',', 'muitoalta', ',', 'com', 'o', 'seu', 'mocho', 'inclinado', ',', 'um', 'cofre', 'de', 'ferro', ',', 'uma', 'pilha', 'de', 'livros', 'de', 'escrituracao', 'mercantil', ',', 'umaprensa', ',', 'o', 'copiador', 'ao', 'lado', 'e', 'mais', 'um', 'copo', 'sujo', 'de', 'po', ',', 'em', 'cujas', 'bordas', 'descansava', 'um', 'pincel', 'chato', 'decabo', 'largo', 'uma', 'cadeira', 'de', 'palhinha', ',', 'um', 'caixao', 'de', 'papeis', 'inuteis', ',', 'um', 'bico', 'de', 'gas', 'e', 'duas', 'escarradeiras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah! ainda havia na parede, sobre a secretária, um calendário do ano e outro da semana, ambos\n",
      "com as algibeiras pejadas de notas e recibos'\n",
      "Tokens gerados: ['ah', 'ainda', 'havia', 'na', 'parede', ',', 'sobre', 'a', 'secretaria', ',', 'um', 'calendario', 'do', 'ano', 'e', 'outro', 'da', 'semana', ',', 'amboscom', 'as', 'algibeiras', 'pejadas', 'de', 'notas', 'e', 'recibos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era isto que Manuel Pedro chamava pamposamente “o seu escritório” e onde fazia a\n",
      "correspondência comercial'\n",
      "Tokens gerados: ['era', 'isto', 'que', 'manuel', 'pedro', 'chamava', 'pamposamente', '“', 'o', 'seu', 'escritorio', '”', 'e', 'onde', 'fazia', 'acorrespondencia', 'comercial']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aí, quando ele de corpo e alma se entregava aos interesses da sua vida, às\n",
      "suas especulações, ao seu trabalho enfim, podiam lá fora até morrer, que o bom homem não dava por\n",
      "isso'\n",
      "Tokens gerados: ['ai', ',', 'quando', 'ele', 'de', 'corpo', 'e', 'alma', 'se', 'entregava', 'aos', 'interesses', 'da', 'sua', 'vida', ',', 'assuas', 'especulacoes', ',', 'ao', 'seu', 'trabalho', 'enfim', ',', 'podiam', 'la', 'fora', 'ate', 'morrer', ',', 'que', 'o', 'bom', 'homem', 'nao', 'dava', 'porisso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Amava deveras o trabalho e seria uma santa criatura se não fora certa maniazinha de querer\n",
      "especular com tudo, o que às vezes lhe desvirtuava as melhores intenções'\n",
      "Tokens gerados: ['amava', 'deveras', 'o', 'trabalho', 'e', 'seria', 'uma', 'santa', 'criatura', 'se', 'nao', 'fora', 'certa', 'maniazinha', 'de', 'quererespecular', 'com', 'tudo', ',', 'o', 'que', 'as', 'vezes', 'lhe', 'desvirtuava', 'as', 'melhores', 'intencoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando os dois entraram, ele foi logo fechando a porta, discretamente, enquanto o outro se\n",
      "esparralhava na cadeira, com um suspiro de cansaço, levantando até ao meio da canela a sua batina\n",
      "lustrosa e de bom talho'\n",
      "Tokens gerados: ['quando', 'os', 'dois', 'entraram', ',', 'ele', 'foi', 'logo', 'fechando', 'a', 'porta', ',', 'discretamente', ',', 'enquanto', 'o', 'outro', 'seesparralhava', 'na', 'cadeira', ',', 'com', 'um', 'suspiro', 'de', 'cansaco', ',', 'levantando', 'ate', 'ao', 'meio', 'da', 'canela', 'a', 'sua', 'batinalustrosa', 'e', 'de', 'bom', 'talho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel havia tomado um cigarro de papel amarelo de cima da carteira e\n",
      "acendia-o sofregamente; o cônego esperava por ele, com uma notícia suspensa dos lábios, como\n",
      "espantado; a boca meio aberta; o tronco inclinado para a frente, as mãos espalmadas nos joelhos, a\n",
      "cabeça erguida e um olhar de sobrancelhas arregaçadas através do cristal dos óculos'\n",
      "Tokens gerados: ['manuel', 'havia', 'tomado', 'um', 'cigarro', 'de', 'papel', 'amarelo', 'de', 'cima', 'da', 'carteira', 'eacendia-o', 'sofregamente', 'o', 'conego', 'esperava', 'por', 'ele', ',', 'com', 'uma', 'noticia', 'suspensa', 'dos', 'labios', ',', 'comoespantado', 'a', 'boca', 'meio', 'aberta', 'o', 'tronco', 'inclinado', 'para', 'a', 'frente', ',', 'as', 'maos', 'espalmadas', 'nos', 'joelhos', ',', 'acabeca', 'erguida', 'e', 'um', 'olhar', 'de', 'sobrancelhas', 'arregacadas', 'atraves', 'do', 'cristal', 'dos', 'oculos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Sabe quem está a chegar por aí?'\n",
      "Tokens gerados: ['—', 'sabe', 'quem', 'esta', 'a', 'chegar', 'por', 'ai', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'perguntou afinal, quando viu Manuel já instalado no\n",
      "mocho da secretária'\n",
      "Tokens gerados: ['perguntou', 'afinal', ',', 'quando', 'viu', 'manuel', 'ja', 'instalado', 'nomocho', 'da', 'secretaria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Quem?\n",
      "— O Raimundo!\n",
      "E o cônego sorveu uma pitada'\n",
      "Tokens gerados: ['—', 'quem', '?', '—', 'o', 'raimundoe', 'o', 'conego', 'sorveu', 'uma', 'pitada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que Raimundo?\n",
      "— O Mundico! o filho do José, homem! teu sobrinho! aquela criança, que teu mano teve da\n",
      "Domingas'\n",
      "Tokens gerados: ['—', 'que', 'raimundo', '?', '—', 'o', 'mundico', 'o', 'filho', 'do', 'jose', ',', 'homem', 'teu', 'sobrinho', 'aquela', 'crianca', ',', 'que', 'teu', 'mano', 'teve', 'dadomingas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Sim, sim, já sei, mas então?'\n",
      "Tokens gerados: ['—', 'sim', ',', 'sim', ',', 'ja', 'sei', ',', 'mas', 'entao', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Está a chegar por dias'\n",
      "Tokens gerados: ['—', 'esta', 'a', 'chegar', 'por', 'dias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ora espera'\n",
      "Tokens gerados: ['ora', 'espera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O padre tirou papéis da algibeira e rebuscou entre eles uma carta, que passou ao negociante'\n",
      "Tokens gerados: ['o', 'padre', 'tirou', 'papeis', 'da', 'algibeira', 'e', 'rebuscou', 'entre', 'eles', 'uma', 'carta', ',', 'que', 'passou', 'ao', 'negociante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— É do Peixoto, o Peixoto de Lisboa'\n",
      "Tokens gerados: ['—', 'e', 'do', 'peixoto', ',', 'o', 'peixoto', 'de', 'lisboa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— De Lisboa, como?\n",
      "— Sim, homem! Do Peixoto de Lisboa, que está há três anos no Rio'\n",
      "Tokens gerados: ['—', 'de', 'lisboa', ',', 'como', '?', '—', 'sim', ',', 'homem', 'do', 'peixoto', 'de', 'lisboa', ',', 'que', 'esta', 'ha', 'tres', 'anos', 'no', 'rio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah!'\n",
      "Tokens gerados: ['—', 'ah']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'isso sim, porque tinha idéia de que o pequeno deveria estar agora na Corte'\n",
      "Tokens gerados: ['isso', 'sim', ',', 'porque', 'tinha', 'ideia', 'de', 'que', 'o', 'pequeno', 'deveria', 'estar', 'agora', 'na', 'corte']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah! chegou\n",
      "o vapor do Sul'\n",
      "Tokens gerados: ['ah', 'chegouo', 'vapor', 'do', 'sul']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pois é'\n",
      "Tokens gerados: ['—', 'pois', 'e']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lê!\n",
      "Manuel armou os óculos no nariz e leu para si a seguinte carta datada do Rio de Janeiro: “Revmo'\n",
      "Tokens gerados: ['lemanuel', 'armou', 'os', 'oculos', 'no', 'nariz', 'e', 'leu', 'para', 'si', 'a', 'seguinte', 'carta', 'datada', 'do', 'rio', 'de', 'janeiro', '“', 'revmo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'amigo e Sr'\n",
      "Tokens gerados: ['amigo', 'e', 'sr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cônego Diogo de Melo'\n",
      "Tokens gerados: ['conego', 'diogo', 'de', 'melo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Folgamos que esta vá encontrar V'\n",
      "Tokens gerados: ['folgamos', 'que', 'esta', 'va', 'encontrar', 'v']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Revma'\n",
      "Tokens gerados: ['revma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'no gozo da mais\n",
      "perfeita saúde'\n",
      "Tokens gerados: ['no', 'gozo', 'da', 'maisperfeita', 'saude']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Temos por fim comunicar a V'\n",
      "Tokens gerados: ['temos', 'por', 'fim', 'comunicar', 'a', 'v']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Reverendíssima que, no paquete de 15 do corrente, segue\n",
      "para essa capital o Dr'\n",
      "Tokens gerados: ['reverendissima', 'que', ',', 'no', 'paquete', 'de', '15', 'do', 'corrente', ',', 'seguepara', 'essa', 'capital', 'o', 'dr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo José da Silva, de quem nos encarregou V'\n",
      "Tokens gerados: ['raimundo', 'jose', 'da', 'silva', ',', 'de', 'quem', 'nos', 'encarregou', 'v']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Revma'\n",
      "Tokens gerados: ['revma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'e o Sr'\n",
      "Tokens gerados: ['e', 'o', 'sr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel\n",
      "Pedro da Silva quando ainda nos achávamos estabelecidos em Lisboa'\n",
      "Tokens gerados: ['manuelpedro', 'da', 'silva', 'quando', 'ainda', 'nos', 'achavamos', 'estabelecidos', 'em', 'lisboa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Temos também a declarar, se\n",
      "bem que já em tempo competente o houvéssemos feito, que envidamos então os melhores esforços para\n",
      "conseguir do nosso recomendado ficasse empregado em nossa casa comercial e que, visto não o\n",
      "conseguirmos, tomamos logo a resolução de remetê-lo para Coimbra com o fim de formar-se ele em\n",
      "Teologia, o que igualmente não se realizou, porque, feito o curso preparatório, escolheu o nosso\n",
      "recomendado a carreira de Direito, na qual se acha formado com distinções e bonitas notas'\n",
      "Tokens gerados: ['temos', 'tambem', 'a', 'declarar', ',', 'sebem', 'que', 'ja', 'em', 'tempo', 'competente', 'o', 'houvessemos', 'feito', ',', 'que', 'envidamos', 'entao', 'os', 'melhores', 'esforcos', 'paraconseguir', 'do', 'nosso', 'recomendado', 'ficasse', 'empregado', 'em', 'nossa', 'casa', 'comercial', 'e', 'que', ',', 'visto', 'nao', 'oconseguirmos', ',', 'tomamos', 'logo', 'a', 'resolucao', 'de', 'remete-lo', 'para', 'coimbra', 'com', 'o', 'fim', 'de', 'formar-se', 'ele', 'emteologia', ',', 'o', 'que', 'igualmente', 'nao', 'se', 'realizou', ',', 'porque', ',', 'feito', 'o', 'curso', 'preparatorio', ',', 'escolheu', 'o', 'nossorecomendado', 'a', 'carreira', 'de', 'direito', ',', 'na', 'qual', 'se', 'acha', 'formado', 'com', 'distincoes', 'e', 'bonitas', 'notas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cumpre-nos ainda declarar com prazer a V'\n",
      "Tokens gerados: ['cumpre-nos', 'ainda', 'declarar', 'com', 'prazer', 'a', 'v']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Revma'\n",
      "Tokens gerados: ['revma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'que o Dr'\n",
      "Tokens gerados: ['que', 'o', 'dr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo foi sempre apreciado\n",
      "pelos seus lentes e condiscípulos e que tem feito boa figura, tanto em Portugal, como depois na Alemanha\n",
      "e na Suíça, e como ultimamente nesta Corte, onde, segundo diz ele, tencionava fundar uma empresa\n",
      "muito importante'\n",
      "Tokens gerados: ['raimundo', 'foi', 'sempre', 'apreciadopelos', 'seus', 'lentes', 'e', 'condiscipulos', 'e', 'que', 'tem', 'feito', 'boa', 'figura', ',', 'tanto', 'em', 'portugal', ',', 'como', 'depois', 'na', 'alemanhae', 'na', 'suica', ',', 'e', 'como', 'ultimamente', 'nesta', 'corte', ',', 'onde', ',', 'segundo', 'diz', 'ele', ',', 'tencionava', 'fundar', 'uma', 'empresamuito', 'importante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, antes de estabelecer-se aqui, deseja o Dr'\n",
      "Tokens gerados: ['mas', ',', 'antes', 'de', 'estabelecer-se', 'aqui', ',', 'deseja', 'o', 'dr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo efetuar nessa província a\n",
      "venda de terras e outras propriedades de que aí dispõe, e com esse fim segue'\n",
      "Tokens gerados: ['raimundo', 'efetuar', 'nessa', 'provincia', 'avenda', 'de', 'terras', 'e', 'outras', 'propriedades', 'de', 'que', 'ai', 'dispoe', ',', 'e', 'com', 'esse', 'fim', 'segue']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por esta mesma via escrevemos ao Sr'\n",
      "Tokens gerados: ['por', 'esta', 'mesma', 'via', 'escrevemos', 'ao', 'sr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel Pedro da Silva, a quem novamente prestamos\n",
      "contas das despesas que fizemos com o sobrinho'\n",
      "Tokens gerados: ['manuel', 'pedro', 'da', 'silva', ',', 'a', 'quem', 'novamente', 'prestamoscontas', 'das', 'despesas', 'que', 'fizemos', 'com', 'o', 'sobrinho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Seguiam-se os cumprimentos do estilo'\n",
      "Tokens gerados: ['”', 'seguiam-se', 'os', 'cumprimentos', 'do', 'estilo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel, terminada a leitura, chamou o Benedito, um moleque da casa, e ordenou-lhe que fosse\n",
      "ao armazém saber se havia já chegado a correspondência do Sul'\n",
      "Tokens gerados: ['manuel', ',', 'terminada', 'a', 'leitura', ',', 'chamou', 'o', 'benedito', ',', 'um', 'moleque', 'da', 'casa', ',', 'e', 'ordenou-lhe', 'que', 'fosseao', 'armazem', 'saber', 'se', 'havia', 'ja', 'chegado', 'a', 'correspondencia', 'do', 'sul']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O moleque voltou pouco depois,\n",
      "dizendo que “ainda não senhor, mas que seu Dias a fora buscar ao correio”'\n",
      "Tokens gerados: ['o', 'moleque', 'voltou', 'pouco', 'depois', ',', 'dizendo', 'que', '“', 'ainda', 'nao', 'senhor', ',', 'mas', 'que', 'seu', 'dias', 'a', 'fora', 'buscar', 'ao', 'correio', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Homem! ele é isso!'\n",
      "Tokens gerados: ['—', 'homem', 'ele', 'e', 'isso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'exclamou Pescada'\n",
      "Tokens gerados: ['exclamou', 'pescada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O rapaz está bem encaminhado, quer liquidar o que\n",
      "tem por cá e estabelecer-se no Rio'\n",
      "Tokens gerados: ['o', 'rapaz', 'esta', 'bem', 'encaminhado', ',', 'quer', 'liquidar', 'o', 'quetem', 'por', 'ca', 'e', 'estabelecer-se', 'no', 'rio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não! Sempre é outro futuro!'\n",
      "Tokens gerados: ['nao', 'sempre', 'e', 'outro', 'futuro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora! ora! ora! soprou o cônego em três tempos'\n",
      "Tokens gerados: ['—', 'ora', 'ora', 'ora', 'soprou', 'o', 'conego', 'em', 'tres', 'tempos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nem falemos nisso! O Rio de Janeiro é o\n",
      "Brasil! Ele faria uma grandíssima asneira se ficasse aqui'\n",
      "Tokens gerados: ['nem', 'falemos', 'nisso', 'o', 'rio', 'de', 'janeiro', 'e', 'obrasil', 'ele', 'faria', 'uma', 'grandissima', 'asneira', 'se', 'ficasse', 'aqui']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Se faria'\n",
      "Tokens gerados: ['—', 'se', 'faria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Até lhe digo mais'\n",
      "Tokens gerados: ['—', 'ate', 'lhe', 'digo', 'mais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'nem precisava cá vir, porque'\n",
      "Tokens gerados: ['nem', 'precisava', 'ca', 'vir', ',', 'porque']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'continuou Diogo, abaixando a voz, ninguém\n",
      "aqui lhe ignora a biografia; todos sabem de quem ele saiu!\n",
      "— Que não viesse, não digo, porque enfim'\n",
      "Tokens gerados: ['continuou', 'diogo', ',', 'abaixando', 'a', 'voz', ',', 'ninguemaqui', 'lhe', 'ignora', 'a', 'biografia', 'todos', 'sabem', 'de', 'quem', 'ele', 'saiu—', 'que', 'nao', 'viesse', ',', 'nao', 'digo', ',', 'porque', 'enfim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“quem quer vai e quem não quer manda”, como lá\n",
      "diz o outro; mas é chegar, aviar o que tem a fazer e levantar de novo o ferro!\n",
      "— Ai, ai!\n",
      "— E demais, que diabo ficava ele fazendo aqui? Enchendo as ruas de pernas e gastando o pouco\n",
      "que tem'\n",
      "Tokens gerados: ['“', 'quem', 'quer', 'vai', 'e', 'quem', 'nao', 'quer', 'manda', '”', ',', 'como', 'ladiz', 'o', 'outro', 'mas', 'e', 'chegar', ',', 'aviar', 'o', 'que', 'tem', 'a', 'fazer', 'e', 'levantar', 'de', 'novo', 'o', 'ferro—', 'ai', ',', 'ai—', 'e', 'demais', ',', 'que', 'diabo', 'ficava', 'ele', 'fazendo', 'aqui', '?', 'enchendo', 'as', 'ruas', 'de', 'pernas', 'e', 'gastando', 'o', 'poucoque', 'tem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sim! que ele tem alguma coisinha para roer'\n",
      "Tokens gerados: ['sim', 'que', 'ele', 'tem', 'alguma', 'coisinha', 'para', 'roer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'tem aquelas moradas de casa em São Pantaleão;\n",
      "tem o seu punhado de ações; tem o jimbo cá na casa, onde por bem dizer é sócio comanditário, e tem as\n",
      "fazendas do Rosário, isto é — a fazenda, porque uma é tapera'\n",
      "Tokens gerados: ['tem', 'aquelas', 'moradas', 'de', 'casa', 'em', 'sao', 'pantaleaotem', 'o', 'seu', 'punhado', 'de', 'acoes', 'tem', 'o', 'jimbo', 'ca', 'na', 'casa', ',', 'onde', 'por', 'bem', 'dizer', 'e', 'socio', 'comanditario', ',', 'e', 'tem', 'asfazendas', 'do', 'rosario', ',', 'isto', 'e', '—', 'a', 'fazenda', ',', 'porque', 'uma', 'e', 'tapera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Essa é que ninguém a quer!'\n",
      "Tokens gerados: ['—', 'essa', 'e', 'que', 'ninguem', 'a', 'quer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'observou o cônego, e ferrou o olhar num ponto, deixando\n",
      "perceber que alguma triste reminiscência o dominava'\n",
      "Tokens gerados: ['observou', 'o', 'conego', ',', 'e', 'ferrou', 'o', 'olhar', 'num', 'ponto', ',', 'deixandoperceber', 'que', 'alguma', 'triste', 'reminiscencia', 'o', 'dominava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Acreditam nas almas doutro mundo'\n",
      "Tokens gerados: ['—', 'acreditam', 'nas', 'almas', 'doutro', 'mundo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'prosseguiu Manuel'\n",
      "Tokens gerados: ['prosseguiu', 'manuel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O caso é que nunca mais consegui\n",
      "dar-lhe destino'\n",
      "Tokens gerados: ['o', 'caso', 'e', 'que', 'nunca', 'mais', 'conseguidar-lhe', 'destino']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois olhe, seu compadre, aquelas terras são bem boas para a cana'\n",
      "Tokens gerados: ['pois', 'olhe', ',', 'seu', 'compadre', ',', 'aquelas', 'terras', 'sao', 'bem', 'boas', 'para', 'a', 'cana']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cônego permanecia preocupado pela lembrança da tapera'\n",
      "Tokens gerados: ['o', 'conego', 'permanecia', 'preocupado', 'pela', 'lembranca', 'da', 'tapera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Agora'\n",
      "Tokens gerados: ['—', 'agora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'acrescentou o outro, o melhor seria que ele se tivesse feito padre'\n",
      "Tokens gerados: ['acrescentou', 'o', 'outro', ',', 'o', 'melhor', 'seria', 'que', 'ele', 'se', 'tivesse', 'feito', 'padre']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cônego despertou'\n",
      "Tokens gerados: ['o', 'conego', 'despertou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Padre?!\n",
      "— Era a vontade do José'\n",
      "Tokens gerados: ['—', 'padre', '?', '—', 'era', 'a', 'vontade', 'do', 'jose']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora, deixe-se disso! retrucou Diogo, levantando-se com ímpeto'\n",
      "Tokens gerados: ['—', 'ora', ',', 'deixe-se', 'disso', 'retrucou', 'diogo', ',', 'levantando-se', 'com', 'impeto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nós já temos por aí muito\n",
      "padre de cor!\n",
      "— Mas, compadre, venha cá, não é isso'\n",
      "Tokens gerados: ['nos', 'ja', 'temos', 'por', 'ai', 'muitopadre', 'de', 'cor—', 'mas', ',', 'compadre', ',', 'venha', 'ca', ',', 'nao', 'e', 'isso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora o quê, homem de Deus! É só — ser padre! é só — ser padre! E no fim de contas estão se\n",
      "vendo, as duas por três, superiores mais negros que as nossas cozinheiras! Então isto tem jeito?'\n",
      "Tokens gerados: ['—', 'ora', 'o', 'que', ',', 'homem', 'de', 'deus', 'e', 'so', '—', 'ser', 'padre', 'e', 'so', '—', 'ser', 'padre', 'e', 'no', 'fim', 'de', 'contas', 'estao', 'sevendo', ',', 'as', 'duas', 'por', 'tres', ',', 'superiores', 'mais', 'negros', 'que', 'as', 'nossas', 'cozinheiras', 'entao', 'isto', 'tem', 'jeito', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O\n",
      "governo — e o cônego inchava as palavras — o governo devia até tomar uma medida séria a este\n",
      "respeito! devia proibir aos cabras certos misteres!\n",
      "— Mas, compadre'\n",
      "Tokens gerados: ['ogoverno', '—', 'e', 'o', 'conego', 'inchava', 'as', 'palavras', '—', 'o', 'governo', 'devia', 'ate', 'tomar', 'uma', 'medida', 'seria', 'a', 'esterespeito', 'devia', 'proibir', 'aos', 'cabras', 'certos', 'misteres—', 'mas', ',', 'compadre']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que conheçam seu lugar!\n",
      "E o cônego transformava-se ao calor daquela indignação'\n",
      "Tokens gerados: ['—', 'que', 'conhecam', 'seu', 'lugare', 'o', 'conego', 'transformava-se', 'ao', 'calor', 'daquela', 'indignacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— E então, parece já de pirraça, bradou, é nascer um moleque nas condições deste'\n",
      "Tokens gerados: ['—', 'e', 'entao', ',', 'parece', 'ja', 'de', 'pirraca', ',', 'bradou', ',', 'e', 'nascer', 'um', 'moleque', 'nas', 'condicoes', 'deste']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E mostrava a carta, esmurrando-a — pode contar-se logo com um homem inteligente! Deviam\n",
      "ser burros! burros! que só prestassem mesmo para nos servir! Malditos!\n",
      "— Mas, compadre, você desta vez não tem razão'\n",
      "Tokens gerados: ['e', 'mostrava', 'a', 'carta', ',', 'esmurrando-a', '—', 'pode', 'contar-se', 'logo', 'com', 'um', 'homem', 'inteligente', 'deviamser', 'burros', 'burros', 'que', 'so', 'prestassem', 'mesmo', 'para', 'nos', 'servir', 'malditos—', 'mas', ',', 'compadre', ',', 'voce', 'desta', 'vez', 'nao', 'tem', 'razao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora o quê, homem de Deus'\n",
      "Tokens gerados: ['—', 'ora', 'o', 'que', ',', 'homem', 'de', 'deus']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não diga asneiras! Pois você queria ver sua filha confessada,\n",
      "casada, por um negro? você queria seu Manuel que a Dona Anica beijasse a mão de um filho da\n",
      "Domingas? Se você viesse a ter netos queria que eles apanhassem palmatoadas de um professor mais\n",
      "negro que esta batina? Ora, seu compadre, você às vezes até me parece tolo!\n",
      "Manuel abaixou a cabeça, derrotado'\n",
      "Tokens gerados: ['nao', 'diga', 'asneiras', 'pois', 'voce', 'queria', 'ver', 'sua', 'filha', 'confessada', ',', 'casada', ',', 'por', 'um', 'negro', '?', 'voce', 'queria', 'seu', 'manuel', 'que', 'a', 'dona', 'anica', 'beijasse', 'a', 'mao', 'de', 'um', 'filho', 'dadomingas', '?', 'se', 'voce', 'viesse', 'a', 'ter', 'netos', 'queria', 'que', 'eles', 'apanhassem', 'palmatoadas', 'de', 'um', 'professor', 'maisnegro', 'que', 'esta', 'batina', '?', 'ora', ',', 'seu', 'compadre', ',', 'voce', 'as', 'vezes', 'ate', 'me', 'parece', 'tolomanuel', 'abaixou', 'a', 'cabeca', ',', 'derrotado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora, ora, ora! respingava o sacerdote, como as últimas gotas de um aguaceiro'\n",
      "Tokens gerados: ['—', 'ora', ',', 'ora', ',', 'ora', 'respingava', 'o', 'sacerdote', ',', 'como', 'as', 'ultimas', 'gotas', 'de', 'um', 'aguaceiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E passeava\n",
      "vivamente em toda a extensão da saleta, atirando de uma para a outra mão o seu lenço fino de seda da\n",
      "Índia'\n",
      "Tokens gerados: ['e', 'passeavavivamente', 'em', 'toda', 'a', 'extensao', 'da', 'saleta', ',', 'atirando', 'de', 'uma', 'para', 'a', 'outra', 'mao', 'o', 'seu', 'lenco', 'fino', 'de', 'seda', 'daindia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora! ora, deixe-se disso, seu compadre! Stultorum honor inglorius!'\n",
      "Tokens gerados: ['—', 'ora', 'ora', ',', 'deixe-se', 'disso', ',', 'seu', 'compadre', 'stultorum', 'honor', 'inglorius']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nisto bateram à porta'\n",
      "Tokens gerados: ['nisto', 'bateram', 'a', 'porta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era o Dias com a correspondência do Sul'\n",
      "Tokens gerados: ['era', 'o', 'dias', 'com', 'a', 'correspondencia', 'do', 'sul']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Dê cá'\n",
      "Tokens gerados: ['—', 'de', 'ca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A carta de Manuel pouco adiantava da outra'\n",
      "Tokens gerados: ['a', 'carta', 'de', 'manuel', 'pouco', 'adiantava', 'da', 'outra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Mas, afinal que acha você, compadre?'\n",
      "Tokens gerados: ['—', 'mas', ',', 'afinal', 'que', 'acha', 'voce', ',', 'compadre', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'disse ele, passando a carta ao cônego, depois de a\n",
      "ler'\n",
      "Tokens gerados: ['disse', 'ele', ',', 'passando', 'a', 'carta', 'ao', 'conego', ',', 'depois', 'de', 'aler']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que diabo posso achar?'\n",
      "Tokens gerados: ['—', 'que', 'diabo', 'posso', 'achar', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A coisa está feita por si'\n",
      "Tokens gerados: ['a', 'coisa', 'esta', 'feita', 'por', 'si']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deixe correr o barco! Você não disse uma\n",
      "vez que queria entrar em negócio com a fazenda do Cancela? Não há melhor ocasião — trate-a com o\n",
      "próprio dono'\n",
      "Tokens gerados: ['deixe', 'correr', 'o', 'barco', 'voce', 'nao', 'disse', 'umavez', 'que', 'queria', 'entrar', 'em', 'negocio', 'com', 'a', 'fazenda', 'do', 'cancela', '?', 'nao', 'ha', 'melhor', 'ocasiao', '—', 'trate-a', 'com', 'oproprio', 'dono']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'mesmo as casas de São Pantaleão convinham-lhe'\n",
      "Tokens gerados: ['mesmo', 'as', 'casas', 'de', 'sao', 'pantaleao', 'convinham-lhe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'olhe se ele as desse em conta, eu\n",
      "talvez ficasse com alguma'\n",
      "Tokens gerados: ['olhe', 'se', 'ele', 'as', 'desse', 'em', 'conta', ',', 'eutalvez', 'ficasse', 'com', 'alguma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Mas o que eu digo, compadre, é se devo recebê-lo na qualidade de meu sobrinho'\n",
      "Tokens gerados: ['—', 'mas', 'o', 'que', 'eu', 'digo', ',', 'compadre', ',', 'e', 'se', 'devo', 'recebe-lo', 'na', 'qualidade', 'de', 'meu', 'sobrinho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Sobrinho bastardo, está claro! Que diabo tem você com as cabeçadas de seu mano José?'\n",
      "Tokens gerados: ['—', 'sobrinho', 'bastardo', ',', 'esta', 'claro', 'que', 'diabo', 'tem', 'voce', 'com', 'as', 'cabecadas', 'de', 'seu', 'mano', 'jose', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Homessa!\n",
      "— Mas, compadre, você acha que não me fica mal?'\n",
      "Tokens gerados: ['homessa—', 'mas', ',', 'compadre', ',', 'voce', 'acha', 'que', 'nao', 'me', 'fica', 'mal', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Mal por quê, homem de Deus? Isso nada tem que ver com você'\n",
      "Tokens gerados: ['—', 'mal', 'por', 'que', ',', 'homem', 'de', 'deus', '?', 'isso', 'nada', 'tem', 'que', 'ver', 'com', 'voce']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Lá isso é verdade'\n",
      "Tokens gerados: ['—', 'la', 'isso', 'e', 'verdade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah! outra coisa! devo hospedá-lo aqui em casa?\n",
      "— É!'\n",
      "Tokens gerados: ['ah', 'outra', 'coisa', 'devo', 'hospeda-lo', 'aqui', 'em', 'casa', '?', '—', 'e']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'por um lado, devia ser assim'\n",
      "Tokens gerados: ['por', 'um', 'lado', ',', 'devia', 'ser', 'assim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Todos sabem as obrigações que você deve ao defunto\n",
      "José e poderiam boquejar por aí, no caso que não lhe hospedasse o filho'\n",
      "Tokens gerados: ['todos', 'sabem', 'as', 'obrigacoes', 'que', 'voce', 'deve', 'ao', 'defuntojose', 'e', 'poderiam', 'boquejar', 'por', 'ai', ',', 'no', 'caso', 'que', 'nao', 'lhe', 'hospedasse', 'o', 'filho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'mas, por outro lado, meu\n",
      "amigo, não sei o que lhe diga!'\n",
      "Tokens gerados: ['mas', ',', 'por', 'outro', 'lado', ',', 'meuamigo', ',', 'nao', 'sei', 'o', 'que', 'lhe', 'diga']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E depois de uma pausa em que o outro não falou:\n",
      "— Homem, seu compadre, isto de meter rapazes em casa'\n",
      "Tokens gerados: ['e', 'depois', 'de', 'uma', 'pausa', 'em', 'que', 'o', 'outro', 'nao', 'falou—', 'homem', ',', 'seu', 'compadre', ',', 'isto', 'de', 'meter', 'rapazes', 'em', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'é o diabo!\n",
      "— De sorte que'\n",
      "Tokens gerados: ['e', 'o', 'diabo—', 'de', 'sorte', 'que']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Omnem aditum malis prejudica!\n",
      "Manuel não compreendeu, porém acrescentou:\n",
      "— Mas eu hospedo constantemente os meus fregueses do interior'\n",
      "Tokens gerados: ['—', 'omnem', 'aditum', 'malis', 'prejudicamanuel', 'nao', 'compreendeu', ',', 'porem', 'acrescentou—', 'mas', 'eu', 'hospedo', 'constantemente', 'os', 'meus', 'fregueses', 'do', 'interior']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Isso é muito diferente!\n",
      "— E meus caixeiros? não moram aqui comigo?'\n",
      "Tokens gerados: ['—', 'isso', 'e', 'muito', 'diferente—', 'e', 'meus', 'caixeiros', '?', 'nao', 'moram', 'aqui', 'comigo', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Sim! disse o cônego, impacientando-se, mas os pobres dos caixeiros são todos uns\n",
      "moscas-mortas, e nós não sabemos a que nos saiu o tal doutor de Coimbra!'\n",
      "Tokens gerados: ['—', 'sim', 'disse', 'o', 'conego', ',', 'impacientando-se', ',', 'mas', 'os', 'pobres', 'dos', 'caixeiros', 'sao', 'todos', 'unsmoscas-mortas', ',', 'e', 'nos', 'nao', 'sabemos', 'a', 'que', 'nos', 'saiu', 'o', 'tal', 'doutor', 'de', 'coimbra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Homem, compadre, o\n",
      "melro vem de Paris, deve estar mitrado!'\n",
      "Tokens gerados: ['homem', ',', 'compadre', ',', 'omelro', 'vem', 'de', 'paris', ',', 'deve', 'estar', 'mitrado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Talvez não'\n",
      "Tokens gerados: ['—', 'talvez', 'nao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Sim, mas é mais natural que esteja!\n",
      "E o cônego intumescia a papada com certo ar experimentado'\n",
      "Tokens gerados: ['—', 'sim', ',', 'mas', 'e', 'mais', 'natural', 'que', 'estejae', 'o', 'conego', 'intumescia', 'a', 'papada', 'com', 'certo', 'ar', 'experimentado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Em todo caso'\n",
      "Tokens gerados: ['—', 'em', 'todo', 'caso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'arriscou Manuel, é por pouco tempo'\n",
      "Tokens gerados: ['arriscou', 'manuel', ',', 'e', 'por', 'pouco', 'tempo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Talvez coisa de um mês'\n",
      "Tokens gerados: ['talvez', 'coisa', 'de', 'um', 'mes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, sopeando a voz, discretamente, com medo: Além disso'\n",
      "Tokens gerados: ['e', ',', 'sopeando', 'a', 'voz', ',', 'discretamente', ',', 'com', 'medo', 'alem', 'disso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'não me convinha desagradar o\n",
      "rapaz'\n",
      "Tokens gerados: ['nao', 'me', 'convinha', 'desagradar', 'orapaz']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sim! tenho de entrar em negócio com ele, e'\n",
      "Tokens gerados: ['sim', 'tenho', 'de', 'entrar', 'em', 'negocio', 'com', 'ele', ',', 'e']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'isto cá para nós'\n",
      "Tokens gerados: ['isto', 'ca', 'para', 'nos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'seria uma fineza, que me ficava\n",
      "a dever'\n",
      "Tokens gerados: ['seria', 'uma', 'fineza', ',', 'que', 'me', 'ficavaa', 'dever']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'porque enfim'\n",
      "Tokens gerados: ['porque', 'enfim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'você sabe que'\n",
      "Tokens gerados: ['voce', 'sabe', 'que']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah! interrompeu o cônego, tomando uma nova atitude'\n",
      "Tokens gerados: ['—', 'ah', 'interrompeu', 'o', 'conego', ',', 'tomando', 'uma', 'nova', 'atitude']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Isso é outro cantar!'\n",
      "Tokens gerados: ['isso', 'e', 'outro', 'cantar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por ai é que\n",
      "você devia ter principiado!\n",
      "— Sim, tornou Manuel, com mais ânimo'\n",
      "Tokens gerados: ['por', 'ai', 'e', 'quevoce', 'devia', 'ter', 'principiado—', 'sim', ',', 'tornou', 'manuel', ',', 'com', 'mais', 'animo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Você bem sabe que não tenho obrigação de estar a\n",
      "moer-me com o nhonhô Mundico'\n",
      "Tokens gerados: ['voce', 'bem', 'sabe', 'que', 'nao', 'tenho', 'obrigacao', 'de', 'estar', 'amoer-me', 'com', 'o', 'nhonho', 'mundico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'e, se bem que'\n",
      "Tokens gerados: ['e', ',', 'se', 'bem', 'que']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pchio!'\n",
      "Tokens gerados: ['—', 'pchio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'fez o padre, cortando a conversa, e disse: — Hospede o homem!\n",
      "E saiu da saleta, revestindo logo o seu pachorrento e estudado ar de santarrão'\n",
      "Tokens gerados: ['fez', 'o', 'padre', ',', 'cortando', 'a', 'conversa', ',', 'e', 'disse', '—', 'hospede', 'o', 'homeme', 'saiu', 'da', 'saleta', ',', 'revestindo', 'logo', 'o', 'seu', 'pachorrento', 'e', 'estudado', 'ar', 'de', 'santarrao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao chegarem à varanda Ana Rosa, já em trajes de passeio, os esperava para sair toda debruçada\n",
      "no parapeito da janela e derramando sobre o Bacanga um olhar mole e cheio de incertezas'\n",
      "Tokens gerados: ['ao', 'chegarem', 'a', 'varanda', 'ana', 'rosa', ',', 'ja', 'em', 'trajes', 'de', 'passeio', ',', 'os', 'esperava', 'para', 'sair', 'toda', 'debrucadano', 'parapeito', 'da', 'janela', 'e', 'derramando', 'sobre', 'o', 'bacanga', 'um', 'olhar', 'mole', 'e', 'cheio', 'de', 'incertezas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então, sempre te resolveste, minha caprichosa?'\n",
      "Tokens gerados: ['—', 'entao', ',', 'sempre', 'te', 'resolveste', ',', 'minha', 'caprichosa', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'disse o pai'\n",
      "Tokens gerados: ['disse', 'o', 'pai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E contemplava a filha, com um risinho de orgulho'\n",
      "Tokens gerados: ['e', 'contemplava', 'a', 'filha', ',', 'com', 'um', 'risinho', 'de', 'orgulho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ela estava realmente boa com o seu vestido\n",
      "muito alvo de fustão, alegre, todo cheirando aos jasmins da gaveta; com o seu chapéu de palhinha de\n",
      "Itália, emoldurando o rosto oval, fresco e bem feito; com o seu cabelo castanho, farto e sedoso, que\n",
      "aparecia em bandós no alto da cabeça e reaparecia no pescoço enrodilhado despretensiosamente'\n",
      "Tokens gerados: ['ela', 'estava', 'realmente', 'boa', 'com', 'o', 'seu', 'vestidomuito', 'alvo', 'de', 'fustao', ',', 'alegre', ',', 'todo', 'cheirando', 'aos', 'jasmins', 'da', 'gaveta', 'com', 'o', 'seu', 'chapeu', 'de', 'palhinha', 'deitalia', ',', 'emoldurando', 'o', 'rosto', 'oval', ',', 'fresco', 'e', 'bem', 'feito', 'com', 'o', 'seu', 'cabelo', 'castanho', ',', 'farto', 'e', 'sedoso', ',', 'queaparecia', 'em', 'bandos', 'no', 'alto', 'da', 'cabeca', 'e', 'reaparecia', 'no', 'pescoco', 'enrodilhado', 'despretensiosamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Tinhas dito que não ias'\n",
      "Tokens gerados: ['—', 'tinhas', 'dito', 'que', 'nao', 'ias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Vá se vestir, papai'\n",
      "Tokens gerados: ['—', 'va', 'se', 'vestir', ',', 'papai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E assentou-se'\n",
      "Tokens gerados: ['e', 'assentou-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Lá vou! Lá vou!\n",
      "Manuel bateu no ombro do cônego:\n",
      "— Meto-lhe inveja, hein, compadre?'\n",
      "Tokens gerados: ['—', 'la', 'vou', 'la', 'voumanuel', 'bateu', 'no', 'ombro', 'do', 'conego—', 'meto-lhe', 'inveja', ',', 'hein', ',', 'compadre', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Olhe como o diacho da pequena está faceira, não é?\n",
      "— Ne insultes miseris!\n",
      "— Quê?'\n",
      "Tokens gerados: ['olhe', 'como', 'o', 'diacho', 'da', 'pequena', 'esta', 'faceira', ',', 'nao', 'e', '?', '—', 'ne', 'insultes', 'miseris—', 'que', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'interjeicionou o negociante, olhando para o relógio da varanda'\n",
      "Tokens gerados: ['interjeicionou', 'o', 'negociante', ',', 'olhando', 'para', 'o', 'relogio', 'da', 'varanda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quatro e meia! E eu que ainda tinha de ir\n",
      "hoje tratar do despacho de um açúcar!'\n",
      "Tokens gerados: ['quatro', 'e', 'meia', 'e', 'eu', 'que', 'ainda', 'tinha', 'de', 'irhoje', 'tratar', 'do', 'despacho', 'de', 'um', 'acucar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E foi entrando apressado no quarto, a gritar para o Benedito “que lhe levasse água morna para\n",
      "banhar o rosto”'\n",
      "Tokens gerados: ['e', 'foi', 'entrando', 'apressado', 'no', 'quarto', ',', 'a', 'gritar', 'para', 'o', 'benedito', '“', 'que', 'lhe', 'levasse', 'agua', 'morna', 'parabanhar', 'o', 'rosto', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cônego assentou-se defronte de Ana Rosa'\n",
      "Tokens gerados: ['o', 'conego', 'assentou-se', 'defronte', 'de', 'ana', 'rosa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então onde é hoje o passeio minha rica afilhada?\n",
      "— À casa do Freitas'\n",
      "Tokens gerados: ['—', 'entao', 'onde', 'e', 'hoje', 'o', 'passeio', 'minha', 'rica', 'afilhada', '?', '—', 'a', 'casa', 'do', 'freitas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não se lembra? Lindoca faz anos hoje'\n",
      "Tokens gerados: ['nao', 'se', 'lembra', '?', 'lindoca', 'faz', 'anos', 'hoje']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Cáspite! Temos então peru de forno!'\n",
      "Tokens gerados: ['—', 'caspite', 'temos', 'entao', 'peru', 'de', 'forno']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Papai fica para o jantar'\n",
      "Tokens gerados: ['—', 'papai', 'fica', 'para', 'o', 'jantar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'vossemecê não vai, dindinho?\n",
      "— Talvez apareça à noite'\n",
      "Tokens gerados: ['vossemece', 'nao', 'vai', ',', 'dindinho', '?', '—', 'talvez', 'apareca', 'a', 'noite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Com certeza há dança'\n",
      "Tokens gerados: ['com', 'certeza', 'ha', 'danca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Hum-hum'\n",
      "Tokens gerados: ['—', 'hum-hum']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'mas creio que o Freitas conta com uma surpresa da Filarmônica'\n",
      "Tokens gerados: ['mas', 'creio', 'que', 'o', 'freitas', 'conta', 'com', 'uma', 'surpresa', 'da', 'filarmonica']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'disse Ana\n",
      "Rosa, entretida a endireitar os folhos do seu vestido com a biqueira da sombrinha'\n",
      "Tokens gerados: ['disse', 'anarosa', ',', 'entretida', 'a', 'endireitar', 'os', 'folhos', 'do', 'seu', 'vestido', 'com', 'a', 'biqueira', 'da', 'sombrinha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nisto, ouviram-se bater embaixo as portas do armazém, que se fechavam com grande ruído de\n",
      "fechaduras, e logo em seguida o som pesado de passos repetidos na escada'\n",
      "Tokens gerados: ['nisto', ',', 'ouviram-se', 'bater', 'embaixo', 'as', 'portas', 'do', 'armazem', ',', 'que', 'se', 'fechavam', 'com', 'grande', 'ruido', 'defechaduras', ',', 'e', 'logo', 'em', 'seguida', 'o', 'som', 'pesado', 'de', 'passos', 'repetidos', 'na', 'escada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eram os caixeiros que\n",
      "subiam para jantar'\n",
      "Tokens gerados: ['eram', 'os', 'caixeiros', 'quesubiam', 'para', 'jantar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entrou primeiro na varanda o Bento Cordeiro'\n",
      "Tokens gerados: ['entrou', 'primeiro', 'na', 'varanda', 'o', 'bento', 'cordeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Português dos seus trinta e tantos anos arruivado,\n",
      "feio, de bigode e barba e cavanhaque'\n",
      "Tokens gerados: ['portugues', 'dos', 'seus', 'trinta', 'e', 'tantos', 'anos', 'arruivado', ',', 'feio', ',', 'de', 'bigode', 'e', 'barba', 'e', 'cavanhaque']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Gabava-se de grande prática de balcão chamavam-lhe “Um\n",
      "alho”'\n",
      "Tokens gerados: ['gabava-se', 'de', 'grande', 'pratica', 'de', 'balcao', 'chamavam-lhe', '“', 'umalho', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Para aviar encomendas do interior não havia outro! Cordeiro “metia no bolso o capurreiro mais\n",
      "sabido”'\n",
      "Tokens gerados: ['para', 'aviar', 'encomendas', 'do', 'interior', 'nao', 'havia', 'outro', 'cordeiro', '“', 'metia', 'no', 'bolso', 'o', 'capurreiro', 'maissabido', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dos empregados da casa era o mais antigo; nunca, porém lograra ter interesse na sociedade;\n",
      "continuava sempre de fora e tinha por isso um ódio surdo ao patrão; ódio, que o patife disfarçava por\n",
      "um constante sorriso de boa vontade'\n",
      "Tokens gerados: ['dos', 'empregados', 'da', 'casa', 'era', 'o', 'mais', 'antigo', 'nunca', ',', 'porem', 'lograra', 'ter', 'interesse', 'na', 'sociedadecontinuava', 'sempre', 'de', 'fora', 'e', 'tinha', 'por', 'isso', 'um', 'odio', 'surdo', 'ao', 'patrao', 'odio', ',', 'que', 'o', 'patife', 'disfarcava', 'porum', 'constante', 'sorriso', 'de', 'boa', 'vontade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas o seu maior defeito o que deveras depunha contra ele aos\n",
      "olhos das — raposas — do comércio; o que explicava na Praça a sua não entrada na sociedade da casa\n",
      "em que trabalhava havia tanto tempo, era sem dúvida a sua queda para o vinho'\n",
      "Tokens gerados: ['mas', 'o', 'seu', 'maior', 'defeito', 'o', 'que', 'deveras', 'depunha', 'contra', 'ele', 'aosolhos', 'das', '—', 'raposas', '—', 'do', 'comercio', 'o', 'que', 'explicava', 'na', 'praca', 'a', 'sua', 'nao', 'entrada', 'na', 'sociedade', 'da', 'casaem', 'que', 'trabalhava', 'havia', 'tanto', 'tempo', ',', 'era', 'sem', 'duvida', 'a', 'sua', 'queda', 'para', 'o', 'vinho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aos domingos metia-se\n",
      "na tiorga e ficava de todo insuportável'\n",
      "Tokens gerados: ['aos', 'domingos', 'metia-sena', 'tiorga', 'e', 'ficava', 'de', 'todo', 'insuportavel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bento atravessou silencioso a varanda cortejando com afetada humildade o cônego e Ana Rosa,\n",
      "e seguiu logo para o mirante, onde moravam todos os caixeiros da casa'\n",
      "Tokens gerados: ['bento', 'atravessou', 'silencioso', 'a', 'varanda', 'cortejando', 'com', 'afetada', 'humildade', 'o', 'conego', 'e', 'ana', 'rosa', ',', 'e', 'seguiu', 'logo', 'para', 'o', 'mirante', ',', 'onde', 'moravam', 'todos', 'os', 'caixeiros', 'da', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O segundo a passar foi Gustavo de Vila Rica; simpático e bonito mocetão de dezesseis anos,\n",
      "com as suas soberbas cores portuguesas, que o clima do Maranhão ainda não tinha conseguido destruir'\n",
      "Tokens gerados: ['o', 'segundo', 'a', 'passar', 'foi', 'gustavo', 'de', 'vila', 'rica', 'simpatico', 'e', 'bonito', 'mocetao', 'de', 'dezesseis', 'anos', ',', 'com', 'as', 'suas', 'soberbas', 'cores', 'portuguesas', ',', 'que', 'o', 'clima', 'do', 'maranhao', 'ainda', 'nao', 'tinha', 'conseguido', 'destruir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estava sempre de bom humor; lisonjeava-se de um apetite inquebrantável e de nunca haver ficado de\n",
      "cama no Brasil'\n",
      "Tokens gerados: ['estava', 'sempre', 'de', 'bom', 'humor', 'lisonjeava-se', 'de', 'um', 'apetite', 'inquebrantavel', 'e', 'de', 'nunca', 'haver', 'ficado', 'decama', 'no', 'brasil']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em casa todavia ganhara fama de extravagante; é que mandava fazer fatos de casimira\n",
      "à moda, para passear aos domingos e para ir aos bailes familiares de contribuição, e queimava charutos\n",
      "de dois vinténs'\n",
      "Tokens gerados: ['em', 'casa', 'todavia', 'ganhara', 'fama', 'de', 'extravagante', 'e', 'que', 'mandava', 'fazer', 'fatos', 'de', 'casimiraa', 'moda', ',', 'para', 'passear', 'aos', 'domingos', 'e', 'para', 'ir', 'aos', 'bailes', 'familiares', 'de', 'contribuicao', ',', 'e', 'queimava', 'charutosde', 'dois', 'vintens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O grande defeito deste era uma assinatura no Gabinete Português, o que levava a boa\n",
      "gente do comércio a dizer “que ele era um grande biltre, um peralta, que estava sempre procurando o\n",
      "que ler!”\n",
      "O Bento Cordeiro bradava-lhe às vezes, furioso:\n",
      "— Com os diabos! o patrão já lhe tem dado a entender que não gosta de caixeiros amigos de\n",
      "gazeta?'\n",
      "Tokens gerados: ['o', 'grande', 'defeito', 'deste', 'era', 'uma', 'assinatura', 'no', 'gabinete', 'portugues', ',', 'o', 'que', 'levava', 'a', 'boagente', 'do', 'comercio', 'a', 'dizer', '“', 'que', 'ele', 'era', 'um', 'grande', 'biltre', ',', 'um', 'peralta', ',', 'que', 'estava', 'sempre', 'procurando', 'oque', 'ler', '”', 'o', 'bento', 'cordeiro', 'bradava-lhe', 'as', 'vezes', ',', 'furioso—', 'com', 'os', 'diabos', 'o', 'patrao', 'ja', 'lhe', 'tem', 'dado', 'a', 'entender', 'que', 'nao', 'gosta', 'de', 'caixeiros', 'amigos', 'degazeta', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se você quer ser letrado, vá pra Coimbra, seu burro!\n",
      "Gustavo ouvia constantemente destas e doutras amabilidades, mas, que fazer? precisava ganhar\n",
      "a vida!'\n",
      "Tokens gerados: ['se', 'voce', 'quer', 'ser', 'letrado', ',', 'va', 'pra', 'coimbra', ',', 'seu', 'burrogustavo', 'ouvia', 'constantemente', 'destas', 'e', 'doutras', 'amabilidades', ',', 'mas', ',', 'que', 'fazer', '?', 'precisava', 'ganhara', 'vida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O outro era caixeiro mais antigo na casa'\n",
      "Tokens gerados: ['o', 'outro', 'era', 'caixeiro', 'mais', 'antigo', 'na', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Conformava-se, sem respingar, e em certas ocasiões\n",
      "até satisfeito, graças ao seu bom humor'\n",
      "Tokens gerados: ['conformava-se', ',', 'sem', 'respingar', ',', 'e', 'em', 'certas', 'ocasioesate', 'satisfeito', ',', 'gracas', 'ao', 'seu', 'bom', 'humor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao passar pela varanda foi menos brusco no seu cumprimento à filha do patrão; chegou mesmo\n",
      "a parar, sorrir, e dizer, inclinando a cabeça: “Minha senhora!'\n",
      "Tokens gerados: ['ao', 'passar', 'pela', 'varanda', 'foi', 'menos', 'brusco', 'no', 'seu', 'cumprimento', 'a', 'filha', 'do', 'patrao', 'chegou', 'mesmoa', 'parar', ',', 'sorrir', ',', 'e', 'dizer', ',', 'inclinando', 'a', 'cabeca', '“', 'minha', 'senhora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "O cônego teve uma risota'\n",
      "Tokens gerados: ['”', 'o', 'conego', 'teve', 'uma', 'risota']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que mitra!'\n",
      "Tokens gerados: ['—', 'que', 'mitra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'julgou com os seus botões'\n",
      "Tokens gerados: ['julgou', 'com', 'os', 'seus', 'botoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em seguida, atravessou a varanda, muito apressado, com as mãos escondidas nas enormes mangas\n",
      "de um jaquetão, cuja gola lhe subia ate à nuca, uma criança de uns dez anos de idade'\n",
      "Tokens gerados: ['em', 'seguida', ',', 'atravessou', 'a', 'varanda', ',', 'muito', 'apressado', ',', 'com', 'as', 'maos', 'escondidas', 'nas', 'enormes', 'mangasde', 'um', 'jaquetao', ',', 'cuja', 'gola', 'lhe', 'subia', 'ate', 'a', 'nuca', ',', 'uma', 'crianca', 'de', 'uns', 'dez', 'anos', 'de', 'idade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha o cabelo à\n",
      "escovinha; os sapatos grandemente desproporcionados; calças de zuarte dobradas na bainha; olhos\n",
      "espantados; gestos desconfiados, e um certo movimento rápido de esconder a cabeça nos ombros, que\n",
      "lhe traía o hábito de levar pescoções'\n",
      "Tokens gerados: ['tinha', 'o', 'cabelo', 'aescovinha', 'os', 'sapatos', 'grandemente', 'desproporcionados', 'calcas', 'de', 'zuarte', 'dobradas', 'na', 'bainha', 'olhosespantados', 'gestos', 'desconfiados', ',', 'e', 'um', 'certo', 'movimento', 'rapido', 'de', 'esconder', 'a', 'cabeca', 'nos', 'ombros', ',', 'quelhe', 'traia', 'o', 'habito', 'de', 'levar', 'pescocoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Este era em tudo mais novo que os outros — em idade, na casa, e no Brasil'\n",
      "Tokens gerados: ['este', 'era', 'em', 'tudo', 'mais', 'novo', 'que', 'os', 'outros', '—', 'em', 'idade', ',', 'na', 'casa', ',', 'e', 'no', 'brasil']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Chegara havia coisa\n",
      "de seis meses da sua aldeia no Porto; dizia chamar-se Manuelzinho e tinha sempre os olhos vermelhos\n",
      "de chorar à noite com saudades da mãe e da terra'\n",
      "Tokens gerados: ['chegara', 'havia', 'coisade', 'seis', 'meses', 'da', 'sua', 'aldeia', 'no', 'porto', 'dizia', 'chamar-se', 'manuelzinho', 'e', 'tinha', 'sempre', 'os', 'olhos', 'vermelhosde', 'chorar', 'a', 'noite', 'com', 'saudades', 'da', 'mae', 'e', 'da', 'terra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por ser o mais novo na casa varria o armazém limpava as balanças e burnia os pesos de latão'\n",
      "Tokens gerados: ['por', 'ser', 'o', 'mais', 'novo', 'na', 'casa', 'varria', 'o', 'armazem', 'limpava', 'as', 'balancas', 'e', 'burnia', 'os', 'pesos', 'de', 'latao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Todos lhe batiam sem responsabilidade, não tinha a quem se queixar'\n",
      "Tokens gerados: ['todos', 'lhe', 'batiam', 'sem', 'responsabilidade', ',', 'nao', 'tinha', 'a', 'quem', 'se', 'queixar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Divertiam-se à custa dele; riam-se\n",
      "com repugnância das suas orelhas cheias de cera escura'\n",
      "Tokens gerados: ['divertiam-se', 'a', 'custa', 'dele', 'riam-secom', 'repugnancia', 'das', 'suas', 'orelhas', 'cheias', 'de', 'cera', 'escura']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desfeava-lhe a testa uma grande cicatriz; foi um trambolhão que levou na primeira noite em\n",
      "que lhe deram uma rede para dormir'\n",
      "Tokens gerados: ['desfeava-lhe', 'a', 'testa', 'uma', 'grande', 'cicatriz', 'foi', 'um', 'trambolhao', 'que', 'levou', 'na', 'primeira', 'noite', 'emque', 'lhe', 'deram', 'uma', 'rede', 'para', 'dormir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O pobre desterradozinho, que não sabia haver-se com semelhante\n",
      "engenhoca, caiu na asneira de meter primeiro os pés, e zás! lá foi por cima de uma caixa de pinho de um\n",
      "dos companheiros'\n",
      "Tokens gerados: ['o', 'pobre', 'desterradozinho', ',', 'que', 'nao', 'sabia', 'haver-se', 'com', 'semelhanteengenhoca', ',', 'caiu', 'na', 'asneira', 'de', 'meter', 'primeiro', 'os', 'pes', ',', 'e', 'zas', 'la', 'foi', 'por', 'cima', 'de', 'uma', 'caixa', 'de', 'pinho', 'de', 'umdos', 'companheiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desde esse dia ficou conhecido em casa pela alcunha de “Salta-chão”'\n",
      "Tokens gerados: ['desde', 'esse', 'dia', 'ficou', 'conhecido', 'em', 'casa', 'pela', 'alcunha', 'de', '“', 'salta-chao', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Punham-lhe\n",
      "nomes feios e chamavam-lhe “Ó coisa! — Ó maroto! — Ó bisca!” tudo servia para o chamarem, menos\n",
      "o seu verdadeiro nome'\n",
      "Tokens gerados: ['punham-lhenomes', 'feios', 'e', 'chamavam-lhe', '“', 'o', 'coisa', '—', 'o', 'maroto', '—', 'o', 'bisca', '”', 'tudo', 'servia', 'para', 'o', 'chamarem', ',', 'menoso', 'seu', 'verdadeiro', 'nome']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ia atravessando a varanda, como um bicho assustado, quase a correr'\n",
      "Tokens gerados: ['ia', 'atravessando', 'a', 'varanda', ',', 'como', 'um', 'bicho', 'assustado', ',', 'quase', 'a', 'correr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cônego gritou por ele:\n",
      "— Ó pequeno? anda cá!\n",
      "Manuelzinho voltou, confuso, coçando a nuca, muito contrariado sem levantar os olhos'\n",
      "Tokens gerados: ['o', 'conego', 'gritou', 'por', 'ele—', 'o', 'pequeno', '?', 'anda', 'camanuelzinho', 'voltou', ',', 'confuso', ',', 'cocando', 'a', 'nuca', ',', 'muito', 'contrariado', 'sem', 'levantar', 'os', 'olhos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ana Rosa teve um olhar de piedade'\n",
      "Tokens gerados: ['ana', 'rosa', 'teve', 'um', 'olhar', 'de', 'piedade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então que e isso? disse o cônego'\n",
      "Tokens gerados: ['—', 'entao', 'que', 'e', 'isso', '?', 'disse', 'o', 'conego']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pareces-me um bicho do mato! Fala direito com a gente,\n",
      "rapaz! Levanta essa cachimônia!\n",
      "E, com a sua mão branca e fina, suspendeu-lhe pelo queixo a cabeça, que Manuelzinho insistia\n",
      "em ter baixa'\n",
      "Tokens gerados: ['pareces-me', 'um', 'bicho', 'do', 'mato', 'fala', 'direito', 'com', 'a', 'gente', ',', 'rapaz', 'levanta', 'essa', 'cachimoniae', ',', 'com', 'a', 'sua', 'mao', 'branca', 'e', 'fina', ',', 'suspendeu-lhe', 'pelo', 'queixo', 'a', 'cabeca', ',', 'que', 'manuelzinho', 'insistiaem', 'ter', 'baixa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Este ainda está muito peludo!'\n",
      "Tokens gerados: ['—', 'este', 'ainda', 'esta', 'muito', 'peludo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'acrescentou'\n",
      "Tokens gerados: ['acrescentou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E perguntou-lhe depois uma porção de coisas:\n",
      "“Se tinha vontade de enriquecer, se não sonhava já com uma comenda; se tinha visto o pássaro guariba,\n",
      "se encontrara a árvore das patacas'\n",
      "Tokens gerados: ['e', 'perguntou-lhe', 'depois', 'uma', 'porcao', 'de', 'coisas', '“', 'se', 'tinha', 'vontade', 'de', 'enriquecer', ',', 'se', 'nao', 'sonhava', 'ja', 'com', 'uma', 'comenda', 'se', 'tinha', 'visto', 'o', 'passaro', 'guariba', ',', 'se', 'encontrara', 'a', 'arvore', 'das', 'patacas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” O pequeno mastigava respostas inarticuladas, com um sorriso\n",
      "aflito'\n",
      "Tokens gerados: ['”', 'o', 'pequeno', 'mastigava', 'respostas', 'inarticuladas', ',', 'com', 'um', 'sorrisoaflito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Como te chamas?\n",
      "Ele não respondeu'\n",
      "Tokens gerados: ['—', 'como', 'te', 'chamas', '?', 'ele', 'nao', 'respondeu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então não respondes?'\n",
      "Tokens gerados: ['—', 'entao', 'nao', 'respondes', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Com certeza és Manuel!\n",
      "O portuguesinho meneou a cabeça afirmativamente, e apertou a boca, para conter o riso que\n",
      "procurava uma válvula'\n",
      "Tokens gerados: ['com', 'certeza', 'es', 'manuelo', 'portuguesinho', 'meneou', 'a', 'cabeca', 'afirmativamente', ',', 'e', 'apertou', 'a', 'boca', ',', 'para', 'conter', 'o', 'riso', 'queprocurava', 'uma', 'valvula']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então é com a cabeça que se responde? Tu não sabes falar, mariola?\n",
      "E, voltando-se para Ana Rosa:\n",
      "— Isto é um sonso, minha afilhada! olhe em que estado ele traz as orelhas! Se tens a alma como\n",
      "tens o corpo, podes dá-la ao diabo! Tu já te confessaste aqui, maroto?\n",
      "Manuelzinho, não podendo já suster os beiços, abriu a boca e, com a força de uma caldeira,\n",
      "soprou o riso que a tanto custo refreava'\n",
      "Tokens gerados: ['—', 'entao', 'e', 'com', 'a', 'cabeca', 'que', 'se', 'responde', '?', 'tu', 'nao', 'sabes', 'falar', ',', 'mariola', '?', 'e', ',', 'voltando-se', 'para', 'ana', 'rosa—', 'isto', 'e', 'um', 'sonso', ',', 'minha', 'afilhada', 'olhe', 'em', 'que', 'estado', 'ele', 'traz', 'as', 'orelhas', 'se', 'tens', 'a', 'alma', 'comotens', 'o', 'corpo', ',', 'podes', 'da-la', 'ao', 'diabo', 'tu', 'ja', 'te', 'confessaste', 'aqui', ',', 'maroto', '?', 'manuelzinho', ',', 'nao', 'podendo', 'ja', 'suster', 'os', 'beicos', ',', 'abriu', 'a', 'boca', 'e', ',', 'com', 'a', 'forca', 'de', 'uma', 'caldeira', ',', 'soprou', 'o', 'riso', 'que', 'a', 'tanto', 'custo', 'refreava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Olha que estás a cuspir-me, ó patife! gritou o cônego'\n",
      "Tokens gerados: ['—', 'olha', 'que', 'estas', 'a', 'cuspir-me', ',', 'o', 'patife', 'gritou', 'o', 'conego']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bom, bom! vai-te! vai-te!\n",
      "Repeliu-o e limpou a batina com o lenço'\n",
      "Tokens gerados: ['bom', ',', 'bom', 'vai-te', 'vai-terepeliu-o', 'e', 'limpou', 'a', 'batina', 'com', 'o', 'lenco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ana Rosa então correu os dedos pela cabeça do menino e puxou-o para si'\n",
      "Tokens gerados: ['ana', 'rosa', 'entao', 'correu', 'os', 'dedos', 'pela', 'cabeca', 'do', 'menino', 'e', 'puxou-o', 'para', 'si']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Arregaçou-lhe as\n",
      "mangas da jaqueta e revistou-lhe as unhas'\n",
      "Tokens gerados: ['arregacou-lhe', 'asmangas', 'da', 'jaqueta', 'e', 'revistou-lhe', 'as', 'unhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estavam crescidas e sujas'\n",
      "Tokens gerados: ['estavam', 'crescidas', 'e', 'sujas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah! censurou ela, você também não é tão pequeno, que se desculpe isto!'\n",
      "Tokens gerados: ['—', 'ah', 'censurou', 'ela', ',', 'voce', 'tambem', 'nao', 'e', 'tao', 'pequeno', ',', 'que', 'se', 'desculpe', 'isto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, tirando do seu indispensável uma tesourinha, começou, com grande surpresa do caixeiro e\n",
      "até do cônego, a limpar as unhas da criança, dizendo ao outro, baixinho:\n",
      "— Não sei como há mães que se separam de filhos desta idade'\n",
      "Tokens gerados: ['e', ',', 'tirando', 'do', 'seu', 'indispensavel', 'uma', 'tesourinha', ',', 'comecou', ',', 'com', 'grande', 'surpresa', 'do', 'caixeiro', 'eate', 'do', 'conego', ',', 'a', 'limpar', 'as', 'unhas', 'da', 'crianca', ',', 'dizendo', 'ao', 'outro', ',', 'baixinho—', 'nao', 'sei', 'como', 'ha', 'maes', 'que', 'se', 'separam', 'de', 'filhos', 'desta', 'idade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Também, coitados! devem\n",
      "amargar muito!'\n",
      "Tokens gerados: ['tambem', ',', 'coitados', 'devemamargar', 'muito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A sua voz tinha já completa solicitude de amor materno'\n",
      "Tokens gerados: ['a', 'sua', 'voz', 'tinha', 'ja', 'completa', 'solicitude', 'de', 'amor', 'materno']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cônego levantou-se e foi encostar-se ao parapeito da varanda, enquanto Ana Rosa, que\n",
      "continuava a cortar as unhas do menino, ia em segredo perguntando a este se não tinha saudades da sua\n",
      "terra e se não chorava ao lembrar-se da mãe'\n",
      "Tokens gerados: ['o', 'conego', 'levantou-se', 'e', 'foi', 'encostar-se', 'ao', 'parapeito', 'da', 'varanda', ',', 'enquanto', 'ana', 'rosa', ',', 'quecontinuava', 'a', 'cortar', 'as', 'unhas', 'do', 'menino', ',', 'ia', 'em', 'segredo', 'perguntando', 'a', 'este', 'se', 'nao', 'tinha', 'saudades', 'da', 'suaterra', 'e', 'se', 'nao', 'chorava', 'ao', 'lembrar-se', 'da', 'mae']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuelzinho estava pasmado'\n",
      "Tokens gerados: ['manuelzinho', 'estava', 'pasmado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era a primeira vez que no Brasil lhe falavam com aquela ternura'\n",
      "Tokens gerados: ['era', 'a', 'primeira', 'vez', 'que', 'no', 'brasil', 'lhe', 'falavam', 'com', 'aquela', 'ternura']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Levantou a cabeça e encarou Ana Rosa; ele, que tinha sempre o olhar baixo e terrestre, procurou, sem\n",
      "vacilar, os olhos da rapariga e fitou-os, cheio de confiança, sentindo por ela um súbito respeito, uma\n",
      "espécie de adoração inesperada'\n",
      "Tokens gerados: ['levantou', 'a', 'cabeca', 'e', 'encarou', 'ana', 'rosa', 'ele', ',', 'que', 'tinha', 'sempre', 'o', 'olhar', 'baixo', 'e', 'terrestre', ',', 'procurou', ',', 'semvacilar', ',', 'os', 'olhos', 'da', 'rapariga', 'e', 'fitou-os', ',', 'cheio', 'de', 'confianca', ',', 'sentindo', 'por', 'ela', 'um', 'subito', 'respeito', ',', 'umaespecie', 'de', 'adoracao', 'inesperada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Afigurava-se extraordinário ao pobrezito desprezado de todos, que\n",
      "aquela senhora brasileira, tão limpa, tão bem vestida, tão perfumada e com as mãos tão macias, estivesse\n",
      "ali a cortar-lhe e assear-lhe as unhas'\n",
      "Tokens gerados: ['afigurava-se', 'extraordinario', 'ao', 'pobrezito', 'desprezado', 'de', 'todos', ',', 'queaquela', 'senhora', 'brasileira', ',', 'tao', 'limpa', ',', 'tao', 'bem', 'vestida', ',', 'tao', 'perfumada', 'e', 'com', 'as', 'maos', 'tao', 'macias', ',', 'estivesseali', 'a', 'cortar-lhe', 'e', 'assear-lhe', 'as', 'unhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A princípio foi isto para ele um sacrifício horrível, um suplício insuportável'\n",
      "Tokens gerados: ['a', 'principio', 'foi', 'isto', 'para', 'ele', 'um', 'sacrificio', 'horrivel', ',', 'um', 'suplicio', 'insuportavel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desejava, de si\n",
      "para si, ver terminada aquela cena incômoda; queria fugir daquela posição difícil; resfolegava, sem\n",
      "ousar mexer com a cabeça, olhando para os lados, de esguelha, como a procura de uma saída, de algum\n",
      "lugar onde se escondesse ou de qualquer pretexto que o arrancasse dali'\n",
      "Tokens gerados: ['desejava', ',', 'de', 'sipara', 'si', ',', 'ver', 'terminada', 'aquela', 'cena', 'incomoda', 'queria', 'fugir', 'daquela', 'posicao', 'dificil', 'resfolegava', ',', 'semousar', 'mexer', 'com', 'a', 'cabeca', ',', 'olhando', 'para', 'os', 'lados', ',', 'de', 'esguelha', ',', 'como', 'a', 'procura', 'de', 'uma', 'saida', ',', 'de', 'algumlugar', 'onde', 'se', 'escondesse', 'ou', 'de', 'qualquer', 'pretexto', 'que', 'o', 'arrancasse', 'dali']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sentia-se mal com aquilo, que dúvida! Não se animava a respirar livremente, receoso de fazer\n",
      "notar o seu hálito pela senhora; já lhe doíam as juntas do corpo, tal era a sua imobilidade contrafeita;\n",
      "não mexia sequer com um dedo'\n",
      "Tokens gerados: ['sentia-se', 'mal', 'com', 'aquilo', ',', 'que', 'duvida', 'nao', 'se', 'animava', 'a', 'respirar', 'livremente', ',', 'receoso', 'de', 'fazernotar', 'o', 'seu', 'halito', 'pela', 'senhora', 'ja', 'lhe', 'doiam', 'as', 'juntas', 'do', 'corpo', ',', 'tal', 'era', 'a', 'sua', 'imobilidade', 'contrafeitanao', 'mexia', 'sequer', 'com', 'um', 'dedo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois do primeiro minuto de sacrifício, o suor começou logo a\n",
      "correr-lhe em bagas da cabeça pela gola do jaquetão, e o pequeno teve verdadeiros calafrios; mas\n",
      "quando Ana Rosa lhe falou da pátria e da mãe, com aquela penetrante meiguice que só as próprias mães\n",
      "sabem fazer, as lágrimas rebentaram-lhe dos olhos e desceram-lhe em silêncio pela cara'\n",
      "Tokens gerados: ['depois', 'do', 'primeiro', 'minuto', 'de', 'sacrificio', ',', 'o', 'suor', 'comecou', 'logo', 'acorrer-lhe', 'em', 'bagas', 'da', 'cabeca', 'pela', 'gola', 'do', 'jaquetao', ',', 'e', 'o', 'pequeno', 'teve', 'verdadeiros', 'calafrios', 'masquando', 'ana', 'rosa', 'lhe', 'falou', 'da', 'patria', 'e', 'da', 'mae', ',', 'com', 'aquela', 'penetrante', 'meiguice', 'que', 'so', 'as', 'proprias', 'maessabem', 'fazer', ',', 'as', 'lagrimas', 'rebentaram-lhe', 'dos', 'olhos', 'e', 'desceram-lhe', 'em', 'silencio', 'pela', 'cara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois se era a primeira vez que no Brasil lhe falavam dessas coisas!'\n",
      "Tokens gerados: ['pois', 'se', 'era', 'a', 'primeira', 'vez', 'que', 'no', 'brasil', 'lhe', 'falavam', 'dessas', 'coisas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cônego assistia a tudo isto, calado, rufando sobre a sua tabaqueira de ouro as unhas burnidas\n",
      "a cinza de charuto e a sorrir como um bom velho'\n",
      "Tokens gerados: ['o', 'conego', 'assistia', 'a', 'tudo', 'isto', ',', 'calado', ',', 'rufando', 'sobre', 'a', 'sua', 'tabaqueira', 'de', 'ouro', 'as', 'unhas', 'burnidasa', 'cinza', 'de', 'charuto', 'e', 'a', 'sorrir', 'como', 'um', 'bom', 'velho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, enquanto Ana Rosa, de cabeça baixa, toda desvelos,\n",
      "tratava do desgraçadinho, provocando-lhe as lágrimas e contendo as próprias, sabe Deus como! passava\n",
      "o Dias pelo fundo da varanda, sem ser sentido, o andar de gato, levando no coração uma grande raiva,\n",
      "só pelo fato de ver a filha do patrão acarinhando o outro'\n",
      "Tokens gerados: ['e', ',', 'enquanto', 'ana', 'rosa', ',', 'de', 'cabeca', 'baixa', ',', 'toda', 'desvelos', ',', 'tratava', 'do', 'desgracadinho', ',', 'provocando-lhe', 'as', 'lagrimas', 'e', 'contendo', 'as', 'proprias', ',', 'sabe', 'deus', 'como', 'passavao', 'dias', 'pelo', 'fundo', 'da', 'varanda', ',', 'sem', 'ser', 'sentido', ',', 'o', 'andar', 'de', 'gato', ',', 'levando', 'no', 'coracao', 'uma', 'grande', 'raiva', ',', 'so', 'pelo', 'fato', 'de', 'ver', 'a', 'filha', 'do', 'patrao', 'acarinhando', 'o', 'outro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ralava-o aquela caridade'\n",
      "Tokens gerados: ['ralava-o', 'aquela', 'caridade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Ele nunca tivera quem lhe cortasse as unhas!'\n",
      "Tokens gerados: ['“', 'ele', 'nunca', 'tivera', 'quem', 'lhe', 'cortasse', 'as', 'unhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” Amofinava-o ver a\n",
      "Sra'\n",
      "Tokens gerados: ['”', 'amofinava-o', 'ver', 'asra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'D'\n",
      "Tokens gerados: ['d']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ana Rosa às voltas com semelhante bisca'\n",
      "Tokens gerados: ['ana', 'rosa', 'as', 'voltas', 'com', 'semelhante', 'bisca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Punha a perder de todo a peste do pequeno! — Ora\n",
      "para que lhe havia de dar!'\n",
      "Tokens gerados: ['“', 'punha', 'a', 'perder', 'de', 'todo', 'a', 'peste', 'do', 'pequeno', '—', 'orapara', 'que', 'lhe', 'havia', 'de', 'dar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'embonecar o súcio! Queria-o com certeza para seu chichisbéu! Contava já\n",
      "com ele para levar-lhe as cartas do desaforo e trazer-lhe os presentinhos de flores e os recados dos\n",
      "pelintras!'\n",
      "Tokens gerados: ['embonecar', 'o', 'sucio', 'queria-o', 'com', 'certeza', 'para', 'seu', 'chichisbeu', 'contava', 'jacom', 'ele', 'para', 'levar-lhe', 'as', 'cartas', 'do', 'desaforo', 'e', 'trazer-lhe', 'os', 'presentinhos', 'de', 'flores', 'e', 'os', 'recados', 'dospelintras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah! mas ele, o Dias, ali estava para lhes cortar as vazas!”\n",
      "O Dias, que completava o pessoal da casa de Manuel Pescada, era um tipo fechado como um\n",
      "ovo, um ovo choco que mal denuncia na casca a podridão interior'\n",
      "Tokens gerados: ['ah', 'mas', 'ele', ',', 'o', 'dias', ',', 'ali', 'estava', 'para', 'lhes', 'cortar', 'as', 'vazas', '”', 'o', 'dias', ',', 'que', 'completava', 'o', 'pessoal', 'da', 'casa', 'de', 'manuel', 'pescada', ',', 'era', 'um', 'tipo', 'fechado', 'como', 'umovo', ',', 'um', 'ovo', 'choco', 'que', 'mal', 'denuncia', 'na', 'casca', 'a', 'podridao', 'interior']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Todavia, nas cores biliosas do rosto,\n",
      "no desprezo do próprio corpo, na taciturnidade paciente daquela exagerada economia, adivinhava-se-lhe\n",
      "uma idéia fixa, um alvo, para o qual caminhava o acrobata, sem olhar dos lados, preocupado, nem que\n",
      "se equilibrasse sobre um corda tesa'\n",
      "Tokens gerados: ['todavia', ',', 'nas', 'cores', 'biliosas', 'do', 'rosto', ',', 'no', 'desprezo', 'do', 'proprio', 'corpo', ',', 'na', 'taciturnidade', 'paciente', 'daquela', 'exagerada', 'economia', ',', 'adivinhava-se-lheuma', 'ideia', 'fixa', ',', 'um', 'alvo', ',', 'para', 'o', 'qual', 'caminhava', 'o', 'acrobata', ',', 'sem', 'olhar', 'dos', 'lados', ',', 'preocupado', ',', 'nem', 'quese', 'equilibrasse', 'sobre', 'um', 'corda', 'tesa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não desdenhava qualquer meio para chegar mais depressa aos fins;\n",
      "aceitava, sem examinar, qualquer caminho, desde que lhe parecesse mais curto; tudo servia, tudo era\n",
      "bom, contanto que o levasse mais rapidamente ao ponto desejado'\n",
      "Tokens gerados: ['nao', 'desdenhava', 'qualquer', 'meio', 'para', 'chegar', 'mais', 'depressa', 'aos', 'finsaceitava', ',', 'sem', 'examinar', ',', 'qualquer', 'caminho', ',', 'desde', 'que', 'lhe', 'parecesse', 'mais', 'curto', 'tudo', 'servia', ',', 'tudo', 'erabom', ',', 'contanto', 'que', 'o', 'levasse', 'mais', 'rapidamente', 'ao', 'ponto', 'desejado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lama ou brasa — havia de passar\n",
      "por cima; havia de chegar ao alvo — enriquecer'\n",
      "Tokens gerados: ['lama', 'ou', 'brasa', '—', 'havia', 'de', 'passarpor', 'cima', 'havia', 'de', 'chegar', 'ao', 'alvo', '—', 'enriquecer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quanto à figura, repugnante: magro e macilento, um tanto baixo, um tanto curvado, pouca\n",
      "barba, testa curta e olhos fundos'\n",
      "Tokens gerados: ['quanto', 'a', 'figura', ',', 'repugnante', 'magro', 'e', 'macilento', ',', 'um', 'tanto', 'baixo', ',', 'um', 'tanto', 'curvado', ',', 'poucabarba', ',', 'testa', 'curta', 'e', 'olhos', 'fundos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O uso constante dos chinelos de trança fizera-lhe os pés monstruosos\n",
      "e chatos; quando ele andava, lançava-os desairosamente para os lados, como o movimento dos palmípedes\n",
      "nadando'\n",
      "Tokens gerados: ['o', 'uso', 'constante', 'dos', 'chinelos', 'de', 'tranca', 'fizera-lhe', 'os', 'pes', 'monstruosose', 'chatos', 'quando', 'ele', 'andava', ',', 'lancava-os', 'desairosamente', 'para', 'os', 'lados', ',', 'como', 'o', 'movimento', 'dos', 'palmipedesnadando']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aborrecia-o o charuto, o passeio, o teatro e as reuniões em que fosse necessário despender\n",
      "alguma coisa; quando estava perto da gente sentia-se logo um cheiro azedo de roupas sujas'\n",
      "Tokens gerados: ['aborrecia-o', 'o', 'charuto', ',', 'o', 'passeio', ',', 'o', 'teatro', 'e', 'as', 'reunioes', 'em', 'que', 'fosse', 'necessario', 'despenderalguma', 'coisa', 'quando', 'estava', 'perto', 'da', 'gente', 'sentia-se', 'logo', 'um', 'cheiro', 'azedo', 'de', 'roupas', 'sujas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ana Rosa não podia conceber como uma mulher de certa ordem pudesse suportar semelhante\n",
      "porco'\n",
      "Tokens gerados: ['ana', 'rosa', 'nao', 'podia', 'conceber', 'como', 'uma', 'mulher', 'de', 'certa', 'ordem', 'pudesse', 'suportar', 'semelhanteporco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Enfim, resumia ela, quando, conversando com amigas, queria dar-lhes uma idéia justa do que\n",
      "era o Dias — sempre há um homem que não tem coragem de comprar uma escova de dentes!” As\n",
      "amigas respondiam “Iche!” mas em geral tinham-no na conta de moço benfazejo e de conduta exemplar'\n",
      "Tokens gerados: ['“', 'enfim', ',', 'resumia', 'ela', ',', 'quando', ',', 'conversando', 'com', 'amigas', ',', 'queria', 'dar-lhes', 'uma', 'ideia', 'justa', 'do', 'queera', 'o', 'dias', '—', 'sempre', 'ha', 'um', 'homem', 'que', 'nao', 'tem', 'coragem', 'de', 'comprar', 'uma', 'escova', 'de', 'dentes', '”', 'asamigas', 'respondiam', '“', 'iche', '”', 'mas', 'em', 'geral', 'tinham-no', 'na', 'conta', 'de', 'moco', 'benfazejo', 'e', 'de', 'conduta', 'exemplar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'À noite só deixava a porta do patrão nos sábados, para ir ao peixe frito em casa de uma mulata\n",
      "gorda, que morava com duas filhas lá para os confins da Rua das Crioulas'\n",
      "Tokens gerados: ['a', 'noite', 'so', 'deixava', 'a', 'porta', 'do', 'patrao', 'nos', 'sabados', ',', 'para', 'ir', 'ao', 'peixe', 'frito', 'em', 'casa', 'de', 'uma', 'mulatagorda', ',', 'que', 'morava', 'com', 'duas', 'filhas', 'la', 'para', 'os', 'confins', 'da', 'rua', 'das', 'crioulas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ia sempre sozinho'\n",
      "Tokens gerados: ['ia', 'sempre', 'sozinho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Nada de\n",
      "troças!”\n",
      "— Não tenho amigos'\n",
      "Tokens gerados: ['“', 'nada', 'detrocas', '”', '—', 'nao', 'tenho', 'amigos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'dizia ele constantemente, tenho apenas alguns conhecidos'\n",
      "Tokens gerados: ['dizia', 'ele', 'constantemente', ',', 'tenho', 'apenas', 'alguns', 'conhecidos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nesses passeios levava às vezes uma garrafa de vinho do Porto ou uma lata de marmelada, e\n",
      "chamava a isso “fazer as suas extravagâncias”'\n",
      "Tokens gerados: ['nesses', 'passeios', 'levava', 'as', 'vezes', 'uma', 'garrafa', 'de', 'vinho', 'do', 'porto', 'ou', 'uma', 'lata', 'de', 'marmelada', ',', 'echamava', 'a', 'isso', '“', 'fazer', 'as', 'suas', 'extravagancias', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mulata votava-lhe grande admiração e punha nele\n",
      "muita confiança: dava-lhe a guardar “os seus ouros” e as suas economias'\n",
      "Tokens gerados: ['a', 'mulata', 'votava-lhe', 'grande', 'admiracao', 'e', 'punha', 'nelemuita', 'confianca', 'dava-lhe', 'a', 'guardar', '“', 'os', 'seus', 'ouros', '”', 'e', 'as', 'suas', 'economias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Além desta, ninguém lhe\n",
      "conhecia outra relação particular; uma bela manhã, porém, o “exemplar moço” aparecera incomodado\n",
      "e pedira ao patrão que lhe deixasse ficar aquele dia no quarto'\n",
      "Tokens gerados: ['alem', 'desta', ',', 'ninguem', 'lheconhecia', 'outra', 'relacao', 'particular', 'uma', 'bela', 'manha', ',', 'porem', ',', 'o', '“', 'exemplar', 'moco', '”', 'aparecera', 'incomodadoe', 'pedira', 'ao', 'patrao', 'que', 'lhe', 'deixasse', 'ficar', 'aquele', 'dia', 'no', 'quarto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel, todo solícito pelo seu bom\n",
      "empregado, mandou-lhe lá o médico'\n",
      "Tokens gerados: ['manuel', ',', 'todo', 'solicito', 'pelo', 'seu', 'bomempregado', ',', 'mandou-lhe', 'la', 'o', 'medico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então, que tinha o rapaz?\n",
      "— Aquilo é mais porcaria que outra coisa, respondeu o facultativo, franzindo o nariz; mas\n",
      "receitou, recomendando banhos mornos'\n",
      "Tokens gerados: ['—', 'entao', ',', 'que', 'tinha', 'o', 'rapaz', '?', '—', 'aquilo', 'e', 'mais', 'porcaria', 'que', 'outra', 'coisa', ',', 'respondeu', 'o', 'facultativo', ',', 'franzindo', 'o', 'nariz', 'masreceitou', ',', 'recomendando', 'banhos', 'mornos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Banhos! de banhos principalmente é que ele precisava!”\n",
      "E, quando viu o doente pela segunda vez, não se pôde ter, que lhe não dissesse:\n",
      "— Olhe lá, meu amigo, que o asseio também faz parte do tratamento!\n",
      "E acabou provando que a limpeza não era menos necessária ao corpo do que a alimentação,\n",
      "principalmente em um clima daqueles em que um homem está sempre a transpirar'\n",
      "Tokens gerados: ['“', 'banhos', 'de', 'banhos', 'principalmente', 'e', 'que', 'ele', 'precisava', '”', 'e', ',', 'quando', 'viu', 'o', 'doente', 'pela', 'segunda', 'vez', ',', 'nao', 'se', 'pode', 'ter', ',', 'que', 'lhe', 'nao', 'dissesse—', 'olhe', 'la', ',', 'meu', 'amigo', ',', 'que', 'o', 'asseio', 'tambem', 'faz', 'parte', 'do', 'tratamentoe', 'acabou', 'provando', 'que', 'a', 'limpeza', 'nao', 'era', 'menos', 'necessaria', 'ao', 'corpo', 'do', 'que', 'a', 'alimentacao', ',', 'principalmente', 'em', 'um', 'clima', 'daqueles', 'em', 'que', 'um', 'homem', 'esta', 'sempre', 'a', 'transpirar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel foi à noite ao quarto do caixeiro'\n",
      "Tokens gerados: ['manuel', 'foi', 'a', 'noite', 'ao', 'quarto', 'do', 'caixeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Falou-lhe com brandura paternal; lamentou-o com\n",
      "palavras amigáveis, e desatou um protesto, em forma de sermão, contra o clima e os costumes do\n",
      "Brasil'\n",
      "Tokens gerados: ['falou-lhe', 'com', 'brandura', 'paternal', 'lamentou-o', 'compalavras', 'amigaveis', ',', 'e', 'desatou', 'um', 'protesto', ',', 'em', 'forma', 'de', 'sermao', ',', 'contra', 'o', 'clima', 'e', 'os', 'costumes', 'dobrasil']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Uma terrinha com que é preciso cuidado! Perigosa! Perigosa! dizia ele'\n",
      "Tokens gerados: ['—', 'uma', 'terrinha', 'com', 'que', 'e', 'preciso', 'cuidado', 'perigosa', 'perigosa', 'dizia', 'ele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aqui a gente tem a\n",
      "vida por um fio de cabelo!\n",
      "Tratou depois, com entusiasmo, de Portugal; lembrou as boas comezainas portuguesas: “As\n",
      "caldeiradas d’eirozes, a orelheira de porco com feijão branco, a açorda, o caldo gordo, o famoso bacalhau\n",
      "do Algarve!”\n",
      "— Ai! o pescado! suspirou o Dias, saudoso pela terra'\n",
      "Tokens gerados: ['aqui', 'a', 'gente', 'tem', 'avida', 'por', 'um', 'fio', 'de', 'cabelotratou', 'depois', ',', 'com', 'entusiasmo', ',', 'de', 'portugal', 'lembrou', 'as', 'boas', 'comezainas', 'portuguesas', '“', 'ascaldeiradas', 'd', '’', 'eirozes', ',', 'a', 'orelheira', 'de', 'porco', 'com', 'feijao', 'branco', ',', 'a', 'acorda', ',', 'o', 'caldo', 'gordo', ',', 'o', 'famoso', 'bacalhaudo', 'algarve', '”', '—', 'ai', 'o', 'pescado', 'suspirou', 'o', 'dias', ',', 'saudoso', 'pela', 'terra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que rico pitéu!\n",
      "— E os nossos figos de comadre, e as nossas castanhas assadas, e o vinho verde?\n",
      "Dias escutava com água na boca'\n",
      "Tokens gerados: ['que', 'rico', 'piteu—', 'e', 'os', 'nossos', 'figos', 'de', 'comadre', ',', 'e', 'as', 'nossas', 'castanhas', 'assadas', ',', 'e', 'o', 'vinho', 'verde', '?', 'dias', 'escutava', 'com', 'agua', 'na', 'boca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ai! a terra!'\n",
      "Tokens gerados: ['—', 'ai', 'a', 'terra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O patrão falou-lhe também das comodidades, dos ares, das frutas e por fim dos divertimentos de\n",
      "Lisboa, terminando por contar fatos de moléstia; casos idênticos ao do Dias; transportou-se rindo ao\n",
      "seu tempo de rapaz, e, já de pé, pronto para sair, bateu-lhe no ombro, carinhosamente:\n",
      "— Você, homem, o que devia era casar!'\n",
      "Tokens gerados: ['o', 'patrao', 'falou-lhe', 'tambem', 'das', 'comodidades', ',', 'dos', 'ares', ',', 'das', 'frutas', 'e', 'por', 'fim', 'dos', 'divertimentos', 'delisboa', ',', 'terminando', 'por', 'contar', 'fatos', 'de', 'molestia', 'casos', 'identicos', 'ao', 'do', 'dias', 'transportou-se', 'rindo', 'aoseu', 'tempo', 'de', 'rapaz', ',', 'e', ',', 'ja', 'de', 'pe', ',', 'pronto', 'para', 'sair', ',', 'bateu-lhe', 'no', 'ombro', ',', 'carinhosamente—', 'voce', ',', 'homem', ',', 'o', 'que', 'devia', 'era', 'casar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E jurou-lhe que o casamento lhe estava mesmo calhando'\n",
      "Tokens gerados: ['e', 'jurou-lhe', 'que', 'o', 'casamento', 'lhe', 'estava', 'mesmo', 'calhando']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“O Dias, com aquele gênio e com\n",
      "aquele método, dava por força um bom marido!'\n",
      "Tokens gerados: ['“', 'o', 'dias', ',', 'com', 'aquele', 'genio', 'e', 'comaquele', 'metodo', ',', 'dava', 'por', 'forca', 'um', 'bom', 'marido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que se casasse, e havia de ver se não teria outra\n",
      "importância!'\n",
      "Tokens gerados: ['que', 'se', 'casasse', ',', 'e', 'havia', 'de', 'ver', 'se', 'nao', 'teria', 'outraimportancia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "— Olhe! concluiu, digo-lhe agora como o doutor “Banhos! banhos, meu amigo” mas que sejam\n",
      "de igreja, compreende?\n",
      "E, rindo com a própria pilhéria e todo cheio de sorrisos de boa intenção, saiu do quarto na ponta\n",
      "dos pés, cautelosamente, para que os outros caixeiros, a quem ele não dava a honra de uma visita\n",
      "daquelas, não lhe ouvissem as pisadas'\n",
      "Tokens gerados: ['”', '—', 'olhe', 'concluiu', ',', 'digo-lhe', 'agora', 'como', 'o', 'doutor', '“', 'banhos', 'banhos', ',', 'meu', 'amigo', '”', 'mas', 'que', 'sejamde', 'igreja', ',', 'compreende', '?', 'e', ',', 'rindo', 'com', 'a', 'propria', 'pilheria', 'e', 'todo', 'cheio', 'de', 'sorrisos', 'de', 'boa', 'intencao', ',', 'saiu', 'do', 'quarto', 'na', 'pontados', 'pes', ',', 'cautelosamente', ',', 'para', 'que', 'os', 'outros', 'caixeiros', ',', 'a', 'quem', 'ele', 'nao', 'dava', 'a', 'honra', 'de', 'uma', 'visitadaquelas', ',', 'nao', 'lhe', 'ouvissem', 'as', 'pisadas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando Ana Rosa acabou de cortar as unhas de Manuelzinho deu-lhe de conselho que estudasse\n",
      "alguma coisa; prometeu que arranjaria com o pai metê-lo em uma aula noturna de primeiras letras, e\n",
      "recomendou-lhe que todos os dias de manhã tomasse o seu banho debaixo da bomba do poço'\n",
      "Tokens gerados: ['quando', 'ana', 'rosa', 'acabou', 'de', 'cortar', 'as', 'unhas', 'de', 'manuelzinho', 'deu-lhe', 'de', 'conselho', 'que', 'estudassealguma', 'coisa', 'prometeu', 'que', 'arranjaria', 'com', 'o', 'pai', 'mete-lo', 'em', 'uma', 'aula', 'noturna', 'de', 'primeiras', 'letras', ',', 'erecomendou-lhe', 'que', 'todos', 'os', 'dias', 'de', 'manha', 'tomasse', 'o', 'seu', 'banho', 'debaixo', 'da', 'bomba', 'do', 'poco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Faça isso, que serei por você, rematou a moça, afastando-o com uma ligeira palmada na\n",
      "cabeça'\n",
      "Tokens gerados: ['—', 'faca', 'isso', ',', 'que', 'serei', 'por', 'voce', ',', 'rematou', 'a', 'moca', ',', 'afastando-o', 'com', 'uma', 'ligeira', 'palmada', 'nacabeca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O menino retirou-se, muito comovido, para o andar de cima, mas o Dias, de pé, no tope da\n",
      "escada, esperava por ele, furioso'\n",
      "Tokens gerados: ['o', 'menino', 'retirou-se', ',', 'muito', 'comovido', ',', 'para', 'o', 'andar', 'de', 'cima', ',', 'mas', 'o', 'dias', ',', 'de', 'pe', ',', 'no', 'tope', 'daescada', ',', 'esperava', 'por', 'ele', ',', 'furioso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que estava fazendo, seu traste?\n",
      "— Nada, respondeu a criança, a tremer'\n",
      "Tokens gerados: ['—', 'que', 'estava', 'fazendo', ',', 'seu', 'traste', '?', '—', 'nada', ',', 'respondeu', 'a', 'crianca', ',', 'a', 'tremer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fora a senhora que o chamara!'\n",
      "Tokens gerados: ['fora', 'a', 'senhora', 'que', 'o', 'chamara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dias, com um murro, explicou que o maroto não podia pôr-se de palestra na varanda, em vez de\n",
      "cuidar das obrigações'\n",
      "Tokens gerados: ['dias', ',', 'com', 'um', 'murro', ',', 'explicou', 'que', 'o', 'maroto', 'nao', 'podia', 'por-se', 'de', 'palestra', 'na', 'varanda', ',', 'em', 'vez', 'decuidar', 'das', 'obrigacoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— E se me constar, acrescentou, cada vez mais zangado, que você me torna a ir com lamúrias\n",
      "para o lado de D'\n",
      "Tokens gerados: ['—', 'e', 'se', 'me', 'constar', ',', 'acrescentou', ',', 'cada', 'vez', 'mais', 'zangado', ',', 'que', 'voce', 'me', 'torna', 'a', 'ir', 'com', 'lamuriaspara', 'o', 'lado', 'de', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Anica, comigo se tem de haver, seu mariola! Vai tudo aos ouvidos do patrão!\n",
      "Manuelzinho arredou-se dali, convencido de que havia praticado uma tremenda falta; no íntimo,\n",
      "porém, ia muito satisfeito com a idéia de que já não estava tão desamparado, e sentindo renascer-lhe, na\n",
      "obscura mágoa do seu desterro, um desejo alegre de continuar a viver'\n",
      "Tokens gerados: ['anica', ',', 'comigo', 'se', 'tem', 'de', 'haver', ',', 'seu', 'mariola', 'vai', 'tudo', 'aos', 'ouvidos', 'do', 'patraomanuelzinho', 'arredou-se', 'dali', ',', 'convencido', 'de', 'que', 'havia', 'praticado', 'uma', 'tremenda', 'falta', 'no', 'intimo', ',', 'porem', ',', 'ia', 'muito', 'satisfeito', 'com', 'a', 'ideia', 'de', 'que', 'ja', 'nao', 'estava', 'tao', 'desamparado', ',', 'e', 'sentindo', 'renascer-lhe', ',', 'naobscura', 'magoa', 'do', 'seu', 'desterro', ',', 'um', 'desejo', 'alegre', 'de', 'continuar', 'a', 'viver']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A reunião em casa do Freitas esteve animada'\n",
      "Tokens gerados: ['a', 'reuniao', 'em', 'casa', 'do', 'freitas', 'esteve', 'animada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Houve violão, cantoria, muita dança'\n",
      "Tokens gerados: ['houve', 'violao', ',', 'cantoria', ',', 'muita', 'danca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Chegaram a\n",
      "deitar chorado da Bahia'\n",
      "Tokens gerados: ['chegaram', 'adeitar', 'chorado', 'da', 'bahia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, pela volta da meia-noite, Ana Rosa, depois de uma valsa, fora acometida de um ataque de\n",
      "nervos'\n",
      "Tokens gerados: ['mas', ',', 'pela', 'volta', 'da', 'meia-noite', ',', 'ana', 'rosa', ',', 'depois', 'de', 'uma', 'valsa', ',', 'fora', 'acometida', 'de', 'um', 'ataque', 'denervos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era o terceiro que lhe dava assim, sem mais nem menos'\n",
      "Tokens gerados: ['era', 'o', 'terceiro', 'que', 'lhe', 'dava', 'assim', ',', 'sem', 'mais', 'nem', 'menos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Felizmente o médico, chamado a toda a pressa afiançou que aquilo não valia nada'\n",
      "Tokens gerados: ['felizmente', 'o', 'medico', ',', 'chamado', 'a', 'toda', 'a', 'pressa', 'afiancou', 'que', 'aquilo', 'nao', 'valia', 'nada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Distrações\n",
      "e bom passadio!” receitou ele, e, ao despedir-se de Manuel, segredou-lhe sorrindo:\n",
      "— Se quiser dar saúde à sua filha, trate de casá-la'\n",
      "Tokens gerados: ['“', 'distracoese', 'bom', 'passadio', '”', 'receitou', 'ele', ',', 'e', ',', 'ao', 'despedir-se', 'de', 'manuel', ',', 'segredou-lhe', 'sorrindo—', 'se', 'quiser', 'dar', 'saude', 'a', 'sua', 'filha', ',', 'trate', 'de', 'casa-la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Mas o que tem ela, doutor?'\n",
      "Tokens gerados: ['—', 'mas', 'o', 'que', 'tem', 'ela', ',', 'doutor', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora o que tem! Tem vinte anos! Está na idade de fazer o ninho! mas, enquanto não chega o\n",
      "casamento, ela que vá dando os seus passeios a pé'\n",
      "Tokens gerados: ['—', 'ora', 'o', 'que', 'tem', 'tem', 'vinte', 'anos', 'esta', 'na', 'idade', 'de', 'fazer', 'o', 'ninho', 'mas', ',', 'enquanto', 'nao', 'chega', 'ocasamento', ',', 'ela', 'que', 'va', 'dando', 'os', 'seus', 'passeios', 'a', 'pe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Banhos frios, exercícios, bom passadio e distrações!\n",
      "Percebe?\n",
      "Manuel, na sua ignorância, imaginou que a filha alimentava ocultamente algum amor mal\n",
      "correspondido'\n",
      "Tokens gerados: ['banhos', 'frios', ',', 'exercicios', ',', 'bom', 'passadio', 'e', 'distracoespercebe', '?', 'manuel', ',', 'na', 'sua', 'ignorancia', ',', 'imaginou', 'que', 'a', 'filha', 'alimentava', 'ocultamente', 'algum', 'amor', 'malcorrespondido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sacudiu os ombros'\n",
      "Tokens gerados: ['sacudiu', 'os', 'ombros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Não era então coisa de cuidado'\n",
      "Tokens gerados: ['“', 'nao', 'era', 'entao', 'coisa', 'de', 'cuidado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” E, em cumprimento as ordens do\n",
      "médico, inaugurou com a enferma longos passeios pela fresca da madrugada'\n",
      "Tokens gerados: ['”', 'e', ',', 'em', 'cumprimento', 'as', 'ordens', 'domedico', ',', 'inaugurou', 'com', 'a', 'enferma', 'longos', 'passeios', 'pela', 'fresca', 'da', 'madrugada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Daí a dias, o cônego Diogo, contra todos os seus hábitos, procurava o compadre às sete horas da\n",
      "manhã'\n",
      "Tokens gerados: ['dai', 'a', 'dias', ',', 'o', 'conego', 'diogo', ',', 'contra', 'todos', 'os', 'seus', 'habitos', ',', 'procurava', 'o', 'compadre', 'as', 'sete', 'horas', 'damanha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Atravessou o armazém, apressado como quem traz grande novidade, e, mal chegou ao negociante,\n",
      "foi lhe dizendo em tom misterioso:\n",
      "— Sabe? Faz sinal de aparecer, e é o Cruzeiro'\n",
      "Tokens gerados: ['atravessou', 'o', 'armazem', ',', 'apressado', 'como', 'quem', 'traz', 'grande', 'novidade', ',', 'e', ',', 'mal', 'chegou', 'ao', 'negociante', ',', 'foi', 'lhe', 'dizendo', 'em', 'tom', 'misterioso—', 'sabe', '?', 'faz', 'sinal', 'de', 'aparecer', ',', 'e', 'e', 'o', 'cruzeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel largou logo de mão o serviço que fazia, subiu à varanda, deu as suas providências para\n",
      "receber um hóspede, e em seguida ganhou a rua com o amigo'\n",
      "Tokens gerados: ['manuel', 'largou', 'logo', 'de', 'mao', 'o', 'servico', 'que', 'fazia', ',', 'subiu', 'a', 'varanda', ',', 'deu', 'as', 'suas', 'providencias', 'parareceber', 'um', 'hospede', ',', 'e', 'em', 'seguida', 'ganhou', 'a', 'rua', 'com', 'o', 'amigo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eles a saírem de casa e a fortaleza de São Marcos a salvar, anunciando com um tiro, a entrada de\n",
      "paquete brasileiro'\n",
      "Tokens gerados: ['eles', 'a', 'sairem', 'de', 'casa', 'e', 'a', 'fortaleza', 'de', 'sao', 'marcos', 'a', 'salvar', ',', 'anunciando', 'com', 'um', 'tiro', ',', 'a', 'entrada', 'depaquete', 'brasileiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os dois tomaram um escaler e foram a bordo'\n",
      "Tokens gerados: ['os', 'dois', 'tomaram', 'um', 'escaler', 'e', 'foram', 'a', 'bordo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_o_mulato_aluisio_azevedo_cap_2.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Daí a pouco, entre as vistas interrogadoras dos curiosos, atravessou a Praça do Comércio um\n",
      "rapaz bem parecido, que ia acompanhado pelo cônego Diogo e por Manuel'\n",
      "Tokens gerados: ['dai', 'a', 'pouco', ',', 'entre', 'as', 'vistas', 'interrogadoras', 'dos', 'curiosos', ',', 'atravessou', 'a', 'praca', 'do', 'comercio', 'umrapaz', 'bem', 'parecido', ',', 'que', 'ia', 'acompanhado', 'pelo', 'conego', 'diogo', 'e', 'por', 'manuel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A novidade foi logo comentada'\n",
      "Tokens gerados: ['a', 'novidade', 'foi', 'logo', 'comentada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os portugueses vinham, com as suas grandes barrigas, às portas dos armazéns de secos\n",
      "e molhados; os barraqueiros espiavam por cima dos óculos de tartaruga; os pretos cangueiros paravam para “mirar o cara-nova”'\n",
      "Tokens gerados: ['os', 'portugueses', 'vinham', ',', 'com', 'as', 'suas', 'grandes', 'barrigas', ',', 'as', 'portas', 'dos', 'armazens', 'de', 'secose', 'molhados', 'os', 'barraqueiros', 'espiavam', 'por', 'cima', 'dos', 'oculos', 'de', 'tartaruga', 'os', 'pretos', 'cangueiros', 'paravam', 'para', '“', 'mirar', 'o', 'cara-nova', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Perua-gorda, em mangas de camisa, como quase todos os outros, acudiu logo à rua:\n",
      "— Quem será esse gajo, ó coisa? perguntou ele ruidosamente a um súcio que passava na ocasião'\n",
      "Tokens gerados: ['o', 'perua-gorda', ',', 'em', 'mangas', 'de', 'camisa', ',', 'como', 'quase', 'todos', 'os', 'outros', ',', 'acudiu', 'logo', 'a', 'rua—', 'quem', 'sera', 'esse', 'gajo', ',', 'o', 'coisa', '?', 'perguntou', 'ele', 'ruidosamente', 'a', 'um', 'sucio', 'que', 'passava', 'na', 'ocasiao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Algum parente ou recomendado do Manuel Pescada'\n",
      "Tokens gerados: ['—', 'algum', 'parente', 'ou', 'recomendado', 'do', 'manuel', 'pescada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Veio do Sul'\n",
      "Tokens gerados: ['veio', 'do', 'sul']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ó aquele! sabes quem é o lanceiro que vai com o Pescada?\n",
      "— Não sei, homem, mas é um rapagão!\n",
      "Manuel apresentou o sobrinho a vários grupos'\n",
      "Tokens gerados: ['—', 'o', 'aquele', 'sabes', 'quem', 'e', 'o', 'lanceiro', 'que', 'vai', 'com', 'o', 'pescada', '?', '—', 'nao', 'sei', ',', 'homem', ',', 'mas', 'e', 'um', 'rapagaomanuel', 'apresentou', 'o', 'sobrinho', 'a', 'varios', 'grupos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Houve sorrisos de delicadezas e grandes apertos\n",
      "de mão'\n",
      "Tokens gerados: ['houve', 'sorrisos', 'de', 'delicadezas', 'e', 'grandes', 'apertosde', 'mao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— É o filho de um mano do Pescada'\n",
      "Tokens gerados: ['—', 'e', 'o', 'filho', 'de', 'um', 'mano', 'do', 'pescada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'diziam depois'\n",
      "Tokens gerados: ['diziam', 'depois']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Conhecemos-lhe muito a vida! Chama-se\n",
      "Raimundo'\n",
      "Tokens gerados: ['conhecemos-lhe', 'muito', 'a', 'vida', 'chama-seraimundo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estava nos estudos'\n",
      "Tokens gerados: ['estava', 'nos', 'estudos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Vem estabelecer-se aqui? indagou o José Buxo'\n",
      "Tokens gerados: ['—', 'vem', 'estabelecer-se', 'aqui', '?', 'indagou', 'o', 'jose', 'buxo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não, creio que vem montar uma companhia'\n",
      "Tokens gerados: ['—', 'nao', ',', 'creio', 'que', 'vem', 'montar', 'uma', 'companhia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Outros afiançavam que Raimundo era sócio capitalista da casa de Manuel'\n",
      "Tokens gerados: ['outros', 'afiancavam', 'que', 'raimundo', 'era', 'socio', 'capitalista', 'da', 'casa', 'de', 'manuel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Discutiam-lhe a\n",
      "roupa, o modo de andar, a cor e os cabelos'\n",
      "Tokens gerados: ['discutiam-lhe', 'aroupa', ',', 'o', 'modo', 'de', 'andar', ',', 'a', 'cor', 'e', 'os', 'cabelos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Luisinho Língua de Prata afirmava que ele “tinha casta”'\n",
      "Tokens gerados: ['o', 'luisinho', 'lingua', 'de', 'prata', 'afirmava', 'que', 'ele', '“', 'tinha', 'casta', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto os três subiam a Rua da Estrela'\n",
      "Tokens gerados: ['entretanto', 'os', 'tres', 'subiam', 'a', 'rua', 'da', 'estrela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Chegados a casa, onde já havia pronto um quarto para o Sr'\n",
      "Tokens gerados: ['chegados', 'a', 'casa', ',', 'onde', 'ja', 'havia', 'pronto', 'um', 'quarto', 'para', 'o', 'sr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dr'\n",
      "Tokens gerados: ['dr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo José da Silva, o\n",
      "cônego e Manuel desfizeram-se em delicadezas com o rapaz'\n",
      "Tokens gerados: ['raimundo', 'jose', 'da', 'silva', ',', 'oconego', 'e', 'manuel', 'desfizeram-se', 'em', 'delicadezas', 'com', 'o', 'rapaz']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Benedito! vê cerveja! Ou prefere conhaque, doutor?'\n",
      "Tokens gerados: ['—', 'benedito', 've', 'cerveja', 'ou', 'prefere', 'conhaque', ',', 'doutor', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Olha moleque, prepara guaraná! Doutor,\n",
      "venha antes para este lado que está mais fresco'\n",
      "Tokens gerados: ['olha', 'moleque', ',', 'prepara', 'guarana', 'doutor', ',', 'venha', 'antes', 'para', 'este', 'lado', 'que', 'esta', 'mais', 'fresco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'não faça cerimônias! Vá entrando! vá entrando para a\n",
      "varanda! O senhor está em sua casa!'\n",
      "Tokens gerados: ['nao', 'faca', 'cerimonias', 'va', 'entrando', 'va', 'entrando', 'para', 'avaranda', 'o', 'senhor', 'esta', 'em', 'sua', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo queixava-se do calor'\n",
      "Tokens gerados: ['raimundo', 'queixava-se', 'do', 'calor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Está horrível! dizia ele, a limpar o rosto com o lenço'\n",
      "Tokens gerados: ['—', 'esta', 'horrivel', 'dizia', 'ele', ',', 'a', 'limpar', 'o', 'rosto', 'com', 'o', 'lenco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nunca suei tanto!\n",
      "— O melhor então é recolher-se um pouco e ficar à vontade'\n",
      "Tokens gerados: ['nunca', 'suei', 'tanto—', 'o', 'melhor', 'entao', 'e', 'recolher-se', 'um', 'pouco', 'e', 'ficar', 'a', 'vontade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pode mudar de roupa, arejar-se'\n",
      "Tokens gerados: ['pode', 'mudar', 'de', 'roupa', ',', 'arejar-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A\n",
      "bagagem não tarda aí'\n",
      "Tokens gerados: ['abagagem', 'nao', 'tarda', 'ai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Olhe, doutor, entre, entre e veja se fica bem aqui!\n",
      "Os três penetraram no quarto destinado ao hóspede'\n",
      "Tokens gerados: ['olhe', ',', 'doutor', ',', 'entre', ',', 'entre', 'e', 'veja', 'se', 'fica', 'bem', 'aquios', 'tres', 'penetraram', 'no', 'quarto', 'destinado', 'ao', 'hospede']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— O senhor, disse Manuel, tem aqui janelas para a rua e para o quintal'\n",
      "Tokens gerados: ['—', 'o', 'senhor', ',', 'disse', 'manuel', ',', 'tem', 'aqui', 'janelas', 'para', 'a', 'rua', 'e', 'para', 'o', 'quintal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ponha-se a gosto'\n",
      "Tokens gerados: ['ponha-se', 'a', 'gosto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se\n",
      "precisar qualquer coisa, é só chamar pelo Benedito'\n",
      "Tokens gerados: ['seprecisar', 'qualquer', 'coisa', ',', 'e', 'so', 'chamar', 'pelo', 'benedito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nada de cerimônias!\n",
      "Raimundo agradeceu muito penhorado'\n",
      "Tokens gerados: ['nada', 'de', 'cerimoniasraimundo', 'agradeceu', 'muito', 'penhorado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Mandei dar-lhe cama, acrescentou o negociante, porque o senhor naturalmente não está\n",
      "afeito à rede, no entanto se quiser'\n",
      "Tokens gerados: ['—', 'mandei', 'dar-lhe', 'cama', ',', 'acrescentou', 'o', 'negociante', ',', 'porque', 'o', 'senhor', 'naturalmente', 'nao', 'estaafeito', 'a', 'rede', ',', 'no', 'entanto', 'se', 'quiser']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não, não, muito obrigado'\n",
      "Tokens gerados: ['—', 'nao', ',', 'nao', ',', 'muito', 'obrigado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Está tudo muito bom'\n",
      "Tokens gerados: ['esta', 'tudo', 'muito', 'bom']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O que desejo é repousar um pouco justamente'\n",
      "Tokens gerados: ['o', 'que', 'desejo', 'e', 'repousar', 'um', 'pouco', 'justamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ainda tenho a cabeça a andar à roda'\n",
      "Tokens gerados: ['ainda', 'tenho', 'a', 'cabeca', 'a', 'andar', 'a', 'roda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pois então descanse, descanse, para depois almoçar com mais apetite… Até logo'\n",
      "Tokens gerados: ['—', 'pois', 'entao', 'descanse', ',', 'descanse', ',', 'para', 'depois', 'almocar', 'com', 'mais', 'apetite', '...', 'ate', 'logo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E Manuel e mais o compadre afastaram-se, cheios de cortesia e sorrisos de afabilidade'\n",
      "Tokens gerados: ['e', 'manuel', 'e', 'mais', 'o', 'compadre', 'afastaram-se', ',', 'cheios', 'de', 'cortesia', 'e', 'sorrisos', 'de', 'afabilidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo tinha vinte e seis anos e seria um tipo acabado de brasileiro se não foram os grandes\n",
      "olhos azuis, que puxara do pai'\n",
      "Tokens gerados: ['raimundo', 'tinha', 'vinte', 'e', 'seis', 'anos', 'e', 'seria', 'um', 'tipo', 'acabado', 'de', 'brasileiro', 'se', 'nao', 'foram', 'os', 'grandesolhos', 'azuis', ',', 'que', 'puxara', 'do', 'pai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cabelos muito pretos, lustrosos e crespos; tez morena e amulatada, mas\n",
      "fina; dentes claros que reluziam sob a negrura do bigode; estatura alta e elegante; pescoço largo, nariz\n",
      "direito e fronte espaçosa'\n",
      "Tokens gerados: ['cabelos', 'muito', 'pretos', ',', 'lustrosos', 'e', 'crespos', 'tez', 'morena', 'e', 'amulatada', ',', 'masfina', 'dentes', 'claros', 'que', 'reluziam', 'sob', 'a', 'negrura', 'do', 'bigode', 'estatura', 'alta', 'e', 'elegante', 'pescoco', 'largo', ',', 'narizdireito', 'e', 'fronte', 'espacosa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A parte mais característica da sua fisionomia era os olhos — grandes,\n",
      "ramalhudos, cheios de sombras azuis; pestanas eriçadas e negras, pálpebras de um roxo vaporoso e\n",
      "úmido; as sobrancelhas, muito desenhadas no rosto, como a nanquim, faziam sobressair a frescura da\n",
      "epiderme, que, no lugar da barba raspada, lembrava os tons suaves e transparentes de uma aquarela\n",
      "sobre papel de arroz'\n",
      "Tokens gerados: ['a', 'parte', 'mais', 'caracteristica', 'da', 'sua', 'fisionomia', 'era', 'os', 'olhos', '—', 'grandes', ',', 'ramalhudos', ',', 'cheios', 'de', 'sombras', 'azuis', 'pestanas', 'ericadas', 'e', 'negras', ',', 'palpebras', 'de', 'um', 'roxo', 'vaporoso', 'eumido', 'as', 'sobrancelhas', ',', 'muito', 'desenhadas', 'no', 'rosto', ',', 'como', 'a', 'nanquim', ',', 'faziam', 'sobressair', 'a', 'frescura', 'daepiderme', ',', 'que', ',', 'no', 'lugar', 'da', 'barba', 'raspada', ',', 'lembrava', 'os', 'tons', 'suaves', 'e', 'transparentes', 'de', 'uma', 'aquarelasobre', 'papel', 'de', 'arroz']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha os gestos bem educados, sóbrios, despidos de pretensão, falava em voz baixa, distintamente\n",
      "sem armar ao efeito; vestia-se com seriedade e bom gosto; amava as artes, as ciências, a literatura e, um\n",
      "pouco menos, a política'\n",
      "Tokens gerados: ['tinha', 'os', 'gestos', 'bem', 'educados', ',', 'sobrios', ',', 'despidos', 'de', 'pretensao', ',', 'falava', 'em', 'voz', 'baixa', ',', 'distintamentesem', 'armar', 'ao', 'efeito', 'vestia-se', 'com', 'seriedade', 'e', 'bom', 'gosto', 'amava', 'as', 'artes', ',', 'as', 'ciencias', ',', 'a', 'literatura', 'e', ',', 'umpouco', 'menos', ',', 'a', 'politica']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em toda a sua vida, sempre longe da pátria, entre povos diversos, cheia de impressões diferentes,\n",
      "tomada de preocupações de estudos, jamais conseguira chegar a uma dedução lógica e satisfatória a\n",
      "respeito da sua procedência'\n",
      "Tokens gerados: ['em', 'toda', 'a', 'sua', 'vida', ',', 'sempre', 'longe', 'da', 'patria', ',', 'entre', 'povos', 'diversos', ',', 'cheia', 'de', 'impressoes', 'diferentes', ',', 'tomada', 'de', 'preocupacoes', 'de', 'estudos', ',', 'jamais', 'conseguira', 'chegar', 'a', 'uma', 'deducao', 'logica', 'e', 'satisfatoria', 'arespeito', 'da', 'sua', 'procedencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não sabia ao certo quais eram as circunstâncias em que viera ao mundo;\n",
      "não sabia a quem devia agradecer a vida e os bens de que dispunha'\n",
      "Tokens gerados: ['nao', 'sabia', 'ao', 'certo', 'quais', 'eram', 'as', 'circunstancias', 'em', 'que', 'viera', 'ao', 'mundonao', 'sabia', 'a', 'quem', 'devia', 'agradecer', 'a', 'vida', 'e', 'os', 'bens', 'de', 'que', 'dispunha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lembrava-se, no entanto, de haver\n",
      "saído em pequeno do Brasil e podia jurar que nunca lhe faltara o necessário e até o supérfluo'\n",
      "Tokens gerados: ['lembrava-se', ',', 'no', 'entanto', ',', 'de', 'haversaido', 'em', 'pequeno', 'do', 'brasil', 'e', 'podia', 'jurar', 'que', 'nunca', 'lhe', 'faltara', 'o', 'necessario', 'e', 'ate', 'o', 'superfluo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em\n",
      "Lisboa tinha ordem franca'\n",
      "Tokens gerados: ['emlisboa', 'tinha', 'ordem', 'franca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas quem vinha a ser essa pessoa encarregada de acompanhá-lo de tão longe?'\n",
      "Tokens gerados: ['mas', 'quem', 'vinha', 'a', 'ser', 'essa', 'pessoa', 'encarregada', 'de', 'acompanha-lo', 'de', 'tao', 'longe', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seu tutor, com\n",
      "certeza, ou coisa que o valha, ou talvez seu próprio tio, pois, quanto ao pai, sabia Raimundo que já o\n",
      "não tinha quando foi para Lisboa'\n",
      "Tokens gerados: ['seu', 'tutor', ',', 'comcerteza', ',', 'ou', 'coisa', 'que', 'o', 'valha', ',', 'ou', 'talvez', 'seu', 'proprio', 'tio', ',', 'pois', ',', 'quanto', 'ao', 'pai', ',', 'sabia', 'raimundo', 'que', 'ja', 'onao', 'tinha', 'quando', 'foi', 'para', 'lisboa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não porque chegasse a conhecê-lo, nem porque se recordasse de ter\n",
      "ouvido de alguém o doce nome de filho, mas sabia-o por intermédio do seu correspondente e pelo que\n",
      "deduzia de algumas vagas reminiscências da meninice'\n",
      "Tokens gerados: ['nao', 'porque', 'chegasse', 'a', 'conhece-lo', ',', 'nem', 'porque', 'se', 'recordasse', 'de', 'terouvido', 'de', 'alguem', 'o', 'doce', 'nome', 'de', 'filho', ',', 'mas', 'sabia-o', 'por', 'intermedio', 'do', 'seu', 'correspondente', 'e', 'pelo', 'quededuzia', 'de', 'algumas', 'vagas', 'reminiscencias', 'da', 'meninice']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Sua mãe, porém, quem seria?'\n",
      "Tokens gerados: ['“', 'sua', 'mae', ',', 'porem', ',', 'quem', 'seria', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” Talvez alguma senhora culpada e receosa de patentear a sua\n",
      "vergonha!'\n",
      "Tokens gerados: ['”', 'talvez', 'alguma', 'senhora', 'culpada', 'e', 'receosa', 'de', 'patentear', 'a', 'suavergonha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Seria boa? Seria virtuosa?'\n",
      "Tokens gerados: ['“', 'seria', 'boa', '?', 'seria', 'virtuosa', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Raimundo perdia-se em conjeturas e, malgrado o seu desprendimento pelo passado, sentia alguma\n",
      "coisa atraí-lo irresistivelmente para a pátria'\n",
      "Tokens gerados: ['”', 'raimundo', 'perdia-se', 'em', 'conjeturas', 'e', ',', 'malgrado', 'o', 'seu', 'desprendimento', 'pelo', 'passado', ',', 'sentia', 'algumacoisa', 'atrai-lo', 'irresistivelmente', 'para', 'a', 'patria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Quem sabia se aí não descobriria a ponta do enigma?'\n",
      "Tokens gerados: ['“', 'quem', 'sabia', 'se', 'ai', 'nao', 'descobriria', 'a', 'ponta', 'do', 'enigma', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ele, que sempre vivera órfão de afeições legítimas e duradouras, como então seria feliz!'\n",
      "Tokens gerados: ['ele', ',', 'que', 'sempre', 'vivera', 'orfao', 'de', 'afeicoes', 'legitimas', 'e', 'duradouras', ',', 'como', 'entao', 'seria', 'feliz']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah, se chegasse\n",
      "a saber quem era sua mãe, perdoar-lhe-ia tudo, tudo!”\n",
      "O quinhão de ternura, que a ela pertencia, estava intacto no coração do filho'\n",
      "Tokens gerados: ['ah', ',', 'se', 'chegassea', 'saber', 'quem', 'era', 'sua', 'mae', ',', 'perdoar-lhe-ia', 'tudo', ',', 'tudo', '”', 'o', 'quinhao', 'de', 'ternura', ',', 'que', 'a', 'ela', 'pertencia', ',', 'estava', 'intacto', 'no', 'coracao', 'do', 'filho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era preciso entregá-lo\n",
      "a alguém! Era preciso desvendar as circunstâncias que determinaram o seu nascimento!\n",
      "“Mas, no fim de contas, refletia Raimundo, em um retrocesso natural de impressões, que diabo\n",
      "tinha ele com tudo isso, se até aí, na ignorância desses fatos, vivera estimado e feliz!'\n",
      "Tokens gerados: ['era', 'preciso', 'entrega-loa', 'alguem', 'era', 'preciso', 'desvendar', 'as', 'circunstancias', 'que', 'determinaram', 'o', 'seu', 'nascimento', '“', 'mas', ',', 'no', 'fim', 'de', 'contas', ',', 'refletia', 'raimundo', ',', 'em', 'um', 'retrocesso', 'natural', 'de', 'impressoes', ',', 'que', 'diabotinha', 'ele', 'com', 'tudo', 'isso', ',', 'se', 'ate', 'ai', ',', 'na', 'ignorancia', 'desses', 'fatos', ',', 'vivera', 'estimado', 'e', 'feliz']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não foi decerto\n",
      "para semelhante coisa que viera à província! Por conseguinte, era liquidar os seus negócios, vender os\n",
      "seus bens e — por aqui é o caminho! O Rio de Janeiro lá estava a sua espera!\n",
      "“Abriria, ao chegar lá, o seu escritório, trabalharia, e, ao lado da mulher com quem casasse e dos\n",
      "filhos que viesse a ter, nem sequer havia de lembrar-se do passado!\n",
      "“Sim, que mais poderia desejar melhor?'\n",
      "Tokens gerados: ['nao', 'foi', 'decertopara', 'semelhante', 'coisa', 'que', 'viera', 'a', 'provincia', 'por', 'conseguinte', ',', 'era', 'liquidar', 'os', 'seus', 'negocios', ',', 'vender', 'osseus', 'bens', 'e', '—', 'por', 'aqui', 'e', 'o', 'caminho', 'o', 'rio', 'de', 'janeiro', 'la', 'estava', 'a', 'sua', 'espera', '“', 'abriria', ',', 'ao', 'chegar', 'la', ',', 'o', 'seu', 'escritorio', ',', 'trabalharia', ',', 'e', ',', 'ao', 'lado', 'da', 'mulher', 'com', 'quem', 'casasse', 'e', 'dosfilhos', 'que', 'viesse', 'a', 'ter', ',', 'nem', 'sequer', 'havia', 'de', 'lembrar-se', 'do', 'passado', '“', 'sim', ',', 'que', 'mais', 'poderia', 'desejar', 'melhor', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Concluíra os estudos, viajara muito, tinha saúde,\n",
      "possuía alguns bens de fortuna'\n",
      "Tokens gerados: ['concluira', 'os', 'estudos', ',', 'viajara', 'muito', ',', 'tinha', 'saude', ',', 'possuia', 'alguns', 'bens', 'de', 'fortuna']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Era caminhar pra frente e deixar em paz o tal — passado! — O\n",
      "passado, passado! Ora adeus!”\n",
      "E, chegando a esta conclusão, sentia-se feliz, independente, seguro contra as misérias da vida,\n",
      "cheio de confiança no futuro'\n",
      "Tokens gerados: ['—', 'era', 'caminhar', 'pra', 'frente', 'e', 'deixar', 'em', 'paz', 'o', 'tal', '—', 'passado', '—', 'opassado', ',', 'passado', 'ora', 'adeus', '”', 'e', ',', 'chegando', 'a', 'esta', 'conclusao', ',', 'sentia-se', 'feliz', ',', 'independente', ',', 'seguro', 'contra', 'as', 'miserias', 'da', 'vida', ',', 'cheio', 'de', 'confianca', 'no', 'futuro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“E por que não havia de fazer carreira? Ninguém podia ter melhores\n",
      "intenções do que ele?'\n",
      "Tokens gerados: ['“', 'e', 'por', 'que', 'nao', 'havia', 'de', 'fazer', 'carreira', '?', 'ninguem', 'podia', 'ter', 'melhoresintencoes', 'do', 'que', 'ele', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não era um vadio, nem homem de maus instintos; aspirava ao casamento, à\n",
      "estabilidade; queria, no remanso de sua casa, entregar-se ao trabalho sério, tirar partido do que estudara,\n",
      "do que aprendera na Alemanha, na França, na Suíça e nos Estados Unidos'\n",
      "Tokens gerados: ['nao', 'era', 'um', 'vadio', ',', 'nem', 'homem', 'de', 'maus', 'instintos', 'aspirava', 'ao', 'casamento', ',', 'aestabilidade', 'queria', ',', 'no', 'remanso', 'de', 'sua', 'casa', ',', 'entregar-se', 'ao', 'trabalho', 'serio', ',', 'tirar', 'partido', 'do', 'que', 'estudara', ',', 'do', 'que', 'aprendera', 'na', 'alemanha', ',', 'na', 'franca', ',', 'na', 'suica', 'e', 'nos', 'estados', 'unidos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Faltava-lhe apenas vir ao\n",
      "Maranhão e liquidar os seus negócios'\n",
      "Tokens gerados: ['faltava-lhe', 'apenas', 'vir', 'aomaranhao', 'e', 'liquidar', 'os', 'seus', 'negocios']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pois bem! cá estava — era aviar e pôr-se de novo a caminho!”\n",
      "Foi com estas idéias que ele chegou à cidade de São Luís'\n",
      "Tokens gerados: ['—', 'pois', 'bem', 'ca', 'estava', '—', 'era', 'aviar', 'e', 'por-se', 'de', 'novo', 'a', 'caminho', '”', 'foi', 'com', 'estas', 'ideias', 'que', 'ele', 'chegou', 'a', 'cidade', 'de', 'sao', 'luis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E agora, na restauradora liberdade do\n",
      "quarto, depois de um banho tépido, o corpo ainda meio quebrado da viagem, o charuto entre os dedos,\n",
      "sentia'\n",
      "Tokens gerados: ['e', 'agora', ',', 'na', 'restauradora', 'liberdade', 'doquarto', ',', 'depois', 'de', 'um', 'banho', 'tepido', ',', 'o', 'corpo', 'ainda', 'meio', 'quebrado', 'da', 'viagem', ',', 'o', 'charuto', 'entre', 'os', 'dedos', ',', 'sentia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se perfeitamente feliz, satisfeito com a sua sorte e com a sua consciência\n",
      "— Ah! bocejou fechando os olhos'\n",
      "Tokens gerados: ['se', 'perfeitamente', 'feliz', ',', 'satisfeito', 'com', 'a', 'sua', 'sorte', 'e', 'com', 'a', 'sua', 'consciencia—', 'ah', 'bocejou', 'fechando', 'os', 'olhos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É liquidar os negócios e pôr-me ao fresco!'\n",
      "Tokens gerados: ['e', 'liquidar', 'os', 'negocios', 'e', 'por-me', 'ao', 'fresco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, com um novo bocejo, deixou cair ao chão o charuto, e adormeceu tranqüilamente'\n",
      "Tokens gerados: ['e', ',', 'com', 'um', 'novo', 'bocejo', ',', 'deixou', 'cair', 'ao', 'chao', 'o', 'charuto', ',', 'e', 'adormeceu', 'tranquilamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No entanto, a história de Raimundo, a história que ele ignorava, era sabida por quantos\n",
      "conheceram os seus parentes no Maranhão'\n",
      "Tokens gerados: ['no', 'entanto', ',', 'a', 'historia', 'de', 'raimundo', ',', 'a', 'historia', 'que', 'ele', 'ignorava', ',', 'era', 'sabida', 'por', 'quantosconheceram', 'os', 'seus', 'parentes', 'no', 'maranhao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nasceu numa fazenda de escravos na Vila do Rosário, muitos anos depois que seu pai, José\n",
      "Pedro da Silva aí se refugiara, corrido do Pará ao grito de “Mata bicudo!” nas revoltas de 1831'\n",
      "Tokens gerados: ['nasceu', 'numa', 'fazenda', 'de', 'escravos', 'na', 'vila', 'do', 'rosario', ',', 'muitos', 'anos', 'depois', 'que', 'seu', 'pai', ',', 'josepedro', 'da', 'silva', 'ai', 'se', 'refugiara', ',', 'corrido', 'do', 'para', 'ao', 'grito', 'de', '“', 'mata', 'bicudo', '”', 'nas', 'revoltas', 'de', '1831']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José da Silva havia enriquecido no contrabando dos negros da África e fora sempre mais ou\n",
      "menos perseguido e malquisto pelo povo do Pará; até que, um belo dia, se levantou contra ele a própria\n",
      "escravatura, que o teria exterminado, se uma das suas escravas mais moças, por nome Domingas, não\n",
      "o prevenisse a tempo'\n",
      "Tokens gerados: ['jose', 'da', 'silva', 'havia', 'enriquecido', 'no', 'contrabando', 'dos', 'negros', 'da', 'africa', 'e', 'fora', 'sempre', 'mais', 'oumenos', 'perseguido', 'e', 'malquisto', 'pelo', 'povo', 'do', 'para', 'ate', 'que', ',', 'um', 'belo', 'dia', ',', 'se', 'levantou', 'contra', 'ele', 'a', 'propriaescravatura', ',', 'que', 'o', 'teria', 'exterminado', ',', 'se', 'uma', 'das', 'suas', 'escravas', 'mais', 'mocas', ',', 'por', 'nome', 'domingas', ',', 'naoo', 'prevenisse', 'a', 'tempo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Logrou passar incólume ao Maranhão, não sem pena de abandonar seus haveres\n",
      "e risco de cair em novos ódios, que esta província, como vizinha e tributária do comércio da outra,\n",
      "sustentava instigada pelo Farol, contra os brasileiros adotivos e contra os portugueses'\n",
      "Tokens gerados: ['logrou', 'passar', 'incolume', 'ao', 'maranhao', ',', 'nao', 'sem', 'pena', 'de', 'abandonar', 'seus', 'haverese', 'risco', 'de', 'cair', 'em', 'novos', 'odios', ',', 'que', 'esta', 'provincia', ',', 'como', 'vizinha', 'e', 'tributaria', 'do', 'comercio', 'da', 'outra', ',', 'sustentava', 'instigada', 'pelo', 'farol', ',', 'contra', 'os', 'brasileiros', 'adotivos', 'e', 'contra', 'os', 'portugueses']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Todavia, conseguiu\n",
      "sempre salvar algum ouro; metal que naquele bom tempo corria abundante por todo o Brasil e que mais\n",
      "tarde a Guerra do Paraguai tinha de transformar em condecorações e fumaça'\n",
      "Tokens gerados: ['todavia', ',', 'conseguiusempre', 'salvar', 'algum', 'ouro', 'metal', 'que', 'naquele', 'bom', 'tempo', 'corria', 'abundante', 'por', 'todo', 'o', 'brasil', 'e', 'que', 'maistarde', 'a', 'guerra', 'do', 'paraguai', 'tinha', 'de', 'transformar', 'em', 'condecoracoes', 'e', 'fumaca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A fuga fizeram eles, senhor e escrava, a pé, por maus caminhos, atravessando os sertões'\n",
      "Tokens gerados: ['a', 'fuga', 'fizeram', 'eles', ',', 'senhor', 'e', 'escrava', ',', 'a', 'pe', ',', 'por', 'maus', 'caminhos', ',', 'atravessando', 'os', 'sertoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ainda\n",
      "não existia a companhia de vapores e os transportes marítimos dependiam então de vagarosas barcas, a\n",
      "vela e remo e, às vezes, puxadas a corda, nos igarapés'\n",
      "Tokens gerados: ['aindanao', 'existia', 'a', 'companhia', 'de', 'vapores', 'e', 'os', 'transportes', 'maritimos', 'dependiam', 'entao', 'de', 'vagarosas', 'barcas', ',', 'avela', 'e', 'remo', 'e', ',', 'as', 'vezes', ',', 'puxadas', 'a', 'corda', ',', 'nos', 'igarapes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foram dar com os ossos no Rosário'\n",
      "Tokens gerados: ['foram', 'dar', 'com', 'os', 'ossos', 'no', 'rosario']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O\n",
      "contrabandista arranjou-se o melhor que pôde com a escrava que lhe restava, e, mais tarde, no lugar\n",
      "denominado São Brás, veio a comprar uma fazendola, onde cultivou café, algodão, tabaco e arroz'\n",
      "Tokens gerados: ['ocontrabandista', 'arranjou-se', 'o', 'melhor', 'que', 'pode', 'com', 'a', 'escrava', 'que', 'lhe', 'restava', ',', 'e', ',', 'mais', 'tarde', ',', 'no', 'lugardenominado', 'sao', 'bras', ',', 'veio', 'a', 'comprar', 'uma', 'fazendola', ',', 'onde', 'cultivou', 'cafe', ',', 'algodao', ',', 'tabaco', 'e', 'arroz']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois de vários abortos, Domingas deu à luz um filho de José da Silva'\n",
      "Tokens gerados: ['depois', 'de', 'varios', 'abortos', ',', 'domingas', 'deu', 'a', 'luz', 'um', 'filho', 'de', 'jose', 'da', 'silva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Chamou-se o vigário\n",
      "da freguesia e, no ato do batismo da criança, esta, como a mãe, receberam solenemente a carta de\n",
      "alforria'\n",
      "Tokens gerados: ['chamou-se', 'o', 'vigarioda', 'freguesia', 'e', ',', 'no', 'ato', 'do', 'batismo', 'da', 'crianca', ',', 'esta', ',', 'como', 'a', 'mae', ',', 'receberam', 'solenemente', 'a', 'carta', 'dealforria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Essa criança era Raimundo'\n",
      "Tokens gerados: ['essa', 'crianca', 'era', 'raimundo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na capital, entretanto, acalmavam-se os ânimos'\n",
      "Tokens gerados: ['na', 'capital', ',', 'entretanto', ',', 'acalmavam-se', 'os', 'animos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José prosperou rapidamente no Rosário; cercou\n",
      "a amante e o filho de cuidados; relacionou-se com a vizinhança, criou amizades, e, no fim de pouco\n",
      "tempo, recebia em casamento a Sra'\n",
      "Tokens gerados: ['jose', 'prosperou', 'rapidamente', 'no', 'rosario', 'cercoua', 'amante', 'e', 'o', 'filho', 'de', 'cuidados', 'relacionou-se', 'com', 'a', 'vizinhanca', ',', 'criou', 'amizades', ',', 'e', ',', 'no', 'fim', 'de', 'poucotempo', ',', 'recebia', 'em', 'casamento', 'a', 'sra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'D'\n",
      "Tokens gerados: ['d']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quitéria Inocência de Freitas Santiago, viúva, brasileira, rica,\n",
      "de muita religião e escrúpulos de sangue, e para quem um escravo não era um homem, e o fato de não\n",
      "ser branco, constituía só por si um crime'\n",
      "Tokens gerados: ['quiteria', 'inocencia', 'de', 'freitas', 'santiago', ',', 'viuva', ',', 'brasileira', ',', 'rica', ',', 'de', 'muita', 'religiao', 'e', 'escrupulos', 'de', 'sangue', ',', 'e', 'para', 'quem', 'um', 'escravo', 'nao', 'era', 'um', 'homem', ',', 'e', 'o', 'fato', 'de', 'naoser', 'branco', ',', 'constituia', 'so', 'por', 'si', 'um', 'crime']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi uma fera! às suas mãos, ou por ordem dela, vários escravos sucumbiram ao relho, ao tronco,\n",
      "à fome, à sede, e ao ferro em brasa'\n",
      "Tokens gerados: ['foi', 'uma', 'fera', 'as', 'suas', 'maos', ',', 'ou', 'por', 'ordem', 'dela', ',', 'varios', 'escravos', 'sucumbiram', 'ao', 'relho', ',', 'ao', 'tronco', ',', 'a', 'fome', ',', 'a', 'sede', ',', 'e', 'ao', 'ferro', 'em', 'brasa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas nunca deixou de ser devota, cheia de superstições; tinha uma\n",
      "capela na fazenda, onde a escravatura, todas as noites, com as mãos inchadas pelos bolos, ou as costas\n",
      "lanhadas pelo chicote, entoava súplicas à Virgem Santíssima, mãe dos infelizes'\n",
      "Tokens gerados: ['mas', 'nunca', 'deixou', 'de', 'ser', 'devota', ',', 'cheia', 'de', 'supersticoes', 'tinha', 'umacapela', 'na', 'fazenda', ',', 'onde', 'a', 'escravatura', ',', 'todas', 'as', 'noites', ',', 'com', 'as', 'maos', 'inchadas', 'pelos', 'bolos', ',', 'ou', 'as', 'costaslanhadas', 'pelo', 'chicote', ',', 'entoava', 'suplicas', 'a', 'virgem', 'santissima', ',', 'mae', 'dos', 'infelizes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao lado da capela o cemitério das suas vítimas'\n",
      "Tokens gerados: ['ao', 'lado', 'da', 'capela', 'o', 'cemiterio', 'das', 'suas', 'vitimas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Casara com José da Silva por dois motivos simplesmente: porque precisava de um homem, e ali\n",
      "não havia muito onde escolher, e porque lhe diziam que os portugueses são brancos de primeira água'\n",
      "Tokens gerados: ['casara', 'com', 'jose', 'da', 'silva', 'por', 'dois', 'motivos', 'simplesmente', 'porque', 'precisava', 'de', 'um', 'homem', ',', 'e', 'alinao', 'havia', 'muito', 'onde', 'escolher', ',', 'e', 'porque', 'lhe', 'diziam', 'que', 'os', 'portugueses', 'sao', 'brancos', 'de', 'primeira', 'agua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nunca tivera filhos'\n",
      "Tokens gerados: ['nunca', 'tivera', 'filhos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um dia reparou que o marido, a título de padrinho, distinguia com certa\n",
      "ternura o crioulo da Domingas e declarou logo que não admitia, nem mais um instante, aquele moleque\n",
      "na fazenda'\n",
      "Tokens gerados: ['um', 'dia', 'reparou', 'que', 'o', 'marido', ',', 'a', 'titulo', 'de', 'padrinho', ',', 'distinguia', 'com', 'certaternura', 'o', 'crioulo', 'da', 'domingas', 'e', 'declarou', 'logo', 'que', 'nao', 'admitia', ',', 'nem', 'mais', 'um', 'instante', ',', 'aquele', 'molequena', 'fazenda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Seu negreiro! gritava ela ao marido, fula de raiva'\n",
      "Tokens gerados: ['—', 'seu', 'negreiro', 'gritava', 'ela', 'ao', 'marido', ',', 'fula', 'de', 'raiva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Você pensa que lhe deixarei criar, em\n",
      "minha companhia, os filhos que você tem das negras?'\n",
      "Tokens gerados: ['voce', 'pensa', 'que', 'lhe', 'deixarei', 'criar', ',', 'emminha', 'companhia', ',', 'os', 'filhos', 'que', 'voce', 'tem', 'das', 'negras', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era só também o que faltava! Não trate de\n",
      "despachar-me, quanto antes, o moleque, que serei eu quem o despacha, mas há de ser para ali, para\n",
      "junto da capela!\n",
      "José, que sabia perfeitamente de quanto ela era capaz, correu logo à vila para dar as providências\n",
      "necessárias à segurança do filho'\n",
      "Tokens gerados: ['era', 'so', 'tambem', 'o', 'que', 'faltava', 'nao', 'trate', 'dedespachar-me', ',', 'quanto', 'antes', ',', 'o', 'moleque', ',', 'que', 'serei', 'eu', 'quem', 'o', 'despacha', ',', 'mas', 'ha', 'de', 'ser', 'para', 'ali', ',', 'parajunto', 'da', 'capelajose', ',', 'que', 'sabia', 'perfeitamente', 'de', 'quanto', 'ela', 'era', 'capaz', ',', 'correu', 'logo', 'a', 'vila', 'para', 'dar', 'as', 'providenciasnecessarias', 'a', 'seguranca', 'do', 'filho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, ao voltar à fazenda, gritos horrorosos atraíram-no ao rancho dos\n",
      "pretos, entrou descoroçoado e viu o seguinte:\n",
      "Estendida por terra, com os pés no tronco, cabeça raspada e mãos amarradas para trás, permanecia\n",
      "Domingas, completamente nua e com as partes genitais queimadas a ferro em brasa'\n",
      "Tokens gerados: ['mas', ',', 'ao', 'voltar', 'a', 'fazenda', ',', 'gritos', 'horrorosos', 'atrairam-no', 'ao', 'rancho', 'dospretos', ',', 'entrou', 'descorocoado', 'e', 'viu', 'o', 'seguinteestendida', 'por', 'terra', ',', 'com', 'os', 'pes', 'no', 'tronco', ',', 'cabeca', 'raspada', 'e', 'maos', 'amarradas', 'para', 'tras', ',', 'permaneciadomingas', ',', 'completamente', 'nua', 'e', 'com', 'as', 'partes', 'genitais', 'queimadas', 'a', 'ferro', 'em', 'brasa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao lado, o filhinho\n",
      "de três anos, gritava como um possesso, tentando abraçá-la, e, de cada vez que ele se aproximava da\n",
      "mãe, dois negros, à ordem de Quitéria, desviavam o relho das costas da escrava para dardejá-lo contra\n",
      "a criança'\n",
      "Tokens gerados: ['ao', 'lado', ',', 'o', 'filhinhode', 'tres', 'anos', ',', 'gritava', 'como', 'um', 'possesso', ',', 'tentando', 'abraca-la', ',', 'e', ',', 'de', 'cada', 'vez', 'que', 'ele', 'se', 'aproximava', 'damae', ',', 'dois', 'negros', ',', 'a', 'ordem', 'de', 'quiteria', ',', 'desviavam', 'o', 'relho', 'das', 'costas', 'da', 'escrava', 'para', 'dardeja-lo', 'contraa', 'crianca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A megera, de pé, horrível, bêbada de cólera, ria-se, praguejava obscenidades, uivando nos\n",
      "espasmos flagrantes da cólera'\n",
      "Tokens gerados: ['a', 'megera', ',', 'de', 'pe', ',', 'horrivel', ',', 'bebada', 'de', 'colera', ',', 'ria-se', ',', 'praguejava', 'obscenidades', ',', 'uivando', 'nosespasmos', 'flagrantes', 'da', 'colera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Domingas, quase morta, gemia, estorcendo-se no chão'\n",
      "Tokens gerados: ['domingas', ',', 'quase', 'morta', ',', 'gemia', ',', 'estorcendo-se', 'no', 'chao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O desarranjo de\n",
      "suas palavras e dos seus gestos denunciava já sintomas de loucura'\n",
      "Tokens gerados: ['o', 'desarranjo', 'desuas', 'palavras', 'e', 'dos', 'seus', 'gestos', 'denunciava', 'ja', 'sintomas', 'de', 'loucura']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O pai de Raimundo, no primeiro assomo de indignação, tão furioso acometeu sobre a esposa,\n",
      "que a fez cair'\n",
      "Tokens gerados: ['o', 'pai', 'de', 'raimundo', ',', 'no', 'primeiro', 'assomo', 'de', 'indignacao', ',', 'tao', 'furioso', 'acometeu', 'sobre', 'a', 'esposa', ',', 'que', 'a', 'fez', 'cair']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em seguida, ordenou que recolhessem Domingas à casa dos brancos e que lhe\n",
      "prodigalizassem todos os cuidados'\n",
      "Tokens gerados: ['em', 'seguida', ',', 'ordenou', 'que', 'recolhessem', 'domingas', 'a', 'casa', 'dos', 'brancos', 'e', 'que', 'lheprodigalizassem', 'todos', 'os', 'cuidados']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quitéria, a conselho do vigário do lugar, um padre ainda moço, chamado Diogo, o mesmo que\n",
      "batizara Raimundo, fugiu essa noite para a fazenda de sua mãe, D'\n",
      "Tokens gerados: ['quiteria', ',', 'a', 'conselho', 'do', 'vigario', 'do', 'lugar', ',', 'um', 'padre', 'ainda', 'moco', ',', 'chamado', 'diogo', ',', 'o', 'mesmo', 'quebatizara', 'raimundo', ',', 'fugiu', 'essa', 'noite', 'para', 'a', 'fazenda', 'de', 'sua', 'mae', ',', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Úrsula Santiago, a meia légua dali'\n",
      "Tokens gerados: ['ursula', 'santiago', ',', 'a', 'meia', 'legua', 'dali']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O vigário era muito da casa das Santiago; dizia-se até aparentado com elas'\n",
      "Tokens gerados: ['o', 'vigario', 'era', 'muito', 'da', 'casa', 'das', 'santiago', 'dizia-se', 'ate', 'aparentado', 'com', 'elas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O caso é que foi na\n",
      "qualidade de confessor, parente e amigo, que ele acompanhou Quitéria'\n",
      "Tokens gerados: ['o', 'caso', 'e', 'que', 'foi', 'naqualidade', 'de', 'confessor', ',', 'parente', 'e', 'amigo', ',', 'que', 'ele', 'acompanhou', 'quiteria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José da Silva, por esse tempo, chegava à cidade de São Luís com o filho'\n",
      "Tokens gerados: ['jose', 'da', 'silva', ',', 'por', 'esse', 'tempo', ',', 'chegava', 'a', 'cidade', 'de', 'sao', 'luis', 'com', 'o', 'filho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Procurou seu irmão\n",
      "mais moço, o Manuel Pedro, e entregou-lhe o pequeno, que ficaria sob as vistas do tio até ter idade para\n",
      "matricular-se num colégio de Lisboa'\n",
      "Tokens gerados: ['procurou', 'seu', 'irmaomais', 'moco', ',', 'o', 'manuel', 'pedro', ',', 'e', 'entregou-lhe', 'o', 'pequeno', ',', 'que', 'ficaria', 'sob', 'as', 'vistas', 'do', 'tio', 'ate', 'ter', 'idade', 'paramatricular-se', 'num', 'colegio', 'de', 'lisboa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Feito isso, tornou de novo para a sua roça'\n",
      "Tokens gerados: ['feito', 'isso', ',', 'tornou', 'de', 'novo', 'para', 'a', 'sua', 'roca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Agora contava viver mais descansado; era natural\n",
      "que a mulher se deixasse ficar em casa da mãe'\n",
      "Tokens gerados: ['“', 'agora', 'contava', 'viver', 'mais', 'descansado', 'era', 'naturalque', 'a', 'mulher', 'se', 'deixasse', 'ficar', 'em', 'casa', 'da', 'mae']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” Ao chegar lá, sabendo que não o esperavam essa noite\n",
      "e como visse luz no quarto da esposa, apeou-se em distância e, para não se encontrar com ela, guardou\n",
      "o cavalo e entrou silenciosamente na fazenda'\n",
      "Tokens gerados: ['”', 'ao', 'chegar', 'la', ',', 'sabendo', 'que', 'nao', 'o', 'esperavam', 'essa', 'noitee', 'como', 'visse', 'luz', 'no', 'quarto', 'da', 'esposa', ',', 'apeou-se', 'em', 'distancia', 'e', ',', 'para', 'nao', 'se', 'encontrar', 'com', 'ela', ',', 'guardouo', 'cavalo', 'e', 'entrou', 'silenciosamente', 'na', 'fazenda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os cães conheceram-no pelo faro e apenas rosnaram'\n",
      "Tokens gerados: ['os', 'caes', 'conheceram-no', 'pelo', 'faro', 'e', 'apenas', 'rosnaram']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, na ocasião em que ele passava defronte\n",
      "do quarto de Quitéria, ouviu aí sussurros de vozes que conversavam'\n",
      "Tokens gerados: ['mas', ',', 'na', 'ocasiao', 'em', 'que', 'ele', 'passava', 'defrontedo', 'quarto', 'de', 'quiteria', ',', 'ouviu', 'ai', 'sussurros', 'de', 'vozes', 'que', 'conversavam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aproximou-se levado pela\n",
      "curiosidade e encostou o ouvido à porta'\n",
      "Tokens gerados: ['aproximou-se', 'levado', 'pelacuriosidade', 'e', 'encostou', 'o', 'ouvido', 'a', 'porta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Reconheceu logo a voz da mulher'\n",
      "Tokens gerados: ['reconheceu', 'logo', 'a', 'voz', 'da', 'mulher']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Mas, com quem, diabo, ela conversaria àquela hora?'\n",
      "Tokens gerados: ['“', 'mas', ',', 'com', 'quem', ',', 'diabo', ',', 'ela', 'conversaria', 'aquela', 'hora', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Conteve a impaciência e esperou de ouvido alerta'\n",
      "Tokens gerados: ['”', 'conteve', 'a', 'impaciencia', 'e', 'esperou', 'de', 'ouvido', 'alerta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Não havia dúvida! — a outra voz era de um homem!'\n",
      "Tokens gerados: ['“', 'nao', 'havia', 'duvida', '—', 'a', 'outra', 'voz', 'era', 'de', 'um', 'homem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Sem esperar mais nada, meteu ombros à porta e, precipitou-se dentro do quarto, atirando-se\n",
      "com fúria sobre a esposa, que perdera logo os sentidos'\n",
      "Tokens gerados: ['”', 'sem', 'esperar', 'mais', 'nada', ',', 'meteu', 'ombros', 'a', 'porta', 'e', ',', 'precipitou-se', 'dentro', 'do', 'quarto', ',', 'atirando-secom', 'furia', 'sobre', 'a', 'esposa', ',', 'que', 'perdera', 'logo', 'os', 'sentidos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O padre Diogo, pois era dele a outra voz, não tivera tempo de fugir e caíra, trêmulo, aos pés de\n",
      "José'\n",
      "Tokens gerados: ['o', 'padre', 'diogo', ',', 'pois', 'era', 'dele', 'a', 'outra', 'voz', ',', 'nao', 'tivera', 'tempo', 'de', 'fugir', 'e', 'caira', ',', 'tremulo', ',', 'aos', 'pes', 'dejose']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando este largou das mãos a traidora, para se apossar do outro, reparou que a tinha estrangulado'\n",
      "Tokens gerados: ['quando', 'este', 'largou', 'das', 'maos', 'a', 'traidora', ',', 'para', 'se', 'apossar', 'do', 'outro', ',', 'reparou', 'que', 'a', 'tinha', 'estrangulado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ficou perplexo e tolhido de assombro'\n",
      "Tokens gerados: ['ficou', 'perplexo', 'e', 'tolhido', 'de', 'assombro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Houve então um silêncio ansioso'\n",
      "Tokens gerados: ['houve', 'entao', 'um', 'silencio', 'ansioso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ouvia-se o resfolegar dos dois homens'\n",
      "Tokens gerados: ['ouvia-se', 'o', 'resfolegar', 'dos', 'dois', 'homens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A situação\n",
      "dificultava-se; mas o vigário, recuperando o sangue-frio, ergueu-se, consertou as roupas e, apontando\n",
      "para o corpo da amante, disse com firmeza:\n",
      "— Matou-a! Você é um criminoso!\n",
      "— Cachorro! E tu?! Tu serás porventura menos criminoso do que eu?\n",
      "— Perante as leis, decerto! porque você nunca poderá provar a minha suposta culpa e, se tentasse\n",
      "fazê-lo, a vergonha do fato recairia toda sobre a sua própria cabeça, ao passo que eu, além do crime de\n",
      "injúria consumado na minha sagrada pessoa, sou testemunha do assassínio desta minha infeliz e inocente\n",
      "confessada, assassínio que facilmente documentarei com o corpo de delito que aqui está!\n",
      "E mostrava a marca das mãos de José na garganta do cadáver'\n",
      "Tokens gerados: ['a', 'situacaodificultava-se', 'mas', 'o', 'vigario', ',', 'recuperando', 'o', 'sangue-frio', ',', 'ergueu-se', ',', 'consertou', 'as', 'roupas', 'e', ',', 'apontandopara', 'o', 'corpo', 'da', 'amante', ',', 'disse', 'com', 'firmeza—', 'matou-a', 'voce', 'e', 'um', 'criminoso—', 'cachorro', 'e', 'tu', '?', 'tu', 'seras', 'porventura', 'menos', 'criminoso', 'do', 'que', 'eu', '?', '—', 'perante', 'as', 'leis', ',', 'decerto', 'porque', 'voce', 'nunca', 'podera', 'provar', 'a', 'minha', 'suposta', 'culpa', 'e', ',', 'se', 'tentassefaze-lo', ',', 'a', 'vergonha', 'do', 'fato', 'recairia', 'toda', 'sobre', 'a', 'sua', 'propria', 'cabeca', ',', 'ao', 'passo', 'que', 'eu', ',', 'alem', 'do', 'crime', 'deinjuria', 'consumado', 'na', 'minha', 'sagrada', 'pessoa', ',', 'sou', 'testemunha', 'do', 'assassinio', 'desta', 'minha', 'infeliz', 'e', 'inocenteconfessada', ',', 'assassinio', 'que', 'facilmente', 'documentarei', 'com', 'o', 'corpo', 'de', 'delito', 'que', 'aqui', 'estae', 'mostrava', 'a', 'marca', 'das', 'maos', 'de', 'jose', 'na', 'garganta', 'do', 'cadaver']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O assassino ficou aterrado e abaixou a cabeça'\n",
      "Tokens gerados: ['o', 'assassino', 'ficou', 'aterrado', 'e', 'abaixou', 'a', 'cabeca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Vamos lá!'\n",
      "Tokens gerados: ['—', 'vamos', 'la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'disse o padre afinal, sorrindo e batendo no ombro do português'\n",
      "Tokens gerados: ['disse', 'o', 'padre', 'afinal', ',', 'sorrindo', 'e', 'batendo', 'no', 'ombro', 'do', 'portugues']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tudo neste\n",
      "mundo se pode arranjar, com a divina ajuda de Deus'\n",
      "Tokens gerados: ['tudo', 'nestemundo', 'se', 'pode', 'arranjar', ',', 'com', 'a', 'divina', 'ajuda', 'de', 'deus']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'só para a morte não há remédio! Se quiser, a\n",
      "defunta será sepultada com todas as formalidades civis e religiosas'\n",
      "Tokens gerados: ['so', 'para', 'a', 'morte', 'nao', 'ha', 'remedio', 'se', 'quiser', ',', 'adefunta', 'sera', 'sepultada', 'com', 'todas', 'as', 'formalidades', 'civis', 'e', 'religiosas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, dando à voz um cunho particular de autoridade: — Apenas, pelo meu silêncio sobre o crime,\n",
      "exijo em troca o seu para a minha culpa'\n",
      "Tokens gerados: ['e', ',', 'dando', 'a', 'voz', 'um', 'cunho', 'particular', 'de', 'autoridade', '—', 'apenas', ',', 'pelo', 'meu', 'silencio', 'sobre', 'o', 'crime', ',', 'exijo', 'em', 'troca', 'o', 'seu', 'para', 'a', 'minha', 'culpa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aceita?\n",
      "José saiu do quarto, cego de cólera, de vergonha e de remorso'\n",
      "Tokens gerados: ['aceita', '?', 'jose', 'saiu', 'do', 'quarto', ',', 'cego', 'de', 'colera', ',', 'de', 'vergonha', 'e', 'de', 'remorso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que vida a sua! exclamava'\n",
      "Tokens gerados: ['—', 'que', 'vida', 'a', 'sua', 'exclamava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que vida, santo Deus!\n",
      "O padre cumpriu a promessa: o cadáver enterrou-se na capela de São Brás, ao lado das suas\n",
      "vítimas; e todos os do lugar, até mesmo os de casa, atribuíram a morte de Quitéria ao espírito maligno\n",
      "que se lhe havia metido no corpo'\n",
      "Tokens gerados: ['que', 'vida', ',', 'santo', 'deuso', 'padre', 'cumpriu', 'a', 'promessa', 'o', 'cadaver', 'enterrou-se', 'na', 'capela', 'de', 'sao', 'bras', ',', 'ao', 'lado', 'das', 'suasvitimas', 'e', 'todos', 'os', 'do', 'lugar', ',', 'ate', 'mesmo', 'os', 'de', 'casa', ',', 'atribuiram', 'a', 'morte', 'de', 'quiteria', 'ao', 'espirito', 'malignoque', 'se', 'lhe', 'havia', 'metido', 'no', 'corpo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O vigário confirmava esses boatos e continuava a pastorar tranqüilamente o seu rebanho, sempre\n",
      "tido por homem de muita santidade e de grandes virtudes teologais'\n",
      "Tokens gerados: ['o', 'vigario', 'confirmava', 'esses', 'boatos', 'e', 'continuava', 'a', 'pastorar', 'tranquilamente', 'o', 'seu', 'rebanho', ',', 'sempretido', 'por', 'homem', 'de', 'muita', 'santidade', 'e', 'de', 'grandes', 'virtudes', 'teologais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os devotos continuaram a trazer-lhe,\n",
      "de muitas léguas de distância, os melhores bácoros, galinhas e perus dos seus cercados'\n",
      "Tokens gerados: ['os', 'devotos', 'continuaram', 'a', 'trazer-lhe', ',', 'de', 'muitas', 'leguas', 'de', 'distancia', ',', 'os', 'melhores', 'bacoros', ',', 'galinhas', 'e', 'perus', 'dos', 'seus', 'cercados']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em breve, as coisas voltavam todas aos eixos: José entregou a fazenda a Domingas e mais três\n",
      "pretos velhos, que alforriou logo, e, acompanhado pelo resto da escravatura, seguiu para a cidade de\n",
      "São Luís, no propósito de liquidar seus bens e recolher-se à pátria com o filho'\n",
      "Tokens gerados: ['em', 'breve', ',', 'as', 'coisas', 'voltavam', 'todas', 'aos', 'eixos', 'jose', 'entregou', 'a', 'fazenda', 'a', 'domingas', 'e', 'mais', 'trespretos', 'velhos', ',', 'que', 'alforriou', 'logo', ',', 'e', ',', 'acompanhado', 'pelo', 'resto', 'da', 'escravatura', ',', 'seguiu', 'para', 'a', 'cidade', 'desao', 'luis', ',', 'no', 'proposito', 'de', 'liquidar', 'seus', 'bens', 'e', 'recolher-se', 'a', 'patria', 'com', 'o', 'filho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mãe de Raimundo conseguiu enfim descansar'\n",
      "Tokens gerados: ['a', 'mae', 'de', 'raimundo', 'conseguiu', 'enfim', 'descansar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'São Brás criou a sua lenda e foi aos poucos\n",
      "ganhando fama de amaldiçoada'\n",
      "Tokens gerados: ['sao', 'bras', 'criou', 'a', 'sua', 'lenda', 'e', 'foi', 'aos', 'poucosganhando', 'fama', 'de', 'amaldicoada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, o pequeno, quando chegou à casa do tio na capital, estava,\n",
      "como facilmente se pode julgar, com a pele sobre os ossos'\n",
      "Tokens gerados: ['entretanto', ',', 'o', 'pequeno', ',', 'quando', 'chegou', 'a', 'casa', 'do', 'tio', 'na', 'capital', ',', 'estava', ',', 'como', 'facilmente', 'se', 'pode', 'julgar', ',', 'com', 'a', 'pele', 'sobre', 'os', 'ossos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A falta de cuidados espalhara-lhe na carinha\n",
      "opada uma expressão triste de moléstia; quase que não conseguia abrir os olhos'\n",
      "Tokens gerados: ['a', 'falta', 'de', 'cuidados', 'espalhara-lhe', 'na', 'carinhaopada', 'uma', 'expressao', 'triste', 'de', 'molestia', 'quase', 'que', 'nao', 'conseguia', 'abrir', 'os', 'olhos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Todo ele era mau trato\n",
      "e fraqueza; tinha o estômago muito sujo, a língua saburrenta, o corpo a finar-se de reumatismo e tosse\n",
      "convulsa, o sangue predisposto à anemia escrofulosa'\n",
      "Tokens gerados: ['todo', 'ele', 'era', 'mau', 'tratoe', 'fraqueza', 'tinha', 'o', 'estomago', 'muito', 'sujo', ',', 'a', 'lingua', 'saburrenta', ',', 'o', 'corpo', 'a', 'finar-se', 'de', 'reumatismo', 'e', 'tosseconvulsa', ',', 'o', 'sangue', 'predisposto', 'a', 'anemia', 'escrofulosa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Apesar do instinto materno, que a tudo resiste e\n",
      "vence, a pobre escrava não podia olhar nunca pelo filho: lá estava Quitéria para desviá-la dele, para\n",
      "cortar-lhe as carícias a chicote; tanto assim, que, quando José lhe anunciou que Raimundo ia para a\n",
      "casa do tio na cidade, a infeliz abençoou com lágrimas desesperadas aquela separação'\n",
      "Tokens gerados: ['apesar', 'do', 'instinto', 'materno', ',', 'que', 'a', 'tudo', 'resiste', 'evence', ',', 'a', 'pobre', 'escrava', 'nao', 'podia', 'olhar', 'nunca', 'pelo', 'filho', 'la', 'estava', 'quiteria', 'para', 'desvia-la', 'dele', ',', 'paracortar-lhe', 'as', 'caricias', 'a', 'chicote', 'tanto', 'assim', ',', 'que', ',', 'quando', 'jose', 'lhe', 'anunciou', 'que', 'raimundo', 'ia', 'para', 'acasa', 'do', 'tio', 'na', 'cidade', ',', 'a', 'infeliz', 'abencoou', 'com', 'lagrimas', 'desesperadas', 'aquela', 'separacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Todavia, o desgraçadinho foi encontrar em Mariana, cunhada de seu pai, a mais carinhosa e\n",
      "terna das protetoras'\n",
      "Tokens gerados: ['todavia', ',', 'o', 'desgracadinho', 'foi', 'encontrar', 'em', 'mariana', ',', 'cunhada', 'de', 'seu', 'pai', ',', 'a', 'mais', 'carinhosa', 'eterna', 'das', 'protetoras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A boa senhora, como sabia que o marido o pouco que tinha devia à generosidade\n",
      "do irmão, julgou-se logo obrigada a servir de mãe ao filho deste'\n",
      "Tokens gerados: ['a', 'boa', 'senhora', ',', 'como', 'sabia', 'que', 'o', 'marido', 'o', 'pouco', 'que', 'tinha', 'devia', 'a', 'generosidadedo', 'irmao', ',', 'julgou-se', 'logo', 'obrigada', 'a', 'servir', 'de', 'mae', 'ao', 'filho', 'deste']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ana Rosa, único fruto do seu casamento,\n",
      "ainda não era nascida nesse tempo, de sorte que as premissas da sua maternidade pertenceram ao\n",
      "pupilo'\n",
      "Tokens gerados: ['ana', 'rosa', ',', 'unico', 'fruto', 'do', 'seu', 'casamento', ',', 'ainda', 'nao', 'era', 'nascida', 'nesse', 'tempo', ',', 'de', 'sorte', 'que', 'as', 'premissas', 'da', 'sua', 'maternidade', 'pertenceram', 'aopupilo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dentro em pouco, no agasalho carinhoso daquelas asas de mãe, Raimundo, de feio que era,\n",
      "tornou-se uma criança forte, sã e bonita'\n",
      "Tokens gerados: ['dentro', 'em', 'pouco', ',', 'no', 'agasalho', 'carinhoso', 'daquelas', 'asas', 'de', 'mae', ',', 'raimundo', ',', 'de', 'feio', 'que', 'era', ',', 'tornou-se', 'uma', 'crianca', 'forte', ',', 'sa', 'e', 'bonita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi então que Ana Rosa veio ao mundo; a princípio muito fraquinha e quase sem dar acordo de\n",
      "si'\n",
      "Tokens gerados: ['foi', 'entao', 'que', 'ana', 'rosa', 'veio', 'ao', 'mundo', 'a', 'principio', 'muito', 'fraquinha', 'e', 'quase', 'sem', 'dar', 'acordo', 'desi']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel andava aflito, com medo de perdê-la'\n",
      "Tokens gerados: ['manuel', 'andava', 'aflito', ',', 'com', 'medo', 'de', 'perde-la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que luta, os três primeiros meses de sua vida! Parecia\n",
      "morrer a todo instante, coitadinha! Ninguém dormia na casa; o negociante chorava como um perdido,\n",
      "enquanto a mulher fazia promessas aos santos da sua devoção'\n",
      "Tokens gerados: ['que', 'luta', ',', 'os', 'tres', 'primeiros', 'meses', 'de', 'sua', 'vida', 'pareciamorrer', 'a', 'todo', 'instante', ',', 'coitadinha', 'ninguem', 'dormia', 'na', 'casa', 'o', 'negociante', 'chorava', 'como', 'um', 'perdido', ',', 'enquanto', 'a', 'mulher', 'fazia', 'promessas', 'aos', 'santos', 'da', 'sua', 'devocao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era por isto que a menina, mais tarde, se recordava agradavelmente de ter feito o anjo da verônica\n",
      "nas procissões da quaresma'\n",
      "Tokens gerados: ['era', 'por', 'isto', 'que', 'a', 'menina', ',', 'mais', 'tarde', ',', 'se', 'recordava', 'agradavelmente', 'de', 'ter', 'feito', 'o', 'anjo', 'da', 'veronicanas', 'procissoes', 'da', 'quaresma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E ao lado de Mariana, que noite e dia velava o berço da filhinha enferma, estava o Mundico, o\n",
      "outro filho, que este também a chamava de mãe e já se não lembrava da verdadeira, da preta que o\n",
      "trouxera nas entranhas'\n",
      "Tokens gerados: ['e', 'ao', 'lado', 'de', 'mariana', ',', 'que', 'noite', 'e', 'dia', 'velava', 'o', 'berco', 'da', 'filhinha', 'enferma', ',', 'estava', 'o', 'mundico', ',', 'ooutro', 'filho', ',', 'que', 'este', 'tambem', 'a', 'chamava', 'de', 'mae', 'e', 'ja', 'se', 'nao', 'lembrava', 'da', 'verdadeira', ',', 'da', 'preta', 'que', 'otrouxera', 'nas', 'entranhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A menina salvou-se, graças aos bons serviços de um médico, que chegara havia pouco da\n",
      "universidade de Montpellier, Dr'\n",
      "Tokens gerados: ['a', 'menina', 'salvou-se', ',', 'gracas', 'aos', 'bons', 'servicos', 'de', 'um', 'medico', ',', 'que', 'chegara', 'havia', 'pouco', 'dauniversidade', 'de', 'montpellier', ',', 'dr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Jauffret, e, a partir daí Manuel não quis saber de outro facultativo em\n",
      "sua casa'\n",
      "Tokens gerados: ['jauffret', ',', 'e', ',', 'a', 'partir', 'dai', 'manuel', 'nao', 'quis', 'saber', 'de', 'outro', 'facultativo', 'emsua', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por essa época, mais ou menos, chegava do Rosário a notícia de haver D'\n",
      "Tokens gerados: ['por', 'essa', 'epoca', ',', 'mais', 'ou', 'menos', ',', 'chegava', 'do', 'rosario', 'a', 'noticia', 'de', 'haver', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quitéria sucumbido a\n",
      "uma congestão cerebral'\n",
      "Tokens gerados: ['quiteria', 'sucumbido', 'auma', 'congestao', 'cerebral']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Deu-lhe de repente! explicava o correio, com o seu saco de couro às costas'\n",
      "Tokens gerados: ['—', 'deu-lhe', 'de', 'repente', 'explicava', 'o', 'correio', ',', 'com', 'o', 'seu', 'saco', 'de', 'couro', 'as', 'costas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi obra do sujo,\n",
      "credo!\n",
      "E, pouco depois, José Pedro da Silva, todo coberto de luto, muito encanecido e desfeito, vinha\n",
      "liquidar os seus negócios e partir logo para Portugal'\n",
      "Tokens gerados: ['foi', 'obra', 'do', 'sujo', ',', 'credoe', ',', 'pouco', 'depois', ',', 'jose', 'pedro', 'da', 'silva', ',', 'todo', 'coberto', 'de', 'luto', ',', 'muito', 'encanecido', 'e', 'desfeito', ',', 'vinhaliquidar', 'os', 'seus', 'negocios', 'e', 'partir', 'logo', 'para', 'portugal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel estimava-o deveras e sentia-se de vê-lo\n",
      "naquele estado'\n",
      "Tokens gerados: ['manuel', 'estimava-o', 'deveras', 'e', 'sentia-se', 'de', 've-lonaquele', 'estado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aprontou-se tudo para a viagem e José recolheu-se a última noite em casa do irmão'\n",
      "Tokens gerados: ['aprontou-se', 'tudo', 'para', 'a', 'viagem', 'e', 'jose', 'recolheu-se', 'a', 'ultima', 'noite', 'em', 'casa', 'do', 'irmao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas não\n",
      "pôde pregar olho, estava excitado, e a lembrança dos terríveis sucessos, que ultimamente se haviam\n",
      "dado com ele, nunca o apoquentara tanto'\n",
      "Tokens gerados: ['mas', 'naopode', 'pregar', 'olho', ',', 'estava', 'excitado', ',', 'e', 'a', 'lembranca', 'dos', 'terriveis', 'sucessos', ',', 'que', 'ultimamente', 'se', 'haviamdado', 'com', 'ele', ',', 'nunca', 'o', 'apoquentara', 'tanto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Levantou-se e começou a passear no quarto, a falar sozinho,\n",
      "nervoso, delirante, vendo surgir espectros de todos os lados'\n",
      "Tokens gerados: ['levantou-se', 'e', 'comecou', 'a', 'passear', 'no', 'quarto', ',', 'a', 'falar', 'sozinho', ',', 'nervoso', ',', 'delirante', ',', 'vendo', 'surgir', 'espectros', 'de', 'todos', 'os', 'lados']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pelas quatro horas da madrugada, Manuel, impressionado, porque, de todas as vezes que acordava,\n",
      "via luz no quarto do hóspede e ouvia-lhe o som dos passos trôpegos e vacilantes, e sentia-lhe os gemidos\n",
      "abafados e o vozear frouxo e doloroso, não se pôde ter e levantou-se'\n",
      "Tokens gerados: ['pelas', 'quatro', 'horas', 'da', 'madrugada', ',', 'manuel', ',', 'impressionado', ',', 'porque', ',', 'de', 'todas', 'as', 'vezes', 'que', 'acordava', ',', 'via', 'luz', 'no', 'quarto', 'do', 'hospede', 'e', 'ouvia-lhe', 'o', 'som', 'dos', 'passos', 'tropegos', 'e', 'vacilantes', ',', 'e', 'sentia-lhe', 'os', 'gemidosabafados', 'e', 'o', 'vozear', 'frouxo', 'e', 'doloroso', ',', 'nao', 'se', 'pode', 'ter', 'e', 'levantou-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Terá alguma coisa o José?'\n",
      "Tokens gerados: ['“', 'tera', 'alguma', 'coisa', 'o', 'jose', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "pensou ele, embrulhando-se no lençol e tomando aquela direção'\n",
      "Tokens gerados: ['”', 'pensou', 'ele', ',', 'embrulhando-se', 'no', 'lencol', 'e', 'tomando', 'aquela', 'direcao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A porta achava-se apenas no trinco,\n",
      "abriu-a devagar e entrou'\n",
      "Tokens gerados: ['a', 'porta', 'achava-se', 'apenas', 'no', 'trinco', ',', 'abriu-a', 'devagar', 'e', 'entrou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O viúvo, ao sentir alguém, voltou-se assombrado e, dando com o fantasma\n",
      "que lhe invadia a alcova, recuou de braços erguidos, entre gritos terror'\n",
      "Tokens gerados: ['o', 'viuvo', ',', 'ao', 'sentir', 'alguem', ',', 'voltou-se', 'assombrado', 'e', ',', 'dando', 'com', 'o', 'fantasmaque', 'lhe', 'invadia', 'a', 'alcova', ',', 'recuou', 'de', 'bracos', 'erguidos', ',', 'entre', 'gritos', 'terror']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel correu sobre ele; mas\n",
      "antes que se desse a conhecer, já o assassino de Quitéria havia caído desamparadamente no chão'\n",
      "Tokens gerados: ['manuel', 'correu', 'sobre', 'ele', 'masantes', 'que', 'se', 'desse', 'a', 'conhecer', ',', 'ja', 'o', 'assassino', 'de', 'quiteria', 'havia', 'caido', 'desamparadamente', 'no', 'chao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fez-se logo um grande motim por toda a casa, que era nesse tempo no Caminho Grande, e na\n",
      "qual os caixeiros do negociante ainda não moravam com o patrão'\n",
      "Tokens gerados: ['fez-se', 'logo', 'um', 'grande', 'motim', 'por', 'toda', 'a', 'casa', ',', 'que', 'era', 'nesse', 'tempo', 'no', 'caminho', 'grande', ',', 'e', 'naqual', 'os', 'caixeiros', 'do', 'negociante', 'ainda', 'nao', 'moravam', 'com', 'o', 'patrao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A boa Mariana acudiu pronta, cheia\n",
      "de zelo'\n",
      "Tokens gerados: ['a', 'boa', 'mariana', 'acudiu', 'pronta', ',', 'cheiade', 'zelo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Um escalda-pés! depressa!” dizia, apalpando os contraídos e volumosos pés do cunhado'\n",
      "Tokens gerados: ['“', 'um', 'escalda-pes', 'depressa', '”', 'dizia', ',', 'apalpando', 'os', 'contraidos', 'e', 'volumosos', 'pes', 'do', 'cunhado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tisanas, mezinhas de toda a espécie, foram lembradas; pôs-se em campo a medicina doméstica, e, daí\n",
      "a uma hora o desfalecido voltava a si'\n",
      "Tokens gerados: ['tisanas', ',', 'mezinhas', 'de', 'toda', 'a', 'especie', ',', 'foram', 'lembradas', 'pos-se', 'em', 'campo', 'a', 'medicina', 'domestica', ',', 'e', ',', 'daia', 'uma', 'hora', 'o', 'desfalecido', 'voltava', 'a', 'si']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas não pôde erguer-se: ficara muito prostrado'\n",
      "Tokens gerados: ['mas', 'nao', 'pode', 'erguer-se', 'ficara', 'muito', 'prostrado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'À síncope sobreveio-lhe uma febre violenta,\n",
      "que durou até à noite, quando chegou afinal o Jauffret'\n",
      "Tokens gerados: ['a', 'sincope', 'sobreveio-lhe', 'uma', 'febre', 'violenta', ',', 'que', 'durou', 'ate', 'a', 'noite', ',', 'quando', 'chegou', 'afinal', 'o', 'jauffret']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era uma febre gástrica, explicou este'\n",
      "Tokens gerados: ['era', 'uma', 'febre', 'gastrica', ',', 'explicou', 'este']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E mais: que a moléstia requeria certo cuidado — muito\n",
      "sossego de espírito! Nada de bulha, principalmente!\n",
      "José, malgrado a recomendação do médico, quis ver o filho'\n",
      "Tokens gerados: ['e', 'mais', 'que', 'a', 'molestia', 'requeria', 'certo', 'cuidado', '—', 'muitosossego', 'de', 'espirito', 'nada', 'de', 'bulha', ',', 'principalmentejose', ',', 'malgrado', 'a', 'recomendacao', 'do', 'medico', ',', 'quis', 'ver', 'o', 'filho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Abraçou-o soluçando, disse-lhe\n",
      "que estava para morrer'\n",
      "Tokens gerados: ['abracou-o', 'solucando', ',', 'disse-lheque', 'estava', 'para', 'morrer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E no outro dia ainda de cama, perfilhou-o; pediu um tabelião, fez testamento e,\n",
      "chorando, chamou Manuel para seu lado'\n",
      "Tokens gerados: ['e', 'no', 'outro', 'dia', 'ainda', 'de', 'cama', ',', 'perfilhou-o', 'pediu', 'um', 'tabeliao', ',', 'fez', 'testamento', 'e', ',', 'chorando', ',', 'chamou', 'manuel', 'para', 'seu', 'lado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Meu irmão, recomendou-lhe'\n",
      "Tokens gerados: ['—', 'meu', 'irmao', ',', 'recomendou-lhe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se eu for desta'\n",
      "Tokens gerados: ['se', 'eu', 'for', 'desta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'o que é possível, remete-me logo o pequeno\n",
      "para a casa do Peixoto em Lisboa'\n",
      "Tokens gerados: ['o', 'que', 'e', 'possivel', ',', 'remete-me', 'logo', 'o', 'pequenopara', 'a', 'casa', 'do', 'peixoto', 'em', 'lisboa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Terminou dizendo “que o queria — com muito saber — que o metessem num colégio de primeira\n",
      "sorte'\n",
      "Tokens gerados: ['terminou', 'dizendo', '“', 'que', 'o', 'queria', '—', 'com', 'muito', 'saber', '—', 'que', 'o', 'metessem', 'num', 'colegio', 'de', 'primeirasorte']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ficava aí bastante dinheiro'\n",
      "Tokens gerados: ['ficava', 'ai', 'bastante', 'dinheiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'não tivessem pena de gastar com o seu filho; que lhe dessem do\n",
      "melhor e do mais fino”'\n",
      "Tokens gerados: ['nao', 'tivessem', 'pena', 'de', 'gastar', 'com', 'o', 'seu', 'filho', 'que', 'lhe', 'dessem', 'domelhor', 'e', 'do', 'mais', 'fino', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estas coisas fizeram-no piorar; já todos os choravam como morto, e, pelos dias\n",
      "de mais risco, quando José delirava na sua febre, apareceu em casa do Manuel o pároco do Rosário;\n",
      "vinha muito solícito, saber do estado do seu amigo José “do seu irmão” dizia ele com uma grande\n",
      "piedade'\n",
      "Tokens gerados: ['estas', 'coisas', 'fizeram-no', 'piorar', 'ja', 'todos', 'os', 'choravam', 'como', 'morto', ',', 'e', ',', 'pelos', 'diasde', 'mais', 'risco', ',', 'quando', 'jose', 'delirava', 'na', 'sua', 'febre', ',', 'apareceu', 'em', 'casa', 'do', 'manuel', 'o', 'paroco', 'do', 'rosariovinha', 'muito', 'solicito', ',', 'saber', 'do', 'estado', 'do', 'seu', 'amigo', 'jose', '“', 'do', 'seu', 'irmao', '”', 'dizia', 'ele', 'com', 'uma', 'grandepiedade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E daí, não abandonava a casa'\n",
      "Tokens gerados: ['e', 'dai', ',', 'nao', 'abandonava', 'a', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prestava-se a um tudo, serviçal, discreto, às vezes choramingando\n",
      "porque lhe vedavam a entrada no quarto do enfermo'\n",
      "Tokens gerados: ['prestava-se', 'a', 'um', 'tudo', ',', 'servical', ',', 'discreto', ',', 'as', 'vezes', 'choramingandoporque', 'lhe', 'vedavam', 'a', 'entrada', 'no', 'quarto', 'do', 'enfermo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel e Mariana não se furtavam de apreciar\n",
      "aquela solicitude do bom padre, o interesse com que ele chegava todos os dias para pedir notícias do\n",
      "amigo'\n",
      "Tokens gerados: ['manuel', 'e', 'mariana', 'nao', 'se', 'furtavam', 'de', 'apreciaraquela', 'solicitude', 'do', 'bom', 'padre', ',', 'o', 'interesse', 'com', 'que', 'ele', 'chegava', 'todos', 'os', 'dias', 'para', 'pedir', 'noticias', 'doamigo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dispensavam-lhe um grande acolhimento; achavam-no meigo, jeitoso e simpático'\n",
      "Tokens gerados: ['dispensavam-lhe', 'um', 'grande', 'acolhimento', 'achavam-no', 'meigo', ',', 'jeitoso', 'e', 'simpatico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— É um santo homem! dizia Manuel convencido'\n",
      "Tokens gerados: ['—', 'e', 'um', 'santo', 'homem', 'dizia', 'manuel', 'convencido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mariana confirmava, acrescentando em voz baixa:\n",
      "— Por adulação não é, coitado! Todos sabem que o padre Diogo não precisa de migalhas!'\n",
      "Tokens gerados: ['mariana', 'confirmava', ',', 'acrescentando', 'em', 'voz', 'baixa—', 'por', 'adulacao', 'nao', 'e', ',', 'coitado', 'todos', 'sabem', 'que', 'o', 'padre', 'diogo', 'nao', 'precisa', 'de', 'migalhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— É remediado de fortuna, pois não! Mas, olhe, que sabe aplicar bem o que possui'\n",
      "Tokens gerados: ['—', 'e', 'remediado', 'de', 'fortuna', ',', 'pois', 'nao', 'mas', ',', 'olhe', ',', 'que', 'sabe', 'aplicar', 'bem', 'o', 'que', 'possui']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seguia-se uma longa resenha dos episódios louváveis da vida do santo vigário; citavam-se\n",
      "rasgos de abnegação, boas esmolas a criaturas desamparadas, perdões de ofensas graves, provas de\n",
      "amizade e provas de desinteresse'\n",
      "Tokens gerados: ['seguia-se', 'uma', 'longa', 'resenha', 'dos', 'episodios', 'louvaveis', 'da', 'vida', 'do', 'santo', 'vigario', 'citavam-serasgos', 'de', 'abnegacao', ',', 'boas', 'esmolas', 'a', 'criaturas', 'desamparadas', ',', 'perdoes', 'de', 'ofensas', 'graves', ',', 'provas', 'deamizade', 'e', 'provas', 'de', 'desinteresse']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Um santo! Um verdadeiro santo!”\n",
      "E assim foi o padre Diogo tomando pé em casa de Manuel e fazendo-se todo de lá'\n",
      "Tokens gerados: ['“', 'um', 'santo', 'um', 'verdadeiro', 'santo', '”', 'e', 'assim', 'foi', 'o', 'padre', 'diogo', 'tomando', 'pe', 'em', 'casa', 'de', 'manuel', 'e', 'fazendo-se', 'todo', 'de', 'la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Já contavam\n",
      "com ele para padrinho de Ana Rosa; esperavam-no todas as tardes com café, e à noite, nos serões da\n",
      "família, marido e mulher não perdiam ocasião de contar as boas pilhérias do senhor vigário, glorificar-lhe\n",
      "as virtudes religiosas e recomendá-lo às visitas como um excelente amigo e magnífico protetor'\n",
      "Tokens gerados: ['ja', 'contavamcom', 'ele', 'para', 'padrinho', 'de', 'ana', 'rosa', 'esperavam-no', 'todas', 'as', 'tardes', 'com', 'cafe', ',', 'e', 'a', 'noite', ',', 'nos', 'seroes', 'dafamilia', ',', 'marido', 'e', 'mulher', 'nao', 'perdiam', 'ocasiao', 'de', 'contar', 'as', 'boas', 'pilherias', 'do', 'senhor', 'vigario', ',', 'glorificar-lheas', 'virtudes', 'religiosas', 'e', 'recomenda-lo', 'as', 'visitas', 'como', 'um', 'excelente', 'amigo', 'e', 'magnifico', 'protetor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um\n",
      "dia, em que ele, como sempre, cheio de solicitude, perguntava pelo “seu doente” disseram-lhe que José\n",
      "estava livre de maior perigo e que o restabelecimento seria completo com a viagem à Europa'\n",
      "Tokens gerados: ['umdia', ',', 'em', 'que', 'ele', ',', 'como', 'sempre', ',', 'cheio', 'de', 'solicitude', ',', 'perguntava', 'pelo', '“', 'seu', 'doente', '”', 'disseram-lhe', 'que', 'joseestava', 'livre', 'de', 'maior', 'perigo', 'e', 'que', 'o', 'restabelecimento', 'seria', 'completo', 'com', 'a', 'viagem', 'a', 'europa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Diogo\n",
      "sorriu, aparentemente satisfeito; mas, se alguém lhe pudesse ouvir o que resmungava ao descer as\n",
      "escadas, ter-se-ia admirado de ouvir estas e outras frases:\n",
      "— Diabo!'\n",
      "Tokens gerados: ['diogosorriu', ',', 'aparentemente', 'satisfeito', 'mas', ',', 'se', 'alguem', 'lhe', 'pudesse', 'ouvir', 'o', 'que', 'resmungava', 'ao', 'descer', 'asescadas', ',', 'ter-se-ia', 'admirado', 'de', 'ouvir', 'estas', 'e', 'outras', 'frases—', 'diabo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Querem ver que ainda não se vai desta, o maldito?'\n",
      "Tokens gerados: ['querem', 'ver', 'que', 'ainda', 'nao', 'se', 'vai', 'desta', ',', 'o', 'maldito', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E eu, que já o tinha por\n",
      "despachado!'\n",
      "Tokens gerados: ['e', 'eu', ',', 'que', 'ja', 'o', 'tinha', 'pordespachado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No dia seguinte, dizia o velhaco ao futuro compadre: — Bom, agora que o nosso homem está\n",
      "livre de perigo, posso ir mais sossegado para a minha paróquia'\n",
      "Tokens gerados: ['no', 'dia', 'seguinte', ',', 'dizia', 'o', 'velhaco', 'ao', 'futuro', 'compadre', '—', 'bom', ',', 'agora', 'que', 'o', 'nosso', 'homem', 'estalivre', 'de', 'perigo', ',', 'posso', 'ir', 'mais', 'sossegado', 'para', 'a', 'minha', 'paroquia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Já não vou sem tempo!'\n",
      "Tokens gerados: ['ja', 'nao', 'vou', 'sem', 'tempo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E despediu-se, todo boas palavras e sorrisos angélicos, acompanhado pelas bênçãos da família'\n",
      "Tokens gerados: ['e', 'despediu-se', ',', 'todo', 'boas', 'palavras', 'e', 'sorrisos', 'angelicos', ',', 'acompanhado', 'pelas', 'bencaos', 'da', 'familia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Senhor vigário! gritou-lhe Mariana do patamar da escada'\n",
      "Tokens gerados: ['—', 'senhor', 'vigario', 'gritou-lhe', 'mariana', 'do', 'patamar', 'da', 'escada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não faça agora como os médicos,\n",
      "que só aparecem com as moléstias!'\n",
      "Tokens gerados: ['nao', 'faca', 'agora', 'como', 'os', 'medicos', ',', 'que', 'so', 'aparecem', 'com', 'as', 'molestias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seja cá de casa!\n",
      "—Venha de vez em quando, padre! acrescentou Manuel'\n",
      "Tokens gerados: ['seja', 'ca', 'de', 'casa—venha', 'de', 'vez', 'em', 'quando', ',', 'padre', 'acrescentou', 'manuel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Apareça!\n",
      "Diogo prometeu vagamente, e nesse mesmo dia atravessou o Boqueirão em demanda da sua\n",
      "freguesia'\n",
      "Tokens gerados: ['aparecadiogo', 'prometeu', 'vagamente', ',', 'e', 'nesse', 'mesmo', 'dia', 'atravessou', 'o', 'boqueirao', 'em', 'demanda', 'da', 'suafreguesia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Essa noite, nas salas de Manuel, só se conversou sobre as boas qualidades e os bons precedentes\n",
      "do estimado cura do Rosário'\n",
      "Tokens gerados: ['essa', 'noite', ',', 'nas', 'salas', 'de', 'manuel', ',', 'so', 'se', 'conversou', 'sobre', 'as', 'boas', 'qualidades', 'e', 'os', 'bons', 'precedentesdo', 'estimado', 'cura', 'do', 'rosario']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José, com geral contentamento dos de casa, convalescia prodigiosamente'\n",
      "Tokens gerados: ['jose', ',', 'com', 'geral', 'contentamento', 'dos', 'de', 'casa', ',', 'convalescia', 'prodigiosamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel e Mariana\n",
      "cercavam-no de afagos, desejosos por fazê-lo esquecer a imprudência da madrugada fatal, o que\n",
      "supunham, fosse o único motivo da moléstia; daí a coisa de um mês, o convalescente resolveu tornar à\n",
      "fazenda, a despeito das instâncias contrárias da cunhada e dos conselhos do irmão'\n",
      "Tokens gerados: ['manuel', 'e', 'marianacercavam-no', 'de', 'afagos', ',', 'desejosos', 'por', 'faze-lo', 'esquecer', 'a', 'imprudencia', 'da', 'madrugada', 'fatal', ',', 'o', 'quesupunham', ',', 'fosse', 'o', 'unico', 'motivo', 'da', 'molestia', 'dai', 'a', 'coisa', 'de', 'um', 'mes', ',', 'o', 'convalescente', 'resolveu', 'tornar', 'afazenda', ',', 'a', 'despeito', 'das', 'instancias', 'contrarias', 'da', 'cunhada', 'e', 'dos', 'conselhos', 'do', 'irmao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que vais lá fazer, homem de Deus? perguntava este'\n",
      "Tokens gerados: ['—', 'que', 'vais', 'la', 'fazer', ',', 'homem', 'de', 'deus', '?', 'perguntava', 'este']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se era por causa da Domingas, que\n",
      "diabo! fizesse-a vir! O melhor porém, segundo a sua fraca opinião, seria deixá-la lá onde estava'\n",
      "Tokens gerados: ['se', 'era', 'por', 'causa', 'da', 'domingas', ',', 'quediabo', 'fizesse-a', 'vir', 'o', 'melhor', 'porem', ',', 'segundo', 'a', 'sua', 'fraca', 'opiniao', ',', 'seria', 'deixa-la', 'la', 'onde', 'estava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma\n",
      "preta da roça, que nunca saiu do mato!'\n",
      "Tokens gerados: ['umapreta', 'da', 'roca', ',', 'que', 'nunca', 'saiu', 'do', 'mato']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não! não era isso! respondia o outro'\n",
      "Tokens gerados: ['nao', 'nao', 'era', 'isso', 'respondia', 'o', 'outro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas não iria para a terra, sem ter dado uma vista d’olhos\n",
      "ao Rosário!\n",
      "— Ao menos não vai só, José'\n",
      "Tokens gerados: ['mas', 'nao', 'iria', 'para', 'a', 'terra', ',', 'sem', 'ter', 'dado', 'uma', 'vista', 'd', '’', 'olhosao', 'rosario—', 'ao', 'menos', 'nao', 'vai', 'so', ',', 'jose']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eu posso acompanhar-te'\n",
      "Tokens gerados: ['eu', 'posso', 'acompanhar-te']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José agradeceu'\n",
      "Tokens gerados: ['jose', 'agradeceu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que já estava perfeitamente bom'\n",
      "Tokens gerados: ['que', 'ja', 'estava', 'perfeitamente', 'bom']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, em caso de necessidade, podia contar com\n",
      "os canoeiros, que eram todos seus homens'\n",
      "Tokens gerados: ['e', ',', 'em', 'caso', 'de', 'necessidade', ',', 'podia', 'contar', 'comos', 'canoeiros', ',', 'que', 'eram', 'todos', 'seus', 'homens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E dizia as inúmeras viagens que tinha feito até ali; contava episódios a respeito do Boqueirão'\n",
      "Tokens gerados: ['e', 'dizia', 'as', 'inumeras', 'viagens', 'que', 'tinha', 'feito', 'ate', 'ali', 'contava', 'episodios', 'a', 'respeito', 'do', 'boqueirao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“E que se deixassem disso! Não estivessem a fazer daquela viagem um bicho de sete cabeças!'\n",
      "Tokens gerados: ['“', 'e', 'que', 'se', 'deixassem', 'disso', 'nao', 'estivessem', 'a', 'fazer', 'daquela', 'viagem', 'um', 'bicho', 'de', 'sete', 'cabecas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Haviam\n",
      "de ver que, antes do fim do mês, estava ele de velas para Lisboa'\n",
      "Tokens gerados: ['haviamde', 'ver', 'que', ',', 'antes', 'do', 'fim', 'do', 'mes', ',', 'estava', 'ele', 'de', 'velas', 'para', 'lisboa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Partiu'\n",
      "Tokens gerados: ['”', 'partiu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A viagem correu-lhe estúpida, como de costume naquele tempo, em que o Maranhão\n",
      "ainda não tinha vapores'\n",
      "Tokens gerados: ['a', 'viagem', 'correu-lhe', 'estupida', ',', 'como', 'de', 'costume', 'naquele', 'tempo', ',', 'em', 'que', 'o', 'maranhaoainda', 'nao', 'tinha', 'vapores']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Demais, a sua fazenda era longe, muito dentro, a cinco léguas da vila'\n",
      "Tokens gerados: ['demais', ',', 'a', 'sua', 'fazenda', 'era', 'longe', ',', 'muito', 'dentro', ',', 'a', 'cinco', 'leguas', 'da', 'vila']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Urgia,\n",
      "por conseguinte, demorar-se aí algumas horas antes de internar-se no mato; comer, beber, tratar dos\n",
      "animais; arranjar condução e fazer a matalotagem'\n",
      "Tokens gerados: ['urgia', ',', 'por', 'conseguinte', ',', 'demorar-se', 'ai', 'algumas', 'horas', 'antes', 'de', 'internar-se', 'no', 'mato', 'comer', ',', 'beber', ',', 'tratar', 'dosanimais', 'arranjar', 'conducao', 'e', 'fazer', 'a', 'matalotagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os poucos familiarizados com tais caminhos tomam sempre, por precaução, um “pajem”, é este\n",
      "o nome que ali romanticamente se dá ao guia; e o pajem menos serve para guiar o viajante, que a\n",
      "estrada é boa, do que para lhe afugentar o terror dos mocambos, das onças e cobras de que falam com\n",
      "assombro os moradores do lugar'\n",
      "Tokens gerados: ['os', 'poucos', 'familiarizados', 'com', 'tais', 'caminhos', 'tomam', 'sempre', ',', 'por', 'precaucao', ',', 'um', '“', 'pajem', '”', ',', 'e', 'esteo', 'nome', 'que', 'ali', 'romanticamente', 'se', 'da', 'ao', 'guia', 'e', 'o', 'pajem', 'menos', 'serve', 'para', 'guiar', 'o', 'viajante', ',', 'que', 'aestrada', 'e', 'boa', ',', 'do', 'que', 'para', 'lhe', 'afugentar', 'o', 'terror', 'dos', 'mocambos', ',', 'das', 'oncas', 'e', 'cobras', 'de', 'que', 'falam', 'comassombro', 'os', 'moradores', 'do', 'lugar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não é tão infundado aquele terror: o sertão da província está cheio de mocambeiros, onde\n",
      "vivem os escravos fugidos com suas mulheres e seus filhos, formando uma grande família de malfeitores'\n",
      "Tokens gerados: ['nao', 'e', 'tao', 'infundado', 'aquele', 'terror', 'o', 'sertao', 'da', 'provincia', 'esta', 'cheio', 'de', 'mocambeiros', ',', 'ondevivem', 'os', 'escravos', 'fugidos', 'com', 'suas', 'mulheres', 'e', 'seus', 'filhos', ',', 'formando', 'uma', 'grande', 'familia', 'de', 'malfeitores']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Esses desgraçados, quando não podem ou não querem viver da caça, que é por lá muito abundante e de\n",
      "fácil venda na vila, lançam-se à rapinagem e atacam na estrada os viajantes; travando-se, às vezes, entre\n",
      "uns e outros, verdadeiras guerrilhas, em que ficam por terra muitas vítimas'\n",
      "Tokens gerados: ['esses', 'desgracados', ',', 'quando', 'nao', 'podem', 'ou', 'nao', 'querem', 'viver', 'da', 'caca', ',', 'que', 'e', 'por', 'la', 'muito', 'abundante', 'e', 'defacil', 'venda', 'na', 'vila', ',', 'lancam-se', 'a', 'rapinagem', 'e', 'atacam', 'na', 'estrada', 'os', 'viajantes', 'travando-se', ',', 'as', 'vezes', ',', 'entreuns', 'e', 'outros', ',', 'verdadeiras', 'guerrilhas', ',', 'em', 'que', 'ficam', 'por', 'terra', 'muitas', 'vitimas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José da Silva comprou na vila o que lhe convinha e seguiu, sem pajem para a fazenda'\n",
      "Tokens gerados: ['jose', 'da', 'silva', 'comprou', 'na', 'vila', 'o', 'que', 'lhe', 'convinha', 'e', 'seguiu', ',', 'sem', 'pajem', 'para', 'a', 'fazenda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah! Ele conhecia perfeitamente essas paragens!'\n",
      "Tokens gerados: ['ah', 'ele', 'conhecia', 'perfeitamente', 'essas', 'paragens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E quantas recordações não lhe despertavam aquelas carnaubeiras solitárias, aqueles pindovais\n",
      "ermos e silenciosos e aqueles trêmulos horizontes de verdura! Quantas vezes, perseguindo uma paca ou\n",
      "um veado, não atravessou ele, a galope, aqueles barrancos perigosos que se perdiam da estrada!\n",
      "Pungia-lhe agora deixar tudo isso; abandonar o encanto selvagem das florestas brasileiras'\n",
      "Tokens gerados: ['e', 'quantas', 'recordacoes', 'nao', 'lhe', 'despertavam', 'aquelas', 'carnaubeiras', 'solitarias', ',', 'aqueles', 'pindovaisermos', 'e', 'silenciosos', 'e', 'aqueles', 'tremulos', 'horizontes', 'de', 'verdura', 'quantas', 'vezes', ',', 'perseguindo', 'uma', 'paca', 'ouum', 'veado', ',', 'nao', 'atravessou', 'ele', ',', 'a', 'galope', ',', 'aqueles', 'barrancos', 'perigosos', 'que', 'se', 'perdiam', 'da', 'estradapungia-lhe', 'agora', 'deixar', 'tudo', 'isso', 'abandonar', 'o', 'encanto', 'selvagem', 'das', 'florestas', 'brasileiras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O\n",
      "europeu sentia-se americano, familiar às vozes misteriosas daqueles caités sempre verdejantes, habituado\n",
      "à companhia austera daquelas árvores seculares, às sestas preguiçosas da fazenda, ao viver amplo da\n",
      "roça, descalço, o peito nu, a rede embalada pela viração cheirosa das matas, o sono vigiado por escravos'\n",
      "Tokens gerados: ['oeuropeu', 'sentia-se', 'americano', ',', 'familiar', 'as', 'vozes', 'misteriosas', 'daqueles', 'caites', 'sempre', 'verdejantes', ',', 'habituadoa', 'companhia', 'austera', 'daquelas', 'arvores', 'seculares', ',', 'as', 'sestas', 'preguicosas', 'da', 'fazenda', ',', 'ao', 'viver', 'amplo', 'daroca', ',', 'descalco', ',', 'o', 'peito', 'nu', ',', 'a', 'rede', 'embalada', 'pela', 'viracao', 'cheirosa', 'das', 'matas', ',', 'o', 'sono', 'vigiado', 'por', 'escravos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E tinha de deixar tudo isso!\n",
      "“Para que negar? Havia de custar-lhe muito!” considerou ele, fazendo estacar o seu animal'\n",
      "Tokens gerados: ['e', 'tinha', 'de', 'deixar', 'tudo', 'isso', '“', 'para', 'que', 'negar', '?', 'havia', 'de', 'custar-lhe', 'muito', '”', 'considerou', 'ele', ',', 'fazendo', 'estacar', 'o', 'seu', 'animal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia andado quatro léguas e precisava comer alguma coisa'\n",
      "Tokens gerados: ['havia', 'andado', 'quatro', 'leguas', 'e', 'precisava', 'comer', 'alguma', 'coisa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No interior do Maranhão o viajante, de ordinário, “pousa” e come nas fazendas que vai\n",
      "encontrando pelo caminho, tanto que todas elas, contando já com isso, têm sempre cômodos especiais,\n",
      "destinados exclusivamente aos hóspedes adventícios; mas com José da Silva, que, aliás muitas e muitas\n",
      "vezes pernoitara em diversas e conhecia de perto a hospitalidade dos seus vizinhos, a coisa mudava\n",
      "agora de figura: não queria de forma alguma suportar a companhia de ninguém; receava que o\n",
      "interrogassem sobre a morte da mulher'\n",
      "Tokens gerados: ['no', 'interior', 'do', 'maranhao', 'o', 'viajante', ',', 'de', 'ordinario', ',', '“', 'pousa', '”', 'e', 'come', 'nas', 'fazendas', 'que', 'vaiencontrando', 'pelo', 'caminho', ',', 'tanto', 'que', 'todas', 'elas', ',', 'contando', 'ja', 'com', 'isso', ',', 'tem', 'sempre', 'comodos', 'especiais', ',', 'destinados', 'exclusivamente', 'aos', 'hospedes', 'adventicios', 'mas', 'com', 'jose', 'da', 'silva', ',', 'que', ',', 'alias', 'muitas', 'e', 'muitasvezes', 'pernoitara', 'em', 'diversas', 'e', 'conhecia', 'de', 'perto', 'a', 'hospitalidade', 'dos', 'seus', 'vizinhos', ',', 'a', 'coisa', 'mudavaagora', 'de', 'figura', 'nao', 'queria', 'de', 'forma', 'alguma', 'suportar', 'a', 'companhia', 'de', 'ninguem', 'receava', 'que', 'ointerrogassem', 'sobre', 'a', 'morte', 'da', 'mulher']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Preferiu pois jantar mesmo ao relento, e seguir logo sua viagem'\n",
      "Tokens gerados: ['preferiu', 'pois', 'jantar', 'mesmo', 'ao', 'relento', ',', 'e', 'seguir', 'logo', 'sua', 'viagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não obstante, ia já escurecendo, as cigarras estridulavam em coro; ouvia-se o lamentoso piar\n",
      "das rolas que se aninhavam para dormir; toda a natureza se embuçava em sombras, bocejando'\n",
      "Tokens gerados: ['nao', 'obstante', ',', 'ia', 'ja', 'escurecendo', ',', 'as', 'cigarras', 'estridulavam', 'em', 'coro', 'ouvia-se', 'o', 'lamentoso', 'piardas', 'rolas', 'que', 'se', 'aninhavam', 'para', 'dormir', 'toda', 'a', 'natureza', 'se', 'embucava', 'em', 'sombras', ',', 'bocejando']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Anoitecia lentamente'\n",
      "Tokens gerados: ['anoitecia', 'lentamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Então, José da Silva sentiu mais negra por dentro a sua viuvez; sentiu um grande desejo de\n",
      "chegar a casa, mas queria encontrar uma boa mesa, onde comesse e bebesse à vontade, como dantes;\n",
      "queria a sua cama larga, de casados, o seu cachimbo, o seu trajo de casa'\n",
      "Tokens gerados: ['entao', ',', 'jose', 'da', 'silva', 'sentiu', 'mais', 'negra', 'por', 'dentro', 'a', 'sua', 'viuvez', 'sentiu', 'um', 'grande', 'desejo', 'dechegar', 'a', 'casa', ',', 'mas', 'queria', 'encontrar', 'uma', 'boa', 'mesa', ',', 'onde', 'comesse', 'e', 'bebesse', 'a', 'vontade', ',', 'como', 'dantesqueria', 'a', 'sua', 'cama', 'larga', ',', 'de', 'casados', ',', 'o', 'seu', 'cachimbo', ',', 'o', 'seu', 'trajo', 'de', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah! Nada disso encontraria!'\n",
      "Tokens gerados: ['ah', 'nada', 'disso', 'encontraria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O quarto, em que ele, durante tantos anos, dormira feliz, devia\n",
      "ser àquela hora um ermo pavoroso; a cozinha devia estar gelada, os armários vazios, a horta murcha, os\n",
      "potes secos, o leito sem mulher!\n",
      "Que desconsolo!\n",
      "Apesar de tudo, sentia fundas saudades da esposa'\n",
      "Tokens gerados: ['o', 'quarto', ',', 'em', 'que', 'ele', ',', 'durante', 'tantos', 'anos', ',', 'dormira', 'feliz', ',', 'deviaser', 'aquela', 'hora', 'um', 'ermo', 'pavoroso', 'a', 'cozinha', 'devia', 'estar', 'gelada', ',', 'os', 'armarios', 'vazios', ',', 'a', 'horta', 'murcha', ',', 'ospotes', 'secos', ',', 'o', 'leito', 'sem', 'mulherque', 'desconsoloapesar', 'de', 'tudo', ',', 'sentia', 'fundas', 'saudades', 'da', 'esposa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Como o homem precisa de família!'\n",
      "Tokens gerados: ['—', 'como', 'o', 'homem', 'precisa', 'de', 'familia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'lamentava ele no isolamento'\n",
      "Tokens gerados: ['lamentava', 'ele', 'no', 'isolamento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah padre! Aquele maldito\n",
      "padre! E daí, quem sabe?'\n",
      "Tokens gerados: ['ah', 'padre', 'aquele', 'malditopadre', 'e', 'dai', ',', 'quem', 'sabe', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'se eu perdoasse?'\n",
      "Tokens gerados: ['se', 'eu', 'perdoasse', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'ela talvez se arrependesse e viesse ainda a dar uma boa\n",
      "companheira, virtuosa e dócil!'\n",
      "Tokens gerados: ['ela', 'talvez', 'se', 'arrependesse', 'e', 'viesse', 'ainda', 'a', 'dar', 'uma', 'boacompanheira', ',', 'virtuosa', 'e', 'docil']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas'\n",
      "Tokens gerados: ['mas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'e ele?'\n",
      "Tokens gerados: ['e', 'ele', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Oh nunca! Ele existiria! A dúvida continuava na mesma!\n",
      "Ele, só ele é que eu devia ter matado!\n",
      "E depois de refletir um instante:\n",
      "— Não! antes assim! Assim foi melhor!\n",
      "Esta conclusão, arrancada só pelo seu espírito religioso, foi seguida de um movimento rápido\n",
      "de esporas'\n",
      "Tokens gerados: ['oh', 'nunca', 'ele', 'existiria', 'a', 'duvida', 'continuava', 'na', 'mesmaele', ',', 'so', 'ele', 'e', 'que', 'eu', 'devia', 'ter', 'matadoe', 'depois', 'de', 'refletir', 'um', 'instante—', 'nao', 'antes', 'assim', 'assim', 'foi', 'melhoresta', 'conclusao', ',', 'arrancada', 'so', 'pelo', 'seu', 'espirito', 'religioso', ',', 'foi', 'seguida', 'de', 'um', 'movimento', 'rapidode', 'esporas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cavalo disparou'\n",
      "Tokens gerados: ['o', 'cavalo', 'disparou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fez-se então um correr vertiginoso, em que José, todo vergado sobre a\n",
      "sela, parecia dormir na cadeia do galope'\n",
      "Tokens gerados: ['fez-se', 'entao', 'um', 'correr', 'vertiginoso', ',', 'em', 'que', 'jose', ',', 'todo', 'vergado', 'sobre', 'asela', ',', 'parecia', 'dormir', 'na', 'cadeia', 'do', 'galope']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, de súbito, contraiu as rédeas e o animal estacou'\n",
      "Tokens gerados: ['mas', ',', 'de', 'subito', ',', 'contraiu', 'as', 'redeas', 'e', 'o', 'animal', 'estacou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cavaleiro torceu a cabeça, concheando a mão atrás da orelha'\n",
      "Tokens gerados: ['o', 'cavaleiro', 'torceu', 'a', 'cabeca', ',', 'concheando', 'a', 'mao', 'atras', 'da', 'orelha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vinha de longe uma toada\n",
      "estranha de vozes sussurrantes, e um confuso tropel de cavalgaduras'\n",
      "Tokens gerados: ['vinha', 'de', 'longe', 'uma', 'toadaestranha', 'de', 'vozes', 'sussurrantes', ',', 'e', 'um', 'confuso', 'tropel', 'de', 'cavalgaduras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A noite exalava da floresta'\n",
      "Tokens gerados: ['a', 'noite', 'exalava', 'da', 'floresta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sentiam-se ainda as derradeiras claridades do dia e já também um\n",
      "crescente acumular de sombras'\n",
      "Tokens gerados: ['sentiam-se', 'ainda', 'as', 'derradeiras', 'claridades', 'do', 'dia', 'e', 'ja', 'tambem', 'umcrescente', 'acumular', 'de', 'sombras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A lua erguia-se, brilhando com a altivez de um novo monarca que\n",
      "inspeciona os seus domínios, e o céu ainda estava todo ensangüentado da púrpura do último sol, que\n",
      "fugia no horizonte trêmulo, como um rei expulso e envergonhado'\n",
      "Tokens gerados: ['a', 'lua', 'erguia-se', ',', 'brilhando', 'com', 'a', 'altivez', 'de', 'um', 'novo', 'monarca', 'queinspeciona', 'os', 'seus', 'dominios', ',', 'e', 'o', 'ceu', 'ainda', 'estava', 'todo', 'ensanguentado', 'da', 'purpura', 'do', 'ultimo', 'sol', ',', 'quefugia', 'no', 'horizonte', 'tremulo', ',', 'como', 'um', 'rei', 'expulso', 'e', 'envergonhado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José da Silva, entregue todo aos seus tormentos, assistia, sem apreciar, ao espetáculo maravilhoso\n",
      "de um crepúsculo de verão no extremo norte do Brasil'\n",
      "Tokens gerados: ['jose', 'da', 'silva', ',', 'entregue', 'todo', 'aos', 'seus', 'tormentos', ',', 'assistia', ',', 'sem', 'apreciar', ',', 'ao', 'espetaculo', 'maravilhosode', 'um', 'crepusculo', 'de', 'verao', 'no', 'extremo', 'norte', 'do', 'brasil']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O sol descambava no ocaso, retocando de tons quentes e vigorosos, com a minuciosidade de um\n",
      "pintor flamengo, tudo aquilo que o cercava'\n",
      "Tokens gerados: ['o', 'sol', 'descambava', 'no', 'ocaso', ',', 'retocando', 'de', 'tons', 'quentes', 'e', 'vigorosos', ',', 'com', 'a', 'minuciosidade', 'de', 'umpintor', 'flamengo', ',', 'tudo', 'aquilo', 'que', 'o', 'cercava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desse lado, montes e vales tinham orlas de ouro; era tudo\n",
      "vermelho e esfogueado: ao passo que, do ponto contrário, lhe opunha o luar o doce contraste da sua luz\n",
      "argentina e fresca, debuxando contra o horizonte o trêmulo e duvidoso perfil das carnaubeiras e dos\n",
      "pindovais'\n",
      "Tokens gerados: ['desse', 'lado', ',', 'montes', 'e', 'vales', 'tinham', 'orlas', 'de', 'ouro', 'era', 'tudovermelho', 'e', 'esfogueado', 'ao', 'passo', 'que', ',', 'do', 'ponto', 'contrario', ',', 'lhe', 'opunha', 'o', 'luar', 'o', 'doce', 'contraste', 'da', 'sua', 'luzargentina', 'e', 'fresca', ',', 'debuxando', 'contra', 'o', 'horizonte', 'o', 'tremulo', 'e', 'duvidoso', 'perfil', 'das', 'carnaubeiras', 'e', 'dospindovais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Destas bandas, no conflito boreal daquelas duas luzes inimigas, um grupo mal-definido e\n",
      "rumoroso agitava-se e crescia progressivamente'\n",
      "Tokens gerados: ['destas', 'bandas', ',', 'no', 'conflito', 'boreal', 'daquelas', 'duas', 'luzes', 'inimigas', ',', 'um', 'grupo', 'mal-definido', 'erumoroso', 'agitava-se', 'e', 'crescia', 'progressivamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era uma caravana de ciganos que se aproximava'\n",
      "Tokens gerados: ['era', 'uma', 'caravana', 'de', 'ciganos', 'que', 'se', 'aproximava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vinha lentamente, com o passo frouxo de uma boiada'\n",
      "Tokens gerados: ['vinha', 'lentamente', ',', 'com', 'o', 'passo', 'frouxo', 'de', 'uma', 'boiada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na solidão tristonha e sombria da floresta\n",
      "iam-se pouco a pouco distinguindo vozes de tons diversos e acentuavam-se grupo de homens, mulheres\n",
      "e crianças, de todas as cores e de todas as idades, cavalgando magníficos animais'\n",
      "Tokens gerados: ['na', 'solidao', 'tristonha', 'e', 'sombria', 'da', 'florestaiam-se', 'pouco', 'a', 'pouco', 'distinguindo', 'vozes', 'de', 'tons', 'diversos', 'e', 'acentuavam-se', 'grupo', 'de', 'homens', ',', 'mulherese', 'criancas', ',', 'de', 'todas', 'as', 'cores', 'e', 'de', 'todas', 'as', 'idades', ',', 'cavalgando', 'magnificos', 'animais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uns cantavam ao\n",
      "embalo monótono da besta; outros tocavam viola; esta acalentava o filho, aquela repetia as modas que\n",
      "lhe ensinara a gajoa'\n",
      "Tokens gerados: ['uns', 'cantavam', 'aoembalo', 'monotono', 'da', 'besta', 'outros', 'tocavam', 'viola', 'esta', 'acalentava', 'o', 'filho', ',', 'aquela', 'repetia', 'as', 'modas', 'quelhe', 'ensinara', 'a', 'gajoa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Viam-se moços, de calça e quinzena, cabelos grandes, o ar indolente, o cachimbo\n",
      "ao canto da boca, o olhar vago e cheio de volúpia, ao lado de raparigas fortes, queimadas do sol, com as\n",
      "melenas muito negras e lisas escorrendo sobre a opulência das espáduas'\n",
      "Tokens gerados: ['viam-se', 'mocos', ',', 'de', 'calca', 'e', 'quinzena', ',', 'cabelos', 'grandes', ',', 'o', 'ar', 'indolente', ',', 'o', 'cachimboao', 'canto', 'da', 'boca', ',', 'o', 'olhar', 'vago', 'e', 'cheio', 'de', 'volupia', ',', 'ao', 'lado', 'de', 'raparigas', 'fortes', ',', 'queimadas', 'do', 'sol', ',', 'com', 'asmelenas', 'muito', 'negras', 'e', 'lisas', 'escorrendo', 'sobre', 'a', 'opulencia', 'das', 'espaduas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sentavam-se à moda de odaliscas\n",
      "em volumosas trouxas, que serviam, a um tempo, de alforje e de sela'\n",
      "Tokens gerados: ['sentavam-se', 'a', 'moda', 'de', 'odaliscasem', 'volumosas', 'trouxas', ',', 'que', 'serviam', ',', 'a', 'um', 'tempo', ',', 'de', 'alforje', 'e', 'de', 'sela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Algumas delas traziam filhos ao\n",
      "colo ou na garupa do cavalo'\n",
      "Tokens gerados: ['algumas', 'delas', 'traziam', 'filhos', 'aocolo', 'ou', 'na', 'garupa', 'do', 'cavalo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, lenta e pesadamente, a caravana dos ciganos se aproximava'\n",
      "Tokens gerados: ['e', ',', 'lenta', 'e', 'pesadamente', ',', 'a', 'caravana', 'dos', 'ciganos', 'se', 'aproximava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José escondeu-se no mato, para\n",
      "a ver passar'\n",
      "Tokens gerados: ['jose', 'escondeu-se', 'no', 'mato', ',', 'paraa', 'ver', 'passar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Com certeza vinha enxotada de alguma fazenda, porque o chefe, um velho membrudo, de grandes\n",
      "barbas brancas, olhos cor de fumo, cavados e sombrios, mas irrequietos e vivos, erguia, de vez em\n",
      "quando, o braço e ameaçava o poente:\n",
      "— Jacarés te piquem diabo! Atravessado tu sejas na boca de um bacamarte!\n",
      "E a voz rouca e profunda do ancião perdia-se na floresta'\n",
      "Tokens gerados: ['com', 'certeza', 'vinha', 'enxotada', 'de', 'alguma', 'fazenda', ',', 'porque', 'o', 'chefe', ',', 'um', 'velho', 'membrudo', ',', 'de', 'grandesbarbas', 'brancas', ',', 'olhos', 'cor', 'de', 'fumo', ',', 'cavados', 'e', 'sombrios', ',', 'mas', 'irrequietos', 'e', 'vivos', ',', 'erguia', ',', 'de', 'vez', 'emquando', ',', 'o', 'braco', 'e', 'ameacava', 'o', 'poente—', 'jacares', 'te', 'piquem', 'diabo', 'atravessado', 'tu', 'sejas', 'na', 'boca', 'de', 'um', 'bacamartee', 'a', 'voz', 'rouca', 'e', 'profunda', 'do', 'anciao', 'perdia-se', 'na', 'floresta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Meio deitada nas pernas dele, cingindo-lhe a cintura, uma mulher bela, o colo nu e fresco, a\n",
      "garganta lisa e carnuda, procurava, com o olhar muito mole de uma ternura úmida e escrava, diminuir-lhe\n",
      "a cólera'\n",
      "Tokens gerados: ['meio', 'deitada', 'nas', 'pernas', 'dele', ',', 'cingindo-lhe', 'a', 'cintura', ',', 'uma', 'mulher', 'bela', ',', 'o', 'colo', 'nu', 'e', 'fresco', ',', 'agarganta', 'lisa', 'e', 'carnuda', ',', 'procurava', ',', 'com', 'o', 'olhar', 'muito', 'mole', 'de', 'uma', 'ternura', 'umida', 'e', 'escrava', ',', 'diminuir-lhea', 'colera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E a caravana, iluminada pelos últimos raios da claridade poente, foi passando'\n",
      "Tokens gerados: ['e', 'a', 'caravana', ',', 'iluminada', 'pelos', 'ultimos', 'raios', 'da', 'claridade', 'poente', ',', 'foi', 'passando']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E a pouco e\n",
      "pouco o sussurrar das vozes foi se perdendo no tristonho murmúrio das matas, como no horizonte se\n",
      "perdia a última réstia de luz vermelha'\n",
      "Tokens gerados: ['e', 'a', 'pouco', 'epouco', 'o', 'sussurrar', 'das', 'vozes', 'foi', 'se', 'perdendo', 'no', 'tristonho', 'murmurio', 'das', 'matas', ',', 'como', 'no', 'horizonte', 'seperdia', 'a', 'ultima', 'restia', 'de', 'luz', 'vermelha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em breve, tudo recaiu no silêncio primitivo, e a lua, do alto, baldeava com a sua luz misteriosa\n",
      "e triste a solidão das clareiras'\n",
      "Tokens gerados: ['em', 'breve', ',', 'tudo', 'recaiu', 'no', 'silencio', 'primitivo', ',', 'e', 'a', 'lua', ',', 'do', 'alto', ',', 'baldeava', 'com', 'a', 'sua', 'luz', 'misteriosae', 'triste', 'a', 'solidao', 'das', 'clareiras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José ficou imóvel, pensativo, perdido num desgosto invencível'\n",
      "Tokens gerados: ['jose', 'ficou', 'imovel', ',', 'pensativo', ',', 'perdido', 'num', 'desgosto', 'invencivel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O espetáculo daquele velho\n",
      "boêmio, abraçado a uma mulher bonita e sem dúvida fiel, mordia-o por dentro com o dente mais agudo\n",
      "da inveja'\n",
      "Tokens gerados: ['o', 'espetaculo', 'daquele', 'velhoboemio', ',', 'abracado', 'a', 'uma', 'mulher', 'bonita', 'e', 'sem', 'duvida', 'fiel', ',', 'mordia-o', 'por', 'dentro', 'com', 'o', 'dente', 'mais', 'agudoda', 'inveja']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Aquele, um vagabundo, um miserável, sem lar, sem dinheiro, sem mocidade ao menos,\n",
      "tinha contudo nesta vida uma fêmea que o acarinhava e seguia como escrava; ao passo que ele, ali, no\n",
      "meio do campo, desacompanhado, inteiramente esquecido, chorava, porque lhe arrancaram tudo, tudo\n",
      "— a casa, a mulher e a felicidade!” E depois pela associação natural das idéias, punha-se a lembrar do\n",
      "rosto pálido de Diogo'\n",
      "Tokens gerados: ['“', 'aquele', ',', 'um', 'vagabundo', ',', 'um', 'miseravel', ',', 'sem', 'lar', ',', 'sem', 'dinheiro', ',', 'sem', 'mocidade', 'ao', 'menos', ',', 'tinha', 'contudo', 'nesta', 'vida', 'uma', 'femea', 'que', 'o', 'acarinhava', 'e', 'seguia', 'como', 'escrava', 'ao', 'passo', 'que', 'ele', ',', 'ali', ',', 'nomeio', 'do', 'campo', ',', 'desacompanhado', ',', 'inteiramente', 'esquecido', ',', 'chorava', ',', 'porque', 'lhe', 'arrancaram', 'tudo', ',', 'tudo—', 'a', 'casa', ',', 'a', 'mulher', 'e', 'a', 'felicidade', '”', 'e', 'depois', 'pela', 'associacao', 'natural', 'das', 'ideias', ',', 'punha-se', 'a', 'lembrar', 'dorosto', 'palido', 'de', 'diogo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A despeito do ódio que lhe votava, achava-o bonito, com o seu cabelo todo\n",
      "anelado, o sorriso terno e piedoso, olhos e lábios de uma expressão sensual e ao mesmo tempo religiosa'\n",
      "Tokens gerados: ['a', 'despeito', 'do', 'odio', 'que', 'lhe', 'votava', ',', 'achava-o', 'bonito', ',', 'com', 'o', 'seu', 'cabelo', 'todoanelado', ',', 'o', 'sorriso', 'terno', 'e', 'piedoso', ',', 'olhos', 'e', 'labios', 'de', 'uma', 'expressao', 'sensual', 'e', 'ao', 'mesmo', 'tempo', 'religiosa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Este contraste devia por força agradar às mulheres, vencê-las pelos mistérios, pelo incognoscível'\n",
      "Tokens gerados: ['este', 'contraste', 'devia', 'por', 'forca', 'agradar', 'as', 'mulheres', ',', 'vence-las', 'pelos', 'misterios', ',', 'pelo', 'incognoscivel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E\n",
      "chorava, chorava cada vez mais'\n",
      "Tokens gerados: ['echorava', ',', 'chorava', 'cada', 'vez', 'mais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Como eles não se amariam!'\n",
      "Tokens gerados: ['“', 'como', 'eles', 'nao', 'se', 'amariam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quanto prazer não teriam desfrutado!'\n",
      "Tokens gerados: ['quanto', 'prazer', 'nao', 'teriam', 'desfrutado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“\n",
      "Instintivamente comparava-se ao padre e, cheio de raiva, de inveja, reconhecia-se inferior'\n",
      "Tokens gerados: ['“', 'instintivamente', 'comparava-se', 'ao', 'padre', 'e', ',', 'cheio', 'de', 'raiva', ',', 'de', 'inveja', ',', 'reconhecia-se', 'inferior']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De\n",
      "repente, veio-lhe esta idéia:\n",
      "“E se eu o matasse?'\n",
      "Tokens gerados: ['derepente', ',', 'veio-lhe', 'esta', 'ideia', '“', 'e', 'se', 'eu', 'o', 'matasse', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Repeliu-a logo, sem querer nem ao menos escutá-la; mas a idéia não ia e agarrava-se-lhe ao\n",
      "cérebro, com uma obstinação de parasita'\n",
      "Tokens gerados: ['”', 'repeliu-a', 'logo', ',', 'sem', 'querer', 'nem', 'ao', 'menos', 'escuta-la', 'mas', 'a', 'ideia', 'nao', 'ia', 'e', 'agarrava-se-lhe', 'aocerebro', ',', 'com', 'uma', 'obstinacao', 'de', 'parasita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Então, vieram-lhe à lembrança, sob uma reminiscência lúcida e saudosa — o seu casamento, os\n",
      "sobressaltos felizes do noivado, o namoro de Quitéria'\n",
      "Tokens gerados: ['entao', ',', 'vieram-lhe', 'a', 'lembranca', ',', 'sob', 'uma', 'reminiscencia', 'lucida', 'e', 'saudosa', '—', 'o', 'seu', 'casamento', ',', 'ossobressaltos', 'felizes', 'do', 'noivado', ',', 'o', 'namoro', 'de', 'quiteria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tudo isso nunca lhe pareceu tão bom, tão apetecível\n",
      "como naquele momento'\n",
      "Tokens gerados: ['tudo', 'isso', 'nunca', 'lhe', 'pareceu', 'tao', 'bom', ',', 'tao', 'apetecivelcomo', 'naquele', 'momento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Agora, descobria na mulher virtudes e belas qualidades, para as quais nunca\n",
      "atentara dantes'\n",
      "Tokens gerados: ['agora', ',', 'descobria', 'na', 'mulher', 'virtudes', 'e', 'belas', 'qualidades', ',', 'para', 'as', 'quais', 'nuncaatentara', 'dantes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Seria eu o culpado de tudo?'\n",
      "Tokens gerados: ['“', 'seria', 'eu', 'o', 'culpado', 'de', 'tudo', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não teria cumprido com os meus deveres de bom esposo?'\n",
      "Tokens gerados: ['nao', 'teria', 'cumprido', 'com', 'os', 'meus', 'deveres', 'de', 'bom', 'esposo', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seriam insuficientes os meus carinhos?'\n",
      "Tokens gerados: ['seriam', 'insuficientes', 'os', 'meus', 'carinhos', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” interrogava ele à própria consciência; esta respondia\n",
      "opondo-lhe dúvidas que valiam acusações'\n",
      "Tokens gerados: ['”', 'interrogava', 'ele', 'a', 'propria', 'consciencia', 'esta', 'respondiaopondo-lhe', 'duvidas', 'que', 'valiam', 'acusacoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ele defendia-se, explicava os fatos, citava provas em favor,\n",
      "lembrava a sua dedicação e a sua amizade pela defunta; mas a maldita rezingueira não se acomodava e\n",
      "não aceitava razões'\n",
      "Tokens gerados: ['ele', 'defendia-se', ',', 'explicava', 'os', 'fatos', ',', 'citava', 'provas', 'em', 'favor', ',', 'lembrava', 'a', 'sua', 'dedicacao', 'e', 'a', 'sua', 'amizade', 'pela', 'defunta', 'mas', 'a', 'maldita', 'rezingueira', 'nao', 'se', 'acomodava', 'enao', 'aceitava', 'razoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E José abriu a chorar como um perdido'\n",
      "Tokens gerados: ['e', 'jose', 'abriu', 'a', 'chorar', 'como', 'um', 'perdido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Surpreendeu-se neste estado; quis fugir de si mesmo, e cravou as esporas no cavalo'\n",
      "Tokens gerados: ['surpreendeu-se', 'neste', 'estado', 'quis', 'fugir', 'de', 'si', 'mesmo', ',', 'e', 'cravou', 'as', 'esporas', 'no', 'cavalo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Correu\n",
      "muito, à rédea solta como se fugira perseguido pela própria sombra'\n",
      "Tokens gerados: ['correumuito', ',', 'a', 'redea', 'solta', 'como', 'se', 'fugira', 'perseguido', 'pela', 'propria', 'sombra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“E se eu o matasse?'\n",
      "Tokens gerados: ['“', 'e', 'se', 'eu', 'o', 'matasse', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Era a maldita idéia que vinha de novo à superfície dos seus pensamentos'\n",
      "Tokens gerados: ['”', 'era', 'a', 'maldita', 'ideia', 'que', 'vinha', 'de', 'novo', 'a', 'superficie', 'dos', 'seus', 'pensamentos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Não! Não!” E ele a repelia de novo empurrando-a para o fundo da sua imaginação, como o\n",
      "assassino que repele no mar o cadáver da sua vítima; ela mergulhava com o impulso, mas logo reaparecia,\n",
      "boiando'\n",
      "Tokens gerados: ['“', 'nao', 'nao', '”', 'e', 'ele', 'a', 'repelia', 'de', 'novo', 'empurrando-a', 'para', 'o', 'fundo', 'da', 'sua', 'imaginacao', ',', 'como', 'oassassino', 'que', 'repele', 'no', 'mar', 'o', 'cadaver', 'da', 'sua', 'vitima', 'ela', 'mergulhava', 'com', 'o', 'impulso', ',', 'mas', 'logo', 'reaparecia', ',', 'boiando']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“E se eu o matasse?'\n",
      "Tokens gerados: ['“', 'e', 'se', 'eu', 'o', 'matasse', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "— Não! não! exclamou, desferindo um grito no silêncio da floresta'\n",
      "Tokens gerados: ['”', '—', 'nao', 'nao', 'exclamou', ',', 'desferindo', 'um', 'grito', 'no', 'silencio', 'da', 'floresta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Já basta a outra!\n",
      "E assanhavam-se-lhe os remorsos'\n",
      "Tokens gerados: ['ja', 'basta', 'a', 'outrae', 'assanhavam-se-lhe', 'os', 'remorsos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nesse momento uma nuvem escondera a lua'\n",
      "Tokens gerados: ['nesse', 'momento', 'uma', 'nuvem', 'escondera', 'a', 'lua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Espectros surgiam no caminho; José suava e\n",
      "tremia sobre a sela; o mais leve mexer de galhos eriçava-lhe os cabelos'\n",
      "Tokens gerados: ['espectros', 'surgiam', 'no', 'caminho', 'jose', 'suava', 'etremia', 'sobre', 'a', 'sela', 'o', 'mais', 'leve', 'mexer', 'de', 'galhos', 'ericava-lhe', 'os', 'cabelos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No entanto — corria'\n",
      "Tokens gerados: ['no', 'entanto', '—', 'corria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pouco lhe faltava já para chegar à fazenda, muito pouco, uma miserável distância, e, contudo,\n",
      "mais lhe custava esse pouco do que todo o resto da viagem'\n",
      "Tokens gerados: ['pouco', 'lhe', 'faltava', 'ja', 'para', 'chegar', 'a', 'fazenda', ',', 'muito', 'pouco', ',', 'uma', 'miseravel', 'distancia', ',', 'e', ',', 'contudo', ',', 'mais', 'lhe', 'custava', 'esse', 'pouco', 'do', 'que', 'todo', 'o', 'resto', 'da', 'viagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fechou os olhos e deixou que o cavalo\n",
      "corresse à toa, galopando ruidosamente na terra úmida de orvalho'\n",
      "Tokens gerados: ['fechou', 'os', 'olhos', 'e', 'deixou', 'que', 'o', 'cavalocorresse', 'a', 'toa', ',', 'galopando', 'ruidosamente', 'na', 'terra', 'umida', 'de', 'orvalho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ele ofegava, acossado por fantasmas'\n",
      "Tokens gerados: ['ele', 'ofegava', ',', 'acossado', 'por', 'fantasmas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Via a sua vítima, com a boca muito aberta, os olhos convulsos, a falar-lhe coisas estranhas numa voz de\n",
      "moribunda, a língua de fora, enorme e negra, entre gorgolhões de sangue'\n",
      "Tokens gerados: ['via', 'a', 'sua', 'vitima', ',', 'com', 'a', 'boca', 'muito', 'aberta', ',', 'os', 'olhos', 'convulsos', ',', 'a', 'falar-lhe', 'coisas', 'estranhas', 'numa', 'voz', 'demoribunda', ',', 'a', 'lingua', 'de', 'fora', ',', 'enorme', 'e', 'negra', ',', 'entre', 'gorgolhoes', 'de', 'sangue']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E via também surgir aquele\n",
      "padre infame, bater-lhe no ombro, apresentar-lhe, sorrindo, um alvitre, propor uma condição e passar\n",
      "logo à ameaça brutal: “Tenho-te na mão, assassino! Se quiseres punir-me, entrego-te à justiça! “\n",
      "E José gritou, como doido, soluçando:\n",
      "— E eu aceitei, diabo! Eu aceitei!\n",
      "Nisto, o cavalo acuou'\n",
      "Tokens gerados: ['e', 'via', 'tambem', 'surgir', 'aquelepadre', 'infame', ',', 'bater-lhe', 'no', 'ombro', ',', 'apresentar-lhe', ',', 'sorrindo', ',', 'um', 'alvitre', ',', 'propor', 'uma', 'condicao', 'e', 'passarlogo', 'a', 'ameaca', 'brutal', '“', 'tenho-te', 'na', 'mao', ',', 'assassino', 'se', 'quiseres', 'punir-me', ',', 'entrego-te', 'a', 'justica', '“', 'e', 'jose', 'gritou', ',', 'como', 'doido', ',', 'solucando—', 'e', 'eu', 'aceitei', ',', 'diabo', 'eu', 'aceiteinisto', ',', 'o', 'cavalo', 'acuou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um vulto negro agitou-se por detrás do tronco de um ingazeiro, e uma\n",
      "bala, seguida pela detonação de um tiro, varou o peito de José da Silva'\n",
      "Tokens gerados: ['um', 'vulto', 'negro', 'agitou-se', 'por', 'detras', 'do', 'tronco', 'de', 'um', 'ingazeiro', ',', 'e', 'umabala', ',', 'seguida', 'pela', 'detonacao', 'de', 'um', 'tiro', ',', 'varou', 'o', 'peito', 'de', 'jose', 'da', 'silva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os negros de São Brás viram aparecer lá o animal às soltas, e todo salpicado de sangue, tinham\n",
      "ouvido um tiro para as bandas da estrada, correram todos nessa direção à procura da vítima'\n",
      "Tokens gerados: ['os', 'negros', 'de', 'sao', 'bras', 'viram', 'aparecer', 'la', 'o', 'animal', 'as', 'soltas', ',', 'e', 'todo', 'salpicado', 'de', 'sangue', ',', 'tinhamouvido', 'um', 'tiro', 'para', 'as', 'bandas', 'da', 'estrada', ',', 'correram', 'todos', 'nessa', 'direcao', 'a', 'procura', 'da', 'vitima']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi Domingas quem a descobriu, e, num delírio, precipitou-se contra o cadáver, a beijar-lhe as\n",
      "mãos e as faces'\n",
      "Tokens gerados: ['foi', 'domingas', 'quem', 'a', 'descobriu', ',', 'e', ',', 'num', 'delirio', ',', 'precipitou-se', 'contra', 'o', 'cadaver', ',', 'a', 'beijar-lhe', 'asmaos', 'e', 'as', 'faces']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Meu senhor! meu querido! meus amores! exclamava ela, a soluçar convulsivamente'\n",
      "Tokens gerados: ['—', 'meu', 'senhor', 'meu', 'querido', 'meus', 'amores', 'exclamava', 'ela', ',', 'a', 'solucar', 'convulsivamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, tomada de uma idéia súbita, ergueu-se, e gritou, apontando vagamente para o lado da vila'\n",
      "Tokens gerados: ['mas', ',', 'tomada', 'de', 'uma', 'ideia', 'subita', ',', 'ergueu-se', ',', 'e', 'gritou', ',', 'apontando', 'vagamente', 'para', 'o', 'lado', 'da', 'vila']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Foi ele! Não foi outro! Foi aquele malvado! Foi aquele padre do diabo!\n",
      "E pôs-se a rir e a dançar, batendo palmas e cantando'\n",
      "Tokens gerados: ['—', 'foi', 'ele', 'nao', 'foi', 'outro', 'foi', 'aquele', 'malvado', 'foi', 'aquele', 'padre', 'do', 'diaboe', 'pos-se', 'a', 'rir', 'e', 'a', 'dancar', ',', 'batendo', 'palmas', 'e', 'cantando']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era a loucura que voltava'\n",
      "Tokens gerados: ['era', 'a', 'loucura', 'que', 'voltava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O crime foi atribuído aos mocambeiros e o corpo de José da Silva enterrado junto à sepultura da\n",
      "mulher, ao lado da capela, que principiava a desmoronar com a míngua dos antigos cuidados'\n",
      "Tokens gerados: ['o', 'crime', 'foi', 'atribuido', 'aos', 'mocambeiros', 'e', 'o', 'corpo', 'de', 'jose', 'da', 'silva', 'enterrado', 'junto', 'a', 'sepultura', 'damulher', ',', 'ao', 'lado', 'da', 'capela', ',', 'que', 'principiava', 'a', 'desmoronar', 'com', 'a', 'mingua', 'dos', 'antigos', 'cuidados']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A fazenda aos poucos se converteu em tapera, e lendas e superstições de todo o gênero se\n",
      "inventaram para explicar-lhe o abandono'\n",
      "Tokens gerados: ['a', 'fazenda', 'aos', 'poucos', 'se', 'converteu', 'em', 'tapera', ',', 'e', 'lendas', 'e', 'supersticoes', 'de', 'todo', 'o', 'genero', 'seinventaram', 'para', 'explicar-lhe', 'o', 'abandono']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O vigário do lugar, pessoa insuspeita e criteriosa, nem só\n",
      "confirmava o que diziam, como aconselhava a que não fossem lá'\n",
      "Tokens gerados: ['o', 'vigario', 'do', 'lugar', ',', 'pessoa', 'insuspeita', 'e', 'criteriosa', ',', 'nem', 'soconfirmava', 'o', 'que', 'diziam', ',', 'como', 'aconselhava', 'a', 'que', 'nao', 'fossem', 'la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '‘’Aquilo eram terras amaldiçoadas!”\n",
      "Anos depois, contavam que nas ruínas de São Brás vivia uma preta feiticeira, que, por alta\n",
      "noite, saía pelos campos a imitar o canto da mãe-da-lua'\n",
      "Tokens gerados: ['‘', '’', 'aquilo', 'eram', 'terras', 'amaldicoadas', '”', 'anos', 'depois', ',', 'contavam', 'que', 'nas', 'ruinas', 'de', 'sao', 'bras', 'vivia', 'uma', 'preta', 'feiticeira', ',', 'que', ',', 'por', 'altanoite', ',', 'saia', 'pelos', 'campos', 'a', 'imitar', 'o', 'canto', 'da', 'mae-da-lua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ninguém se animava a passar perto dali, e o caminheiro descuidado, que se perdesse em tais\n",
      "paragens, via percorrer o cemitério, a cantar e a rodar, um vulto alto e magro de mulher, coberto de\n",
      "andrajos'\n",
      "Tokens gerados: ['ninguem', 'se', 'animava', 'a', 'passar', 'perto', 'dali', ',', 'e', 'o', 'caminheiro', 'descuidado', ',', 'que', 'se', 'perdesse', 'em', 'taisparagens', ',', 'via', 'percorrer', 'o', 'cemiterio', ',', 'a', 'cantar', 'e', 'a', 'rodar', ',', 'um', 'vulto', 'alto', 'e', 'magro', 'de', 'mulher', ',', 'coberto', 'deandrajos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A morte inesperada de José causou grande abalo no irmão e ainda mais em Mariana'\n",
      "Tokens gerados: ['a', 'morte', 'inesperada', 'de', 'jose', 'causou', 'grande', 'abalo', 'no', 'irmao', 'e', 'ainda', 'mais', 'em', 'mariana']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo\n",
      "era muito criança, não a compreendeu; por esse tempo teria ele cinco anos, se tanto'\n",
      "Tokens gerados: ['raimundoera', 'muito', 'crianca', ',', 'nao', 'a', 'compreendeu', 'por', 'esse', 'tempo', 'teria', 'ele', 'cinco', 'anos', ',', 'se', 'tanto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vestiram-no de\n",
      "sarja preta e disseram-lhe que estava de luto pelo pai'\n",
      "Tokens gerados: ['vestiram-no', 'desarja', 'preta', 'e', 'disseram-lhe', 'que', 'estava', 'de', 'luto', 'pelo', 'pai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Manuel tratou do inventário; recebeu o que lhe\n",
      "coube e mais a mulher na herança; depositou no recém-criado banco da província o que pertencia ao\n",
      "órfão e, apesar das vantagens que propôs para vender ou arrendar a fazenda de São Brás, ninguém a\n",
      "quis'\n",
      "Tokens gerados: ['manuel', 'tratou', 'do', 'inventario', 'recebeu', 'o', 'que', 'lhecoube', 'e', 'mais', 'a', 'mulher', 'na', 'heranca', 'depositou', 'no', 'recem-criado', 'banco', 'da', 'provincia', 'o', 'que', 'pertencia', 'aoorfao', 'e', ',', 'apesar', 'das', 'vantagens', 'que', 'propos', 'para', 'vender', 'ou', 'arrendar', 'a', 'fazenda', 'de', 'sao', 'bras', ',', 'ninguem', 'aquis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Isto feito, escreveu logo para Lisboa, pedindo esclarecimentos à Casa Peixoto, Costa & Cia'\n",
      "Tokens gerados: ['isto', 'feito', ',', 'escreveu', 'logo', 'para', 'lisboa', ',', 'pedindo', 'esclarecimentos', 'a', 'casa', 'peixoto', ',', 'costa', '&', 'cia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): ', e\n",
      "uma vez bem informado no que desejava, remeteu o sobrinho para um colégio daquela cidade'\n",
      "Tokens gerados: [',', 'euma', 'vez', 'bem', 'informado', 'no', 'que', 'desejava', ',', 'remeteu', 'o', 'sobrinho', 'para', 'um', 'colegio', 'daquela', 'cidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Muito custou à bondosa Mariana separar-se de Raimundo'\n",
      "Tokens gerados: ['muito', 'custou', 'a', 'bondosa', 'mariana', 'separar-se', 'de', 'raimundo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Doía aquele coração amoroso ver\n",
      "expatriar-se, assim, tão sem mãe, uma pobre criança de cinco anos'\n",
      "Tokens gerados: ['doia', 'aquele', 'coracao', 'amoroso', 'verexpatriar-se', ',', 'assim', ',', 'tao', 'sem', 'mae', ',', 'uma', 'pobre', 'crianca', 'de', 'cinco', 'anos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O pequeno, todavia, depois de\n",
      "preparado com todo o desvelo, foi metido, a chorar, dentro de um navio, e partiu'\n",
      "Tokens gerados: ['o', 'pequeno', ',', 'todavia', ',', 'depois', 'depreparado', 'com', 'todo', 'o', 'desvelo', ',', 'foi', 'metido', ',', 'a', 'chorar', ',', 'dentro', 'de', 'um', 'navio', ',', 'e', 'partiu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ia recomendado ao comandante e lamentava-se muito em viagem'\n",
      "Tokens gerados: ['ia', 'recomendado', 'ao', 'comandante', 'e', 'lamentava-se', 'muito', 'em', 'viagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando chegou a Lisboa\n",
      "teve horror de tudo que o cercava'\n",
      "Tokens gerados: ['quando', 'chegou', 'a', 'lisboateve', 'horror', 'de', 'tudo', 'que', 'o', 'cercava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, foi sempre bem tratado: seu correspondente hospedou-o\n",
      "como a um parente, tratou-o como filho; depois, meteu-o num colégio dos melhores'\n",
      "Tokens gerados: ['entretanto', ',', 'foi', 'sempre', 'bem', 'tratado', 'seu', 'correspondente', 'hospedou-ocomo', 'a', 'um', 'parente', ',', 'tratou-o', 'como', 'filho', 'depois', ',', 'meteu-o', 'num', 'colegio', 'dos', 'melhores']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo envergou o uniforme da casa, recebeu um número, e freqüentou as aulas'\n",
      "Tokens gerados: ['raimundo', 'envergou', 'o', 'uniforme', 'da', 'casa', ',', 'recebeu', 'um', 'numero', ',', 'e', 'frequentou', 'as', 'aulas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A princípio,\n",
      "logo que o deixavam sozinho, punha-se a chorar'\n",
      "Tokens gerados: ['a', 'principio', ',', 'logo', 'que', 'o', 'deixavam', 'sozinho', ',', 'punha-se', 'a', 'chorar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha muito medo do escuro; à noite, cosia-se contra\n",
      "a parede, abraçado aos travesseiros'\n",
      "Tokens gerados: ['tinha', 'muito', 'medo', 'do', 'escuro', 'a', 'noite', ',', 'cosia-se', 'contraa', 'parede', ',', 'abracado', 'aos', 'travesseiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não gostava dos outros meninos, porque lhe chamavam\n",
      "“Macaquinho”'\n",
      "Tokens gerados: ['nao', 'gostava', 'dos', 'outros', 'meninos', ',', 'porque', 'lhe', 'chamavam', '“', 'macaquinho', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era teimoso, cheio de caprichos, ressentia-se muito da má educação que os portugueses\n",
      "trouxeram para o Brasil'\n",
      "Tokens gerados: ['era', 'teimoso', ',', 'cheio', 'de', 'caprichos', ',', 'ressentia-se', 'muito', 'da', 'ma', 'educacao', 'que', 'os', 'portuguesestrouxeram', 'para', 'o', 'brasil']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No colégio era o único estudante que se chamava Raimundo e os colegas ridicularizavam-lhe o\n",
      "nome, “Raimundo Mundico Nico!” diziam-lhe, puxando-lhe a blusa e batendo-lhe na cabeça tosquiada\n",
      "à escovinha; até que ele se retirava enfiado, sem querer tornar ao recreio, a chorar e a berrar que o\n",
      "mandassem para a sua terra'\n",
      "Tokens gerados: ['no', 'colegio', 'era', 'o', 'unico', 'estudante', 'que', 'se', 'chamava', 'raimundo', 'e', 'os', 'colegas', 'ridicularizavam-lhe', 'onome', ',', '“', 'raimundo', 'mundico', 'nico', '”', 'diziam-lhe', ',', 'puxando-lhe', 'a', 'blusa', 'e', 'batendo-lhe', 'na', 'cabeca', 'tosquiadaa', 'escovinha', 'ate', 'que', 'ele', 'se', 'retirava', 'enfiado', ',', 'sem', 'querer', 'tornar', 'ao', 'recreio', ',', 'a', 'chorar', 'e', 'a', 'berrar', 'que', 'omandassem', 'para', 'a', 'sua', 'terra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, com o tempo, apareceram-lhe amigos e a vida então se lhe afigurou\n",
      "melhor'\n",
      "Tokens gerados: ['mas', ',', 'com', 'o', 'tempo', ',', 'apareceram-lhe', 'amigos', 'e', 'a', 'vida', 'entao', 'se', 'lhe', 'afiguroumelhor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Já faziam as suas palestras; os companheiros não se cansavam de pedir-lhe informações sobre\n",
      "o Brasil'\n",
      "Tokens gerados: ['ja', 'faziam', 'as', 'suas', 'palestras', 'os', 'companheiros', 'nao', 'se', 'cansavam', 'de', 'pedir-lhe', 'informacoes', 'sobreo', 'brasil']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Como eram os selvagens?'\n",
      "Tokens gerados: ['“', 'como', 'eram', 'os', 'selvagens', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E se a gente encontrava, pelas ruas, mulheres despidas; e se\n",
      "Raimundo nunca fora varado por alguma flecha dos caboclos'\n",
      "Tokens gerados: ['e', 'se', 'a', 'gente', 'encontrava', ',', 'pelas', 'ruas', ',', 'mulheres', 'despidas', 'e', 'seraimundo', 'nunca', 'fora', 'varado', 'por', 'alguma', 'flecha', 'dos', 'caboclos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Um dia recebeu uma carta de Mariana e, pela primeira vez, deu-se ao cuidado de pensar em si'\n",
      "Tokens gerados: ['”', 'um', 'dia', 'recebeu', 'uma', 'carta', 'de', 'mariana', 'e', ',', 'pela', 'primeira', 'vez', ',', 'deu-se', 'ao', 'cuidado', 'de', 'pensar', 'em', 'si']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas as suas reminiscências não iam além da casa do tio; no entanto, queria parecer-lhe que a sua\n",
      "verdadeira mãe não era aquela senhora, aquela vinha a ser sua tia, porque era a mulher de seu tio\n",
      "Manuel; e até, se lhe não falhava a memória, por mais de uma vez ouvira dela própria falar na outra, na\n",
      "sua verdadeira mãe'\n",
      "Tokens gerados: ['mas', 'as', 'suas', 'reminiscencias', 'nao', 'iam', 'alem', 'da', 'casa', 'do', 'tio', 'no', 'entanto', ',', 'queria', 'parecer-lhe', 'que', 'a', 'suaverdadeira', 'mae', 'nao', 'era', 'aquela', 'senhora', ',', 'aquela', 'vinha', 'a', 'ser', 'sua', 'tia', ',', 'porque', 'era', 'a', 'mulher', 'de', 'seu', 'tiomanuel', 'e', 'ate', ',', 'se', 'lhe', 'nao', 'falhava', 'a', 'memoria', ',', 'por', 'mais', 'de', 'uma', 'vez', 'ouvira', 'dela', 'propria', 'falar', 'na', 'outra', ',', 'nasua', 'verdadeira', 'mae']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Mas quem seria a outra? Como se chamava?'\n",
      "Tokens gerados: ['“', 'mas', 'quem', 'seria', 'a', 'outra', '?', 'como', 'se', 'chamava', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nunca lho disseram!'\n",
      "Tokens gerados: ['nunca', 'lho', 'disseram']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Quanto a seu pai, devia ser aquele homem barbado que, uma noite, lhe apareceu, muito pálido\n",
      "e aflito, e por quem pouco depois o cobriram de luto'\n",
      "Tokens gerados: ['”', 'quanto', 'a', 'seu', 'pai', ',', 'devia', 'ser', 'aquele', 'homem', 'barbado', 'que', ',', 'uma', 'noite', ',', 'lhe', 'apareceu', ',', 'muito', 'palidoe', 'aflito', ',', 'e', 'por', 'quem', 'pouco', 'depois', 'o', 'cobriram', 'de', 'luto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Da cena dessa noite lembrava-se perfeitamente!\n",
      "Já estava recolhido, foram buscá-lo à rede e trouxeram-no, estremunhado, para as pernas do tal sujeito,\n",
      "por sinal que as suas barbas tinham na ocasião certa umidade aborrecida, que Raimundo agora calculava\n",
      "ser produzida pelas lágrimas; depois foi se deitar e não pensou mais nisso'\n",
      "Tokens gerados: ['da', 'cena', 'dessa', 'noite', 'lembrava-se', 'perfeitamenteja', 'estava', 'recolhido', ',', 'foram', 'busca-lo', 'a', 'rede', 'e', 'trouxeram-no', ',', 'estremunhado', ',', 'para', 'as', 'pernas', 'do', 'tal', 'sujeito', ',', 'por', 'sinal', 'que', 'as', 'suas', 'barbas', 'tinham', 'na', 'ocasiao', 'certa', 'umidade', 'aborrecida', ',', 'que', 'raimundo', 'agora', 'calculavaser', 'produzida', 'pelas', 'lagrimas', 'depois', 'foi', 'se', 'deitar', 'e', 'nao', 'pensou', 'mais', 'nisso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Recordava-se também, mas\n",
      "não com tamanha lucidez, do tempo em que aquele mesmo homem esteve doente, lembrava-se de ter\n",
      "recebido dele muitos beijos e abraços, e só agora notava que todos esses afagos eram sempre ocultos e\n",
      "assustados, feitos como que ilegalmente, às escondidas, e quase sempre acompanhados de choro'\n",
      "Tokens gerados: ['recordava-se', 'tambem', ',', 'masnao', 'com', 'tamanha', 'lucidez', ',', 'do', 'tempo', 'em', 'que', 'aquele', 'mesmo', 'homem', 'esteve', 'doente', ',', 'lembrava-se', 'de', 'terrecebido', 'dele', 'muitos', 'beijos', 'e', 'abracos', ',', 'e', 'so', 'agora', 'notava', 'que', 'todos', 'esses', 'afagos', 'eram', 'sempre', 'ocultos', 'eassustados', ',', 'feitos', 'como', 'que', 'ilegalmente', ',', 'as', 'escondidas', ',', 'e', 'quase', 'sempre', 'acompanhados', 'de', 'choro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois destas e outras divagações pelo passado, Raimundo, se bem que muito novo ainda,\n",
      "punha-se a pensar e os véus misteriosos da sua infância assombravam-lhe já o coração com uma tristeza\n",
      "vaga e obscura, numa perplexidade cheia de desgosto'\n",
      "Tokens gerados: ['depois', 'destas', 'e', 'outras', 'divagacoes', 'pelo', 'passado', ',', 'raimundo', ',', 'se', 'bem', 'que', 'muito', 'novo', 'ainda', ',', 'punha-se', 'a', 'pensar', 'e', 'os', 'veus', 'misteriosos', 'da', 'sua', 'infancia', 'assombravam-lhe', 'ja', 'o', 'coracao', 'com', 'uma', 'tristezavaga', 'e', 'obscura', ',', 'numa', 'perplexidade', 'cheia', 'de', 'desgosto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Todo o seu desejo era correr aos braços de\n",
      "Mariana e pedir-lhe que lhe dissesse, por amor de Deus, quem afinal vinha a ser seu pai e, principalmente,\n",
      "sua mãe'\n",
      "Tokens gerados: ['todo', 'o', 'seu', 'desejo', 'era', 'correr', 'aos', 'bracos', 'demariana', 'e', 'pedir-lhe', 'que', 'lhe', 'dissesse', ',', 'por', 'amor', 'de', 'deus', ',', 'quem', 'afinal', 'vinha', 'a', 'ser', 'seu', 'pai', 'e', ',', 'principalmente', ',', 'sua', 'mae']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Passaram-se anos, e ele permaneceu enleado nas mesmas dúvidas'\n",
      "Tokens gerados: ['passaram-se', 'anos', ',', 'e', 'ele', 'permaneceu', 'enleado', 'nas', 'mesmas', 'duvidas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Concluiu os seus preparatórios,\n",
      "habilitou-se a entrar para a Academia'\n",
      "Tokens gerados: ['concluiu', 'os', 'seus', 'preparatorios', ',', 'habilitou-se', 'a', 'entrar', 'para', 'a', 'academia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E sempre as mesmas incertezas a respeito da sua procedência'\n",
      "Tokens gerados: ['e', 'sempre', 'as', 'mesmas', 'incertezas', 'a', 'respeito', 'da', 'sua', 'procedencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Matriculou-se em Coimbra'\n",
      "Tokens gerados: ['matriculou-se', 'em', 'coimbra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desde então a sua vida mudou radicalmente; todo ele se transformou\n",
      "nos seus modos de ver e julgar'\n",
      "Tokens gerados: ['desde', 'entao', 'a', 'sua', 'vida', 'mudou', 'radicalmente', 'todo', 'ele', 'se', 'transformounos', 'seus', 'modos', 'de', 'ver', 'e', 'julgar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Principiou a ser alegre'\n",
      "Tokens gerados: ['principiou', 'a', 'ser', 'alegre']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas um golpe terrível veio de novo entristecê-lo — a morte da sua mãe adotiva'\n",
      "Tokens gerados: ['mas', 'um', 'golpe', 'terrivel', 'veio', 'de', 'novo', 'entristece-lo', '—', 'a', 'morte', 'da', 'sua', 'mae', 'adotiva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Chorou-a\n",
      "longa e amargamente; não só por ela, mas também muito por si próprio: perdendo Mariana, perdia tudo\n",
      "que o ligava ao passado e à pátria'\n",
      "Tokens gerados: ['chorou-alonga', 'e', 'amargamente', 'nao', 'so', 'por', 'ela', ',', 'mas', 'tambem', 'muito', 'por', 'si', 'proprio', 'perdendo', 'mariana', ',', 'perdia', 'tudoque', 'o', 'ligava', 'ao', 'passado', 'e', 'a', 'patria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nunca se considerou tão órfão'\n",
      "Tokens gerados: ['nunca', 'se', 'considerou', 'tao', 'orfao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Todavia, com o correr dos tempos,\n",
      "dispersaram-se-lhe as mágoas e a mocidade triunfou; a criança melancólica produziu um rapaz cheio\n",
      "de vida e bom humor; sentiu-se bem dentro da sua romântica batina de estudante; meteu-se em pândegas\n",
      "com os colegas; contraiu novos amigos, e afinal reparou que tinha talento e graça; escreveu sátiras,\n",
      "ridicularizando os professores antipatizados; ganhou ódios e admiradores; teve quem o temesse e teve\n",
      "quem o imitasse'\n",
      "Tokens gerados: ['todavia', ',', 'com', 'o', 'correr', 'dos', 'tempos', ',', 'dispersaram-se-lhe', 'as', 'magoas', 'e', 'a', 'mocidade', 'triunfou', 'a', 'crianca', 'melancolica', 'produziu', 'um', 'rapaz', 'cheiode', 'vida', 'e', 'bom', 'humor', 'sentiu-se', 'bem', 'dentro', 'da', 'sua', 'romantica', 'batina', 'de', 'estudante', 'meteu-se', 'em', 'pandegascom', 'os', 'colegas', 'contraiu', 'novos', 'amigos', ',', 'e', 'afinal', 'reparou', 'que', 'tinha', 'talento', 'e', 'graca', 'escreveu', 'satiras', ',', 'ridicularizando', 'os', 'professores', 'antipatizados', 'ganhou', 'odios', 'e', 'admiradores', 'teve', 'quem', 'o', 'temesse', 'e', 'tevequem', 'o', 'imitasse']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No segundo ano deu para namorador: atirou-se aos versos líricos, cantou o amor em\n",
      "todos os metros depois vieram-lhe idéias revolucionárias, meteu-se em clubes incendiários, falou muito,\n",
      "e foi aplaudido pelos seus companheiros'\n",
      "Tokens gerados: ['no', 'segundo', 'ano', 'deu', 'para', 'namorador', 'atirou-se', 'aos', 'versos', 'liricos', ',', 'cantou', 'o', 'amor', 'emtodos', 'os', 'metros', 'depois', 'vieram-lhe', 'ideias', 'revolucionarias', ',', 'meteu-se', 'em', 'clubes', 'incendiarios', ',', 'falou', 'muito', ',', 'e', 'foi', 'aplaudido', 'pelos', 'seus', 'companheiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No terceiro ano tornou-se janota, gastou mais do que nos\n",
      "outros, teve amantes, em compensação veio-lhe a febre dos jornais, escreveu com entusiasmo sobre\n",
      "todos os assuntos, desde o artigo de fundo até à crônica teatral'\n",
      "Tokens gerados: ['no', 'terceiro', 'ano', 'tornou-se', 'janota', ',', 'gastou', 'mais', 'do', 'que', 'nosoutros', ',', 'teve', 'amantes', ',', 'em', 'compensacao', 'veio-lhe', 'a', 'febre', 'dos', 'jornais', ',', 'escreveu', 'com', 'entusiasmo', 'sobretodos', 'os', 'assuntos', ',', 'desde', 'o', 'artigo', 'de', 'fundo', 'ate', 'a', 'cronica', 'teatral']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No quarto, porém, distinguiu-se na\n",
      "Academia, criou gosto pela ciência, e daí em diante fez-se homem, firmou a sua imputabilidade, tornou-se\n",
      "muito estudioso e sério'\n",
      "Tokens gerados: ['no', 'quarto', ',', 'porem', ',', 'distinguiu-se', 'naacademia', ',', 'criou', 'gosto', 'pela', 'ciencia', ',', 'e', 'dai', 'em', 'diante', 'fez-se', 'homem', ',', 'firmou', 'a', 'sua', 'imputabilidade', ',', 'tornou-semuito', 'estudioso', 'e', 'serio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seus discursos acadêmicos foram apreciados; elogiaram-lhe a tese'\n",
      "Tokens gerados: ['seus', 'discursos', 'academicos', 'foram', 'apreciados', 'elogiaram-lhe', 'a', 'tese']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Formou-se'\n",
      "Tokens gerados: ['formou-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Veio-lhe então à idéia fazer uma viagem'\n",
      "Tokens gerados: ['veio-lhe', 'entao', 'a', 'ideia', 'fazer', 'uma', 'viagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em Coimbra todos o diziam rico; tinha ordem franca'\n",
      "Tokens gerados: ['em', 'coimbra', 'todos', 'o', 'diziam', 'rico', 'tinha', 'ordem', 'franca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Preparou as malas'\n",
      "Tokens gerados: ['preparou', 'as', 'malas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sua principal ambição era instruir-se, instruir-se muito, abranger a maior quantidade\n",
      "de conhecimentos que pudesse; e sentia-se cheio de coragem para a luta e cheio de confiança no seu\n",
      "esforço'\n",
      "Tokens gerados: ['sua', 'principal', 'ambicao', 'era', 'instruir-se', ',', 'instruir-se', 'muito', ',', 'abranger', 'a', 'maior', 'quantidadede', 'conhecimentos', 'que', 'pudesse', 'e', 'sentia-se', 'cheio', 'de', 'coragem', 'para', 'a', 'luta', 'e', 'cheio', 'de', 'confianca', 'no', 'seuesforco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Às vezes, porém uma sombra de tristeza mesquinha toldava-lhe as aspirações — não sabia ao\n",
      "certo de quem descendia, e de que modo, e por quem, fora adquirido aquele dinheiro que lhe enchia as\n",
      "algibeiras'\n",
      "Tokens gerados: ['as', 'vezes', ',', 'porem', 'uma', 'sombra', 'de', 'tristeza', 'mesquinha', 'toldava-lhe', 'as', 'aspiracoes', '—', 'nao', 'sabia', 'aocerto', 'de', 'quem', 'descendia', ',', 'e', 'de', 'que', 'modo', ',', 'e', 'por', 'quem', ',', 'fora', 'adquirido', 'aquele', 'dinheiro', 'que', 'lhe', 'enchia', 'asalgibeiras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Procurou o seu correspondente em Lisboa, pediu-lhe esclarecimentos a esse respeito —\n",
      "Nada! O Peixoto dizia-lhe, em tom muito seco, “que o pai de Raimundo havia morrido antes da chegada\n",
      "deste a Portugal, e o tio, o tutor, esse estava no Maranhão, estabelecido na Rua da Estrela com um\n",
      "armazém de fazendas por atacado”'\n",
      "Tokens gerados: ['procurou', 'o', 'seu', 'correspondente', 'em', 'lisboa', ',', 'pediu-lhe', 'esclarecimentos', 'a', 'esse', 'respeito', '—nada', 'o', 'peixoto', 'dizia-lhe', ',', 'em', 'tom', 'muito', 'seco', ',', '“', 'que', 'o', 'pai', 'de', 'raimundo', 'havia', 'morrido', 'antes', 'da', 'chegadadeste', 'a', 'portugal', ',', 'e', 'o', 'tio', ',', 'o', 'tutor', ',', 'esse', 'estava', 'no', 'maranhao', ',', 'estabelecido', 'na', 'rua', 'da', 'estrela', 'com', 'umarmazem', 'de', 'fazendas', 'por', 'atacado', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De sua mãe — nem uma palavra, nem uma atribuição!'\n",
      "Tokens gerados: ['de', 'sua', 'mae', '—', 'nem', 'uma', 'palavra', ',', 'nem', 'uma', 'atribuicao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era para enlouquecer! “Mas, afinal, quem seria ela?'\n",
      "Tokens gerados: ['era', 'para', 'enlouquecer', '“', 'mas', ',', 'afinal', ',', 'quem', 'seria', 'ela', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Talvez irmã daquela santa senhora que\n",
      "foi para ele uma segunda mãe'\n",
      "Tokens gerados: ['talvez', 'irma', 'daquela', 'santa', 'senhora', 'quefoi', 'para', 'ele', 'uma', 'segunda', 'mae']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas então por que tanto mistério?'\n",
      "Tokens gerados: ['mas', 'entao', 'por', 'que', 'tanto', 'misterio', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seria alguma história, a tal ponto\n",
      "vergonhosa, que ninguém se atrevesse a revelar-lhe?'\n",
      "Tokens gerados: ['seria', 'alguma', 'historia', ',', 'a', 'tal', 'pontovergonhosa', ',', 'que', 'ninguem', 'se', 'atrevesse', 'a', 'revelar-lhe', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seria ele enjeitado?'\n",
      "Tokens gerados: ['seria', 'ele', 'enjeitado', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não, decerto, porque era\n",
      "herdeiro de seu pai'\n",
      "Tokens gerados: ['nao', ',', 'decerto', ',', 'porque', 'eraherdeiro', 'de', 'seu', 'pai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” E Raimundo, quanto mais tentava pôr a limpo a sua existência, mais e mais se\n",
      "perdia no dédalo das conjeturas'\n",
      "Tokens gerados: ['”', 'e', 'raimundo', ',', 'quanto', 'mais', 'tentava', 'por', 'a', 'limpo', 'a', 'sua', 'existencia', ',', 'mais', 'e', 'mais', 'seperdia', 'no', 'dedalo', 'das', 'conjeturas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Das cartas que recebia do Brasil, nem uma só lhe falava no passado, e todavia, era tanto o seu\n",
      "empenho em penetrá-lo, que às vezes, com muito esforço de memória, conseguia reconstruir e articular\n",
      "fragmentos dispersos de algumas reminiscências, incompletas e vagas, da sua infância'\n",
      "Tokens gerados: ['das', 'cartas', 'que', 'recebia', 'do', 'brasil', ',', 'nem', 'uma', 'so', 'lhe', 'falava', 'no', 'passado', ',', 'e', 'todavia', ',', 'era', 'tanto', 'o', 'seuempenho', 'em', 'penetra-lo', ',', 'que', 'as', 'vezes', ',', 'com', 'muito', 'esforco', 'de', 'memoria', ',', 'conseguia', 'reconstruir', 'e', 'articularfragmentos', 'dispersos', 'de', 'algumas', 'reminiscencias', ',', 'incompletas', 'e', 'vagas', ',', 'da', 'sua', 'infancia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lograva\n",
      "recordar-se da Aniquinha, que tantas noites, adormecera a seu lado, na mesma esteira, ouvindo cantar\n",
      "por D'\n",
      "Tokens gerados: ['logravarecordar-se', 'da', 'aniquinha', ',', 'que', 'tantas', 'noites', ',', 'adormecera', 'a', 'seu', 'lado', ',', 'na', 'mesma', 'esteira', ',', 'ouvindo', 'cantarpor', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mariana o “Boizinho do curral, vem papar neném”; recordava-se também da Sra'\n",
      "Tokens gerados: ['mariana', 'o', '“', 'boizinho', 'do', 'curral', ',', 'vem', 'papar', 'nenem', '”', 'recordava-se', 'tambem', 'da', 'sra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'D'\n",
      "Tokens gerados: ['d']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Maria\n",
      "Bárbara, a sogra de Manuel, que ia, com muito aparato, visitar a neta; passar dias'\n",
      "Tokens gerados: ['mariabarbara', ',', 'a', 'sogra', 'de', 'manuel', ',', 'que', 'ia', ',', 'com', 'muito', 'aparato', ',', 'visitar', 'a', 'neta', 'passar', 'dias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em geral, ela chegava\n",
      "à boca da noite, no seu palanquim carregado por dois escravos, vestida de enorme roda, cercada de\n",
      "crias e moleques, precedida por um preto encarregado de alumiar a rua com um lampião de folha,\n",
      "oitavado, duas velas no centro'\n",
      "Tokens gerados: ['em', 'geral', ',', 'ela', 'chegavaa', 'boca', 'da', 'noite', ',', 'no', 'seu', 'palanquim', 'carregado', 'por', 'dois', 'escravos', ',', 'vestida', 'de', 'enorme', 'roda', ',', 'cercada', 'decrias', 'e', 'moleques', ',', 'precedida', 'por', 'um', 'preto', 'encarregado', 'de', 'alumiar', 'a', 'rua', 'com', 'um', 'lampiao', 'de', 'folha', ',', 'oitavado', ',', 'duas', 'velas', 'no', 'centro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o demônio da mulher sempre a ralhar, sempre zangada, batendo nos\n",
      "negros e a implicar com ele, Raimundo, a quem, todas as vezes que lhe dava a mão a beijar, pespegava\n",
      "com as costas desta uma pancada na boca'\n",
      "Tokens gerados: ['e', 'o', 'demonio', 'da', 'mulher', 'sempre', 'a', 'ralhar', ',', 'sempre', 'zangada', ',', 'batendo', 'nosnegros', 'e', 'a', 'implicar', 'com', 'ele', ',', 'raimundo', ',', 'a', 'quem', ',', 'todas', 'as', 'vezes', 'que', 'lhe', 'dava', 'a', 'mao', 'a', 'beijar', ',', 'pespegavacom', 'as', 'costas', 'desta', 'uma', 'pancada', 'na', 'boca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E recordava-se bem do rosto macilento de Maria Bárbara, já\n",
      "então meio descaído; recordava-se dos seus olhos castanho-claros, de seus dentes triangulares, truncados\n",
      "a navalha, como barbaramente faziam dantes, por luxo, as senhoras do Maranhão, criadas em fazenda'\n",
      "Tokens gerados: ['e', 'recordava-se', 'bem', 'do', 'rosto', 'macilento', 'de', 'maria', 'barbara', ',', 'jaentao', 'meio', 'descaido', 'recordava-se', 'dos', 'seus', 'olhos', 'castanho-claros', ',', 'de', 'seus', 'dentes', 'triangulares', ',', 'truncadosa', 'navalha', ',', 'como', 'barbaramente', 'faziam', 'dantes', ',', 'por', 'luxo', ',', 'as', 'senhoras', 'do', 'maranhao', ',', 'criadas', 'em', 'fazenda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo, uma vez, ainda em Coimbra, aspirando o cheiro de alfazema queimada, sentiu, como por encanto, surgirem-lhe\n",
      "à memória muitos fatos de que nunca se recordara até então'\n",
      "Tokens gerados: ['raimundo', ',', 'uma', 'vez', ',', 'ainda', 'em', 'coimbra', ',', 'aspirando', 'o', 'cheiro', 'de', 'alfazema', 'queimada', ',', 'sentiu', ',', 'como', 'por', 'encanto', ',', 'surgirem-lhea', 'memoria', 'muitos', 'fatos', 'de', 'que', 'nunca', 'se', 'recordara', 'ate', 'entao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lembrou-se logo do nascimento de Ana Rosa: A casa estava\n",
      "toda silenciosa e impregnada daquele odor; Mariana gemia no seu quarto; Manuel andava, de um para outro lado da varanda,\n",
      "inquieto e desorientado; mas, de repente, apareceu na porta do quarto uma mulata gorda, a quem davam o tratamento de\n",
      "“Inhá comadre”, e esta, que vinha alvoroçada, chamou de parte o dono da casa, disse-lhe alguma coisa em segredo, e daí a\n",
      "pouco estavam todos felizes e satisfeitos'\n",
      "Tokens gerados: ['lembrou-se', 'logo', 'do', 'nascimento', 'de', 'ana', 'rosa', 'a', 'casa', 'estavatoda', 'silenciosa', 'e', 'impregnada', 'daquele', 'odor', 'mariana', 'gemia', 'no', 'seu', 'quarto', 'manuel', 'andava', ',', 'de', 'um', 'para', 'outro', 'lado', 'da', 'varanda', ',', 'inquieto', 'e', 'desorientado', 'mas', ',', 'de', 'repente', ',', 'apareceu', 'na', 'porta', 'do', 'quarto', 'uma', 'mulata', 'gorda', ',', 'a', 'quem', 'davam', 'o', 'tratamento', 'de', '“', 'inha', 'comadre', '”', ',', 'e', 'esta', ',', 'que', 'vinha', 'alvorocada', ',', 'chamou', 'de', 'parte', 'o', 'dono', 'da', 'casa', ',', 'disse-lhe', 'alguma', 'coisa', 'em', 'segredo', ',', 'e', 'dai', 'apouco', 'estavam', 'todos', 'felizes', 'e', 'satisfeitos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E ouvia-se vir lá de dentro um grunhido fanhoso, que parecia uma gaita'\n",
      "Tokens gerados: ['e', 'ouvia-se', 'vir', 'la', 'de', 'dentro', 'um', 'grunhido', 'fanhoso', ',', 'que', 'parecia', 'uma', 'gaita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na\n",
      "ocasião, Raimundo nada compreendeu de tudo isto; disseram-lhe que Mariana recebera uma menina de França, e ele acreditou\n",
      "piamente'\n",
      "Tokens gerados: ['naocasiao', ',', 'raimundo', 'nada', 'compreendeu', 'de', 'tudo', 'isto', 'disseram-lhe', 'que', 'mariana', 'recebera', 'uma', 'menina', 'de', 'franca', ',', 'e', 'ele', 'acreditoupiamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Assim lhe acudiam outras recordações; por exemplo a do macassar cheiroso, então muito em\n",
      "uso na província, com que D'\n",
      "Tokens gerados: ['assim', 'lhe', 'acudiam', 'outras', 'recordacoes', 'por', 'exemplo', 'a', 'do', 'macassar', 'cheiroso', ',', 'entao', 'muito', 'emuso', 'na', 'provincia', ',', 'com', 'que', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mariana lhe perfumava os cabelos todas as manhãs antes do café; mas,\n",
      "dentre tudo, do que melhor ele se recordava era dos lampiões com que iluminavam a cidade'\n",
      "Tokens gerados: ['mariana', 'lhe', 'perfumava', 'os', 'cabelos', 'todas', 'as', 'manhas', 'antes', 'do', 'cafe', 'mas', ',', 'dentre', 'tudo', ',', 'do', 'que', 'melhor', 'ele', 'se', 'recordava', 'era', 'dos', 'lampioes', 'com', 'que', 'iluminavam', 'a', 'cidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ainda lá\n",
      "não havia gás, nem querosene; ao bater d’Ave-Marias vinha o acendedor, desatava a corrente do lampião,\n",
      "descia-o, abria-o, despejava-lhe dentro aguarrás misturada com álcool, acendia-lhe o pavio, guindava-o\n",
      "novamente para o seu lugar, e seguia adiante'\n",
      "Tokens gerados: ['ainda', 'lanao', 'havia', 'gas', ',', 'nem', 'querosene', 'ao', 'bater', 'd', '’', 'ave-marias', 'vinha', 'o', 'acendedor', ',', 'desatava', 'a', 'corrente', 'do', 'lampiao', ',', 'descia-o', ',', 'abria-o', ',', 'despejava-lhe', 'dentro', 'aguarras', 'misturada', 'com', 'alcool', ',', 'acendia-lhe', 'o', 'pavio', ',', 'guindava-onovamente', 'para', 'o', 'seu', 'lugar', ',', 'e', 'seguia', 'adiante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“E que mau cheiro em todas as esquinas em que havia\n",
      "iluminação!'\n",
      "Tokens gerados: ['“', 'e', 'que', 'mau', 'cheiro', 'em', 'todas', 'as', 'esquinas', 'em', 'que', 'haviailuminacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Oh! a não ser que estivesse muito transformada a sua província devia ser simplesmente\n",
      "horrível!”\n",
      "Não obstante, queria lá ir'\n",
      "Tokens gerados: ['oh', 'a', 'nao', 'ser', 'que', 'estivesse', 'muito', 'transformada', 'a', 'sua', 'provincia', 'devia', 'ser', 'simplesmentehorrivel', '”', 'nao', 'obstante', ',', 'queria', 'la', 'ir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sentia atrações por essa pátria, quase tão desconhecida para ele como\n",
      "o seu próprio nascimento misterioso'\n",
      "Tokens gerados: ['sentia', 'atracoes', 'por', 'essa', 'patria', ',', 'quase', 'tao', 'desconhecida', 'para', 'ele', 'comoo', 'seu', 'proprio', 'nascimento', 'misterioso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Com a viagem descobriria tudo! Mas, primeiro, era preciso dar\n",
      "um passeio à Europa'\n",
      "Tokens gerados: ['“', 'com', 'a', 'viagem', 'descobriria', 'tudo', 'mas', ',', 'primeiro', ',', 'era', 'preciso', 'darum', 'passeio', 'a', 'europa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "E, resolvido, foi ao escritório de Peixoto, Costa & Cia'\n",
      "Tokens gerados: ['”', 'e', ',', 'resolvido', ',', 'foi', 'ao', 'escritorio', 'de', 'peixoto', ',', 'costa', '&', 'cia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): ', sacou a quantia de que precisava, abraçou\n",
      "os amigos, e fez-se de vela para a França'\n",
      "Tokens gerados: [',', 'sacou', 'a', 'quantia', 'de', 'que', 'precisava', ',', 'abracouos', 'amigos', ',', 'e', 'fez-se', 'de', 'vela', 'para', 'a', 'franca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Passou pela Espanha, visitou a Itália, foi à Suíça, esteve na Alemanha, percorreu a Inglaterra, e,\n",
      "no fim de três anos de viagem, chegou ao Rio de Janeiro, onde encontrou os seus antigos correspondentes\n",
      "de Lisboa'\n",
      "Tokens gerados: ['passou', 'pela', 'espanha', ',', 'visitou', 'a', 'italia', ',', 'foi', 'a', 'suica', ',', 'esteve', 'na', 'alemanha', ',', 'percorreu', 'a', 'inglaterra', ',', 'e', ',', 'no', 'fim', 'de', 'tres', 'anos', 'de', 'viagem', ',', 'chegou', 'ao', 'rio', 'de', 'janeiro', ',', 'onde', 'encontrou', 'os', 'seus', 'antigos', 'correspondentesde', 'lisboa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Demorou-se um ano na Corte, gostou da cidade, relacionou-se, fez projetos de vida e resolveu\n",
      "estabelecer aí a sua residência'\n",
      "Tokens gerados: ['demorou-se', 'um', 'ano', 'na', 'corte', ',', 'gostou', 'da', 'cidade', ',', 'relacionou-se', ',', 'fez', 'projetos', 'de', 'vida', 'e', 'resolveuestabelecer', 'ai', 'a', 'sua', 'residencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“E o Maranhão?'\n",
      "Tokens gerados: ['“', 'e', 'o', 'maranhao', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Oh, que maçada! Mas não podia deixar de lá ir! Não podia instalar-se na\n",
      "Corte, sem ter ido primeiro à sua província! Era indispensável conhecer a família; liquidar os seus bens\n",
      "e'\n",
      "Tokens gerados: ['oh', ',', 'que', 'macada', 'mas', 'nao', 'podia', 'deixar', 'de', 'la', 'ir', 'nao', 'podia', 'instalar-se', 'nacorte', ',', 'sem', 'ter', 'ido', 'primeiro', 'a', 'sua', 'provincia', 'era', 'indispensavel', 'conhecer', 'a', 'familia', 'liquidar', 'os', 'seus', 'bense']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "— Verdade, verdade, dizia ele, conversando com um amigo, a quem confiara os seus projetos,\n",
      "a coisa não é tão feia como quer parecer, porque, no fim de contas, fico conhecendo todo o norte do\n",
      "Brasil, dou um pulo ao Pará e ao Amazonas, que desejo ver, e, afinal, volto descansado para cá com a\n",
      "vida em ordem, a consciência descarregada e o pouco que possuo reduzido a moeda'\n",
      "Tokens gerados: ['”', '—', 'verdade', ',', 'verdade', ',', 'dizia', 'ele', ',', 'conversando', 'com', 'um', 'amigo', ',', 'a', 'quem', 'confiara', 'os', 'seus', 'projetos', ',', 'a', 'coisa', 'nao', 'e', 'tao', 'feia', 'como', 'quer', 'parecer', ',', 'porque', ',', 'no', 'fim', 'de', 'contas', ',', 'fico', 'conhecendo', 'todo', 'o', 'norte', 'dobrasil', ',', 'dou', 'um', 'pulo', 'ao', 'para', 'e', 'ao', 'amazonas', ',', 'que', 'desejo', 'ver', ',', 'e', ',', 'afinal', ',', 'volto', 'descansado', 'para', 'ca', 'com', 'avida', 'em', 'ordem', ',', 'a', 'consciencia', 'descarregada', 'e', 'o', 'pouco', 'que', 'possuo', 'reduzido', 'a', 'moeda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não posso\n",
      "queixar-me da sorte!\n",
      "O passeio à Europa não só lhe beneficiara o espírito, como o corpo'\n",
      "Tokens gerados: ['nao', 'possoqueixar-me', 'da', 'sorteo', 'passeio', 'a', 'europa', 'nao', 'so', 'lhe', 'beneficiara', 'o', 'espirito', ',', 'como', 'o', 'corpo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estava muito mais forte\n",
      "bem exercitado e com uma saúde invejável'\n",
      "Tokens gerados: ['estava', 'muito', 'mais', 'fortebem', 'exercitado', 'e', 'com', 'uma', 'saude', 'invejavel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Gabava-se de ter adquirido grande experiência do mundo;\n",
      "conversava à vontade sobre qualquer assunto tão bem sabia entrar numa sala de primeira ordem como\n",
      "dar uma palestra entre rapazes numa redação de jornal ou na caixa de um teatro'\n",
      "Tokens gerados: ['gabava-se', 'de', 'ter', 'adquirido', 'grande', 'experiencia', 'do', 'mundoconversava', 'a', 'vontade', 'sobre', 'qualquer', 'assunto', 'tao', 'bem', 'sabia', 'entrar', 'numa', 'sala', 'de', 'primeira', 'ordem', 'comodar', 'uma', 'palestra', 'entre', 'rapazes', 'numa', 'redacao', 'de', 'jornal', 'ou', 'na', 'caixa', 'de', 'um', 'teatro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E em pontos de honra\n",
      "e lealdade, não admitia, com todo o direito, que houvesse alguém mais escrupuloso do que ele'\n",
      "Tokens gerados: ['e', 'em', 'pontos', 'de', 'honrae', 'lealdade', ',', 'nao', 'admitia', ',', 'com', 'todo', 'o', 'direito', ',', 'que', 'houvesse', 'alguem', 'mais', 'escrupuloso', 'do', 'que', 'ele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi nessa bela disposição de espírito, feliz e cheio de esperanças no futuro que Raimundo tomou\n",
      "o “Cruzeiro” e partiu para a capital de São Luís do Maranhão'\n",
      "Tokens gerados: ['foi', 'nessa', 'bela', 'disposicao', 'de', 'espirito', ',', 'feliz', 'e', 'cheio', 'de', 'esperancas', 'no', 'futuro', 'que', 'raimundo', 'tomouo', '“', 'cruzeiro', '”', 'e', 'partiu', 'para', 'a', 'capital', 'de', 'sao', 'luis', 'do', 'maranhao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_o_mulato_aluisio_azevedo_cap_3.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, com a chegada de Raimundo, reuniram-se em casa de Manuel as velhas amizades\n",
      "da família'\n",
      "Tokens gerados: ['entretanto', ',', 'com', 'a', 'chegada', 'de', 'raimundo', ',', 'reuniram-se', 'em', 'casa', 'de', 'manuel', 'as', 'velhas', 'amizadesda', 'familia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vieram as Sarmentos com os seus enormes penteados; moças feias, mas de grandes cabelos,\n",
      "muito elogiados e conhecidos na província'\n",
      "Tokens gerados: ['vieram', 'as', 'sarmentos', 'com', 'os', 'seus', 'enormes', 'penteados', 'mocas', 'feias', ',', 'mas', 'de', 'grandes', 'cabelos', ',', 'muito', 'elogiados', 'e', 'conhecidos', 'na', 'provincia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Tranças como as das Sarmentos!'\n",
      "Tokens gerados: ['“', 'trancas', 'como', 'as', 'das', 'sarmentos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cabelo bonito como o\n",
      "das Sarmentos! Cachos como os das Sarmentos!'\n",
      "Tokens gerados: ['cabelo', 'bonito', 'como', 'odas', 'sarmentos', 'cachos', 'como', 'os', 'das', 'sarmentos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” Estas e outras tantas frases se haviam convertido\n",
      "em preceitos invariáveis'\n",
      "Tokens gerados: ['”', 'estas', 'e', 'outras', 'tantas', 'frases', 'se', 'haviam', 'convertidoem', 'preceitos', 'invariaveis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fora das Sarmentos não conheciam termo de comparação para cabelos; e\n",
      "elas, cônscias daquela popularidade, ostentavam sempre o objeto de tais admirações em penteados\n",
      "assustadores, de tamanhos fantásticos'\n",
      "Tokens gerados: ['fora', 'das', 'sarmentos', 'nao', 'conheciam', 'termo', 'de', 'comparacao', 'para', 'cabelos', 'eelas', ',', 'conscias', 'daquela', 'popularidade', ',', 'ostentavam', 'sempre', 'o', 'objeto', 'de', 'tais', 'admiracoes', 'em', 'penteadosassustadores', ',', 'de', 'tamanhos', 'fantasticos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Tenho pena, afetava às vezes D'\n",
      "Tokens gerados: ['—', 'tenho', 'pena', ',', 'afetava', 'as', 'vezes', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bibina Sarmento (esta era Bernardina) de ter tanto cabelo!'\n",
      "Tokens gerados: ['bibina', 'sarmento', 'esta', 'era', 'bernardina', 'de', 'ter', 'tanto', 'cabelo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Para desembrulhá-lo é um martírio'\n",
      "Tokens gerados: ['para', 'desembrulha-lo', 'e', 'um', 'martirio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, quando depois do banho, não me penteio logo, ou quando passo\n",
      "um dia sem botar óleo'\n",
      "Tokens gerados: ['e', ',', 'quando', 'depois', 'do', 'banho', ',', 'nao', 'me', 'penteio', 'logo', ',', 'ou', 'quando', 'passoum', 'dia', 'sem', 'botar', 'oleo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah, dona, nem lhe digo nada!'\n",
      "Tokens gerados: ['ah', ',', 'dona', ',', 'nem', 'lhe', 'digo', 'nada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E arregalava os olhos e sacudia a juba, como se descrevesse uma caçada de leões'\n",
      "Tokens gerados: ['e', 'arregalava', 'os', 'olhos', 'e', 'sacudia', 'a', 'juba', ',', 'como', 'se', 'descrevesse', 'uma', 'cacada', 'de', 'leoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A família Sarmento compunha-se, além desta D'\n",
      "Tokens gerados: ['a', 'familia', 'sarmento', 'compunha-se', ',', 'alem', 'desta', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bibina, de outra rapariga e de uma senhora de\n",
      "cinqüenta anos, muito nervosa, tia das duas moças'\n",
      "Tokens gerados: ['bibina', ',', 'de', 'outra', 'rapariga', 'e', 'de', 'uma', 'senhora', 'decinquenta', 'anos', ',', 'muito', 'nervosa', ',', 'tia', 'das', 'duas', 'mocas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A velha só falava em moléstias e sabia remédios\n",
      "para tudo; tinha um grosso livro de receitas, que ela em geral trazia no bolso; em casa uma variadíssima\n",
      "coleção de vidros, garrafas e púcaros; guardava sempre as cascas de laranja, de romã e os caroços de\n",
      "tuturubá, os quais, dizia pateticamente “Abaixo de Deus, eram santo remédio para as dores de ouvido!”\n",
      "Chamava-se Maria do Carmo, e as sobrinhas tratavam-na por “Mamãe outrinha”'\n",
      "Tokens gerados: ['a', 'velha', 'so', 'falava', 'em', 'molestias', 'e', 'sabia', 'remediospara', 'tudo', 'tinha', 'um', 'grosso', 'livro', 'de', 'receitas', ',', 'que', 'ela', 'em', 'geral', 'trazia', 'no', 'bolso', 'em', 'casa', 'uma', 'variadissimacolecao', 'de', 'vidros', ',', 'garrafas', 'e', 'pucaros', 'guardava', 'sempre', 'as', 'cascas', 'de', 'laranja', ',', 'de', 'roma', 'e', 'os', 'carocos', 'detuturuba', ',', 'os', 'quais', ',', 'dizia', 'pateticamente', '“', 'abaixo', 'de', 'deus', ',', 'eram', 'santo', 'remedio', 'para', 'as', 'dores', 'de', 'ouvido', '”', 'chamava-se', 'maria', 'do', 'carmo', ',', 'e', 'as', 'sobrinhas', 'tratavam-na', 'por', '“', 'mamae', 'outrinha', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era sumamente\n",
      "apreensiva e entendida de doces'\n",
      "Tokens gerados: ['era', 'sumamenteapreensiva', 'e', 'entendida', 'de', 'doces']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Viúva'\n",
      "Tokens gerados: ['viuva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Passara a mocidade no Recolhimento de Nossa Senhora da Anunciação e Remédios,\n",
      "onde concebera o seu primeiro filho do homem com quem depois veio a casar — o tenente Espigão,\n",
      "tenente do exército, um espalhafateiro dos quatro costados, que andava sempre de farda e desembainhava\n",
      "a durindana por dá cá aquela palha'\n",
      "Tokens gerados: ['passara', 'a', 'mocidade', 'no', 'recolhimento', 'de', 'nossa', 'senhora', 'da', 'anunciacao', 'e', 'remedios', ',', 'onde', 'concebera', 'o', 'seu', 'primeiro', 'filho', 'do', 'homem', 'com', 'quem', 'depois', 'veio', 'a', 'casar', '—', 'o', 'tenente', 'espigao', ',', 'tenente', 'do', 'exercito', ',', 'um', 'espalhafateiro', 'dos', 'quatro', 'costados', ',', 'que', 'andava', 'sempre', 'de', 'farda', 'e', 'desembainhavaa', 'durindana', 'por', 'da', 'ca', 'aquela', 'palha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Contavam dele que, um dia, num jantar de festa, perdendo a paciência\n",
      "com o peru assado, que parecia disposto a resistir ao trinchante, arranca do chanfalho e esquarteja a\n",
      "golpes de espada o inocente animal'\n",
      "Tokens gerados: ['contavam', 'dele', 'que', ',', 'um', 'dia', ',', 'num', 'jantar', 'de', 'festa', ',', 'perdendo', 'a', 'pacienciacom', 'o', 'peru', 'assado', ',', 'que', 'parecia', 'disposto', 'a', 'resistir', 'ao', 'trinchante', ',', 'arranca', 'do', 'chanfalho', 'e', 'esquarteja', 'agolpes', 'de', 'espada', 'o', 'inocente', 'animal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Gostava de fazer medo as crianças, fingindo que as prendia ou afiando a lâmina reluzente no\n",
      "tijolo do chão; e ficava muito lisonjeado quando lhe diziam que se parecia com o Pedro II'\n",
      "Tokens gerados: ['gostava', 'de', 'fazer', 'medo', 'as', 'criancas', ',', 'fingindo', 'que', 'as', 'prendia', 'ou', 'afiando', 'a', 'lamina', 'reluzente', 'notijolo', 'do', 'chao', 'e', 'ficava', 'muito', 'lisonjeado', 'quando', 'lhe', 'diziam', 'que', 'se', 'parecia', 'com', 'o', 'pedro', 'ii']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha-se na\n",
      "conta de muito atilado e a todos contava que fora poeta em rapaz: referia-se a meia dúzia de acrósticos\n",
      "e recitativos, que lhe inspirava D'\n",
      "Tokens gerados: ['tinha-se', 'naconta', 'de', 'muito', 'atilado', 'e', 'a', 'todos', 'contava', 'que', 'fora', 'poeta', 'em', 'rapaz', 'referia-se', 'a', 'meia', 'duzia', 'de', 'acrosticose', 'recitativos', ',', 'que', 'lhe', 'inspirava', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Maria do Carmo, no seu tempo de recolhida'\n",
      "Tokens gerados: ['maria', 'do', 'carmo', ',', 'no', 'seu', 'tempo', 'de', 'recolhida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Coitado! Morreu de uma tremenda indigestão no dia seguinte a uma ceia, ainda mais tremenda,\n",
      "na qual praticara a imprudência de comer uma salada inteira de pepinos, seu pratinho predileto'\n",
      "Tokens gerados: ['coitado', 'morreu', 'de', 'uma', 'tremenda', 'indigestao', 'no', 'dia', 'seguinte', 'a', 'uma', 'ceia', ',', 'ainda', 'mais', 'tremenda', ',', 'na', 'qual', 'praticara', 'a', 'imprudencia', 'de', 'comer', 'uma', 'salada', 'inteira', 'de', 'pepinos', ',', 'seu', 'pratinho', 'predileto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A viúva\n",
      "ficou inconsolável, e, em homenagem à memória do Espigão, nunca mais comeu daquele legume; seu\n",
      "ódio estendeu-se implacável por toda a família do maldito; não quis ouvir mais falar de maxixes, nem\n",
      "de abóboras, nem de jerimuns'\n",
      "Tokens gerados: ['a', 'viuvaficou', 'inconsolavel', ',', 'e', ',', 'em', 'homenagem', 'a', 'memoria', 'do', 'espigao', ',', 'nunca', 'mais', 'comeu', 'daquele', 'legume', 'seuodio', 'estendeu-se', 'implacavel', 'por', 'toda', 'a', 'familia', 'do', 'maldito', 'nao', 'quis', 'ouvir', 'mais', 'falar', 'de', 'maxixes', ',', 'nemde', 'aboboras', ',', 'nem', 'de', 'jerimuns']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ai o meu rico tenente! lamentava-se ela quando alguém lhe lembrava o esposo'\n",
      "Tokens gerados: ['—', 'ai', 'o', 'meu', 'rico', 'tenente', 'lamentava-se', 'ela', 'quando', 'alguem', 'lhe', 'lembrava', 'o', 'esposo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que maneiras\n",
      "de homem! que coração de pomba! aquilo é que era um marido como hoje em dia não se vê!'\n",
      "Tokens gerados: ['que', 'maneirasde', 'homem', 'que', 'coracao', 'de', 'pomba', 'aquilo', 'e', 'que', 'era', 'um', 'marido', 'como', 'hoje', 'em', 'dia', 'nao', 'se', 've']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A outra sobrinha de D'\n",
      "Tokens gerados: ['a', 'outra', 'sobrinha', 'de', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Maria do Carmo, chamava-se Etelvina'\n",
      "Tokens gerados: ['maria', 'do', 'carmo', ',', 'chamava-se', 'etelvina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Criaturinha sumamente magra,\n",
      "e tão nervosa como a tia; nariz muito fino grande e gelado, mãos ossudas e frias, olhos sensuais e dentes\n",
      "podres'\n",
      "Tokens gerados: ['criaturinha', 'sumamente', 'magra', ',', 'e', 'tao', 'nervosa', 'como', 'a', 'tia', 'nariz', 'muito', 'fino', 'grande', 'e', 'gelado', ',', 'maos', 'ossudas', 'e', 'frias', ',', 'olhos', 'sensuais', 'e', 'dentespodres']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era detestável: os rapazes do comércio chamavam-lhe “Lagartixa”'\n",
      "Tokens gerados: ['era', 'detestavel', 'os', 'rapazes', 'do', 'comercio', 'chamavam-lhe', '“', 'lagartixa', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fazia-se muito romântica; prezava a sua cor horrivelmente pálida; suspirava de cinco em cinco minutos e sabia estropiar\n",
      "modinhas sentimentais ao violão'\n",
      "Tokens gerados: ['fazia-se', 'muito', 'romantica', 'prezava', 'a', 'sua', 'cor', 'horrivelmente', 'palida', 'suspirava', 'de', 'cinco', 'em', 'cinco', 'minutos', 'e', 'sabia', 'estropiarmodinhas', 'sentimentais', 'ao', 'violao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Diziam, em ar muito sério, que ela tivera aos dezesseis anos uma formidável paixão por\n",
      "um italiano, professor de canto, o qual fugira aos credores para o Pará e que, desde então, Etelvina nunca mais tomara corpo'\n",
      "Tokens gerados: ['diziam', ',', 'em', 'ar', 'muito', 'serio', ',', 'que', 'ela', 'tivera', 'aos', 'dezesseis', 'anos', 'uma', 'formidavel', 'paixao', 'porum', 'italiano', ',', 'professor', 'de', 'canto', ',', 'o', 'qual', 'fugira', 'aos', 'credores', 'para', 'o', 'para', 'e', 'que', ',', 'desde', 'entao', ',', 'etelvina', 'nunca', 'mais', 'tomara', 'corpo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Apresentou-se também em casa de Manuel a sra'\n",
      "Tokens gerados: ['apresentou-se', 'tambem', 'em', 'casa', 'de', 'manuel', 'a', 'sra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'D'\n",
      "Tokens gerados: ['d']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Amância Sousellas, velha de grande memória\n",
      "para citar fatos, datas e nomes; lembrava-se sempre do aniversário natalício dos seus inúmeros\n",
      "conhecidos, e nesse dia filava-lhes impreterivelmente o jantar'\n",
      "Tokens gerados: ['amancia', 'sousellas', ',', 'velha', 'de', 'grande', 'memoriapara', 'citar', 'fatos', ',', 'datas', 'e', 'nomes', 'lembrava-se', 'sempre', 'do', 'aniversario', 'natalicio', 'dos', 'seus', 'inumerosconhecidos', ',', 'e', 'nesse', 'dia', 'filava-lhes', 'impreterivelmente', 'o', 'jantar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estava sempre a falar mal da vida alheia,\n",
      "à sombra da qual aliás, vivia; quinze dias em casa de uma amiga, outros quinze em casa de um parente,\n",
      "o mês seguinte em casa de um parente e amigo, e assim por diante; sempre, sempre de passeio'\n",
      "Tokens gerados: ['estava', 'sempre', 'a', 'falar', 'mal', 'da', 'vida', 'alheia', ',', 'a', 'sombra', 'da', 'qual', 'alias', ',', 'vivia', 'quinze', 'dias', 'em', 'casa', 'de', 'uma', 'amiga', ',', 'outros', 'quinze', 'em', 'casa', 'de', 'um', 'parente', ',', 'o', 'mes', 'seguinte', 'em', 'casa', 'de', 'um', 'parente', 'e', 'amigo', ',', 'e', 'assim', 'por', 'diante', 'sempre', ',', 'sempre', 'de', 'passeio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ia a\n",
      "qualquer parte, fosse ou não fosse desejada, e, às duas por três, era da casa'\n",
      "Tokens gerados: ['ia', 'aqualquer', 'parte', ',', 'fosse', 'ou', 'nao', 'fosse', 'desejada', ',', 'e', ',', 'as', 'duas', 'por', 'tres', ',', 'era', 'da', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Conhecia todo o Maranhão;\n",
      "contava, sem reservas, os escândalos que lhe caíam no bico, e andava sozinha na rua, passarinhando\n",
      "por toda a cidade, de xale, metendo o nariz em tudo'\n",
      "Tokens gerados: ['conhecia', 'todo', 'o', 'maranhaocontava', ',', 'sem', 'reservas', ',', 'os', 'escandalos', 'que', 'lhe', 'caiam', 'no', 'bico', ',', 'e', 'andava', 'sozinha', 'na', 'rua', ',', 'passarinhandopor', 'toda', 'a', 'cidade', ',', 'de', 'xale', ',', 'metendo', 'o', 'nariz', 'em', 'tudo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se morria algum conhecido seu, lá estava ela, a\n",
      "vestir o cadáver, a cortar-lhe as unhas, a dizer os lugares-comuns da consolação, tida e citada por muito\n",
      "serviçal, ativa e prestimosa'\n",
      "Tokens gerados: ['se', 'morria', 'algum', 'conhecido', 'seu', ',', 'la', 'estava', 'ela', ',', 'avestir', 'o', 'cadaver', ',', 'a', 'cortar-lhe', 'as', 'unhas', ',', 'a', 'dizer', 'os', 'lugares-comuns', 'da', 'consolacao', ',', 'tida', 'e', 'citada', 'por', 'muitoservical', ',', 'ativa', 'e', 'prestimosa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era cronicamente virgem, mas afirmava que em moça, rejeitara muito casamento bom'\n",
      "Tokens gerados: ['era', 'cronicamente', 'virgem', ',', 'mas', 'afirmava', 'que', 'em', 'moca', ',', 'rejeitara', 'muito', 'casamento', 'bom']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dava-se\n",
      "a coisas de igreja; sabia vestir anjos de procissão e pintava os cabelos com cosmético preto'\n",
      "Tokens gerados: ['dava-sea', 'coisas', 'de', 'igreja', 'sabia', 'vestir', 'anjos', 'de', 'procissao', 'e', 'pintava', 'os', 'cabelos', 'com', 'cosmetico', 'preto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Detestava o progresso'\n",
      "Tokens gerados: ['detestava', 'o', 'progresso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— No seu tempo, dizia ela com azedume, as meninas tinham a sua tarefa de costura para tantas\n",
      "horas e haviam de pôr pr’ali o trabalho! se o acabavam mais cedo iam descansar?'\n",
      "Tokens gerados: ['—', 'no', 'seu', 'tempo', ',', 'dizia', 'ela', 'com', 'azedume', ',', 'as', 'meninas', 'tinham', 'a', 'sua', 'tarefa', 'de', 'costura', 'para', 'tantashoras', 'e', 'haviam', 'de', 'por', 'pr', '’', 'ali', 'o', 'trabalho', 'se', 'o', 'acabavam', 'mais', 'cedo', 'iam', 'descansar', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Boas! desmanchavam\n",
      "minha senhora! desmanchavam para fazer de novo! E hoje?'\n",
      "Tokens gerados: ['boas', 'desmanchavamminha', 'senhora', 'desmanchavam', 'para', 'fazer', 'de', 'novo', 'e', 'hoje', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'perguntava, dando um pulinho, com as\n",
      "mãos nas ilhargas — hoje é o maquiavelismo da máquina de costura! Dá-se uma tarefa grande e é só\n",
      "“zuc-zuc-zuc!” e está pronto o serviço! E daí, vai a sirigaita pôr-se de leitura nos jornais, tomar conta\n",
      "do romance ou então vai para a indecência do piano!\n",
      "E jurava que filha sua não havia de aprender semelhante instrumento, porque as desavergonhadas\n",
      "só queriam aquilo para melhor conversar com os namorados, sem que os outros dessem pela patifaria!\n",
      "Também dizia mal da iluminação a gás:\n",
      "— Dantes os escravos tinham que fazer! Mal serviam a janta iam aprontar e acender os candeeiros,\n",
      "deitar-lhes novo azeite e colocá-los no seu lugar'\n",
      "Tokens gerados: ['perguntava', ',', 'dando', 'um', 'pulinho', ',', 'com', 'asmaos', 'nas', 'ilhargas', '—', 'hoje', 'e', 'o', 'maquiavelismo', 'da', 'maquina', 'de', 'costura', 'da-se', 'uma', 'tarefa', 'grande', 'e', 'e', 'so', '“', 'zuc-zuc-zuc', '”', 'e', 'esta', 'pronto', 'o', 'servico', 'e', 'dai', ',', 'vai', 'a', 'sirigaita', 'por-se', 'de', 'leitura', 'nos', 'jornais', ',', 'tomar', 'contado', 'romance', 'ou', 'entao', 'vai', 'para', 'a', 'indecencia', 'do', 'pianoe', 'jurava', 'que', 'filha', 'sua', 'nao', 'havia', 'de', 'aprender', 'semelhante', 'instrumento', ',', 'porque', 'as', 'desavergonhadasso', 'queriam', 'aquilo', 'para', 'melhor', 'conversar', 'com', 'os', 'namorados', ',', 'sem', 'que', 'os', 'outros', 'dessem', 'pela', 'patifariatambem', 'dizia', 'mal', 'da', 'iluminacao', 'a', 'gas—', 'dantes', 'os', 'escravos', 'tinham', 'que', 'fazer', 'mal', 'serviam', 'a', 'janta', 'iam', 'aprontar', 'e', 'acender', 'os', 'candeeiros', ',', 'deitar-lhes', 'novo', 'azeite', 'e', 'coloca-los', 'no', 'seu', 'lugar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E hoje? É só chegar o palitinho de fogo à bruxaria do\n",
      "bico de gás e'\n",
      "Tokens gerados: ['e', 'hoje', '?', 'e', 'so', 'chegar', 'o', 'palitinho', 'de', 'fogo', 'a', 'bruxaria', 'dobico', 'de', 'gas', 'e']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'caia-se na pândega! Já não há tarefa! Já não há cativeiro! É por isso que eles andam tão\n",
      "descarados! Chicote! chicote, até dizer basta! que é do que eles precisam'\n",
      "Tokens gerados: ['caia-se', 'na', 'pandega', 'ja', 'nao', 'ha', 'tarefa', 'ja', 'nao', 'ha', 'cativeiro', 'e', 'por', 'isso', 'que', 'eles', 'andam', 'taodescarados', 'chicote', 'chicote', ',', 'ate', 'dizer', 'basta', 'que', 'e', 'do', 'que', 'eles', 'precisam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tivesse eu muitos, que lhes\n",
      "juro, pela bênção de minha madrinha, que lhes havia de tirar sangue do lombo!\n",
      "Mas a especialidade de D'\n",
      "Tokens gerados: ['tivesse', 'eu', 'muitos', ',', 'que', 'lhesjuro', ',', 'pela', 'bencao', 'de', 'minha', 'madrinha', ',', 'que', 'lhes', 'havia', 'de', 'tirar', 'sangue', 'do', 'lombomas', 'a', 'especialidade', 'de', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Amância Sousellas, o que a tornava adorável para certos rapazes e\n",
      "detestada por muitos pais de família que iam de nariz torcido lhe recebendo visitas e obséquios de\n",
      "cortesia, era, sem dúvida, o seu antigo hábito de contar anedotas baixas e grosseiras'\n",
      "Tokens gerados: ['amancia', 'sousellas', ',', 'o', 'que', 'a', 'tornava', 'adoravel', 'para', 'certos', 'rapazes', 'edetestada', 'por', 'muitos', 'pais', 'de', 'familia', 'que', 'iam', 'de', 'nariz', 'torcido', 'lhe', 'recebendo', 'visitas', 'e', 'obsequios', 'decortesia', ',', 'era', ',', 'sem', 'duvida', ',', 'o', 'seu', 'antigo', 'habito', 'de', 'contar', 'anedotas', 'baixas', 'e', 'grosseiras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sempre fora muito\n",
      "desbocada; no entanto alguns basbaques da sua roda, diziam dela, num frouxo de riso: “Com a D'\n",
      "Tokens gerados: ['sempre', 'fora', 'muitodesbocada', 'no', 'entanto', 'alguns', 'basbaques', 'da', 'sua', 'roda', ',', 'diziam', 'dela', ',', 'num', 'frouxo', 'de', 'riso', '“', 'com', 'a', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Amância não pode a gente estar séria! — O diabo da velha tem uma graça!'\n",
      "Tokens gerados: ['amancia', 'nao', 'pode', 'a', 'gente', 'estar', 'seria', '—', 'o', 'diabo', 'da', 'velha', 'tem', 'uma', 'graca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Lá estava também em casa de Manuel a Eufrasinha, viúva do oficial de infantaria'\n",
      "Tokens gerados: ['”', 'la', 'estava', 'tambem', 'em', 'casa', 'de', 'manuel', 'a', 'eufrasinha', ',', 'viuva', 'do', 'oficial', 'de', 'infantaria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Toda enfeitada\n",
      "de lacinhos de fita roxa, moreninha apesar da superabundância do pó-de-arroz; as feições muito\n",
      "desenhadas à superfície do rosto e com um sinal de nitrato de prata ao lado esquerdo da boca,\n",
      "desastradamente imitado do de uma francesa ex-cantora com quem ela se dava'\n",
      "Tokens gerados: ['toda', 'enfeitadade', 'lacinhos', 'de', 'fita', 'roxa', ',', 'moreninha', 'apesar', 'da', 'superabundancia', 'do', 'po-de-arroz', 'as', 'feicoes', 'muitodesenhadas', 'a', 'superficie', 'do', 'rosto', 'e', 'com', 'um', 'sinal', 'de', 'nitrato', 'de', 'prata', 'ao', 'lado', 'esquerdo', 'da', 'boca', ',', 'desastradamente', 'imitado', 'do', 'de', 'uma', 'francesa', 'ex-cantora', 'com', 'quem', 'ela', 'se', 'dava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O sinal era para ficar do\n",
      "tamanho de uma pulga e saiu do tamanho e do feitio de um feijão-preto'\n",
      "Tokens gerados: ['o', 'sinal', 'era', 'para', 'ficar', 'dotamanho', 'de', 'uma', 'pulga', 'e', 'saiu', 'do', 'tamanho', 'e', 'do', 'feitio', 'de', 'um', 'feijao-preto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Saracoteava-se, cheia de\n",
      "novidades, levantando-se de vez em quando, para ir dizer um segredinho ao ouvido de Ana Rosa,\n",
      "enquanto disfarçadamente lhe endireitava o penteado; nestes passeios olhava de esguelha para os quartos\n",
      "e para a varanda — dando fé — e voltava à sua cadeira, mirando-se a furto nos espelhos da sala, sempre\n",
      "muito curiosa, irrequieta, querendo achar em tudo que lhe diziam uma significação dupla, trejeitando\n",
      "sorrisos e momices expressivas quando não entendia, para fingir que compreendera perfeitamente'\n",
      "Tokens gerados: ['saracoteava-se', ',', 'cheia', 'denovidades', ',', 'levantando-se', 'de', 'vez', 'em', 'quando', ',', 'para', 'ir', 'dizer', 'um', 'segredinho', 'ao', 'ouvido', 'de', 'ana', 'rosa', ',', 'enquanto', 'disfarcadamente', 'lhe', 'endireitava', 'o', 'penteado', 'nestes', 'passeios', 'olhava', 'de', 'esguelha', 'para', 'os', 'quartose', 'para', 'a', 'varanda', '—', 'dando', 'fe', '—', 'e', 'voltava', 'a', 'sua', 'cadeira', ',', 'mirando-se', 'a', 'furto', 'nos', 'espelhos', 'da', 'sala', ',', 'sempremuito', 'curiosa', ',', 'irrequieta', ',', 'querendo', 'achar', 'em', 'tudo', 'que', 'lhe', 'diziam', 'uma', 'significacao', 'dupla', ',', 'trejeitandosorrisos', 'e', 'momices', 'expressivas', 'quando', 'nao', 'entendia', ',', 'para', 'fingir', 'que', 'compreendera', 'perfeitamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha a voz sibilante e afetada, assoviava os SS, e dizia silabadas'\n",
      "Tokens gerados: ['tinha', 'a', 'voz', 'sibilante', 'e', 'afetada', ',', 'assoviava', 'os', 'ss', ',', 'e', 'dizia', 'silabadas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Freitas, em cuja casa Ana Rosa tivera o seu último histérico, também se achava presente, com\n",
      "a filha, a sua querida Lindoca'\n",
      "Tokens gerados: ['o', 'freitas', ',', 'em', 'cuja', 'casa', 'ana', 'rosa', 'tivera', 'o', 'seu', 'ultimo', 'histerico', ',', 'tambem', 'se', 'achava', 'presente', ',', 'coma', 'filha', ',', 'a', 'sua', 'querida', 'lindoca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Freitas era um homem desquitado da mulher “que se atirara aos cães”, explicava friamente,\n",
      "muito teso, magro, alto, com o pescocinho comprido no seu grande colarinho em pé'\n",
      "Tokens gerados: ['o', 'freitas', 'era', 'um', 'homem', 'desquitado', 'da', 'mulher', '“', 'que', 'se', 'atirara', 'aos', 'caes', '”', ',', 'explicava', 'friamente', ',', 'muito', 'teso', ',', 'magro', ',', 'alto', ',', 'com', 'o', 'pescocinho', 'comprido', 'no', 'seu', 'grande', 'colarinho', 'em', 'pe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não relaxava as\n",
      "calças brancas, e gabava-se do segredo de conservá-las limpas e engomadas durante uma semana;\n",
      "trazia sempre, apesar do calor da província, o colarinho duro e o peito da camisa irrepreensível; gravata\n",
      "preta — invariavelmente'\n",
      "Tokens gerados: ['nao', 'relaxava', 'ascalcas', 'brancas', ',', 'e', 'gabava-se', 'do', 'segredo', 'de', 'conserva-las', 'limpas', 'e', 'engomadas', 'durante', 'uma', 'semanatrazia', 'sempre', ',', 'apesar', 'do', 'calor', 'da', 'provincia', ',', 'o', 'colarinho', 'duro', 'e', 'o', 'peito', 'da', 'camisa', 'irrepreensivel', 'gravatapreta', '—', 'invariavelmente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tratava uma enorme unha no dedo mínimo, com a qual costumava pentear o\n",
      "bigode, feito de longos fios, tingidos e lisos, que lhe velavam a boca'\n",
      "Tokens gerados: ['tratava', 'uma', 'enorme', 'unha', 'no', 'dedo', 'minimo', ',', 'com', 'a', 'qual', 'costumava', 'pentear', 'obigode', ',', 'feito', 'de', 'longos', 'fios', ',', 'tingidos', 'e', 'lisos', ',', 'que', 'lhe', 'velavam', 'a', 'boca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Jamais consentia que barbeiro\n",
      "algum “lhe encostasse a mão no rosto”; fazia ele mesmo a sua barba, um dia sim, outro não'\n",
      "Tokens gerados: ['jamais', 'consentia', 'que', 'barbeiroalgum', '“', 'lhe', 'encostasse', 'a', 'mao', 'no', 'rosto', '”', 'fazia', 'ele', 'mesmo', 'a', 'sua', 'barba', ',', 'um', 'dia', 'sim', ',', 'outro', 'nao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Escondia\n",
      "a calva com as compridíssimas farripas do cabelo, muito espichadas, como que grudadas a goma-arábica\n",
      "sobre o crânio'\n",
      "Tokens gerados: ['escondiaa', 'calva', 'com', 'as', 'compridissimas', 'farripas', 'do', 'cabelo', ',', 'muito', 'espichadas', ',', 'como', 'que', 'grudadas', 'a', 'goma-arabicasobre', 'o', 'cranio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dispunha de uma memória prodigiosa, gabada por toda a cidade; fazia-se grande\n",
      "conhecedor da história antiga; quando falava escolhia termos, procurava fazer estilo, e, sempre que se\n",
      "referia ao Imperador dizia gravemente: “O nosso defensor perpétuo!” Afiançavam que era habilidoso;\n",
      "em tempo fizera, com muita paciência, uma árvore genealógica de sua família e mandara-a litografar\n",
      "no Rio de Janeiro'\n",
      "Tokens gerados: ['dispunha', 'de', 'uma', 'memoria', 'prodigiosa', ',', 'gabada', 'por', 'toda', 'a', 'cidade', 'fazia-se', 'grandeconhecedor', 'da', 'historia', 'antiga', 'quando', 'falava', 'escolhia', 'termos', ',', 'procurava', 'fazer', 'estilo', ',', 'e', ',', 'sempre', 'que', 'sereferia', 'ao', 'imperador', 'dizia', 'gravemente', '“', 'o', 'nosso', 'defensor', 'perpetuo', '”', 'afiancavam', 'que', 'era', 'habilidosoem', 'tempo', 'fizera', ',', 'com', 'muita', 'paciencia', ',', 'uma', 'arvore', 'genealogica', 'de', 'sua', 'familia', 'e', 'mandara-a', 'litografarno', 'rio', 'de', 'janeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Este trabalho foi muito apreciado e comentado na província'\n",
      "Tokens gerados: ['este', 'trabalho', 'foi', 'muito', 'apreciado', 'e', 'comentado', 'na', 'provincia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era empregado público havia vinte e cinco anos e só faltara à repartição três vezes — por uma\n",
      "queda, um antraz, e no dia do seu malfadado casamento; contava isto a todos, com glória'\n",
      "Tokens gerados: ['era', 'empregado', 'publico', 'havia', 'vinte', 'e', 'cinco', 'anos', 'e', 'so', 'faltara', 'a', 'reparticao', 'tres', 'vezes', '—', 'por', 'umaqueda', ',', 'um', 'antraz', ',', 'e', 'no', 'dia', 'do', 'seu', 'malfadado', 'casamento', 'contava', 'isto', 'a', 'todos', ',', 'com', 'gloria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando\n",
      "temia constipar-se, aspirava cautelosamente o fartum do conhaque'\n",
      "Tokens gerados: ['quandotemia', 'constipar-se', ',', 'aspirava', 'cautelosamente', 'o', 'fartum', 'do', 'conhaque']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Isto e o bastante para me fazer\n",
      "ficar tonto!'\n",
      "Tokens gerados: ['“', 'isto', 'e', 'o', 'bastante', 'para', 'me', 'fazerficar', 'tonto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” afirmava com uma repugnância virtuosa'\n",
      "Tokens gerados: ['”', 'afirmava', 'com', 'uma', 'repugnancia', 'virtuosa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha honor às cartas e sabia tocar clarinete,\n",
      "mas nunca tocava, porque o médico lhe dissera ‘’não achar prudente”'\n",
      "Tokens gerados: ['tinha', 'honor', 'as', 'cartas', 'e', 'sabia', 'tocar', 'clarinete', ',', 'mas', 'nunca', 'tocava', ',', 'porque', 'o', 'medico', 'lhe', 'dissera', '‘', '’', 'nao', 'achar', 'prudente', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fumara em tempo, mas o médico\n",
      "dissera do charuto o mesmo que do clarinete'\n",
      "Tokens gerados: ['fumara', 'em', 'tempo', ',', 'mas', 'o', 'medicodissera', 'do', 'charuto', 'o', 'mesmo', 'que', 'do', 'clarinete']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Nunca mais fumou'\n",
      "Tokens gerados: ['—', 'nunca', 'mais', 'fumou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não dançava, para não suar;\n",
      "falava com raiva das mulheres e, nem caindo de fome, seria capaz de comer à noite'\n",
      "Tokens gerados: ['nao', 'dancava', ',', 'para', 'nao', 'suarfalava', 'com', 'raiva', 'das', 'mulheres', 'e', ',', 'nem', 'caindo', 'de', 'fome', ',', 'seria', 'capaz', 'de', 'comer', 'a', 'noite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Além do chá,\n",
      "nada! nada!” protestava com firmeza; estivesse onde estivesse, havia de retirar-se impreterivelmente à\n",
      "meia-noite'\n",
      "Tokens gerados: ['“', 'alem', 'do', 'cha', ',', 'nada', 'nada', '”', 'protestava', 'com', 'firmeza', 'estivesse', 'onde', 'estivesse', ',', 'havia', 'de', 'retirar-se', 'impreterivelmente', 'ameia-noite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Usava sapatos rasos, de polimento, e nunca se esquecia do chapéu-de-sol'\n",
      "Tokens gerados: ['usava', 'sapatos', 'rasos', ',', 'de', 'polimento', ',', 'e', 'nunca', 'se', 'esquecia', 'do', 'chapeu-de-sol']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Jamais arredara o pé da ilha de São Luís do Maranhão, tal era o medo que tinha do mar'\n",
      "Tokens gerados: ['jamais', 'arredara', 'o', 'pe', 'da', 'ilha', 'de', 'sao', 'luis', 'do', 'maranhao', ',', 'tal', 'era', 'o', 'medo', 'que', 'tinha', 'do', 'mar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Nem para ir a Alcântara! jurava ele, conversando essa noite em casa do Manuel'\n",
      "Tokens gerados: ['—', 'nem', 'para', 'ir', 'a', 'alcantara', 'jurava', 'ele', ',', 'conversando', 'essa', 'noite', 'em', 'casa', 'do', 'manuel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Daqui —\n",
      "para o Gavião! Nada, meu caro senhor, quero morrer na minha caminha, sossegado, bem com Deus!\n",
      "— Com toda a comodidade, observou Raimundo, a rir'\n",
      "Tokens gerados: ['daqui', '—para', 'o', 'gaviao', 'nada', ',', 'meu', 'caro', 'senhor', ',', 'quero', 'morrer', 'na', 'minha', 'caminha', ',', 'sossegado', ',', 'bem', 'com', 'deus—', 'com', 'toda', 'a', 'comodidade', ',', 'observou', 'raimundo', ',', 'a', 'rir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era devoto: todos os anos carregava na procissão o andor do milagroso Senhor Bom Jesus dos\n",
      "Passos'\n",
      "Tokens gerados: ['era', 'devoto', 'todos', 'os', 'anos', 'carregava', 'na', 'procissao', 'o', 'andor', 'do', 'milagroso', 'senhor', 'bom', 'jesus', 'dospassos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E muito arranjadinho: “Em casa dele havia de tudo, como na botica'\n",
      "Tokens gerados: ['e', 'muito', 'arranjadinho', '“', 'em', 'casa', 'dele', 'havia', 'de', 'tudo', ',', 'como', 'na', 'botica']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” Diziam os seus íntimos'\n",
      "Tokens gerados: ['”', 'diziam', 'os', 'seus', 'intimos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Só falta dinheiro'\n",
      "Tokens gerados: ['“', 'so', 'falta', 'dinheiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” completava o Freitas em ar discreto de pilhéria'\n",
      "Tokens gerados: ['”', 'completava', 'o', 'freitas', 'em', 'ar', 'discreto', 'de', 'pilheria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No mais: — sempre o mesmo\n",
      "homem; nunca fora de estroinices; mesmo em rapaz, era já metido consigo; não gostava de dever a\n",
      "ninguém; colecionava selos velhos; dava homeopatia de graça, aos amigos, e tinha a fama do maior\n",
      "maçante do Maranhão'\n",
      "Tokens gerados: ['no', 'mais', '—', 'sempre', 'o', 'mesmohomem', 'nunca', 'fora', 'de', 'estroinices', 'mesmo', 'em', 'rapaz', ',', 'era', 'ja', 'metido', 'consigo', 'nao', 'gostava', 'de', 'dever', 'aninguem', 'colecionava', 'selos', 'velhos', 'dava', 'homeopatia', 'de', 'graca', ',', 'aos', 'amigos', ',', 'e', 'tinha', 'a', 'fama', 'do', 'maiormacante', 'do', 'maranhao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A tal “sua querida Lindoca” era uma menina de dezesseis anos, pequenina, extremamente gorda,\n",
      "quase redonda, bonitinha de feições, curta de idéias, bom coração e temperamento honesto'\n",
      "Tokens gerados: ['a', 'tal', '“', 'sua', 'querida', 'lindoca', '”', 'era', 'uma', 'menina', 'de', 'dezesseis', 'anos', ',', 'pequenina', ',', 'extremamente', 'gorda', ',', 'quase', 'redonda', ',', 'bonitinha', 'de', 'feicoes', ',', 'curta', 'de', 'ideias', ',', 'bom', 'coracao', 'e', 'temperamento', 'honesto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A Etelvina\n",
      "dissera uma vez que ela estava engordando até nos miolos'\n",
      "Tokens gerados: ['a', 'etelvinadissera', 'uma', 'vez', 'que', 'ela', 'estava', 'engordando', 'ate', 'nos', 'miolos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lindoca Freitas não escondia o seu desejo de casar e amava extremosamente o pai, a quem só\n",
      "tratava por “Nhozinho”'\n",
      "Tokens gerados: ['lindoca', 'freitas', 'nao', 'escondia', 'o', 'seu', 'desejo', 'de', 'casar', 'e', 'amava', 'extremosamente', 'o', 'pai', ',', 'a', 'quem', 'sotratava', 'por', '“', 'nhozinho', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Tenho um desgosto desta gordura!'\n",
      "Tokens gerados: ['—', 'tenho', 'um', 'desgosto', 'desta', 'gordura']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lamentava-se ela às camaradas, que lhe elogiavam a\n",
      "exuberância adiposa'\n",
      "Tokens gerados: ['lamentava-se', 'ela', 'as', 'camaradas', ',', 'que', 'lhe', 'elogiavam', 'aexuberancia', 'adiposa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se eu soubesse de um remédio para emagrecer'\n",
      "Tokens gerados: ['se', 'eu', 'soubesse', 'de', 'um', 'remedio', 'para', 'emagrecer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'tomava!\n",
      "As amigas procuravam consolá-la: “Dá-me gordura que te darei formosura! — Gordura é saúde!”\n",
      "Mas a repolhuda moça não se conformava com aquela desgraça'\n",
      "Tokens gerados: ['tomavaas', 'amigas', 'procuravam', 'consola-la', '“', 'da-me', 'gordura', 'que', 'te', 'darei', 'formosura', '—', 'gordura', 'e', 'saude', '”', 'mas', 'a', 'repolhuda', 'moca', 'nao', 'se', 'conformava', 'com', 'aquela', 'desgraca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vivia triste'\n",
      "Tokens gerados: ['vivia', 'triste']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As banhas\n",
      "cresciam-lhe cada vez mais; estava vermelha; cansava por cinco passos'\n",
      "Tokens gerados: ['as', 'banhascresciam-lhe', 'cada', 'vez', 'mais', 'estava', 'vermelha', 'cansava', 'por', 'cinco', 'passos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era um desgosto sério! Recorria\n",
      "ao vinagre; dava-se a longos exercícios pela varanda; mas qual! — as enxúndias aumentavam sempre'\n",
      "Tokens gerados: ['era', 'um', 'desgosto', 'serio', 'recorriaao', 'vinagre', 'dava-se', 'a', 'longos', 'exercicios', 'pela', 'varanda', 'mas', 'qual', '—', 'as', 'enxundias', 'aumentavam', 'sempre']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lindoca estava cada vez mais redonda, mais boleada; a casa estremecia cada vez mais com o seu peso;\n",
      "os olhos desapareciam-lhe na abundância das bochechas; o seu nariz parecia um lombinho; as suas\n",
      "costas uma almofada'\n",
      "Tokens gerados: ['lindoca', 'estava', 'cada', 'vez', 'mais', 'redonda', ',', 'mais', 'boleada', 'a', 'casa', 'estremecia', 'cada', 'vez', 'mais', 'com', 'o', 'seu', 'pesoos', 'olhos', 'desapareciam-lhe', 'na', 'abundancia', 'das', 'bochechas', 'o', 'seu', 'nariz', 'parecia', 'um', 'lombinho', 'as', 'suascostas', 'uma', 'almofada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bufava'\n",
      "Tokens gerados: ['bufava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dias, o piedoso, o doce Luís Dias, também comparecera aquela noite à sala do patrão'\n",
      "Tokens gerados: ['dias', ',', 'o', 'piedoso', ',', 'o', 'doce', 'luis', 'dias', ',', 'tambem', 'comparecera', 'aquela', 'noite', 'a', 'sala', 'do', 'patrao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lá estava,\n",
      "metido a um canto, roendo ferozmente as unhas, o olhar imóvel sobre Ana Rosa, que, ao piano,\n",
      "dispunha-se a tocar alguma coisa e experimentava as teclas'\n",
      "Tokens gerados: ['la', 'estava', ',', 'metido', 'a', 'um', 'canto', ',', 'roendo', 'ferozmente', 'as', 'unhas', ',', 'o', 'olhar', 'imovel', 'sobre', 'ana', 'rosa', ',', 'que', ',', 'ao', 'piano', ',', 'dispunha-se', 'a', 'tocar', 'alguma', 'coisa', 'e', 'experimentava', 'as', 'teclas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em uma das janelas da frente, encostados contra a sacada, Manuel e o cônego Diogo ouviam de\n",
      "Raimundo a descrição em voz baixa de um passeio de Paris à Suíça'\n",
      "Tokens gerados: ['em', 'uma', 'das', 'janelas', 'da', 'frente', ',', 'encostados', 'contra', 'a', 'sacada', ',', 'manuel', 'e', 'o', 'conego', 'diogo', 'ouviam', 'deraimundo', 'a', 'descricao', 'em', 'voz', 'baixa', 'de', 'um', 'passeio', 'de', 'paris', 'a', 'suica']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No resto da sala corria o sussurro\n",
      "das senhoras, que conversavam'\n",
      "Tokens gerados: ['no', 'resto', 'da', 'sala', 'corria', 'o', 'sussurrodas', 'senhoras', ',', 'que', 'conversavam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então! Estamos passando o Boqueirão? exclamou o Freitas, erguendo-se do sofá, a sacudir\n",
      "as calças, para evitar as joelheiras'\n",
      "Tokens gerados: ['—', 'entao', 'estamos', 'passando', 'o', 'boqueirao', '?', 'exclamou', 'o', 'freitas', ',', 'erguendo-se', 'do', 'sofa', ',', 'a', 'sacudiras', 'calcas', ',', 'para', 'evitar', 'as', 'joelheiras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, voltando-se para uma das sobrinhas de D'\n",
      "Tokens gerados: ['e', ',', 'voltando-se', 'para', 'uma', 'das', 'sobrinhas', 'de', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Maria do Carmo: —\n",
      "Diga alguma coisa, D'\n",
      "Tokens gerados: ['maria', 'do', 'carmo', '—diga', 'alguma', 'coisa', ',', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Etelvina!'\n",
      "Tokens gerados: ['etelvina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Etelvina ergueu os olhos para o teto e soltou um suspiro'\n",
      "Tokens gerados: ['etelvina', 'ergueu', 'os', 'olhos', 'para', 'o', 'teto', 'e', 'soltou', 'um', 'suspiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Por quem suspiras? perguntou-lhe, em misterioso falsete, a velha Amância que lhe ficava ao\n",
      "lado'\n",
      "Tokens gerados: ['—', 'por', 'quem', 'suspiras', '?', 'perguntou-lhe', ',', 'em', 'misterioso', 'falsete', ',', 'a', 'velha', 'amancia', 'que', 'lhe', 'ficava', 'aolado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Por ninguém'\n",
      "Tokens gerados: ['—', 'por', 'ninguem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'respondeu a Lagartixa, sorrindo melancolicamente com os caquinhos dos\n",
      "dentes'\n",
      "Tokens gerados: ['respondeu', 'a', 'lagartixa', ',', 'sorrindo', 'melancolicamente', 'com', 'os', 'caquinhos', 'dosdentes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ele não é feio'\n",
      "Tokens gerados: ['—', 'ele', 'nao', 'e', 'feio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'a senhora não acha D'\n",
      "Tokens gerados: ['a', 'senhora', 'nao', 'acha', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bibina?'\n",
      "Tokens gerados: ['bibina', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'segredava Lindoca à outra sobrinha de D'\n",
      "Tokens gerados: ['segredava', 'lindoca', 'a', 'outra', 'sobrinha', 'de', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Maria do Carmo, olhando furtivamente para o lado de Raimundo'\n",
      "Tokens gerados: ['maria', 'do', 'carmo', ',', 'olhando', 'furtivamente', 'para', 'o', 'lado', 'de', 'raimundo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Quem? O primo d’Ana Rosa?\n",
      "— Primo? Eu creio que ele não é primo, dona!\n",
      "— É! sustentou Bibina quase com arrelia'\n",
      "Tokens gerados: ['—', 'quem', '?', 'o', 'primo', 'd', '’', 'ana', 'rosa', '?', '—', 'primo', '?', 'eu', 'creio', 'que', 'ele', 'nao', 'e', 'primo', ',', 'dona—', 'e', 'sustentou', 'bibina', 'quase', 'com', 'arrelia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É primo, sim, por parte de pai!'\n",
      "Tokens gerados: ['e', 'primo', ',', 'sim', ',', 'por', 'parte', 'de', 'pai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E olhe, ali está quem\n",
      "lhe sabe bem a história!'\n",
      "Tokens gerados: ['e', 'olhe', ',', 'ali', 'esta', 'quemlhe', 'sabe', 'bem', 'a', 'historia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E indicava a tia com o beiço inferior'\n",
      "Tokens gerados: ['e', 'indicava', 'a', 'tia', 'com', 'o', 'beico', 'inferior']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— An'\n",
      "Tokens gerados: ['—', 'an']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'resmungou a gorducha, passando a considerar da cabeça aos pés o objeto da discussão'\n",
      "Tokens gerados: ['resmungou', 'a', 'gorducha', ',', 'passando', 'a', 'considerar', 'da', 'cabeca', 'aos', 'pes', 'o', 'objeto', 'da', 'discussao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por outro lado, Maria do Carmo segredava a Amância Sousellas:\n",
      "— Pois é o que lhe digo, D'\n",
      "Tokens gerados: ['por', 'outro', 'lado', ',', 'maria', 'do', 'carmo', 'segredava', 'a', 'amancia', 'sousellas—', 'pois', 'e', 'o', 'que', 'lhe', 'digo', ',', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Amância: muito boa preta!'\n",
      "Tokens gerados: ['amancia', 'muito', 'boa', 'preta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'negra como este vestido! Cá está quem\n",
      "a conheceu!'\n",
      "Tokens gerados: ['negra', 'como', 'este', 'vestido', 'ca', 'esta', 'quema', 'conheceu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E batia no seu peito sem seios'\n",
      "Tokens gerados: ['e', 'batia', 'no', 'seu', 'peito', 'sem', 'seios']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Muita vez a vi no relho'\n",
      "Tokens gerados: ['—', 'muita', 'vez', 'a', 'vi', 'no', 'relho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Iche!\n",
      "— Ora quem houvera de dizer!'\n",
      "Tokens gerados: ['iche—', 'ora', 'quem', 'houvera', 'de', 'dizer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'resmungou a outra, fingindo ignorar da existência de Domingas,\n",
      "para ouvir mais'\n",
      "Tokens gerados: ['resmungou', 'a', 'outra', ',', 'fingindo', 'ignorar', 'da', 'existencia', 'de', 'domingas', ',', 'para', 'ouvir', 'mais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma coisa assim só no Maranhão! Credo!\n",
      "— É como lhe digo, minha rica! O sujeitinho foi forro à pia, e hoje, olhe só pr’aquilo! está todo\n",
      "cheio de fumaças e de filáucias!'\n",
      "Tokens gerados: ['uma', 'coisa', 'assim', 'so', 'no', 'maranhao', 'credo—', 'e', 'como', 'lhe', 'digo', ',', 'minha', 'rica', 'o', 'sujeitinho', 'foi', 'forro', 'a', 'pia', ',', 'e', 'hoje', ',', 'olhe', 'so', 'pr', '’', 'aquilo', 'esta', 'todocheio', 'de', 'fumacas', 'e', 'de', 'filaucias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pergunte ao cônego, que está ao lado dele!\n",
      "— Cruz! T’arrenego, pé-de-pato!\n",
      "E Amância bateu por hábito nas faces engelhadas'\n",
      "Tokens gerados: ['pergunte', 'ao', 'conego', ',', 'que', 'esta', 'ao', 'lado', 'dele—', 'cruz', 't', '’', 'arrenego', ',', 'pe-de-patoe', 'amancia', 'bateu', 'por', 'habito', 'nas', 'faces', 'engelhadas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nisto, ouviu-se um grande motim, que vinha da varanda'\n",
      "Tokens gerados: ['nisto', ',', 'ouviu-se', 'um', 'grande', 'motim', ',', 'que', 'vinha', 'da', 'varanda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ó Benedito! Moleque! Ó peste! Está dormindo, sem-vergonha?!\n",
      "E logo o estalo de uma bofetada'\n",
      "Tokens gerados: ['—', 'o', 'benedito', 'moleque', 'o', 'peste', 'esta', 'dormindo', ',', 'sem-vergonha', '?', 'e', 'logo', 'o', 'estalo', 'de', 'uma', 'bofetada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Arre! que até me fazes zangar com visitas na sala!'\n",
      "Tokens gerados: ['—', 'arre', 'que', 'ate', 'me', 'fazes', 'zangar', 'com', 'visitas', 'na', 'sala']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era Maria Bárbara, que andava às voltas com o Benedito'\n",
      "Tokens gerados: ['era', 'maria', 'barbara', ',', 'que', 'andava', 'as', 'voltas', 'com', 'o', 'benedito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Vai deitar a mesa do chá moleque!\n",
      "Manuel correu logo à varanda, contrariado'\n",
      "Tokens gerados: ['—', 'vai', 'deitar', 'a', 'mesa', 'do', 'cha', 'molequemanuel', 'correu', 'logo', 'a', 'varanda', ',', 'contrariado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ó senhora!'\n",
      "Tokens gerados: ['—', 'o', 'senhora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'disse à sogra'\n",
      "Tokens gerados: ['disse', 'a', 'sogra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que inferneira! Olhe que está aí gente de fora!'\n",
      "Tokens gerados: ['que', 'inferneira', 'olhe', 'que', 'esta', 'ai', 'gente', 'de', 'fora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Freitas passou-se à janela de Raimundo, e aproveitou a oportunidade para despejar contra este\n",
      "uma estopada a respeito do mau serviço doméstico feito pelos escravos'\n",
      "Tokens gerados: ['freitas', 'passou-se', 'a', 'janela', 'de', 'raimundo', ',', 'e', 'aproveitou', 'a', 'oportunidade', 'para', 'despejar', 'contra', 'esteuma', 'estopada', 'a', 'respeito', 'do', 'mau', 'servico', 'domestico', 'feito', 'pelos', 'escravos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Reconheço que nos são necessários, reconheço!'\n",
      "Tokens gerados: ['—', 'reconheco', 'que', 'nos', 'sao', 'necessarios', ',', 'reconheco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'mas não podem ser mais imorais do que\n",
      "são!'\n",
      "Tokens gerados: ['mas', 'nao', 'podem', 'ser', 'mais', 'imorais', 'do', 'quesao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As negras, principalmente as negras!'\n",
      "Tokens gerados: ['as', 'negras', ',', 'principalmente', 'as', 'negras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'São umas muruxabas, que um pai de família tem em casa,\n",
      "e que dormem debaixo da rede das filhas e que lhes contam histórias indecentes! É uma imoralidade!\n",
      "Ainda outro dia, em certa casa, uma menina, coitada, apareceu coberta de piolhos indecorosos, que\n",
      "pegara da negra! Sei de outro caso de uma escrava que contagiou a uma família inteira de impigens e\n",
      "dartros de caráter feio! E note, doutor, que isto é o menos, o pior é que elas contam às suas sinhazinhas\n",
      "tudo o que praticam aí por essas ruas! Ficam as pobres moças sujas de corpo e alma na companhia de\n",
      "semelhante corja! Afianço-lhe, meu caro senhor doutor, que, se conservo pretos ao meu serviço, é\n",
      "porque não tenho outro remédio! Contudo'\n",
      "Tokens gerados: ['sao', 'umas', 'muruxabas', ',', 'que', 'um', 'pai', 'de', 'familia', 'tem', 'em', 'casa', ',', 'e', 'que', 'dormem', 'debaixo', 'da', 'rede', 'das', 'filhas', 'e', 'que', 'lhes', 'contam', 'historias', 'indecentes', 'e', 'uma', 'imoralidadeainda', 'outro', 'dia', ',', 'em', 'certa', 'casa', ',', 'uma', 'menina', ',', 'coitada', ',', 'apareceu', 'coberta', 'de', 'piolhos', 'indecorosos', ',', 'quepegara', 'da', 'negra', 'sei', 'de', 'outro', 'caso', 'de', 'uma', 'escrava', 'que', 'contagiou', 'a', 'uma', 'familia', 'inteira', 'de', 'impigens', 'edartros', 'de', 'carater', 'feio', 'e', 'note', ',', 'doutor', ',', 'que', 'isto', 'e', 'o', 'menos', ',', 'o', 'pior', 'e', 'que', 'elas', 'contam', 'as', 'suas', 'sinhazinhastudo', 'o', 'que', 'praticam', 'ai', 'por', 'essas', 'ruas', 'ficam', 'as', 'pobres', 'mocas', 'sujas', 'de', 'corpo', 'e', 'alma', 'na', 'companhia', 'desemelhante', 'corja', 'afianco-lhe', ',', 'meu', 'caro', 'senhor', 'doutor', ',', 'que', ',', 'se', 'conservo', 'pretos', 'ao', 'meu', 'servico', ',', 'eporque', 'nao', 'tenho', 'outro', 'remedio', 'contudo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi interrompido por Benedito que, nu da cintura para cima e acossado pela velha Bárbara,\n",
      "atravessou a sala com agilidade de macaco'\n",
      "Tokens gerados: ['foi', 'interrompido', 'por', 'benedito', 'que', ',', 'nu', 'da', 'cintura', 'para', 'cima', 'e', 'acossado', 'pela', 'velha', 'barbara', ',', 'atravessou', 'a', 'sala', 'com', 'agilidade', 'de', 'macaco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As senhoras espantaram-se, mas abriram logo em gargalhadas'\n",
      "Tokens gerados: ['as', 'senhoras', 'espantaram-se', ',', 'mas', 'abriram', 'logo', 'em', 'gargalhadas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O moleque alcançara a porta da escada e fugira'\n",
      "Tokens gerados: ['o', 'moleque', 'alcancara', 'a', 'porta', 'da', 'escada', 'e', 'fugira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Então, o Dias, que até aí se conservara quieto no seu\n",
      "canto, ergueu-se de um pulo e deitou a correr atrás dele'\n",
      "Tokens gerados: ['entao', ',', 'o', 'dias', ',', 'que', 'ate', 'ai', 'se', 'conservara', 'quieto', 'no', 'seucanto', ',', 'ergueu-se', 'de', 'um', 'pulo', 'e', 'deitou', 'a', 'correr', 'atras', 'dele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desapareceram ambos'\n",
      "Tokens gerados: ['desapareceram', 'ambos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Benedito era cria de Maria Bárbara; um pretinho seco, retinto, muito levado dos diabos; pernas\n",
      "compridas, beiços enormes, dentes branquíssimos'\n",
      "Tokens gerados: ['benedito', 'era', 'cria', 'de', 'maria', 'barbara', 'um', 'pretinho', 'seco', ',', 'retinto', ',', 'muito', 'levado', 'dos', 'diabos', 'pernascompridas', ',', 'beicos', 'enormes', ',', 'dentes', 'branquissimos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quebrava muita louça e fugia de casa constantemente'\n",
      "Tokens gerados: ['quebrava', 'muita', 'louca', 'e', 'fugia', 'de', 'casa', 'constantemente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A velha estacara no meio da sala furiosa'\n",
      "Tokens gerados: ['a', 'velha', 'estacara', 'no', 'meio', 'da', 'sala', 'furiosa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ai, gentes! não reparem!'\n",
      "Tokens gerados: ['—', 'ai', ',', 'gentes', 'nao', 'reparem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'bradou'\n",
      "Tokens gerados: ['bradou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aquele não-sei-que-diga, aquele maldito moleque!'\n",
      "Tokens gerados: ['aquele', 'nao-sei-que-diga', ',', 'aquele', 'maldito', 'moleque']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois o desavergonhado não queria vir trazer água na sala, sem pôr uma camisa?'\n",
      "Tokens gerados: ['pois', 'o', 'desavergonhado', 'nao', 'queria', 'vir', 'trazer', 'agua', 'na', 'sala', ',', 'sem', 'por', 'uma', 'camisa', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Patife! Ah, se o\n",
      "pego!'\n",
      "Tokens gerados: ['patife', 'ah', ',', 'se', 'opego']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas deixa estar, que não as perdes, malvado!\n",
      "E correndo à janela: — Se seu Dias não te alcançar, tens amanhã um campeche te seguindo a\n",
      "pista, sem-vergonha!\n",
      "E saiu de novo para a varanda, muito atarefada, gritando pela Brígida:\n",
      "— Ó Brígida! Também estás dormindo, seu diabo?!\n",
      "Na sala as visitas discutiam rindo a cena do moleque e o mau gênio de Maria Bárbara, mas\n",
      "tiveram de abafar a voz, porque Ana Rosa pôs-se a tocar uma polca ao piano'\n",
      "Tokens gerados: ['mas', 'deixa', 'estar', ',', 'que', 'nao', 'as', 'perdes', ',', 'malvadoe', 'correndo', 'a', 'janela', '—', 'se', 'seu', 'dias', 'nao', 'te', 'alcancar', ',', 'tens', 'amanha', 'um', 'campeche', 'te', 'seguindo', 'apista', ',', 'sem-vergonhae', 'saiu', 'de', 'novo', 'para', 'a', 'varanda', ',', 'muito', 'atarefada', ',', 'gritando', 'pela', 'brigida—', 'o', 'brigida', 'tambem', 'estas', 'dormindo', ',', 'seu', 'diabo', '?', 'na', 'sala', 'as', 'visitas', 'discutiam', 'rindo', 'a', 'cena', 'do', 'moleque', 'e', 'o', 'mau', 'genio', 'de', 'maria', 'barbara', ',', 'mastiveram', 'de', 'abafar', 'a', 'voz', ',', 'porque', 'ana', 'rosa', 'pos-se', 'a', 'tocar', 'uma', 'polca', 'ao', 'piano']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pouco depois, ouviu-se um farfalhar de saias engomadas, e em seguida apresentou-se a Brígida,\n",
      "uma mulata corpulenta a carapinha muito trançada e cheia de flores, um vestido de chita com três\n",
      "palmos de cauda, recendendo a cumaru'\n",
      "Tokens gerados: ['pouco', 'depois', ',', 'ouviu-se', 'um', 'farfalhar', 'de', 'saias', 'engomadas', ',', 'e', 'em', 'seguida', 'apresentou-se', 'a', 'brigida', ',', 'uma', 'mulata', 'corpulenta', 'a', 'carapinha', 'muito', 'trancada', 'e', 'cheia', 'de', 'flores', ',', 'um', 'vestido', 'de', 'chita', 'com', 'trespalmos', 'de', 'cauda', ',', 'recendendo', 'a', 'cumaru']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Preparava-se daquele modo, para ir à sala, oferecer água'\n",
      "Tokens gerados: ['preparava-se', 'daquele', 'modo', ',', 'para', 'ir', 'a', 'sala', ',', 'oferecer', 'agua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E,\n",
      "segurando com ambas as mãos uma enorme salva de prata, cheia de copos, dirigia-se a todos, um por\n",
      "um, a bambalear as ancas volumosas'\n",
      "Tokens gerados: ['e', ',', 'segurando', 'com', 'ambas', 'as', 'maos', 'uma', 'enorme', 'salva', 'de', 'prata', ',', 'cheia', 'de', 'copos', ',', 'dirigia-se', 'a', 'todos', ',', 'um', 'porum', ',', 'a', 'bambalear', 'as', 'ancas', 'volumosas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A criadagem de Manuel e Maria Bárbara, contava, além de Brígida e Benedito, de uma cafuza\n",
      "já idosa, chamada Mônica, que amamentara Ana Rosa e lavava a roupa da casa, e mais de uma preta só\n",
      "para engomar, e outra só para cozinhar, e outra só para sacudir o pó dos trastes e levar recados à rua'\n",
      "Tokens gerados: ['a', 'criadagem', 'de', 'manuel', 'e', 'maria', 'barbara', ',', 'contava', ',', 'alem', 'de', 'brigida', 'e', 'benedito', ',', 'de', 'uma', 'cafuzaja', 'idosa', ',', 'chamada', 'monica', ',', 'que', 'amamentara', 'ana', 'rosa', 'e', 'lavava', 'a', 'roupa', 'da', 'casa', ',', 'e', 'mais', 'de', 'uma', 'preta', 'sopara', 'engomar', ',', 'e', 'outra', 'so', 'para', 'cozinhar', ',', 'e', 'outra', 'so', 'para', 'sacudir', 'o', 'po', 'dos', 'trastes', 'e', 'levar', 'recados', 'a', 'rua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois, apesar deste pessoal, o serviço era sempre tardio e malfeito'\n",
      "Tokens gerados: ['pois', ',', 'apesar', 'deste', 'pessoal', ',', 'o', 'servico', 'era', 'sempre', 'tardio', 'e', 'malfeito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Estas escravas de hoje têm luxos!'\n",
      "Tokens gerados: ['—', 'estas', 'escravas', 'de', 'hoje', 'tem', 'luxos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'observou Amância em voz baixa a Maria do Carmo,\n",
      "apontando com o olhar para o vulto empantufado de Brígida'\n",
      "Tokens gerados: ['observou', 'amancia', 'em', 'voz', 'baixa', 'a', 'maria', 'do', 'carmo', ',', 'apontando', 'com', 'o', 'olhar', 'para', 'o', 'vulto', 'empantufado', 'de', 'brigida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E entraram a conversar sobre o escândalo das mulatas se prepararem tão bem como as senhoras'\n",
      "Tokens gerados: ['e', 'entraram', 'a', 'conversar', 'sobre', 'o', 'escandalo', 'das', 'mulatas', 'se', 'prepararem', 'tao', 'bem', 'como', 'as', 'senhoras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Já se não contentavam com a sua saia curta e cabeção de renda; queriam vestido de cauda; em vez das\n",
      "chinelas, queriam botinas! Uma patifaria!” Depois falaram nos caixeiros, que roubavam do patrão para\n",
      "enfeitar as suas pininchas; e, por uma transição natural, estenderam a crítica até aos passeios a carro, às\n",
      "festas de largo e os bailes dos pretos'\n",
      "Tokens gerados: ['“', 'ja', 'se', 'nao', 'contentavam', 'com', 'a', 'sua', 'saia', 'curta', 'e', 'cabecao', 'de', 'renda', 'queriam', 'vestido', 'de', 'cauda', 'em', 'vez', 'daschinelas', ',', 'queriam', 'botinas', 'uma', 'patifaria', '”', 'depois', 'falaram', 'nos', 'caixeiros', ',', 'que', 'roubavam', 'do', 'patrao', 'paraenfeitar', 'as', 'suas', 'pininchas', 'e', ',', 'por', 'uma', 'transicao', 'natural', ',', 'estenderam', 'a', 'critica', 'ate', 'aos', 'passeios', 'a', 'carro', ',', 'asfestas', 'de', 'largo', 'e', 'os', 'bailes', 'dos', 'pretos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Os chinfrins, como lhes chamava o meu defunto Espigão, acudiu Maria do Carmo, conheço!\n",
      "ora se conheço!'\n",
      "Tokens gerados: ['—', 'os', 'chinfrins', ',', 'como', 'lhes', 'chamava', 'o', 'meu', 'defunto', 'espigao', ',', 'acudiu', 'maria', 'do', 'carmo', ',', 'conhecoora', 'se', 'conheco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bastante quizília tivemos nós por amor deles!'\n",
      "Tokens gerados: ['bastante', 'quizilia', 'tivemos', 'nos', 'por', 'amor', 'deles']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— É uma sem-vergonheira! Ver as escravas todas de cambraia, laços de fita, água de cheiro no\n",
      "lenço, a requebrarem as chandangas na dança!'\n",
      "Tokens gerados: ['—', 'e', 'uma', 'sem-vergonheira', 'ver', 'as', 'escravas', 'todas', 'de', 'cambraia', ',', 'lacos', 'de', 'fita', ',', 'agua', 'de', 'cheiro', 'nolenco', ',', 'a', 'requebrarem', 'as', 'chandangas', 'na', 'danca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah, um bom chicote!'\n",
      "Tokens gerados: ['—', 'ah', ',', 'um', 'bom', 'chicote']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'disseram as duas velhas ao mesmo tempo'\n",
      "Tokens gerados: ['disseram', 'as', 'duas', 'velhas', 'ao', 'mesmo', 'tempo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— E elas dançam direito?'\n",
      "Tokens gerados: ['—', 'e', 'elas', 'dancam', 'direito', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'perguntou a do Carmo,\n",
      "— Se dançam!'\n",
      "Tokens gerados: ['perguntou', 'a', 'do', 'carmo', ',', '—', 'se', 'dancam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O serviço é que não sabem fazer a tempo e a horas! Lá para dançar estão\n",
      "sempre prontas! Nem o João Enxova!\n",
      "A indignação secava-lhe a voz'\n",
      "Tokens gerados: ['o', 'servico', 'e', 'que', 'nao', 'sabem', 'fazer', 'a', 'tempo', 'e', 'a', 'horas', 'la', 'para', 'dancar', 'estaosempre', 'prontas', 'nem', 'o', 'joao', 'enxovaa', 'indignacao', 'secava-lhe', 'a', 'voz']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Até parecem senhoras, Deus me perdoe! Todas a se fazerem de gente! os negros a darem-lhes\n",
      "excelência'\n",
      "Tokens gerados: ['—', 'ate', 'parecem', 'senhoras', ',', 'deus', 'me', 'perdoe', 'todas', 'a', 'se', 'fazerem', 'de', 'gente', 'os', 'negros', 'a', 'darem-lhesexcelencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“E porque minha senhora pra cá! Vossa Senhoria pra lá!” É uma pouca vergonha, a senhora\n",
      "não imagina!'\n",
      "Tokens gerados: ['“', 'e', 'porque', 'minha', 'senhora', 'pra', 'ca', 'vossa', 'senhoria', 'pra', 'la', '”', 'e', 'uma', 'pouca', 'vergonha', ',', 'a', 'senhoranao', 'imagina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma vez, em que fui espiar um chinfrim, porque me disseram que o meu defunto estava\n",
      "lá metido, fiquei pasma! E o melhor é que os descarados não se tratam pelo nome deles, tratam-se pelo\n",
      "nome dos seus senhores!'\n",
      "Tokens gerados: ['uma', 'vez', ',', 'em', 'que', 'fui', 'espiar', 'um', 'chinfrim', ',', 'porque', 'me', 'disseram', 'que', 'o', 'meu', 'defunto', 'estavala', 'metido', ',', 'fiquei', 'pasma', 'e', 'o', 'melhor', 'e', 'que', 'os', 'descarados', 'nao', 'se', 'tratam', 'pelo', 'nome', 'deles', ',', 'tratam-se', 'pelonome', 'dos', 'seus', 'senhores']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não sabe Filomeno?'\n",
      "Tokens gerados: ['nao', 'sabe', 'filomeno', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'aquele mulato do presidente?'\n",
      "Tokens gerados: ['aquele', 'mulato', 'do', 'presidente', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois a esse só davam\n",
      "“Sr'\n",
      "Tokens gerados: ['pois', 'a', 'esse', 'so', 'davam', '“', 'sr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Presidente!”'\n",
      "Tokens gerados: ['presidente', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Outros são “Srs'\n",
      "Tokens gerados: ['outros', 'sao', '“', 'srs']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desembargadores, Doutores, Majores e Coronéis!”'\n",
      "Tokens gerados: ['desembargadores', ',', 'doutores', ',', 'majores', 'e', 'coroneis', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um desaforo\n",
      "que deveria acabar na palmatória da polícia!\n",
      "Ana Rosa terminou a sua polca'\n",
      "Tokens gerados: ['um', 'desaforoque', 'deveria', 'acabar', 'na', 'palmatoria', 'da', 'policiaana', 'rosa', 'terminou', 'a', 'sua', 'polca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Bravo! Bravo!\n",
      "— Muito bem, D'\n",
      "Tokens gerados: ['—', 'bravo', 'bravo—', 'muito', 'bem', ',', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Anica!\n",
      "E estalaram palmas'\n",
      "Tokens gerados: ['anicae', 'estalaram', 'palmas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Tocou às mil maravilhas!'\n",
      "Tokens gerados: ['—', 'tocou', 'as', 'mil', 'maravilhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não senhor, foi uma polca do Marinho'\n",
      "Tokens gerados: ['—', 'nao', 'senhor', ',', 'foi', 'uma', 'polca', 'do', 'marinho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Correram a cumprimentar a pianista'\n",
      "Tokens gerados: ['correram', 'a', 'cumprimentar', 'a', 'pianista']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Freitas profetizou logo “que ali estava um segundo\n",
      "Lira!”\n",
      "Raimundo foi o único que não se abalou'\n",
      "Tokens gerados: ['o', 'freitas', 'profetizou', 'logo', '“', 'que', 'ali', 'estava', 'um', 'segundolira', '”', 'raimundo', 'foi', 'o', 'unico', 'que', 'nao', 'se', 'abalou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estava fumando à janela, e fumando deixou-se ficar'\n",
      "Tokens gerados: ['estava', 'fumando', 'a', 'janela', ',', 'e', 'fumando', 'deixou-se', 'ficar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ana Rosa, sem dar a perceber, sentiu por isso uma ligeira decepção'\n",
      "Tokens gerados: ['ana', 'rosa', ',', 'sem', 'dar', 'a', 'perceber', ',', 'sentiu', 'por', 'isso', 'uma', 'ligeira', 'decepcao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Esforçara-se por tocar bem e ele,\n",
      "nem assim! “Até parecia não ter notado nada!'\n",
      "Tokens gerados: ['esforcara-se', 'por', 'tocar', 'bem', 'e', 'ele', ',', 'nem', 'assim', '“', 'ate', 'parecia', 'nao', 'ter', 'notado', 'nada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É um malcriado!” concluiu ela, de si para si'\n",
      "Tokens gerados: ['e', 'um', 'malcriado', '”', 'concluiu', 'ela', ',', 'de', 'si', 'para', 'si']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, com\n",
      "uma pontinha de mau humor, assentou-se ao lado de Lindoca'\n",
      "Tokens gerados: ['e', ',', 'comuma', 'pontinha', 'de', 'mau', 'humor', ',', 'assentou-se', 'ao', 'lado', 'de', 'lindoca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eufrásia correu logo para junto da amiga'\n",
      "Tokens gerados: ['eufrasia', 'correu', 'logo', 'para', 'junto', 'da', 'amiga']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que tal o achas?'\n",
      "Tokens gerados: ['—', 'que', 'tal', 'o', 'achas', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'perguntou em segredo, assentando-se, com muito interesse'\n",
      "Tokens gerados: ['perguntou', 'em', 'segredo', ',', 'assentando-se', ',', 'com', 'muito', 'interesse']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Quem? disse Ana Rosa, fingindo distração e franzindo o nariz'\n",
      "Tokens gerados: ['—', 'quem', '?', 'disse', 'ana', 'rosa', ',', 'fingindo', 'distracao', 'e', 'franzindo', 'o', 'nariz']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A outra indicou misteriosamente a janela com um dos polegares'\n",
      "Tokens gerados: ['a', 'outra', 'indicou', 'misteriosamente', 'a', 'janela', 'com', 'um', 'dos', 'polegares']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Assim, assim'\n",
      "Tokens gerados: ['—', 'assim', ',', 'assim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E a filha do negociante fez um bico de indiferença'\n",
      "Tokens gerados: ['e', 'a', 'filha', 'do', 'negociante', 'fez', 'um', 'bico', 'de', 'indiferenca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Nem por isso!'\n",
      "Tokens gerados: ['—', 'nem', 'por', 'isso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Um peixão! opinou Eufrásia com entusiasmo'\n",
      "Tokens gerados: ['—', 'um', 'peixao', 'opinou', 'eufrasia', 'com', 'entusiasmo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Gentes!'\n",
      "Tokens gerados: ['—', 'gentes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que é isto, Eufrasinha?'\n",
      "Tokens gerados: ['que', 'e', 'isto', ',', 'eufrasinha', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— É uma tetéia!\n",
      "E a viúva mordia os beiços'\n",
      "Tokens gerados: ['—', 'e', 'uma', 'teteiae', 'a', 'viuva', 'mordia', 'os', 'beicos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Sim, ele não é feio'\n",
      "Tokens gerados: ['—', 'sim', ',', 'ele', 'nao', 'e', 'feio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'tornou Ana Rosa, impacientando-se'\n",
      "Tokens gerados: ['tornou', 'ana', 'rosa', ',', 'impacientando-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas também não é lá essas coisas!'\n",
      "Tokens gerados: ['mas', 'tambem', 'nao', 'e', 'la', 'essas', 'coisas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que olhos! que cabelos! e que gestos!'\n",
      "Tokens gerados: ['—', 'que', 'olhos', 'que', 'cabelos', 'e', 'que', 'gestos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'olha, olha, menina! como ele brinca com o charuto!'\n",
      "Tokens gerados: ['olha', ',', 'olha', ',', 'menina', 'como', 'ele', 'brinca', 'com', 'o', 'charuto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'olha como ele se encosta à grade da janela!'\n",
      "Tokens gerados: ['olha', 'como', 'ele', 'se', 'encosta', 'a', 'grade', 'da', 'janela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Parece um fidalgo, o diabo do homem!'\n",
      "Tokens gerados: ['parece', 'um', 'fidalgo', ',', 'o', 'diabo', 'do', 'homem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ana Rosa, sem desfranzir o nariz, enviesava os olhos contra o primo e sentia melhor do que a\n",
      "amiga, a evidência do que esta lhe dizia'\n",
      "Tokens gerados: ['ana', 'rosa', ',', 'sem', 'desfranzir', 'o', 'nariz', ',', 'enviesava', 'os', 'olhos', 'contra', 'o', 'primo', 'e', 'sentia', 'melhor', 'do', 'que', 'aamiga', ',', 'a', 'evidencia', 'do', 'que', 'esta', 'lhe', 'dizia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Raimundo era com efeito elegante e bem bonito, mas, que\n",
      "diabo, desde que chegara ainda lhe não tinha dispensado uma única palavra de distinção, um só gesto\n",
      "que a especializasse, quando ali, no entanto, era ela, incontestavelmente, a mais chique, a mais simpática,\n",
      "e, além disso — sua prima! (Ana Rosa pouco, ou nada, sabia ao certo do grau do seu parentesco com\n",
      "ele) Não! Não fora correto! Falara-lhe como às outras, igualmente frio e reservado; não fizera como os\n",
      "rapazes do Maranhão, que, mal se aproximavam dela estavam desfeitos em elogios e protestos de\n",
      "amor!” Aquela indiferença de Raimundo doía-lhe como uma injustiça: sentia-se lesada, roubada, nos\n",
      "seus direitos de moça irresistível'\n",
      "Tokens gerados: ['“', 'raimundo', 'era', 'com', 'efeito', 'elegante', 'e', 'bem', 'bonito', ',', 'mas', ',', 'quediabo', ',', 'desde', 'que', 'chegara', 'ainda', 'lhe', 'nao', 'tinha', 'dispensado', 'uma', 'unica', 'palavra', 'de', 'distincao', ',', 'um', 'so', 'gestoque', 'a', 'especializasse', ',', 'quando', 'ali', ',', 'no', 'entanto', ',', 'era', 'ela', ',', 'incontestavelmente', ',', 'a', 'mais', 'chique', ',', 'a', 'mais', 'simpatica', ',', 'e', ',', 'alem', 'disso', '—', 'sua', 'prima', 'ana', 'rosa', 'pouco', ',', 'ou', 'nada', ',', 'sabia', 'ao', 'certo', 'do', 'grau', 'do', 'seu', 'parentesco', 'comele', 'nao', 'nao', 'fora', 'correto', 'falara-lhe', 'como', 'as', 'outras', ',', 'igualmente', 'frio', 'e', 'reservado', 'nao', 'fizera', 'como', 'osrapazes', 'do', 'maranhao', ',', 'que', ',', 'mal', 'se', 'aproximavam', 'dela', 'estavam', 'desfeitos', 'em', 'elogios', 'e', 'protestos', 'deamor', '”', 'aquela', 'indiferenca', 'de', 'raimundo', 'doia-lhe', 'como', 'uma', 'injustica', 'sentia-se', 'lesada', ',', 'roubada', ',', 'nosseus', 'direitos', 'de', 'moca', 'irresistivel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Um pedante é o que ele é! Um enfatuado! Pensa que vale muito,\n",
      "porque se formou em Coimbra e correu a Europa! Um tolo!'\n",
      "Tokens gerados: ['“', 'um', 'pedante', 'e', 'o', 'que', 'ele', 'e', 'um', 'enfatuado', 'pensa', 'que', 'vale', 'muito', ',', 'porque', 'se', 'formou', 'em', 'coimbra', 'e', 'correu', 'a', 'europa', 'um', 'tolo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Nessa ocasião, entraram na sala, com ruídos, dois novos tipos — o José Roberto e o Sebastião\n",
      "Campos'\n",
      "Tokens gerados: ['”', 'nessa', 'ocasiao', ',', 'entraram', 'na', 'sala', ',', 'com', 'ruidos', ',', 'dois', 'novos', 'tipos', '—', 'o', 'jose', 'roberto', 'e', 'o', 'sebastiaocampos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foram logo apresentados a Raimundo e seguiram a cumprimentar as senhoras, dando a cada\n",
      "qual uma frase ou uma palavra ou um gesto de galanteio familiar: “D'\n",
      "Tokens gerados: ['foram', 'logo', 'apresentados', 'a', 'raimundo', 'e', 'seguiram', 'a', 'cumprimentar', 'as', 'senhoras', ',', 'dando', 'a', 'cadaqual', 'uma', 'frase', 'ou', 'uma', 'palavra', 'ou', 'um', 'gesto', 'de', 'galanteio', 'familiar', '“', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eufrasinha sempre bela como os\n",
      "amores, que pena ser eu já papel queimado! — Então, D'\n",
      "Tokens gerados: ['eufrasinha', 'sempre', 'bela', 'como', 'osamores', ',', 'que', 'pena', 'ser', 'eu', 'ja', 'papel', 'queimado', '—', 'entao', ',', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lindoca, onde vai com essa gordura? divida\n",
      "a metade comigo! — Quando se come doce desse casamento, D'\n",
      "Tokens gerados: ['lindoca', ',', 'onde', 'vai', 'com', 'essa', 'gordura', '?', 'dividaa', 'metade', 'comigo', '—', 'quando', 'se', 'come', 'doce', 'desse', 'casamento', ',', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bibina?'\n",
      "Tokens gerados: ['bibina', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E tinham sempre na ponta\n",
      "da língua uma pilhéria, um dito, para bulir com as moças; coisas desengraçadas e sediças, mas que as\n",
      "faziam rebentar de riso'\n",
      "Tokens gerados: ['e', 'tinham', 'sempre', 'na', 'pontada', 'lingua', 'uma', 'pilheria', ',', 'um', 'dito', ',', 'para', 'bulir', 'com', 'as', 'mocas', 'coisas', 'desengracadas', 'e', 'sedicas', ',', 'mas', 'que', 'asfaziam', 'rebentar', 'de', 'riso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Deus os fez e o diabo os ajuntou! explodiu, com um estalo de boca, a velha Amância quando\n",
      "os dois passaram por ela'\n",
      "Tokens gerados: ['—', 'deus', 'os', 'fez', 'e', 'o', 'diabo', 'os', 'ajuntou', 'explodiu', ',', 'com', 'um', 'estalo', 'de', 'boca', ',', 'a', 'velha', 'amancia', 'quandoos', 'dois', 'passaram', 'por', 'ela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José Roberto, a quem só tratavam por “Seu Casusa”, era moço de vinte e tantos anos; magro,\n",
      "moreno, crivado de espinhas, olhos muito negros, boca em ruínas, uma enorme cabeleira, rica, toda\n",
      "encaracolada e reluzente de óleo cheiroso, preta, bem preta, dividida pacientemente ao meio da cabeça'\n",
      "Tokens gerados: ['jose', 'roberto', ',', 'a', 'quem', 'so', 'tratavam', 'por', '“', 'seu', 'casusa', '”', ',', 'era', 'moco', 'de', 'vinte', 'e', 'tantos', 'anos', 'magro', ',', 'moreno', ',', 'crivado', 'de', 'espinhas', ',', 'olhos', 'muito', 'negros', ',', 'boca', 'em', 'ruinas', ',', 'uma', 'enorme', 'cabeleira', ',', 'rica', ',', 'todaencaracolada', 'e', 'reluzente', 'de', 'oleo', 'cheiroso', ',', 'preta', ',', 'bem', 'preta', ',', 'dividida', 'pacientemente', 'ao', 'meio', 'da', 'cabeca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Usava lunetas azuis e cantava ao violão modinhas da sua própria lavra e de outros, apimentadas à\n",
      "baiana com o travo sensual e árabe dos lundus africanos'\n",
      "Tokens gerados: ['usava', 'lunetas', 'azuis', 'e', 'cantava', 'ao', 'violao', 'modinhas', 'da', 'sua', 'propria', 'lavra', 'e', 'de', 'outros', ',', 'apimentadas', 'abaiana', 'com', 'o', 'travo', 'sensual', 'e', 'arabe', 'dos', 'lundus', 'africanos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando tocava, tinha o amaneirado voluptuoso\n",
      "do trovador de esquina; vergava-se todo sobre o instrumento, picando as notas com as unhas cujos\n",
      "dedos pareciam as pernas de um caranguejo doido, ou abafando com a palma da mão o som das cordas,\n",
      "que gemiam e choravam como gente'\n",
      "Tokens gerados: ['quando', 'tocava', ',', 'tinha', 'o', 'amaneirado', 'voluptuosodo', 'trovador', 'de', 'esquina', 'vergava-se', 'todo', 'sobre', 'o', 'instrumento', ',', 'picando', 'as', 'notas', 'com', 'as', 'unhas', 'cujosdedos', 'pareciam', 'as', 'pernas', 'de', 'um', 'caranguejo', 'doido', ',', 'ou', 'abafando', 'com', 'a', 'palma', 'da', 'mao', 'o', 'som', 'das', 'cordas', ',', 'que', 'gemiam', 'e', 'choravam', 'como', 'gente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tipo do Norte, perfeito, cheio de franquezas, com horror ao dinheiro, muito orgulhoso e prevenido\n",
      "contra os portugueses, a quem perseguia com as suas constantes chalaças, imitando-lhes o sotaque, o\n",
      "andar e os gestos'\n",
      "Tokens gerados: ['tipo', 'do', 'norte', ',', 'perfeito', ',', 'cheio', 'de', 'franquezas', ',', 'com', 'horror', 'ao', 'dinheiro', ',', 'muito', 'orgulhoso', 'e', 'prevenidocontra', 'os', 'portugueses', ',', 'a', 'quem', 'perseguia', 'com', 'as', 'suas', 'constantes', 'chalacas', ',', 'imitando-lhes', 'o', 'sotaque', ',', 'oandar', 'e', 'os', 'gestos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha alguma coisinha de seu e passava por estróina'\n",
      "Tokens gerados: ['tinha', 'alguma', 'coisinha', 'de', 'seu', 'e', 'passava', 'por', 'estroina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Gostava das serenatas, das\n",
      "pândegas com moças; pilhando dança — não perdia quadrilha nem pulada, mas no dia seguinte ficava\n",
      "de cama, estrompado'\n",
      "Tokens gerados: ['gostava', 'das', 'serenatas', ',', 'daspandegas', 'com', 'mocas', 'pilhando', 'danca', '—', 'nao', 'perdia', 'quadrilha', 'nem', 'pulada', ',', 'mas', 'no', 'dia', 'seguinte', 'ficavade', 'cama', ',', 'estrompado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia muito que José Roberto procurava agradar a Ana Rosa; esta sempre o repelia, a rir'\n",
      "Tokens gerados: ['havia', 'muito', 'que', 'jose', 'roberto', 'procurava', 'agradar', 'a', 'ana', 'rosa', 'esta', 'sempre', 'o', 'repelia', ',', 'a', 'rir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Também\n",
      "poucos o tomavam a sério: “Um pancada”, diziam; mas queriam-lhe bem'\n",
      "Tokens gerados: ['tambempoucos', 'o', 'tomavam', 'a', 'serio', '“', 'um', 'pancada', '”', ',', 'diziam', 'mas', 'queriam-lhe', 'bem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Sebastião Campos, esse era viúvo da primeira filha de Maria Bárbara e, como aquele, um tipo\n",
      "legítimo do Maranhão; nada, porém, tinha do outro senão o orgulho e a birra aos portugueses, a quem\n",
      "na ausência só chamava “marinheiros — puças — galegos”'\n",
      "Tokens gerados: ['o', 'sebastiao', 'campos', ',', 'esse', 'era', 'viuvo', 'da', 'primeira', 'filha', 'de', 'maria', 'barbara', 'e', ',', 'como', 'aquele', ',', 'um', 'tipolegitimo', 'do', 'maranhao', 'nada', ',', 'porem', ',', 'tinha', 'do', 'outro', 'senao', 'o', 'orgulho', 'e', 'a', 'birra', 'aos', 'portugueses', ',', 'a', 'quemna', 'ausencia', 'so', 'chamava', '“', 'marinheiros', '—', 'pucas', '—', 'galegos', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Senhor de engenho, de um engenho de cana, lá para as bandas do Munim, onde passava três\n",
      "meses no tempo da colheita; o resto do ano passava-o na cidade'\n",
      "Tokens gerados: ['senhor', 'de', 'engenho', ',', 'de', 'um', 'engenho', 'de', 'cana', ',', 'la', 'para', 'as', 'bandas', 'do', 'munim', ',', 'onde', 'passava', 'tresmeses', 'no', 'tempo', 'da', 'colheita', 'o', 'resto', 'do', 'ano', 'passava-o', 'na', 'cidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Devia ter quase o duplo da idade de\n",
      "José Roberto, baixote, muito asseado, mas com a roupa sempre malfeita'\n",
      "Tokens gerados: ['devia', 'ter', 'quase', 'o', 'duplo', 'da', 'idade', 'dejose', 'roberto', ',', 'baixote', ',', 'muito', 'asseado', ',', 'mas', 'com', 'a', 'roupa', 'sempre', 'malfeita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Usava calças curtas, em geral\n",
      "brancas, deixando aparecer, desde o tornozelo, os seus pezinhos ridiculamente pequenos e mimosos;\n",
      "barba cerrada, ainda preta, e cabelo à escovinha; olhos de pássaro, vivos e lascivos, nariz de criança e\n",
      "testa enorme; uma grande cabeça, desproporcionada do corpo, beiços grossos e vermelhos, mostrando\n",
      "a dentadura miudinha e gasta, porém muito bem tratada, tratada a mel de fumo de corda, que era com\n",
      "que ele asseava a boca'\n",
      "Tokens gerados: ['usava', 'calcas', 'curtas', ',', 'em', 'geralbrancas', ',', 'deixando', 'aparecer', ',', 'desde', 'o', 'tornozelo', ',', 'os', 'seus', 'pezinhos', 'ridiculamente', 'pequenos', 'e', 'mimososbarba', 'cerrada', ',', 'ainda', 'preta', ',', 'e', 'cabelo', 'a', 'escovinha', 'olhos', 'de', 'passaro', ',', 'vivos', 'e', 'lascivos', ',', 'nariz', 'de', 'crianca', 'etesta', 'enorme', 'uma', 'grande', 'cabeca', ',', 'desproporcionada', 'do', 'corpo', ',', 'beicos', 'grossos', 'e', 'vermelhos', ',', 'mostrandoa', 'dentadura', 'miudinha', 'e', 'gasta', ',', 'porem', 'muito', 'bem', 'tratada', ',', 'tratada', 'a', 'mel', 'de', 'fumo', 'de', 'corda', ',', 'que', 'era', 'comque', 'ele', 'asseava', 'a', 'boca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bairrista, isso ao último ponto: a tudo preferia o que fosse nacional'\n",
      "Tokens gerados: ['bairrista', ',', 'isso', 'ao', 'ultimo', 'ponto', 'a', 'tudo', 'preferia', 'o', 'que', 'fosse', 'nacional']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Não trocava a sua boa\n",
      "cana-capim — e o seu vinho de caju por quantos cognacs e vinhos do Porto havia por aí! nem o seu\n",
      "gostoso e cheiroso fumo de molho, fabricado no Maranhão, pelo melhor tabaco estrangeiro, ou mesmo\n",
      "importado das outras províncias! Ou bem que se era maranhense ou bem que se não era!”\n",
      "Não cochilava com os seus escravos'\n",
      "Tokens gerados: ['“', 'nao', 'trocava', 'a', 'sua', 'boacana-capim', '—', 'e', 'o', 'seu', 'vinho', 'de', 'caju', 'por', 'quantos', 'cognacs', 'e', 'vinhos', 'do', 'porto', 'havia', 'por', 'ai', 'nem', 'o', 'seugostoso', 'e', 'cheiroso', 'fumo', 'de', 'molho', ',', 'fabricado', 'no', 'maranhao', ',', 'pelo', 'melhor', 'tabaco', 'estrangeiro', ',', 'ou', 'mesmoimportado', 'das', 'outras', 'provincias', 'ou', 'bem', 'que', 'se', 'era', 'maranhense', 'ou', 'bem', 'que', 'se', 'nao', 'era', '”', 'nao', 'cochilava', 'com', 'os', 'seus', 'escravos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na roça era temido até pelo feitor, um pouco devoto e\n",
      "cheio de escrúpulos de raça'\n",
      "Tokens gerados: ['na', 'roca', 'era', 'temido', 'ate', 'pelo', 'feitor', ',', 'um', 'pouco', 'devoto', 'echeio', 'de', 'escrupulos', 'de', 'raca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Preto é preto; branco é branco! Moleque é moleque; menino é menino!”\n",
      "E estava sempre a repetir que o Brasil teria ganho muito, se perdesse a Guerra dos Guararapes'\n",
      "Tokens gerados: ['“', 'preto', 'e', 'preto', 'branco', 'e', 'branco', 'moleque', 'e', 'moleque', 'menino', 'e', 'menino', '”', 'e', 'estava', 'sempre', 'a', 'repetir', 'que', 'o', 'brasil', 'teria', 'ganho', 'muito', ',', 'se', 'perdesse', 'a', 'guerra', 'dos', 'guararapes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— A nossa desgraça, rezava ele, é termos caído nas mãos destas bestas! Uns lesmas! Uma gente\n",
      "sem progresso, que só cuida de encher o papo e aferrolhar dinheiro!\n",
      "Favores, de quem quer que fosse, não os aceitava “que não queria dever obrigações a nenhum\n",
      "filho da mãe!'\n",
      "Tokens gerados: ['—', 'a', 'nossa', 'desgraca', ',', 'rezava', 'ele', ',', 'e', 'termos', 'caido', 'nas', 'maos', 'destas', 'bestas', 'uns', 'lesmas', 'uma', 'gentesem', 'progresso', ',', 'que', 'so', 'cuida', 'de', 'encher', 'o', 'papo', 'e', 'aferrolhar', 'dinheirofavores', ',', 'de', 'quem', 'quer', 'que', 'fosse', ',', 'nao', 'os', 'aceitava', '“', 'que', 'nao', 'queria', 'dever', 'obrigacoes', 'a', 'nenhumfilho', 'da', 'mae']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” Mas também, quando dava para meter as botas em qualquer pessoa — era aquela\n",
      "desgraça! Não tinha papas na língua! Era nervoso e ativo; gostava todavia de ler ou conversar,\n",
      "escarranchado na rede durante horas esquecidas, em ceroulas, fumando o seu cachimbo de cabeça\n",
      "preta, fabricado na província'\n",
      "Tokens gerados: ['”', 'mas', 'tambem', ',', 'quando', 'dava', 'para', 'meter', 'as', 'botas', 'em', 'qualquer', 'pessoa', '—', 'era', 'aqueladesgraca', 'nao', 'tinha', 'papas', 'na', 'lingua', 'era', 'nervoso', 'e', 'ativo', 'gostava', 'todavia', 'de', 'ler', 'ou', 'conversar', ',', 'escarranchado', 'na', 'rede', 'durante', 'horas', 'esquecidas', ',', 'em', 'ceroulas', ',', 'fumando', 'o', 'seu', 'cachimbo', 'de', 'cabecapreta', ',', 'fabricado', 'na', 'provincia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na rua, encontravam-no de sobrecasaca aberta, coletinho de chamalote,\n",
      "camisa bordada, guarnecida por três brilhantes grandes; ao pescoço, prendendo o cebolão, um trancelim\n",
      "muito comprido, de ouro maciço, obra antiga, com passador'\n",
      "Tokens gerados: ['na', 'rua', ',', 'encontravam-no', 'de', 'sobrecasaca', 'aberta', ',', 'coletinho', 'de', 'chamalote', ',', 'camisa', 'bordada', ',', 'guarnecida', 'por', 'tres', 'brilhantes', 'grandes', 'ao', 'pescoco', ',', 'prendendo', 'o', 'cebolao', ',', 'um', 'trancelimmuito', 'comprido', ',', 'de', 'ouro', 'macico', ',', 'obra', 'antiga', ',', 'com', 'passador']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Adorava os perfumes ativos, as jóias e as\n",
      "cores vivas; para ele, nada havia, porém, como um passeio ao sítio embarcado, à fresca da madrugada,\n",
      "bebericando o seu trago de cachaça e pitando o seu fumo do Codó'\n",
      "Tokens gerados: ['adorava', 'os', 'perfumes', 'ativos', ',', 'as', 'joias', 'e', 'ascores', 'vivas', 'para', 'ele', ',', 'nada', 'havia', ',', 'porem', ',', 'como', 'um', 'passeio', 'ao', 'sitio', 'embarcado', ',', 'a', 'fresca', 'da', 'madrugada', ',', 'bebericando', 'o', 'seu', 'trago', 'de', 'cachaca', 'e', 'pitando', 'o', 'seu', 'fumo', 'do', 'codo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em casa muito obsequiador'\n",
      "Tokens gerados: ['em', 'casa', 'muito', 'obsequiador']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Passava\n",
      "à farta'\n",
      "Tokens gerados: ['passavaa', 'farta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Com a vinda destes dois, a reunião tornou-se mais animada'\n",
      "Tokens gerados: ['com', 'a', 'vinda', 'destes', 'dois', ',', 'a', 'reuniao', 'tornou-se', 'mais', 'animada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Reclamou-se logo o violão, e seu\n",
      "Casusa, depois de muito rogado, afinou o instrumento e principiou a cantar Gonçalves Dias:\n",
      "“Se queres saber o meio\n",
      "Por que às vezes me arrebata\n",
      "Nas asas do pensamento\n",
      "A poesia tão grata;”\n",
      "Nisto, rebentou uma corda do violão'\n",
      "Tokens gerados: ['reclamou-se', 'logo', 'o', 'violao', ',', 'e', 'seucasusa', ',', 'depois', 'de', 'muito', 'rogado', ',', 'afinou', 'o', 'instrumento', 'e', 'principiou', 'a', 'cantar', 'goncalves', 'dias', '“', 'se', 'queres', 'saber', 'o', 'meiopor', 'que', 'as', 'vezes', 'me', 'arrebatanas', 'asas', 'do', 'pensamentoa', 'poesia', 'tao', 'grata', '”', 'nisto', ',', 'rebentou', 'uma', 'corda', 'do', 'violao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora pistolas!'\n",
      "Tokens gerados: ['—', 'ora', 'pistolas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'resmungou o trovador'\n",
      "Tokens gerados: ['resmungou', 'o', 'trovador']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E gritou: — Ó D'\n",
      "Tokens gerados: ['e', 'gritou', '—', 'o', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Anica! a senhora não terá uma\n",
      "prima?\n",
      "Ana Rosa foi ver se tinha, andou remexendo lá por dentro da casa, e voltou com uma segunda'\n",
      "Tokens gerados: ['anica', 'a', 'senhora', 'nao', 'tera', 'umaprima', '?', 'ana', 'rosa', 'foi', 'ver', 'se', 'tinha', ',', 'andou', 'remexendo', 'la', 'por', 'dentro', 'da', 'casa', ',', 'e', 'voltou', 'com', 'uma', 'segunda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Era o que havia'\n",
      "Tokens gerados: ['“', 'era', 'o', 'que', 'havia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” O Casusa arranjou-se com a segunda e prosseguiu, depois de repetir os versos já\n",
      "cantados; ao passo que o Freitas, na janela, importunava Raimundo, a propósito do autor daquela\n",
      "poesia e de outros vultos notáveis do Maranhão “da sua Atenas brasileira” como a denominava ele'\n",
      "Tokens gerados: ['”', 'o', 'casusa', 'arranjou-se', 'com', 'a', 'segunda', 'e', 'prosseguiu', ',', 'depois', 'de', 'repetir', 'os', 'versos', 'jacantados', 'ao', 'passo', 'que', 'o', 'freitas', ',', 'na', 'janela', ',', 'importunava', 'raimundo', ',', 'a', 'proposito', 'do', 'autor', 'daquelapoesia', 'e', 'de', 'outros', 'vultos', 'notaveis', 'do', 'maranhao', '“', 'da', 'sua', 'atenas', 'brasileira', '”', 'como', 'a', 'denominava', 'ele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O\n",
      "cônego fugiu logo para a varanda, covardemente, com medo à seca'\n",
      "Tokens gerados: ['oconego', 'fugiu', 'logo', 'para', 'a', 'varanda', ',', 'covardemente', ',', 'com', 'medo', 'a', 'seca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não sou bairrista, não senhor'\n",
      "Tokens gerados: ['—', 'nao', 'sou', 'bairrista', ',', 'nao', 'senhor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'dizia o maçante, mas o nosso Maranhãozinho é um torrão\n",
      "privilegiado!'\n",
      "Tokens gerados: ['dizia', 'o', 'macante', ',', 'mas', 'o', 'nosso', 'maranhaozinho', 'e', 'um', 'torraoprivilegiado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E citava, com orgulho, “os Cunha, os Odorico Mendes, os Pindaré e os Sotero et cetera! et\n",
      "cetera!’’ O seu modo de dizer et cetera era esplêndido!\n",
      "— Temos os nossos faustos, temos!\n",
      "Passou então a falar nas belezas da sua Atenas: no dique das Mercês, “estava em construção,\n",
      "mas havia de ficar obra muito de se ver e gostar'\n",
      "Tokens gerados: ['e', 'citava', ',', 'com', 'orgulho', ',', '“', 'os', 'cunha', ',', 'os', 'odorico', 'mendes', ',', 'os', 'pindare', 'e', 'os', 'sotero', 'et', 'cetera', 'etcetera', '’', '’', 'o', 'seu', 'modo', 'de', 'dizer', 'et', 'cetera', 'era', 'esplendido—', 'temos', 'os', 'nossos', 'faustos', ',', 'temospassou', 'entao', 'a', 'falar', 'nas', 'belezas', 'da', 'sua', 'atenas', 'no', 'dique', 'das', 'merces', ',', '“', 'estava', 'em', 'construcao', ',', 'mas', 'havia', 'de', 'ficar', 'obra', 'muito', 'de', 'se', 'ver', 'e', 'gostar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” afiançava ele cheio de gestos respeitosos'\n",
      "Tokens gerados: ['”', 'afiancava', 'ele', 'cheio', 'de', 'gestos', 'respeitosos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Falou do\n",
      "Cais da Sagração, “também não estava concluído”, dos Quartéis, “iam entrar em conserto”, na igreja de\n",
      "Santo Antônio, “nunca chegaram a terminá-la, mas se o conseguissem, seria um belo templo!” Elogiou\n",
      "muito o teatro São Luís'\n",
      "Tokens gerados: ['falou', 'docais', 'da', 'sagracao', ',', '“', 'tambem', 'nao', 'estava', 'concluido', '”', ',', 'dos', 'quarteis', ',', '“', 'iam', 'entrar', 'em', 'conserto', '”', ',', 'na', 'igreja', 'desanto', 'antonio', ',', '“', 'nunca', 'chegaram', 'a', 'termina-la', ',', 'mas', 'se', 'o', 'conseguissem', ',', 'seria', 'um', 'belo', 'templo', '”', 'elogioumuito', 'o', 'teatro', 'sao', 'luis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Dizia o cônego que era o São Carlos de Lisboa, em ponto pequeno!” Lembrou\n",
      "respeitosamente a companhia lírica do Ramonda, o Remorini, o tenor “morrera de febre amarela, depois\n",
      "de ser muito aplaudido na Gemma de Vergi'\n",
      "Tokens gerados: ['“', 'dizia', 'o', 'conego', 'que', 'era', 'o', 'sao', 'carlos', 'de', 'lisboa', ',', 'em', 'ponto', 'pequeno', '”', 'lembrourespeitosamente', 'a', 'companhia', 'lirica', 'do', 'ramonda', ',', 'o', 'remorini', ',', 'o', 'tenor', '“', 'morrera', 'de', 'febre', 'amarela', ',', 'depoisde', 'ser', 'muito', 'aplaudido', 'na', 'gemma', 'de', 'vergi']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah, como aquela, jurava não voltaria outra companhia ao\n",
      "Maranhão! Mas que, mesmo na província, havia moços de grande habilidade'\n",
      "Tokens gerados: ['ah', ',', 'como', 'aquela', ',', 'jurava', 'nao', 'voltaria', 'outra', 'companhia', 'aomaranhao', 'mas', 'que', ',', 'mesmo', 'na', 'provincia', ',', 'havia', 'mocos', 'de', 'grande', 'habilidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” Referia-se a uma\n",
      "sociedade particular, de curiosos'\n",
      "Tokens gerados: ['”', 'referia-se', 'a', 'umasociedade', 'particular', ',', 'de', 'curiosos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Tinham seu jeito, sim senhor!” E, engrossando a voz, com muita\n",
      "autoridade: “Representavam Os Sete Infantes de Lara! — Os Renegados! — O Homem da Máscara\n",
      "Negra, e outras peças de igual merecimento! Tinham a sua queda para a coisa, tinham!'\n",
      "Tokens gerados: ['“', 'tinham', 'seu', 'jeito', ',', 'sim', 'senhor', '”', 'e', ',', 'engrossando', 'a', 'voz', ',', 'com', 'muitaautoridade', '“', 'representavam', 'os', 'sete', 'infantes', 'de', 'lara', '—', 'os', 'renegados', '—', 'o', 'homem', 'da', 'mascaranegra', ',', 'e', 'outras', 'pecas', 'de', 'igual', 'merecimento', 'tinham', 'a', 'sua', 'queda', 'para', 'a', 'coisa', ',', 'tinham']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não se pode\n",
      "negar!'\n",
      "Tokens gerados: ['nao', 'se', 'podenegar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” E assoava-se, meneando a cabeça, convencido'\n",
      "Tokens gerados: ['”', 'e', 'assoava-se', ',', 'meneando', 'a', 'cabeca', ',', 'convencido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Principalmente a dama'\n",
      "Tokens gerados: ['“', 'principalmente', 'a', 'dama']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'sim! o moço que\n",
      "fazia de dama!'\n",
      "Tokens gerados: ['sim', 'o', 'moco', 'quefazia', 'de', 'dama']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não havia que desejar — o pegar do leque, o revirar dos olhos, certos requebros,\n",
      "certas faceirices!'\n",
      "Tokens gerados: ['nao', 'havia', 'que', 'desejar', '—', 'o', 'pegar', 'do', 'leque', ',', 'o', 'revirar', 'dos', 'olhos', ',', 'certos', 'requebros', ',', 'certas', 'faceirices']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Enfim, senhores, era perfeito, perfeito, perfeito!”\n",
      "Raimundo bocejava'\n",
      "Tokens gerados: ['enfim', ',', 'senhores', ',', 'era', 'perfeito', ',', 'perfeito', ',', 'perfeito', '”', 'raimundo', 'bocejava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o Freitas nem cuspia'\n",
      "Tokens gerados: ['e', 'o', 'freitas', 'nem', 'cuspia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Acudiam-lhe fatos engraçados sobre o teatrinho; soltava as anedotas em\n",
      "rebanho, sem intervalos'\n",
      "Tokens gerados: ['acudiam-lhe', 'fatos', 'engracados', 'sobre', 'o', 'teatrinho', 'soltava', 'as', 'anedotas', 'emrebanho', ',', 'sem', 'intervalos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo já não achava posição na janela; virava-se da esquerda, da direita,\n",
      "firmava-se ora numa perna, ora na outra deixando afinal pender a cabeça e olhando para os pés,\n",
      "entristecido pelo tédio'\n",
      "Tokens gerados: ['raimundo', 'ja', 'nao', 'achava', 'posicao', 'na', 'janela', 'virava-se', 'da', 'esquerda', ',', 'da', 'direita', ',', 'firmava-se', 'ora', 'numa', 'perna', ',', 'ora', 'na', 'outra', 'deixando', 'afinal', 'pender', 'a', 'cabeca', 'e', 'olhando', 'para', 'os', 'pes', ',', 'entristecido', 'pelo', 'tedio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Que maçante!'\n",
      "Tokens gerados: ['“', 'que', 'macante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” pensava'\n",
      "Tokens gerados: ['”', 'pensava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, o Freitas a sacudir-lhe a manga do fraque, que Raimundo sujara na caliça da janela,\n",
      "ia confessando que “estavam em vazante de divertimentos; que a sua distração única era cavaquear um\n",
      "bocado com os amigos'\n",
      "Tokens gerados: ['entretanto', ',', 'o', 'freitas', 'a', 'sacudir-lhe', 'a', 'manga', 'do', 'fraque', ',', 'que', 'raimundo', 'sujara', 'na', 'calica', 'da', 'janela', ',', 'ia', 'confessando', 'que', '“', 'estavam', 'em', 'vazante', 'de', 'divertimentos', 'que', 'a', 'sua', 'distracao', 'unica', 'era', 'cavaquear', 'umbocado', 'com', 'os', 'amigos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "— Ah! exclamou, minto! minto! Há uma festa nova! — a de Santa Filomena! Mas não será\n",
      "como a dos Remédios, isso, tenham paciência!'\n",
      "Tokens gerados: ['”', '—', 'ah', 'exclamou', ',', 'minto', 'minto', 'ha', 'uma', 'festa', 'nova', '—', 'a', 'de', 'santa', 'filomena', 'mas', 'nao', 'seracomo', 'a', 'dos', 'remedios', ',', 'isso', ',', 'tenham', 'paciencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Sim, decerto, balbuciou Raimundo, fingindo prestar atenção'\n",
      "Tokens gerados: ['—', 'sim', ',', 'decerto', ',', 'balbuciou', 'raimundo', ',', 'fingindo', 'prestar', 'atencao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E espreguiçou-se'\n",
      "Tokens gerados: ['e', 'espreguicou-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— A festa dos Remédios!'\n",
      "Tokens gerados: ['—', 'a', 'festa', 'dos', 'remedios']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'repetiu o outro, estalando os dedos e assoviando prolongadamente,\n",
      "como quem diz: “Vai longe!”\n",
      "Raimundo estremeceu, ficou gelado até a raiz dos cabelos; percebeu aquela tremenda ameaça e\n",
      "mediu instintivamente a altura da janela, como se premeditasse uma fuga'\n",
      "Tokens gerados: ['repetiu', 'o', 'outro', ',', 'estalando', 'os', 'dedos', 'e', 'assoviando', 'prolongadamente', ',', 'como', 'quem', 'diz', '“', 'vai', 'longe', '”', 'raimundo', 'estremeceu', ',', 'ficou', 'gelado', 'ate', 'a', 'raiz', 'dos', 'cabelos', 'percebeu', 'aquela', 'tremenda', 'ameaca', 'emediu', 'instintivamente', 'a', 'altura', 'da', 'janela', ',', 'como', 'se', 'premeditasse', 'uma', 'fuga']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— O nosso João Lisboa'\n",
      "Tokens gerados: ['—', 'o', 'nosso', 'joao', 'lisboa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'disse o Freitas'\n",
      "Tokens gerados: ['disse', 'o', 'freitas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E meteu profundamente as mãos nas algibeiras das\n",
      "calças'\n",
      "Tokens gerados: ['e', 'meteu', 'profundamente', 'as', 'maos', 'nas', 'algibeiras', 'dascalcas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O nosso João Lisboa já, em um folhetim publicado no número'\n",
      "Tokens gerados: ['o', 'nosso', 'joao', 'lisboa', 'ja', ',', 'em', 'um', 'folhetim', 'publicado', 'no', 'numero']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ora qual é o número do Publicador\n",
      "Maranhense?'\n",
      "Tokens gerados: ['ora', 'qual', 'e', 'o', 'numero', 'do', 'publicadormaranhense', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Espere!'\n",
      "Tokens gerados: ['espere']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E fitou o teto'\n",
      "Tokens gerados: ['e', 'fitou', 'o', 'teto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— 1173 — Sim! 1173, de 15 de outubro de 1851'\n",
      "Tokens gerados: ['—', '1173', '—', 'sim', '1173', ',', 'de', '15', 'de', 'outubro', 'de', '1851']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois nesse folhetim descreve ele,\n",
      "circunstanciadamente e com muito donaire e gentilezas de estilo, a nossa popular e pitoresca festa dos\n",
      "Remédios'\n",
      "Tokens gerados: ['pois', 'nesse', 'folhetim', 'descreve', 'ele', ',', 'circunstanciadamente', 'e', 'com', 'muito', 'donaire', 'e', 'gentilezas', 'de', 'estilo', ',', 'a', 'nossa', 'popular', 'e', 'pitoresca', 'festa', 'dosremedios']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo, aterrado, prometeu, sob palavra de honra, ler o tal folhetim na primeira ocasião'\n",
      "Tokens gerados: ['raimundo', ',', 'aterrado', ',', 'prometeu', ',', 'sob', 'palavra', 'de', 'honra', ',', 'ler', 'o', 'tal', 'folhetim', 'na', 'primeira', 'ocasiao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah!'\n",
      "Tokens gerados: ['—', 'ah']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'volveu terrível o Freitas, é que ela hoje é outra coisa!'\n",
      "Tokens gerados: ['volveu', 'terrivel', 'o', 'freitas', ',', 'e', 'que', 'ela', 'hoje', 'e', 'outra', 'coisa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Hoje não se compara! — há\n",
      "muito mais luxo, mas muito!\n",
      "E, segurando com ambas as mãos a gola do fraque de Raimundo e ferrando-lhe em cima dos\n",
      "olhos arregalados, acrescentou energicamente: — Creia, meu doutor, mete pena o dinheirão que se\n",
      "gasta naquela festa! faz dó ver as sedas, os veludos, as anáguas de renda, arrastarem-se pela terra\n",
      "vermelha dos Remédios!'\n",
      "Tokens gerados: ['hoje', 'nao', 'se', 'compara', '—', 'hamuito', 'mais', 'luxo', ',', 'mas', 'muitoe', ',', 'segurando', 'com', 'ambas', 'as', 'maos', 'a', 'gola', 'do', 'fraque', 'de', 'raimundo', 'e', 'ferrando-lhe', 'em', 'cima', 'dosolhos', 'arregalados', ',', 'acrescentou', 'energicamente', '—', 'creia', ',', 'meu', 'doutor', ',', 'mete', 'pena', 'o', 'dinheirao', 'que', 'segasta', 'naquela', 'festa', 'faz', 'do', 'ver', 'as', 'sedas', ',', 'os', 'veludos', ',', 'as', 'anaguas', 'de', 'renda', ',', 'arrastarem-se', 'pela', 'terravermelha', 'dos', 'remedios']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo empenhou a cabeça como faria idéia aproximada'\n",
      "Tokens gerados: ['raimundo', 'empenhou', 'a', 'cabeca', 'como', 'faria', 'ideia', 'aproximada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Qual! Qual! Tenha paciência meu amigo, não é possível! E Freitas repeliu com força a\n",
      "vítima'\n",
      "Tokens gerados: ['—', 'qual', 'qual', 'tenha', 'paciencia', 'meu', 'amigo', ',', 'nao', 'e', 'possivel', 'e', 'freitas', 'repeliu', 'com', 'forca', 'avitima']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aquilo só vendo e sentindo, Sr'\n",
      "Tokens gerados: ['aquilo', 'so', 'vendo', 'e', 'sentindo', ',', 'sr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dr'\n",
      "Tokens gerados: ['dr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo José da Silva!\n",
      "E descreveu minuciosamente a cor, a sutileza da terra; como a maldita manchava o lugar em que\n",
      "caía; como se insinuava pelas costuras dos vestidos, das botas, nas abas dos chapéus, nas máquinas dos\n",
      "relógios; como se introduzia pelo nariz, pela boca, pelas unhas, por todos os poros!\n",
      "— Aquilo, meu caro amigo'\n",
      "Tokens gerados: ['raimundo', 'jose', 'da', 'silvae', 'descreveu', 'minuciosamente', 'a', 'cor', ',', 'a', 'sutileza', 'da', 'terra', 'como', 'a', 'maldita', 'manchava', 'o', 'lugar', 'em', 'quecaia', 'como', 'se', 'insinuava', 'pelas', 'costuras', 'dos', 'vestidos', ',', 'das', 'botas', ',', 'nas', 'abas', 'dos', 'chapeus', ',', 'nas', 'maquinas', 'dosrelogios', 'como', 'se', 'introduzia', 'pelo', 'nariz', ',', 'pela', 'boca', ',', 'pelas', 'unhas', ',', 'por', 'todos', 'os', 'poros—', 'aquilo', ',', 'meu', 'caro', 'amigo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo queixou-se inopinadamente de que tinha muito calor'\n",
      "Tokens gerados: ['raimundo', 'queixou-se', 'inopinadamente', 'de', 'que', 'tinha', 'muito', 'calor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Freitas levou-o pelo braço até a varanda; deu-lhe uma preguiçosa, passou-lhe uma ventarola de\n",
      "Bristol, preparou-lhe uma garapada, e, depois de havê-lo regalado bem, como antigamente se fazia com\n",
      "os sentenciados antes do suplício, de pé, implacável, verdadeiro carrasco em face do paciente, despejou\n",
      "inteira uma descrição do dia da festa dos Remédios, recorrendo a todos os mistérios da tortura, escolhendo\n",
      "palavras e gestos, repetindo as frases, frisando os termos, repisando o que lhe parecia de mais interesse,\n",
      "cheio de atitudes como se discursasse para um grande auditório'\n",
      "Tokens gerados: ['freitas', 'levou-o', 'pelo', 'braco', 'ate', 'a', 'varanda', 'deu-lhe', 'uma', 'preguicosa', ',', 'passou-lhe', 'uma', 'ventarola', 'debristol', ',', 'preparou-lhe', 'uma', 'garapada', ',', 'e', ',', 'depois', 'de', 'have-lo', 'regalado', 'bem', ',', 'como', 'antigamente', 'se', 'fazia', 'comos', 'sentenciados', 'antes', 'do', 'suplicio', ',', 'de', 'pe', ',', 'implacavel', ',', 'verdadeiro', 'carrasco', 'em', 'face', 'do', 'paciente', ',', 'despejouinteira', 'uma', 'descricao', 'do', 'dia', 'da', 'festa', 'dos', 'remedios', ',', 'recorrendo', 'a', 'todos', 'os', 'misterios', 'da', 'tortura', ',', 'escolhendopalavras', 'e', 'gestos', ',', 'repetindo', 'as', 'frases', ',', 'frisando', 'os', 'termos', ',', 'repisando', 'o', 'que', 'lhe', 'parecia', 'de', 'mais', 'interesse', ',', 'cheio', 'de', 'atitudes', 'como', 'se', 'discursasse', 'para', 'um', 'grande', 'auditorio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Principiou expondo minuciosamente o Largo dos Remédios, com a sua ermida toda branca,\n",
      "seus bancos em derredor; muitos ariris, muita bandeira, muito foguete, muito toque de sino'\n",
      "Tokens gerados: ['principiou', 'expondo', 'minuciosamente', 'o', 'largo', 'dos', 'remedios', ',', 'com', 'a', 'sua', 'ermida', 'toda', 'branca', ',', 'seus', 'bancos', 'em', 'derredor', 'muitos', 'ariris', ',', 'muita', 'bandeira', ',', 'muito', 'foguete', ',', 'muito', 'toque', 'de', 'sino']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Descreveu\n",
      "com assombro o luxo exagerado em que se apresentavam todos, todos! para a missa das seis e para a\n",
      "missa das dez, nas quais, dizia ele circunspectamente, “reúne-se a nata da nossa judiciosa sociedade!'\n",
      "Tokens gerados: ['descreveucom', 'assombro', 'o', 'luxo', 'exagerado', 'em', 'que', 'se', 'apresentavam', 'todos', ',', 'todos', 'para', 'a', 'missa', 'das', 'seis', 'e', 'para', 'amissa', 'das', 'dez', ',', 'nas', 'quais', ',', 'dizia', 'ele', 'circunspectamente', ',', '“', 'reune-se', 'a', 'nata', 'da', 'nossa', 'judiciosa', 'sociedade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '”\n",
      "Era tudo em folha, e do mais caro, e do mais fino'\n",
      "Tokens gerados: ['”', 'era', 'tudo', 'em', 'folha', ',', 'e', 'do', 'mais', 'caro', ',', 'e', 'do', 'mais', 'fino']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nesse dia todos luxavam, desde o capitalista até o\n",
      "ralé caixeiro de balcão; velho ou moço, branco ou preto, ninguém lá ia, sem se haver preparado da\n",
      "cabeça aos pés; não se encontrava roupa velha, nem coração triste!\n",
      "— Às quatro horas da tarde, acrescentou o narrador, torna-se o largo a encher'\n",
      "Tokens gerados: ['nesse', 'dia', 'todos', 'luxavam', ',', 'desde', 'o', 'capitalista', 'ate', 'orale', 'caixeiro', 'de', 'balcao', 'velho', 'ou', 'moco', ',', 'branco', 'ou', 'preto', ',', 'ninguem', 'la', 'ia', ',', 'sem', 'se', 'haver', 'preparado', 'dacabeca', 'aos', 'pes', 'nao', 'se', 'encontrava', 'roupa', 'velha', ',', 'nem', 'coracao', 'triste—', 'as', 'quatro', 'horas', 'da', 'tarde', ',', 'acrescentou', 'o', 'narrador', ',', 'torna-se', 'o', 'largo', 'a', 'encher']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pensará talvez o\n",
      "meu amigo que tragam a mesma fatiota da manhã'\n",
      "Tokens gerados: ['pensara', 'talvez', 'omeu', 'amigo', 'que', 'tragam', 'a', 'mesma', 'fatiota', 'da', 'manha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Naturalmente'\n",
      "Tokens gerados: ['—', 'naturalmente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pois engana-se! é tudo outra vez novo! são novos vestidos, novas calças, novas'\n",
      "Tokens gerados: ['—', 'pois', 'engana-se', 'e', 'tudo', 'outra', 'vez', 'novo', 'sao', 'novos', 'vestidos', ',', 'novas', 'calcas', ',', 'novas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Etc'\n",
      "Tokens gerados: ['—', 'etc']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): ', etc'\n",
      "Tokens gerados: [',', 'etc']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '! Vamos adiante'\n",
      "Tokens gerados: ['vamos', 'adiante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Afirmam alguns estrangeiros'\n",
      "Tokens gerados: ['—', 'afirmam', 'alguns', 'estrangeiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'e dizendo isto tenho dito tudo!'\n",
      "Tokens gerados: ['e', 'dizendo', 'isto', 'tenho', 'dito', 'tudo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'que não há, em parte alguma\n",
      "do mundo festa de mais luxo!'\n",
      "Tokens gerados: ['que', 'nao', 'ha', ',', 'em', 'parte', 'algumado', 'mundo', 'festa', 'de', 'mais', 'luxo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E a voz do maçante tomava a solenidade de um juramento'\n",
      "Tokens gerados: ['e', 'a', 'voz', 'do', 'macante', 'tomava', 'a', 'solenidade', 'de', 'um', 'juramento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— O que lhe posso afiançar, doutor, é que não há criança que, nessa tarde, não tenha a sua\n",
      "pratinha amarrada na ponta do lenço'\n",
      "Tokens gerados: ['—', 'o', 'que', 'lhe', 'posso', 'afiancar', ',', 'doutor', ',', 'e', 'que', 'nao', 'ha', 'crianca', 'que', ',', 'nessa', 'tarde', ',', 'nao', 'tenha', 'a', 'suapratinha', 'amarrada', 'na', 'ponta', 'do', 'lenco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aparecem cédulas gordas, moedas amarelas; troca-se dinheiro;\n",
      "queimam-se charutos caros, no bazar (há um bazar) as prendas sobem a um preço escandaloso! Digo-lhe\n",
      "mais: nesse dia não há homem, por mais pichelingue, que não gaste seu bocado nos leilões, nas barracas,\n",
      "nos tabuleiros de doce ou nas casas de sorte; nem há mulher, senhora ou moça-dama, que não arrote\n",
      "grandeza, pelo menos seu vestidinho novo de popelina'\n",
      "Tokens gerados: ['aparecem', 'cedulas', 'gordas', ',', 'moedas', 'amarelas', 'troca-se', 'dinheiroqueimam-se', 'charutos', 'caros', ',', 'no', 'bazar', 'ha', 'um', 'bazar', 'as', 'prendas', 'sobem', 'a', 'um', 'preco', 'escandaloso', 'digo-lhemais', 'nesse', 'dia', 'nao', 'ha', 'homem', ',', 'por', 'mais', 'pichelingue', ',', 'que', 'nao', 'gaste', 'seu', 'bocado', 'nos', 'leiloes', ',', 'nas', 'barracas', ',', 'nos', 'tabuleiros', 'de', 'doce', 'ou', 'nas', 'casas', 'de', 'sorte', 'nem', 'ha', 'mulher', ',', 'senhora', 'ou', 'moca-dama', ',', 'que', 'nao', 'arrotegrandeza', ',', 'pelo', 'menos', 'seu', 'vestidinho', 'novo', 'de', 'popelina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vêem-se enormes trouxas de doce seco, corações\n",
      "unidos de cocada, navios de massa com mastreação de alfenim, jurarás dourados, cutias enfeitadas\n",
      "dentro da gaiola, pombos cheios de fitas, frascos de compota de murici, bacuri, buriti, o diabo, meu\n",
      "caro senhor! As pretas-minas, cativas ou forras, surgem com os seus ouros, as suas ricas telhas de\n",
      "tartaruga, as suas ricas toalhas de rendas, suas belas saias de veludo, suas chinelas de polimento, seus\n",
      "anéis em todos os dedos, aos dois e aos três em cada um'\n",
      "Tokens gerados: ['veem-se', 'enormes', 'trouxas', 'de', 'doce', 'seco', ',', 'coracoesunidos', 'de', 'cocada', ',', 'navios', 'de', 'massa', 'com', 'mastreacao', 'de', 'alfenim', ',', 'juraras', 'dourados', ',', 'cutias', 'enfeitadasdentro', 'da', 'gaiola', ',', 'pombos', 'cheios', 'de', 'fitas', ',', 'frascos', 'de', 'compota', 'de', 'murici', ',', 'bacuri', ',', 'buriti', ',', 'o', 'diabo', ',', 'meucaro', 'senhor', 'as', 'pretas-minas', ',', 'cativas', 'ou', 'forras', ',', 'surgem', 'com', 'os', 'seus', 'ouros', ',', 'as', 'suas', 'ricas', 'telhas', 'detartaruga', ',', 'as', 'suas', 'ricas', 'toalhas', 'de', 'rendas', ',', 'suas', 'belas', 'saias', 'de', 'veludo', ',', 'suas', 'chinelas', 'de', 'polimento', ',', 'seusaneis', 'em', 'todos', 'os', 'dedos', ',', 'aos', 'dois', 'e', 'aos', 'tres', 'em', 'cada', 'um']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E este povo mesclado, coberto de luxo,\n",
      "radiante, com a barriga confortada e o coração contente, passeia, exibe-se, ancho de si, pensando\n",
      "erradamente chamar a atenção de todos, quando aliás cada qual só pensa e repara em si próprio e na sua\n",
      "própria roupa!\n",
      "Raimundo ria-se por delicadeza, e espreguiçava-se na cadeira, bocejando'\n",
      "Tokens gerados: ['e', 'este', 'povo', 'mesclado', ',', 'coberto', 'de', 'luxo', ',', 'radiante', ',', 'com', 'a', 'barriga', 'confortada', 'e', 'o', 'coracao', 'contente', ',', 'passeia', ',', 'exibe-se', ',', 'ancho', 'de', 'si', ',', 'pensandoerradamente', 'chamar', 'a', 'atencao', 'de', 'todos', ',', 'quando', 'alias', 'cada', 'qual', 'so', 'pensa', 'e', 'repara', 'em', 'si', 'proprio', 'e', 'na', 'suapropria', 'rouparaimundo', 'ria-se', 'por', 'delicadeza', ',', 'e', 'espreguicava-se', 'na', 'cadeira', ',', 'bocejando']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— À noite, continuou o Freitas, ilumina-se todo o largo'\n",
      "Tokens gerados: ['—', 'a', 'noite', ',', 'continuou', 'o', 'freitas', ',', 'ilumina-se', 'todo', 'o', 'largo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Armam-se grandes e deslumbrantes\n",
      "arcos transparentes, com a imagem da santa e os emblemas do Comércio e da Navegação, que Nossa\n",
      "Senhora dos Remédios é padroeira do Comércio, e é este que lhe dá a festa'\n",
      "Tokens gerados: ['armam-se', 'grandes', 'e', 'deslumbrantesarcos', 'transparentes', ',', 'com', 'a', 'imagem', 'da', 'santa', 'e', 'os', 'emblemas', 'do', 'comercio', 'e', 'da', 'navegacao', ',', 'que', 'nossasenhora', 'dos', 'remedios', 'e', 'padroeira', 'do', 'comercio', ',', 'e', 'e', 'este', 'que', 'lhe', 'da', 'a', 'festa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas bem, faz-se a iluminação\n",
      "— armas brasileiras, estrelas, vasos caprichosos, o nome da santa, tudo a bico de gás, não contando\n",
      "uma infinidade de balõezinhos chineses, que brilham por entre as bandeiras, os florões os ariris, as\n",
      "casas de música; em uma palavra fica tudo, tudo, claro como o dia!\n",
      "Raimundo soltou um suspiro profundo e mudou de posição'\n",
      "Tokens gerados: ['mas', 'bem', ',', 'faz-se', 'a', 'iluminacao—', 'armas', 'brasileiras', ',', 'estrelas', ',', 'vasos', 'caprichosos', ',', 'o', 'nome', 'da', 'santa', ',', 'tudo', 'a', 'bico', 'de', 'gas', ',', 'nao', 'contandouma', 'infinidade', 'de', 'baloezinhos', 'chineses', ',', 'que', 'brilham', 'por', 'entre', 'as', 'bandeiras', ',', 'os', 'floroes', 'os', 'ariris', ',', 'ascasas', 'de', 'musica', 'em', 'uma', 'palavra', 'fica', 'tudo', ',', 'tudo', ',', 'claro', 'como', 'o', 'diaraimundo', 'soltou', 'um', 'suspiro', 'profundo', 'e', 'mudou', 'de', 'posicao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Há também, para os moleques, um pau-de-sebo, balanços e cavalinhos'\n",
      "Tokens gerados: ['—', 'ha', 'tambem', ',', 'para', 'os', 'moleques', ',', 'um', 'pau-de-sebo', ',', 'balancos', 'e', 'cavalinhos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É verdade! o doutor\n",
      "sabe o que e um pau-de-sebo?'\n",
      "Tokens gerados: ['e', 'verdade', 'o', 'doutorsabe', 'o', 'que', 'e', 'um', 'pau-de-sebo', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Perfeitamente'\n",
      "Tokens gerados: ['—', 'perfeitamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tenha a bondade de não explicar'\n",
      "Tokens gerados: ['tenha', 'a', 'bondade', 'de', 'nao', 'explicar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Com franqueza! Se não sabe, diga, que eu posso'\n",
      "Tokens gerados: ['—', 'com', 'franqueza', 'se', 'nao', 'sabe', ',', 'diga', ',', 'que', 'eu', 'posso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora, por amor de Deus! faz-me o favor em não se incomodar, juro-lhe! Estou impaciente\n",
      "pelo resultado da festa'\n",
      "Tokens gerados: ['—', 'ora', ',', 'por', 'amor', 'de', 'deus', 'faz-me', 'o', 'favor', 'em', 'nao', 'se', 'incomodar', ',', 'juro-lhe', 'estou', 'impacientepelo', 'resultado', 'da', 'festa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Continue!\n",
      "— Pois sim, senhor'\n",
      "Tokens gerados: ['continue—', 'pois', 'sim', ',', 'senhor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dão oito horas'\n",
      "Tokens gerados: ['dao', 'oito', 'horas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah, meu caro amigo! então surge de todos os cantos da\n",
      "cidade uma aluvião interminável de famílias, de velhos, moços, meninos, mulatinhas e negrinhas, que\n",
      "enchem o largo que nem um ovo! Pretos de ambos os sexos e de todas as idades; desde o moleque até\n",
      "o tio velho, acodem, trazendo equilibradas nas cabeças imensas pilhas de cadeiras, e, com estas cadeiras,\n",
      "formam-se grandes rodas mesmo na praça, ao ar livre, e as famílias, ou ficam aí assentadas, ou, a título\n",
      "de passeio, acotovelam-se entre o povo'\n",
      "Tokens gerados: ['ah', ',', 'meu', 'caro', 'amigo', 'entao', 'surge', 'de', 'todos', 'os', 'cantos', 'dacidade', 'uma', 'aluviao', 'interminavel', 'de', 'familias', ',', 'de', 'velhos', ',', 'mocos', ',', 'meninos', ',', 'mulatinhas', 'e', 'negrinhas', ',', 'queenchem', 'o', 'largo', 'que', 'nem', 'um', 'ovo', 'pretos', 'de', 'ambos', 'os', 'sexos', 'e', 'de', 'todas', 'as', 'idades', 'desde', 'o', 'moleque', 'ateo', 'tio', 'velho', ',', 'acodem', ',', 'trazendo', 'equilibradas', 'nas', 'cabecas', 'imensas', 'pilhas', 'de', 'cadeiras', ',', 'e', ',', 'com', 'estas', 'cadeiras', ',', 'formam-se', 'grandes', 'rodas', 'mesmo', 'na', 'praca', ',', 'ao', 'ar', 'livre', ',', 'e', 'as', 'familias', ',', 'ou', 'ficam', 'ai', 'assentadas', ',', 'ou', ',', 'a', 'titulode', 'passeio', ',', 'acotovelam-se', 'entre', 'o', 'povo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fazem-se grupos, a gente ri, discute, critica, namora, zanga-se,\n",
      "ralha'\n",
      "Tokens gerados: ['fazem-se', 'grupos', ',', 'a', 'gente', 'ri', ',', 'discute', ',', 'critica', ',', 'namora', ',', 'zanga-se', ',', 'ralha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ralha?\n",
      "— Ora! Já houve uma senhora que castigou um moleque a chicote, lá mesmo no largo!\n",
      "— A chicote?\n",
      "— Sim, a chicote! Aquilo, meu caro doutor, é uma espécie de romaria! As famílias levam\n",
      "consigo potes de água, cuscuz, castanhas assadas, biscoitos e o mais'\n",
      "Tokens gerados: ['—', 'ralha', '?', '—', 'ora', 'ja', 'houve', 'uma', 'senhora', 'que', 'castigou', 'um', 'moleque', 'a', 'chicote', ',', 'la', 'mesmo', 'no', 'largo—', 'a', 'chicote', '?', '—', 'sim', ',', 'a', 'chicote', 'aquilo', ',', 'meu', 'caro', 'doutor', ',', 'e', 'uma', 'especie', 'de', 'romaria', 'as', 'familias', 'levamconsigo', 'potes', 'de', 'agua', ',', 'cuscuz', ',', 'castanhas', 'assadas', ',', 'biscoitos', 'e', 'o', 'mais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E tudo isto ao som desordenado\n",
      "da pancadaria de três bandas de música, dos gritos do leiloeiro e da inqualificável algazarra do povo!\n",
      "Raimundo quis levantar-se; o outro obrigou-o a ficar sentado, pondo-lhe as mãos nos ombros'\n",
      "Tokens gerados: ['e', 'tudo', 'isto', 'ao', 'som', 'desordenadoda', 'pancadaria', 'de', 'tres', 'bandas', 'de', 'musica', ',', 'dos', 'gritos', 'do', 'leiloeiro', 'e', 'da', 'inqualificavel', 'algazarra', 'do', 'povoraimundo', 'quis', 'levantar-se', 'o', 'outro', 'obrigou-o', 'a', 'ficar', 'sentado', ',', 'pondo-lhe', 'as', 'maos', 'nos', 'ombros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Estamos no apogeu da festa! exclamou o maçante'\n",
      "Tokens gerados: ['—', 'estamos', 'no', 'apogeu', 'da', 'festa', 'exclamou', 'o', 'macante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah! gemeu Raimundo'\n",
      "Tokens gerados: ['—', 'ah', 'gemeu', 'raimundo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Soltam-se balões de papel fino; cruzam-se moças aos pares; giram aos pares os janotas;\n",
      "vendem-se roletos de cana, sorvetes, garapa, cerveja, doces, pastéis, chupas de laranja; sentem-se arder\n",
      "charutos de canela; gastam-se os últimos cartuchos; esvaziam-se de todo as algibeiras e, finalmente,\n",
      "com grande júbilo geral arde o invariável fogo de artifício'\n",
      "Tokens gerados: ['—', 'soltam-se', 'baloes', 'de', 'papel', 'fino', 'cruzam-se', 'mocas', 'aos', 'pares', 'giram', 'aos', 'pares', 'os', 'janotasvendem-se', 'roletos', 'de', 'cana', ',', 'sorvetes', ',', 'garapa', ',', 'cerveja', ',', 'doces', ',', 'pasteis', ',', 'chupas', 'de', 'laranja', 'sentem-se', 'ardercharutos', 'de', 'canela', 'gastam-se', 'os', 'ultimos', 'cartuchos', 'esvaziam-se', 'de', 'todo', 'as', 'algibeiras', 'e', ',', 'finalmente', ',', 'com', 'grande', 'jubilo', 'geral', 'arde', 'o', 'invariavel', 'fogo', 'de', 'artificio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Então rebentam todas as bandas de música\n",
      "a um só tempo, levanta-se uma fumarada capaz de sufocar um fole, e, no meio do estralejar das bombas\n",
      "e do infrene entusiasmo da multidão, aparece no castelo, deslumbrante de luzes, a imagem de Nossa\n",
      "Senhora dos Remédios'\n",
      "Tokens gerados: ['entao', 'rebentam', 'todas', 'as', 'bandas', 'de', 'musicaa', 'um', 'so', 'tempo', ',', 'levanta-se', 'uma', 'fumarada', 'capaz', 'de', 'sufocar', 'um', 'fole', ',', 'e', ',', 'no', 'meio', 'do', 'estralejar', 'das', 'bombase', 'do', 'infrene', 'entusiasmo', 'da', 'multidao', ',', 'aparece', 'no', 'castelo', ',', 'deslumbrante', 'de', 'luzes', ',', 'a', 'imagem', 'de', 'nossasenhora', 'dos', 'remedios']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foguetes de lágrimas voam aos milhares pelo espaço; o céu some-se'\n",
      "Tokens gerados: ['foguetes', 'de', 'lagrimas', 'voam', 'aos', 'milhares', 'pelo', 'espaco', 'o', 'ceu', 'some-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Todos se\n",
      "descobrem, em atenção à santa, e abrem o chapéu-de-sol com medo das tabocas'\n",
      "Tokens gerados: ['todos', 'sedescobrem', ',', 'em', 'atencao', 'a', 'santa', ',', 'e', 'abrem', 'o', 'chapeu-de-sol', 'com', 'medo', 'das', 'tabocas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Há uma chuva de\n",
      "luzes multicores; tudo se ilumina fantasticamente; todos os grupos, todas as fisionomias, todas as casas,\n",
      "tomam sucessivamente as irradiações do prisma'\n",
      "Tokens gerados: ['ha', 'uma', 'chuva', 'deluzes', 'multicores', 'tudo', 'se', 'ilumina', 'fantasticamente', 'todos', 'os', 'grupos', ',', 'todas', 'as', 'fisionomias', ',', 'todas', 'as', 'casas', ',', 'tomam', 'sucessivamente', 'as', 'irradiacoes', 'do', 'prisma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Durante esta apoteose o povo se concentra numa\n",
      "contemplação mística, terminada a qual, está terminada a festa!\n",
      "E Freitas tomou fôlego'\n",
      "Tokens gerados: ['durante', 'esta', 'apoteose', 'o', 'povo', 'se', 'concentra', 'numacontemplacao', 'mistica', ',', 'terminada', 'a', 'qual', ',', 'esta', 'terminada', 'a', 'festae', 'freitas', 'tomou', 'folego']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo ia falar, ele atalhou:\n",
      "— De repente, o povo acorda e quer sair! Corre, precipita-se em massa à Rua dos Remédios,\n",
      "aglomera-se, disputa os carros, pragueja, assanha-se! Cada um entende que deve chegar primeiro à\n",
      "casa; há trambolhões, descomposturas, gritos, gargalhadas, gemidos, rinchos de cavalos, tabuleiros de\n",
      "doce derramados, vestidos rotos, pés esmagados, crianças perdidas, homens bêbados; mas, de súbito,\n",
      "como por encanto, esvazia-se o largo e desaparece a multidão!\n",
      "— Como? por quê?\n",
      "— Daí a pouco estão todos recolhidos, sonhando já com a festa do ano seguinte, calculando\n",
      "economias, pensando em ganhar dinheiro, para na outra fazer ainda melhor figura!\n",
      "E o Freitas resfolegou prostrado, com a língua seca'\n",
      "Tokens gerados: ['raimundo', 'ia', 'falar', ',', 'ele', 'atalhou—', 'de', 'repente', ',', 'o', 'povo', 'acorda', 'e', 'quer', 'sair', 'corre', ',', 'precipita-se', 'em', 'massa', 'a', 'rua', 'dos', 'remedios', ',', 'aglomera-se', ',', 'disputa', 'os', 'carros', ',', 'pragueja', ',', 'assanha-se', 'cada', 'um', 'entende', 'que', 'deve', 'chegar', 'primeiro', 'acasa', 'ha', 'trambolhoes', ',', 'descomposturas', ',', 'gritos', ',', 'gargalhadas', ',', 'gemidos', ',', 'rinchos', 'de', 'cavalos', ',', 'tabuleiros', 'dedoce', 'derramados', ',', 'vestidos', 'rotos', ',', 'pes', 'esmagados', ',', 'criancas', 'perdidas', ',', 'homens', 'bebados', 'mas', ',', 'de', 'subito', ',', 'como', 'por', 'encanto', ',', 'esvazia-se', 'o', 'largo', 'e', 'desaparece', 'a', 'multidao—', 'como', '?', 'por', 'que', '?', '—', 'dai', 'a', 'pouco', 'estao', 'todos', 'recolhidos', ',', 'sonhando', 'ja', 'com', 'a', 'festa', 'do', 'ano', 'seguinte', ',', 'calculandoeconomias', ',', 'pensando', 'em', 'ganhar', 'dinheiro', ',', 'para', 'na', 'outra', 'fazer', 'ainda', 'melhor', 'figurae', 'o', 'freitas', 'resfolegou', 'prostrado', ',', 'com', 'a', 'lingua', 'seca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Mas por que diabo se retiram tão depressa?'\n",
      "Tokens gerados: ['—', 'mas', 'por', 'que', 'diabo', 'se', 'retiram', 'tao', 'depressa', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'perguntou Raimundo'\n",
      "Tokens gerados: ['perguntou', 'raimundo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Freitas engoliu sofregamente três goles de água e voltou-se logo'\n",
      "Tokens gerados: ['freitas', 'engoliu', 'sofregamente', 'tres', 'goles', 'de', 'agua', 'e', 'voltou-se', 'logo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— É porque este povinho, por fogo de vista, é pior que macaco por banana! Tirem-lhe de lá o\n",
      "fogo que ninguém se abalará de casa!\n",
      "— Com efeito! E é muito antiga esta festa, sabe?\n",
      "— Bastante'\n",
      "Tokens gerados: ['—', 'e', 'porque', 'este', 'povinho', ',', 'por', 'fogo', 'de', 'vista', ',', 'e', 'pior', 'que', 'macaco', 'por', 'banana', 'tirem-lhe', 'de', 'la', 'ofogo', 'que', 'ninguem', 'se', 'abalara', 'de', 'casa—', 'com', 'efeito', 'e', 'e', 'muito', 'antiga', 'esta', 'festa', ',', 'sabe', '?', '—', 'bastante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ela já tem seu tempo'\n",
      "Tokens gerados: ['ela', 'ja', 'tem', 'seu', 'tempo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ora espere!\n",
      "E o memorião atirou logo o olhar para o teto'\n",
      "Tokens gerados: ['ora', 'esperee', 'o', 'memoriao', 'atirou', 'logo', 'o', 'olhar', 'para', 'o', 'teto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— No tempo dos governadores portugueses, disse, depois de uma pausa, era ali o convento de\n",
      "São Francisco; isso foi'\n",
      "Tokens gerados: ['—', 'no', 'tempo', 'dos', 'governadores', 'portugueses', ',', 'disse', ',', 'depois', 'de', 'uma', 'pausa', ',', 'era', 'ali', 'o', 'convento', 'desao', 'francisco', 'isso', 'foi']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'poderia ser'\n",
      "Tokens gerados: ['poderia', 'ser']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'em'\n",
      "Tokens gerados: ['em']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'em mil, setecentos'\n",
      "Tokens gerados: ['em', 'mil', ',', 'setecentos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'e dezenove! Chamava-se então a\n",
      "ponta, que forma hoje o Largo dos Remédios, “Ponta do Romeu”'\n",
      "Tokens gerados: ['e', 'dezenove', 'chamava-se', 'entao', 'aponta', ',', 'que', 'forma', 'hoje', 'o', 'largo', 'dos', 'remedios', ',', '“', 'ponta', 'do', 'romeu', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ora, os frades cederam esse terreno\n",
      "a um tal Monteiro de Carvalho, que fez a ermida, como se pode calcular, no mato'\n",
      "Tokens gerados: ['ora', ',', 'os', 'frades', 'cederam', 'esse', 'terrenoa', 'um', 'tal', 'monteiro', 'de', 'carvalho', ',', 'que', 'fez', 'a', 'ermida', ',', 'como', 'se', 'pode', 'calcular', ',', 'no', 'mato']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma ocasião, porém,\n",
      "um preto fugido matou nesse lugar o seu senhor, e os romeiros, que lá iam constantemente, abandonaram\n",
      "receosos a devoção'\n",
      "Tokens gerados: ['uma', 'ocasiao', ',', 'porem', ',', 'um', 'preto', 'fugido', 'matou', 'nesse', 'lugar', 'o', 'seu', 'senhor', ',', 'e', 'os', 'romeiros', ',', 'que', 'la', 'iam', 'constantemente', ',', 'abandonaramreceosos', 'a', 'devocao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Só depois de cinqüenta e seis anos, é que o governador Joaquim de Melo e Póvoas\n",
      "mandou abrir uma boa estrada, a qual vem a ser hoje a nossa pitoresca Rua dos Remédios'\n",
      "Tokens gerados: ['so', 'depois', 'de', 'cinquenta', 'e', 'seis', 'anos', ',', 'e', 'que', 'o', 'governador', 'joaquim', 'de', 'melo', 'e', 'povoasmandou', 'abrir', 'uma', 'boa', 'estrada', ',', 'a', 'qual', 'vem', 'a', 'ser', 'hoje', 'a', 'nossa', 'pitoresca', 'rua', 'dos', 'remedios']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A ermida\n",
      "caiu em ruínas, mas o ermitão, Francisco Xavier, mandou, em 1818, construir a que lá está presentemente;\n",
      "e daí data a festa, que tive a honra e o gosto de descrever-lhe'\n",
      "Tokens gerados: ['a', 'ermidacaiu', 'em', 'ruinas', ',', 'mas', 'o', 'ermitao', ',', 'francisco', 'xavier', ',', 'mandou', ',', 'em', '1818', ',', 'construir', 'a', 'que', 'la', 'esta', 'presentementee', 'dai', 'data', 'a', 'festa', ',', 'que', 'tive', 'a', 'honra', 'e', 'o', 'gosto', 'de', 'descrever-lhe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— De tudo isso, aventurou Raimundo, o que mais me admira é a sua memória: o senhor com\n",
      "efeito tem uma memória de anjo'\n",
      "Tokens gerados: ['—', 'de', 'tudo', 'isso', ',', 'aventurou', 'raimundo', ',', 'o', 'que', 'mais', 'me', 'admira', 'e', 'a', 'sua', 'memoria', 'o', 'senhor', 'comefeito', 'tem', 'uma', 'memoria', 'de', 'anjo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora! O senhor ainda não viu nada! Vou contar-lhe'\n",
      "Tokens gerados: ['—', 'ora', 'o', 'senhor', 'ainda', 'nao', 'viu', 'nada', 'vou', 'contar-lhe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O outro ia disparatar sem mais considerações, quando, felizmente, acudiram todos à varanda'\n",
      "Tokens gerados: ['o', 'outro', 'ia', 'disparatar', 'sem', 'mais', 'consideracoes', ',', 'quando', ',', 'felizmente', ',', 'acudiram', 'todos', 'a', 'varanda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Criou alma nova'\n",
      "Tokens gerados: ['criou', 'alma', 'nova']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Apre! disse Raimundo consigo, respirando'\n",
      "Tokens gerados: ['—', 'apre', 'disse', 'raimundo', 'consigo', ',', 'respirando']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É de primeira força!'\n",
      "Tokens gerados: ['e', 'de', 'primeira', 'forca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Serviu-se o chocolate'\n",
      "Tokens gerados: ['serviu-se', 'o', 'chocolate']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cônego vinha a discretear para Manuel em voz soturna:\n",
      "— Pois é o que lhe digo, compadre, fique você com as casas e divida-as em meias-moradas, que\n",
      "rendem?'\n",
      "Tokens gerados: ['o', 'conego', 'vinha', 'a', 'discretear', 'para', 'manuel', 'em', 'voz', 'soturna—', 'pois', 'e', 'o', 'que', 'lhe', 'digo', ',', 'compadre', ',', 'fique', 'voce', 'com', 'as', 'casas', 'e', 'divida-as', 'em', 'meias-moradas', ',', 'querendem', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Acha então que vou bem, dando quatro contos de réis por cada uma'\n",
      "Tokens gerados: ['—', 'acha', 'entao', 'que', 'vou', 'bem', ',', 'dando', 'quatro', 'contos', 'de', 'reis', 'por', 'cada', 'uma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Decerto, são de graça!'\n",
      "Tokens gerados: ['—', 'decerto', ',', 'sao', 'de', 'graca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Homem aquilo é pedra e cal — construção antiga! — deita séculos!\n",
      "Além disso, as casinhas têm bom quintal, bom poço e não são devassadas pela vizinhança'\n",
      "Tokens gerados: ['homem', 'aquilo', 'e', 'pedra', 'e', 'cal', '—', 'construcao', 'antiga', '—', 'deita', 'seculosalem', 'disso', ',', 'as', 'casinhas', 'tem', 'bom', 'quintal', ',', 'bom', 'poco', 'e', 'nao', 'sao', 'devassadas', 'pela', 'vizinhanca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'verdade é\n",
      "que não deixam de ser um bocadinho quentes, mas'\n",
      "Tokens gerados: ['verdade', 'eque', 'nao', 'deixam', 'de', 'ser', 'um', 'bocadinho', 'quentes', ',', 'mas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Abrem-se-lhe janelas para o nascente, concluiu o negociante'\n",
      "Tokens gerados: ['—', 'abrem-se-lhe', 'janelas', 'para', 'o', 'nascente', ',', 'concluiu', 'o', 'negociante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, assim, conversando, chegaram à varanda, onde já estavam à mesa'\n",
      "Tokens gerados: ['e', ',', 'assim', ',', 'conversando', ',', 'chegaram', 'a', 'varanda', ',', 'onde', 'ja', 'estavam', 'a', 'mesa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José Roberto e Sebastião Campos serviam às senhoras, acompanhando com uma pilhéria cada\n",
      "prato que lhes ofereciam'\n",
      "Tokens gerados: ['jose', 'roberto', 'e', 'sebastiao', 'campos', 'serviam', 'as', 'senhoras', ',', 'acompanhando', 'com', 'uma', 'pilheria', 'cadaprato', 'que', 'lhes', 'ofereciam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo pediu dispensa do chá, com medo do Freitas que lhe abrira um\n",
      "lugar ao lado do seu'\n",
      "Tokens gerados: ['raimundo', 'pediu', 'dispensa', 'do', 'cha', ',', 'com', 'medo', 'do', 'freitas', 'que', 'lhe', 'abrira', 'umlugar', 'ao', 'lado', 'do', 'seu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ouviu-se mastigar as torradas e sorver, aos golinhos, o chocolate quente'\n",
      "Tokens gerados: ['ouviu-se', 'mastigar', 'as', 'torradas', 'e', 'sorver', ',', 'aos', 'golinhos', ',', 'o', 'chocolate', 'quente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Doutor, exclamou o cônego, procurando espetar com o garfo uma fatia de um bolo de tapioca'\n",
      "Tokens gerados: ['—', 'doutor', ',', 'exclamou', 'o', 'conego', ',', 'procurando', 'espetar', 'com', 'o', 'garfo', 'uma', 'fatia', 'de', 'um', 'bolo', 'de', 'tapioca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prove ao menos do nosso “Bolo do Maranhão”'\n",
      "Tokens gerados: ['prove', 'ao', 'menos', 'do', 'nosso', '“', 'bolo', 'do', 'maranhao', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Também o chamam por aí “Bolo podre”'\n",
      "Tokens gerados: ['tambem', 'o', 'chamam', 'por', 'ai', '“', 'bolo', 'podre', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prove, que\n",
      "isto não há fora de cá'\n",
      "Tokens gerados: ['prove', ',', 'queisto', 'nao', 'ha', 'fora', 'de', 'ca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'é uma especialidade da terra!\n",
      "— Não é mau'\n",
      "Tokens gerados: ['e', 'uma', 'especialidade', 'da', 'terra—', 'nao', 'e', 'mau']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'disse Raimundo, fazendo-lhe a vontade'\n",
      "Tokens gerados: ['disse', 'raimundo', ',', 'fazendo-lhe', 'a', 'vontade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Muito saboroso, mas parece-me um\n",
      "tanto pesado'\n",
      "Tokens gerados: ['muito', 'saboroso', ',', 'mas', 'parece-me', 'umtanto', 'pesado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— É de substância — acrescentou Maria Bárbara'\n",
      "Tokens gerados: ['—', 'e', 'de', 'substancia', '—', 'acrescentou', 'maria', 'barbara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Faz-se de tapioca de forno e ovos'\n",
      "Tokens gerados: ['faz-se', 'de', 'tapioca', 'de', 'forno', 'e', 'ovos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— D'\n",
      "Tokens gerados: ['—', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bibina! chamou Ana Rosa, apontando para os beijus'\n",
      "Tokens gerados: ['bibina', 'chamou', 'ana', 'rosa', ',', 'apontando', 'para', 'os', 'beijus']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'São fresquinhos'\n",
      "Tokens gerados: ['sao', 'fresquinhos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Amância, com a boca cheia, dizia baixo a Maria do Carmo:\n",
      "— Pois, minha amiga, quando precisar de missa com cerimônia, não tem mais do que se entender\n",
      "com o padre que lhe digo'\n",
      "Tokens gerados: ['amancia', ',', 'com', 'a', 'boca', 'cheia', ',', 'dizia', 'baixo', 'a', 'maria', 'do', 'carmo—', 'pois', ',', 'minha', 'amiga', ',', 'quando', 'precisar', 'de', 'missa', 'com', 'cerimonia', ',', 'nao', 'tem', 'mais', 'do', 'que', 'se', 'entendercom', 'o', 'padre', 'que', 'lhe', 'digo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É muito pontual e contenta-se com o que a gente lhe dá! Est’r’o dia,\n",
      "apanhou-me dezoito mil-réis por uma missinha cantada, mas também podia se ver a obra que o homem\n",
      "apresentou!'\n",
      "Tokens gerados: ['e', 'muito', 'pontual', 'e', 'contenta-se', 'com', 'o', 'que', 'a', 'gente', 'lhe', 'da', 'est', '’', 'r', '’', 'o', 'dia', ',', 'apanhou-me', 'dezoito', 'mil-reis', 'por', 'uma', 'missinha', 'cantada', ',', 'mas', 'tambem', 'podia', 'se', 'ver', 'a', 'obra', 'que', 'o', 'homemapresentou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois então! Há de dar uma criatura seus cobrinhos, que tanto custam a juntar, a muito\n",
      "padre, como há por aí, desses que, mal chegam ao altar, estão pensando no almoço e na comadre?'\n",
      "Tokens gerados: ['pois', 'entao', 'ha', 'de', 'dar', 'uma', 'criatura', 'seus', 'cobrinhos', ',', 'que', 'tanto', 'custam', 'a', 'juntar', ',', 'a', 'muitopadre', ',', 'como', 'ha', 'por', 'ai', ',', 'desses', 'que', ',', 'mal', 'chegam', 'ao', 'altar', ',', 'estao', 'pensando', 'no', 'almoco', 'e', 'na', 'comadre', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deus te livre, credo! Até pesa na consciência de um cristão!\n",
      "— Como o padre Murta!'\n",
      "Tokens gerados: ['deus', 'te', 'livre', ',', 'credo', 'ate', 'pesa', 'na', 'consciencia', 'de', 'um', 'cristao—', 'como', 'o', 'padre', 'murta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'lembrou a outra'\n",
      "Tokens gerados: ['lembrou', 'a', 'outra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Oh! Esse, nem se fala! Às vezes, Deus me perdoe! nos enterros, até se apresenta bêbado!\n",
      "E Maria do Carmo bateu na boca — Cá está, acrescentou, quem já o viu a todo o pano encomendar\n",
      "o corpo de José Caroxo!'\n",
      "Tokens gerados: ['—', 'oh', 'esse', ',', 'nem', 'se', 'fala', 'as', 'vezes', ',', 'deus', 'me', 'perdoe', 'nos', 'enterros', ',', 'ate', 'se', 'apresenta', 'bebadoe', 'maria', 'do', 'carmo', 'bateu', 'na', 'boca', '—', 'ca', 'esta', ',', 'acrescentou', ',', 'quem', 'ja', 'o', 'viu', 'a', 'todo', 'o', 'pano', 'encomendaro', 'corpo', 'de', 'jose', 'caroxo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não! que hoj’em dia a gente perde a fé'\n",
      "Tokens gerados: ['—', 'nao', 'que', 'hoj', '’', 'em', 'dia', 'a', 'gente', 'perde', 'a', 'fe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'isso está se metendo pelos olhos!'\n",
      "Tokens gerados: ['isso', 'esta', 'se', 'metendo', 'pelos', 'olhos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas é o que já\n",
      "não tem o outro'\n",
      "Tokens gerados: ['mas', 'e', 'o', 'que', 'janao', 'tem', 'o', 'outro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'porta-se muito bem! muito bem procedido! muito cumpridor das suas obrigações!\n",
      "Zeloso da religião! Acredite, minha amiga, que faz gosto'\n",
      "Tokens gerados: ['porta-se', 'muito', 'bem', 'muito', 'bem', 'procedido', 'muito', 'cumpridor', 'das', 'suas', 'obrigacoeszeloso', 'da', 'religiao', 'acredite', ',', 'minha', 'amiga', ',', 'que', 'faz', 'gosto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dizem até'\n",
      "Tokens gerados: ['dizem', 'ate']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E Amância segredou alguma coisa à vizinha'\n",
      "Tokens gerados: ['e', 'amancia', 'segredou', 'alguma', 'coisa', 'a', 'vizinha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Maria do Carmo baixou os olhos, e resmungou\n",
      "beaticamente:\n",
      "— Deus lhe leve em conta, coitado!\n",
      "Houve um rumor de cadeiras que se arrastam'\n",
      "Tokens gerados: ['maria', 'do', 'carmo', 'baixou', 'os', 'olhos', ',', 'e', 'resmungoubeaticamente—', 'deus', 'lhe', 'leve', 'em', 'conta', ',', 'coitadohouve', 'um', 'rumor', 'de', 'cadeiras', 'que', 'se', 'arrastam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os comensais afastaram-se dos seus lugares'\n",
      "Tokens gerados: ['os', 'comensais', 'afastaram-se', 'dos', 'seus', 'lugares']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Mesa feita'\n",
      "Tokens gerados: ['—', 'mesa', 'feita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'companhia desfeita!'\n",
      "Tokens gerados: ['companhia', 'desfeita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'gritou logo José Roberto, chupando os restos dos dentes'\n",
      "Tokens gerados: ['gritou', 'logo', 'jose', 'roberto', ',', 'chupando', 'os', 'restos', 'dos', 'dentes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E\n",
      "tratou de seguir as senhoras, que se encaminhavam silenciosas para a sala'\n",
      "Tokens gerados: ['etratou', 'de', 'seguir', 'as', 'senhoras', ',', 'que', 'se', 'encaminhavam', 'silenciosas', 'para', 'a', 'sala']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nisto, entrou o Dias, trazendo o Benedito pelo cós'\n",
      "Tokens gerados: ['nisto', ',', 'entrou', 'o', 'dias', ',', 'trazendo', 'o', 'benedito', 'pelo', 'cos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vinha a deitar os bofes pela boca e, quase\n",
      "sem poder falar, contou que “seguira o ladrão até o fim da Rua Grande, e que o ladrão quebrara para o\n",
      "Largo dos Quartéis e quase que alcança o mato da Camboa”'\n",
      "Tokens gerados: ['vinha', 'a', 'deitar', 'os', 'bofes', 'pela', 'boca', 'e', ',', 'quasesem', 'poder', 'falar', ',', 'contou', 'que', '“', 'seguira', 'o', 'ladrao', 'ate', 'o', 'fim', 'da', 'rua', 'grande', ',', 'e', 'que', 'o', 'ladrao', 'quebrara', 'para', 'olargo', 'dos', 'quarteis', 'e', 'quase', 'que', 'alcanca', 'o', 'mato', 'da', 'camboa', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dito isto, conduziu ele mesmo o moleque\n",
      "lá para dentro'\n",
      "Tokens gerados: ['dito', 'isto', ',', 'conduziu', 'ele', 'mesmo', 'o', 'molequela', 'para', 'dentro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Anda, peste! Vai preparando o pêlo, que ainda hoje te metes em relho!”\n",
      "Apreciaram muito o serviço do Dias, e conversaram sobre aquele ato de dedicação, elogiando o\n",
      "zelo do bom amigo e caixeiro de Manuel'\n",
      "Tokens gerados: ['“', 'anda', ',', 'peste', 'vai', 'preparando', 'o', 'pelo', ',', 'que', 'ainda', 'hoje', 'te', 'metes', 'em', 'relho', '”', 'apreciaram', 'muito', 'o', 'servico', 'do', 'dias', ',', 'e', 'conversaram', 'sobre', 'aquele', 'ato', 'de', 'dedicacao', ',', 'elogiando', 'ozelo', 'do', 'bom', 'amigo', 'e', 'caixeiro', 'de', 'manuel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Daí a uma hora despediam-se as moças, entre grande barafunda\n",
      "de beijos e abraços'\n",
      "Tokens gerados: ['dai', 'a', 'uma', 'hora', 'despediam-se', 'as', 'mocas', ',', 'entre', 'grande', 'barafundade', 'beijos', 'e', 'abracos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Lindoca! gritava Ana Rosa, agora não arribe de novo, ouviu?'\n",
      "Tokens gerados: ['—', 'lindoca', 'gritava', 'ana', 'rosa', ',', 'agora', 'nao', 'arribe', 'de', 'novo', ',', 'ouviu', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Sim, minha vida, hei de aparecer'\n",
      "Tokens gerados: ['—', 'sim', ',', 'minha', 'vida', ',', 'hei', 'de', 'aparecer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'olha!\n",
      "E subiu dois degraus para lhe dizer um segredinho'\n",
      "Tokens gerados: ['olhae', 'subiu', 'dois', 'degraus', 'para', 'lhe', 'dizer', 'um', 'segredinho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Sim, sim! Eufrasinha, adeus! D'\n",
      "Tokens gerados: ['—', 'sim', ',', 'sim', 'eufrasinha', ',', 'adeus', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Maria do Carmo, não deixe de levar essas meninas à quinta\n",
      "no dia de São João'\n",
      "Tokens gerados: ['maria', 'do', 'carmo', ',', 'nao', 'deixe', 'de', 'levar', 'essas', 'meninas', 'a', 'quintano', 'dia', 'de', 'sao', 'joao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Temos torta de caranguejos, olhe lá!\n",
      "— Adeus, coração!\n",
      "— Etelvina, não se esqueça daquilo!'\n",
      "Tokens gerados: ['temos', 'torta', 'de', 'caranguejos', ',', 'olhe', 'la—', 'adeus', ',', 'coracao—', 'etelvina', ',', 'nao', 'se', 'esqueca', 'daquilo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Bibina, despeça-se da gente!'\n",
      "Tokens gerados: ['—', 'bibina', ',', 'despeca-se', 'da', 'gente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'guarde seus quatro vinténs!'\n",
      "Tokens gerados: ['guarde', 'seus', 'quatro', 'vintens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Olhe, observou o Sebastião Campos, que as tais moças, para se despedirem'\n",
      "Tokens gerados: ['—', 'olhe', ',', 'observou', 'o', 'sebastiao', 'campos', ',', 'que', 'as', 'tais', 'mocas', ',', 'para', 'se', 'despedirem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'são temíveis!\n",
      "— “Pudesse uma só nau contê-las todas'\n",
      "Tokens gerados: ['sao', 'temiveis—', '“', 'pudesse', 'uma', 'so', 'nau', 'conte-las', 'todas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” recitou o Freitas, coçando o bigode com a sua unha\n",
      "de estimação, “e o piloto fosse eu'\n",
      "Tokens gerados: ['”', 'recitou', 'o', 'freitas', ',', 'cocando', 'o', 'bigode', 'com', 'a', 'sua', 'unhade', 'estimacao', ',', '“', 'e', 'o', 'piloto', 'fosse', 'eu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'triunfo eterno!'\n",
      "Tokens gerados: ['triunfo', 'eterno']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” E, após uma gargalhada seca, voltou-se para\n",
      "Raimundo e ofereceu-lhe com ar pretensioso “um talher na sua parca mesa”'\n",
      "Tokens gerados: ['”', 'e', ',', 'apos', 'uma', 'gargalhada', 'seca', ',', 'voltou-se', 'pararaimundo', 'e', 'ofereceu-lhe', 'com', 'ar', 'pretensioso', '“', 'um', 'talher', 'na', 'sua', 'parca', 'mesa', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Vá doutor, vá por aquela choupana, disse'\n",
      "Tokens gerados: ['—', 'va', 'doutor', ',', 'va', 'por', 'aquela', 'choupana', ',', 'disse']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vá aborrecer-se um pouco'\n",
      "Tokens gerados: ['va', 'aborrecer-se', 'um', 'pouco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo prometeu distraidamente'\n",
      "Tokens gerados: ['raimundo', 'prometeu', 'distraidamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bocejava'\n",
      "Tokens gerados: ['bocejava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por mera delicadeza, perguntou se alguma das\n",
      "senhoras “queria um criado para acompanhá-las a casa”'\n",
      "Tokens gerados: ['por', 'mera', 'delicadeza', ',', 'perguntou', 'se', 'alguma', 'dassenhoras', '“', 'queria', 'um', 'criado', 'para', 'acompanha-las', 'a', 'casa', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As Sarmentos aceitaram logo, com muitos trejeitos de cortesia'\n",
      "Tokens gerados: ['as', 'sarmentos', 'aceitaram', 'logo', ',', 'com', 'muitos', 'trejeitos', 'de', 'cortesia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ele, interiormente contrariado,\n",
      "levou-as até às Mercês, onde moravam, ali mesmo, perto'\n",
      "Tokens gerados: ['ele', ',', 'interiormente', 'contrariado', ',', 'levou-as', 'ate', 'as', 'merces', ',', 'onde', 'moravam', ',', 'ali', 'mesmo', ',', 'perto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Voltou pouco depois'\n",
      "Tokens gerados: ['voltou', 'pouco', 'depois']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Recolha-se, doutor, trate de recolher-se'\n",
      "Tokens gerados: ['—', 'recolha-se', ',', 'doutor', ',', 'trate', 'de', 'recolher-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'aconselhou-lhe Manuel, que o esperava de pé'\n",
      "Tokens gerados: ['aconselhou-lhe', 'manuel', ',', 'que', 'o', 'esperava', 'de', 'pe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O\n",
      "senhor deve estar com o corpo a pedir descanso'\n",
      "Tokens gerados: ['osenhor', 'deve', 'estar', 'com', 'o', 'corpo', 'a', 'pedir', 'descanso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Raimundo confessou que sim, apertou-lhe a mão'\n",
      "Tokens gerados: ['raimundo', 'confessou', 'que', 'sim', ',', 'apertou-lhe', 'a', 'mao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Boas noites, e obrigado”'\n",
      "Tokens gerados: ['“', 'boas', 'noites', ',', 'e', 'obrigado', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Até amanhã! Olhe! se precisar de qualquer coisa, chame pelo Benedito, ele dorme na varanda'\n",
      "Tokens gerados: ['—', 'ate', 'amanha', 'olhe', 'se', 'precisar', 'de', 'qualquer', 'coisa', ',', 'chame', 'pelo', 'benedito', ',', 'ele', 'dorme', 'na', 'varanda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas deve estar\n",
      "tudo lá; a Brígida é cuidadosa'\n",
      "Tokens gerados: ['mas', 'deve', 'estartudo', 'la', 'a', 'brigida', 'e', 'cuidadosa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Passe bem!\n",
      "Raimundo fechou-se no quarto; despiu-se, acendeu um cigarro e deitou-se'\n",
      "Tokens gerados: ['passe', 'bemraimundo', 'fechou-se', 'no', 'quarto', 'despiu-se', ',', 'acendeu', 'um', 'cigarro', 'e', 'deitou-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Abriu por hábito um\n",
      "livro; mas, no fim da primeira página, as pálpebras se lhe fechavam'\n",
      "Tokens gerados: ['abriu', 'por', 'habito', 'umlivro', 'mas', ',', 'no', 'fim', 'da', 'primeira', 'pagina', ',', 'as', 'palpebras', 'se', 'lhe', 'fechavam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Soprou a vela'\n",
      "Tokens gerados: ['soprou', 'a', 'vela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Então sentiu um\n",
      "bem-estar infinito, profundamente agradável; abraçou-se aos travesseiros e, antes que algum dos\n",
      "acontecimentos desse dia lhe assaltasse o espírito, adormeceu'\n",
      "Tokens gerados: ['entao', 'sentiu', 'umbem-estar', 'infinito', ',', 'profundamente', 'agradavel', 'abracou-se', 'aos', 'travesseiros', 'e', ',', 'antes', 'que', 'algum', 'dosacontecimentos', 'desse', 'dia', 'lhe', 'assaltasse', 'o', 'espirito', ',', 'adormeceu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Todavia, a pouca distância dali, alguém velava, pensando nele'\n",
      "Tokens gerados: ['todavia', ',', 'a', 'pouca', 'distancia', 'dali', ',', 'alguem', 'velava', ',', 'pensando', 'nele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_o_mulato_aluisio_azevedo_cap_4.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Há anos raiou no céu fluminense uma nova estrela'\n",
      "Tokens gerados: ['ha', 'anos', 'raiou', 'no', 'ceu', 'fluminense', 'uma', 'nova', 'estrela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desde o momento de sua ascensão ninguém lhe disputou o cetro; foi proclamada a rainha dos\n",
      "salões'\n",
      "Tokens gerados: ['desde', 'o', 'momento', 'de', 'sua', 'ascensao', 'ninguem', 'lhe', 'disputou', 'o', 'cetro', 'foi', 'proclamada', 'a', 'rainha', 'dossaloes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tornou-se a deusa dos bailes; a musa dos poetas e o ídolo dos noivos em disponibilidade'\n",
      "Tokens gerados: ['tornou-se', 'a', 'deusa', 'dos', 'bailes', 'a', 'musa', 'dos', 'poetas', 'e', 'o', 'idolo', 'dos', 'noivos', 'em', 'disponibilidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era rica e formosa'\n",
      "Tokens gerados: ['era', 'rica', 'e', 'formosa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Duas opulências, que se realçam como a flor em vaso de alabastro; dois esplendores que se\n",
      "refletem, como o raio de sol no prisma do diamante'\n",
      "Tokens gerados: ['duas', 'opulencias', ',', 'que', 'se', 'realcam', 'como', 'a', 'flor', 'em', 'vaso', 'de', 'alabastro', 'dois', 'esplendores', 'que', 'serefletem', ',', 'como', 'o', 'raio', 'de', 'sol', 'no', 'prisma', 'do', 'diamante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quem não se recorda da Aurélia Camargo, que atravessou o firmamento da Corte como brilhante\n",
      "meteoro, e apagou-se de repente no meio do deslumbramento que produzira o seu -fulgor?\n",
      "Tinha ela dezoito anos quando apareceu a primeira vez na sociedade'\n",
      "Tokens gerados: ['quem', 'nao', 'se', 'recorda', 'da', 'aurelia', 'camargo', ',', 'que', 'atravessou', 'o', 'firmamento', 'da', 'corte', 'como', 'brilhantemeteoro', ',', 'e', 'apagou-se', 'de', 'repente', 'no', 'meio', 'do', 'deslumbramento', 'que', 'produzira', 'o', 'seu', '-fulgor', '?', 'tinha', 'ela', 'dezoito', 'anos', 'quando', 'apareceu', 'a', 'primeira', 'vez', 'na', 'sociedade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não a conheciam; e logo\n",
      "buscaram todos com avidez informações acerca da grande novidade do dia'\n",
      "Tokens gerados: ['nao', 'a', 'conheciam', 'e', 'logobuscaram', 'todos', 'com', 'avidez', 'informacoes', 'acerca', 'da', 'grande', 'novidade', 'do', 'dia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dizia-se muita coisa que não repetirei agora, pois a seu tempo saberemos a verdade, sem os\n",
      "comentos malévolos de que usam vesti-la os noveleiros'\n",
      "Tokens gerados: ['dizia-se', 'muita', 'coisa', 'que', 'nao', 'repetirei', 'agora', ',', 'pois', 'a', 'seu', 'tempo', 'saberemos', 'a', 'verdade', ',', 'sem', 'oscomentos', 'malevolos', 'de', 'que', 'usam', 'vesti-la', 'os', 'noveleiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aurélia era órfã; e tinha em sua companhia uma velha parenta, viúva, D'\n",
      "Tokens gerados: ['aurelia', 'era', 'orfa', 'e', 'tinha', 'em', 'sua', 'companhia', 'uma', 'velha', 'parenta', ',', 'viuva', ',', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina Mascarenhas, que\n",
      "sempre a acompanhava na sociedade'\n",
      "Tokens gerados: ['firmina', 'mascarenhas', ',', 'quesempre', 'a', 'acompanhava', 'na', 'sociedade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas essa parenta não passava de mãe de encomenda, para condescender com os escrúpulos da\n",
      "sociedade brasileira, que naquele tempo não tinha admitido ainda certa emancipação feminina'\n",
      "Tokens gerados: ['mas', 'essa', 'parenta', 'nao', 'passava', 'de', 'mae', 'de', 'encomenda', ',', 'para', 'condescender', 'com', 'os', 'escrupulos', 'dasociedade', 'brasileira', ',', 'que', 'naquele', 'tempo', 'nao', 'tinha', 'admitido', 'ainda', 'certa', 'emancipacao', 'feminina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Guardando com a viúva as deferências devidas à idade, a moça não declinava um instante do firme\n",
      "propósito de governar sua casa e dirigir suas ações como entendesse'\n",
      "Tokens gerados: ['guardando', 'com', 'a', 'viuva', 'as', 'deferencias', 'devidas', 'a', 'idade', ',', 'a', 'moca', 'nao', 'declinava', 'um', 'instante', 'do', 'firmeproposito', 'de', 'governar', 'sua', 'casa', 'e', 'dirigir', 'suas', 'acoes', 'como', 'entendesse']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Constava também que Aurélia tinha um tutor; mas essa entidade desconhecida, a julgar pelo caráter\n",
      "da pupila, não devia exercer maior influência em sua vontade, do que a velha pa-renta'\n",
      "Tokens gerados: ['constava', 'tambem', 'que', 'aurelia', 'tinha', 'um', 'tutor', 'mas', 'essa', 'entidade', 'desconhecida', ',', 'a', 'julgar', 'pelo', 'caraterda', 'pupila', ',', 'nao', 'devia', 'exercer', 'maior', 'influencia', 'em', 'sua', 'vontade', ',', 'do', 'que', 'a', 'velha', 'pa-renta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A convicção geral era que o futuro da moça dependia exclusivamente de suas inclinações ou de seu\n",
      "capricho; e por isso todas as adorações se iam prostrar aos próprios pés do ídolo'\n",
      "Tokens gerados: ['a', 'conviccao', 'geral', 'era', 'que', 'o', 'futuro', 'da', 'moca', 'dependia', 'exclusivamente', 'de', 'suas', 'inclinacoes', 'ou', 'de', 'seucapricho', 'e', 'por', 'isso', 'todas', 'as', 'adoracoes', 'se', 'iam', 'prostrar', 'aos', 'proprios', 'pes', 'do', 'idolo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Assaltada por uma turba de pretendentes que a disputavam como o prêmio da vitória, Aurélia, com\n",
      "sagacidade admirável em sua idade, avaliou da situação difícil em que se achava, e dos perigos que\n",
      "a ameaçavam'\n",
      "Tokens gerados: ['assaltada', 'por', 'uma', 'turba', 'de', 'pretendentes', 'que', 'a', 'disputavam', 'como', 'o', 'premio', 'da', 'vitoria', ',', 'aurelia', ',', 'comsagacidade', 'admiravel', 'em', 'sua', 'idade', ',', 'avaliou', 'da', 'situacao', 'dificil', 'em', 'que', 'se', 'achava', ',', 'e', 'dos', 'perigos', 'quea', 'ameacavam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Daí provinha talvez a expressão cheia de desdém e um certo ar provocador, que eriçavam a sua\n",
      "beleza aliás tão correta e cinzelada para a meiga e serena expansão d’alma'\n",
      "Tokens gerados: ['dai', 'provinha', 'talvez', 'a', 'expressao', 'cheia', 'de', 'desdem', 'e', 'um', 'certo', 'ar', 'provocador', ',', 'que', 'ericavam', 'a', 'suabeleza', 'alias', 'tao', 'correta', 'e', 'cinzelada', 'para', 'a', 'meiga', 'e', 'serena', 'expansao', 'd', '’', 'alma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se o lindo semblante não se impregnasse constantemente, ainda nos momentos de cisma e\n",
      "distração, dessa tinta de sarcasmo, ninguém veria nela a verdadeira fisionomia de Aurélia, e sim a\n",
      "máscara de alguma profunda decepção'\n",
      "Tokens gerados: ['se', 'o', 'lindo', 'semblante', 'nao', 'se', 'impregnasse', 'constantemente', ',', 'ainda', 'nos', 'momentos', 'de', 'cisma', 'edistracao', ',', 'dessa', 'tinta', 'de', 'sarcasmo', ',', 'ninguem', 'veria', 'nela', 'a', 'verdadeira', 'fisionomia', 'de', 'aurelia', ',', 'e', 'sim', 'amascara', 'de', 'alguma', 'profunda', 'decepcao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Como acreditar que a natureza houvesse traçado as linhas tão puras e límpidas daquele perfil para\n",
      "quebrar-lhes a harmonia com o riso de uma pungente ironia?\n",
      "Os olhos grandes e rasgados, Deus não os aveludaria com a mais inefável ternura, se os destinasse\n",
      "para vibrar chispas de escárnio'\n",
      "Tokens gerados: ['como', 'acreditar', 'que', 'a', 'natureza', 'houvesse', 'tracado', 'as', 'linhas', 'tao', 'puras', 'e', 'limpidas', 'daquele', 'perfil', 'paraquebrar-lhes', 'a', 'harmonia', 'com', 'o', 'riso', 'de', 'uma', 'pungente', 'ironia', '?', 'os', 'olhos', 'grandes', 'e', 'rasgados', ',', 'deus', 'nao', 'os', 'aveludaria', 'com', 'a', 'mais', 'inefavel', 'ternura', ',', 'se', 'os', 'destinassepara', 'vibrar', 'chispas', 'de', 'escarnio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Para que a perfeição estatuária do talhe de sílfide, se em vez de arfar ao suave influxo do amor, ele\n",
      "devia ser agitado pelos assomos do desprezo?\n",
      "Na sala, cercada de adoradores, no meio das esplêndidas reverberações de sua beleza, Aurélia bem\n",
      "longe de inebriar-se da adoração produzida por sua formosura, e do culto que lhe rendiam; ao\n",
      "contrário parecia unicamente possuída de indignação por essa turba vil e abjeta'\n",
      "Tokens gerados: ['para', 'que', 'a', 'perfeicao', 'estatuaria', 'do', 'talhe', 'de', 'silfide', ',', 'se', 'em', 'vez', 'de', 'arfar', 'ao', 'suave', 'influxo', 'do', 'amor', ',', 'eledevia', 'ser', 'agitado', 'pelos', 'assomos', 'do', 'desprezo', '?', 'na', 'sala', ',', 'cercada', 'de', 'adoradores', ',', 'no', 'meio', 'das', 'esplendidas', 'reverberacoes', 'de', 'sua', 'beleza', ',', 'aurelia', 'bemlonge', 'de', 'inebriar-se', 'da', 'adoracao', 'produzida', 'por', 'sua', 'formosura', ',', 'e', 'do', 'culto', 'que', 'lhe', 'rendiam', 'aocontrario', 'parecia', 'unicamente', 'possuida', 'de', 'indignacao', 'por', 'essa', 'turba', 'vil', 'e', 'abjeta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não era um triunfo que ela julgasse digno de si, a torpe humilhação dessa gente ante sua riqueza'\n",
      "Tokens gerados: ['nao', 'era', 'um', 'triunfo', 'que', 'ela', 'julgasse', 'digno', 'de', 'si', ',', 'a', 'torpe', 'humilhacao', 'dessa', 'gente', 'ante', 'sua', 'riqueza']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era um desafio, que lançava ao mundo; orgulhosa de esmagá-lo sob a planta, como a um réptil\n",
      "venenoso'\n",
      "Tokens gerados: ['era', 'um', 'desafio', ',', 'que', 'lancava', 'ao', 'mundo', 'orgulhosa', 'de', 'esmaga-lo', 'sob', 'a', 'planta', ',', 'como', 'a', 'um', 'reptilvenenoso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o mundo é assim feito; que foi o fulgor satânico da beleza dessa mulher, a sua maior sedução'\n",
      "Tokens gerados: ['e', 'o', 'mundo', 'e', 'assim', 'feito', 'que', 'foi', 'o', 'fulgor', 'satanico', 'da', 'beleza', 'dessa', 'mulher', ',', 'a', 'sua', 'maior', 'seducao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na\n",
      "acerba veemência da alma revolta, pressentiam-se abismos de paixão; e entrevia-se que procelas de\n",
      "volúpia havia de ter o amor da virgem bacante'\n",
      "Tokens gerados: ['naacerba', 'veemencia', 'da', 'alma', 'revolta', ',', 'pressentiam-se', 'abismos', 'de', 'paixao', 'e', 'entrevia-se', 'que', 'procelas', 'devolupia', 'havia', 'de', 'ter', 'o', 'amor', 'da', 'virgem', 'bacante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se o sinistro vislumbre se apagasse de súbito, deixando a formosa estátua na penumbra suave da\n",
      "candura e inocência, o anjo casto e puro que havia naquela, como há em todas as moças, talvez\n",
      "passasse desapercebido pelo turbilhão'\n",
      "Tokens gerados: ['se', 'o', 'sinistro', 'vislumbre', 'se', 'apagasse', 'de', 'subito', ',', 'deixando', 'a', 'formosa', 'estatua', 'na', 'penumbra', 'suave', 'dacandura', 'e', 'inocencia', ',', 'o', 'anjo', 'casto', 'e', 'puro', 'que', 'havia', 'naquela', ',', 'como', 'ha', 'em', 'todas', 'as', 'mocas', ',', 'talvezpassasse', 'desapercebido', 'pelo', 'turbilhao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As revoltas mais impetuosas de Aurélia eram justamente contra a riqueza que lhe servia de trono, e\n",
      "sem a qual nunca por certo, apesar de suas prendas, receberia como rainha desdenhosa, a\n",
      "vassalagem que lhe rendiam'\n",
      "Tokens gerados: ['as', 'revoltas', 'mais', 'impetuosas', 'de', 'aurelia', 'eram', 'justamente', 'contra', 'a', 'riqueza', 'que', 'lhe', 'servia', 'de', 'trono', ',', 'esem', 'a', 'qual', 'nunca', 'por', 'certo', ',', 'apesar', 'de', 'suas', 'prendas', ',', 'receberia', 'como', 'rainha', 'desdenhosa', ',', 'avassalagem', 'que', 'lhe', 'rendiam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por isso mesmo considerava ela o ouro, um vil metal que rebaixava os homens; e no íntimo sentiase profundamente humilhada pensando que para toda essa gente que a cercava, ela, a sua pessoa,\n",
      "não merecia uma só das bajulações que tributavam a cada um de seus mil contos de réis'\n",
      "Tokens gerados: ['por', 'isso', 'mesmo', 'considerava', 'ela', 'o', 'ouro', ',', 'um', 'vil', 'metal', 'que', 'rebaixava', 'os', 'homens', 'e', 'no', 'intimo', 'sentiase', 'profundamente', 'humilhada', 'pensando', 'que', 'para', 'toda', 'essa', 'gente', 'que', 'a', 'cercava', ',', 'ela', ',', 'a', 'sua', 'pessoa', ',', 'nao', 'merecia', 'uma', 'so', 'das', 'bajulacoes', 'que', 'tributavam', 'a', 'cada', 'um', 'de', 'seus', 'mil', 'contos', 'de', 'reis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nunca da pena de algum Chatterton desconhecido saíram mais cruciantes apóstrofes contra o\n",
      "dinheiro, do que vibrava muitas vezes o lábio perfumado dessa feiticeira menina, no seio de sua\n",
      "opulência'\n",
      "Tokens gerados: ['nunca', 'da', 'pena', 'de', 'algum', 'chatterton', 'desconhecido', 'sairam', 'mais', 'cruciantes', 'apostrofes', 'contra', 'odinheiro', ',', 'do', 'que', 'vibrava', 'muitas', 'vezes', 'o', 'labio', 'perfumado', 'dessa', 'feiticeira', 'menina', ',', 'no', 'seio', 'de', 'suaopulencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um traço basta para desenhá-la sob esta face'\n",
      "Tokens gerados: ['um', 'traco', 'basta', 'para', 'desenha-la', 'sob', 'esta', 'face']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Convencida de que todos os seus inúmeros apaixonados, sem exceção de um, a pretendiam\n",
      "unicamente pela riqueza, Aurélia reagia contra essa afronta, aplicando a esses indivíduos o mesmo\n",
      "estalão'\n",
      "Tokens gerados: ['convencida', 'de', 'que', 'todos', 'os', 'seus', 'inumeros', 'apaixonados', ',', 'sem', 'excecao', 'de', 'um', ',', 'a', 'pretendiamunicamente', 'pela', 'riqueza', ',', 'aurelia', 'reagia', 'contra', 'essa', 'afronta', ',', 'aplicando', 'a', 'esses', 'individuos', 'o', 'mesmoestalao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Assim costumava ela indicar o merecimento de cada um dos pretendentes, dando-lhes certo valor\n",
      "monetário'\n",
      "Tokens gerados: ['assim', 'costumava', 'ela', 'indicar', 'o', 'merecimento', 'de', 'cada', 'um', 'dos', 'pretendentes', ',', 'dando-lhes', 'certo', 'valormonetario']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em linguagem financeira, Aurélia cotava os seus adoradores pelo preço que\n",
      "razoavelmente poderiam obter no mercado matrimonial'\n",
      "Tokens gerados: ['em', 'linguagem', 'financeira', ',', 'aurelia', 'cotava', 'os', 'seus', 'adoradores', 'pelo', 'preco', 'querazoavelmente', 'poderiam', 'obter', 'no', 'mercado', 'matrimonial']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma noite, no Cassino, a Lísia Soares, que fazia-se íntima com ela, e desejava ardentemente vê-la\n",
      "casada, dirigiu-lhe um gracejo acerca do Alfredo Moreira, rapaz elegante que chegara recentemente\n",
      "da Europa:\n",
      "- É um moço muito distinto, respondeu Aurélia sorrindo; vale bem como noivo cem contos de réis;\n",
      "mas eu tenho dinheiro para pagar um marido de maior preço, Lísia; não me contento com esse'\n",
      "Tokens gerados: ['uma', 'noite', ',', 'no', 'cassino', ',', 'a', 'lisia', 'soares', ',', 'que', 'fazia-se', 'intima', 'com', 'ela', ',', 'e', 'desejava', 'ardentemente', 've-lacasada', ',', 'dirigiu-lhe', 'um', 'gracejo', 'acerca', 'do', 'alfredo', 'moreira', ',', 'rapaz', 'elegante', 'que', 'chegara', 'recentementeda', 'europa-', 'e', 'um', 'moco', 'muito', 'distinto', ',', 'respondeu', 'aurelia', 'sorrindo', 'vale', 'bem', 'como', 'noivo', 'cem', 'contos', 'de', 'reismas', 'eu', 'tenho', 'dinheiro', 'para', 'pagar', 'um', 'marido', 'de', 'maior', 'preco', ',', 'lisia', 'nao', 'me', 'contento', 'com', 'esse']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Riam-se todos destes ditos de Aurélia, e os lançavam à conta de gracinhas de moça espirituosa;\n",
      "porém a maior parte das senhoras, sobretudo aquelas que tinham filhas moças, não cansavam de\n",
      "criticar desses modos desenvoltos, impróprios de meninas bem-educadas'\n",
      "Tokens gerados: ['riam-se', 'todos', 'destes', 'ditos', 'de', 'aurelia', ',', 'e', 'os', 'lancavam', 'a', 'conta', 'de', 'gracinhas', 'de', 'moca', 'espirituosaporem', 'a', 'maior', 'parte', 'das', 'senhoras', ',', 'sobretudo', 'aquelas', 'que', 'tinham', 'filhas', 'mocas', ',', 'nao', 'cansavam', 'decriticar', 'desses', 'modos', 'desenvoltos', ',', 'improprios', 'de', 'meninas', 'bem-educadas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os adoradores de Aurélia sabiam, pois ela não fazia mistério, do preço de sua cotação no rol da\n",
      "moça; e longe de se agastarem com a franqueza, divertiam-se com o jogo que muitas vezes\n",
      "resultava do ágio de suas ações naquela empresa nupcial'\n",
      "Tokens gerados: ['os', 'adoradores', 'de', 'aurelia', 'sabiam', ',', 'pois', 'ela', 'nao', 'fazia', 'misterio', ',', 'do', 'preco', 'de', 'sua', 'cotacao', 'no', 'rol', 'damoca', 'e', 'longe', 'de', 'se', 'agastarem', 'com', 'a', 'franqueza', ',', 'divertiam-se', 'com', 'o', 'jogo', 'que', 'muitas', 'vezesresultava', 'do', 'agio', 'de', 'suas', 'acoes', 'naquela', 'empresa', 'nupcial']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dava-se isto quando qualquer dos apaixonados tinha a felicidade de fazer alguma cousa a contento\n",
      "da moça e satisfazer-lhe as fantasias; porque nesse caso ela elevava-lhe a cotação, assim como\n",
      "abaixava a daquele que a contrariava ou incorria em seu desagrado'\n",
      "Tokens gerados: ['dava-se', 'isto', 'quando', 'qualquer', 'dos', 'apaixonados', 'tinha', 'a', 'felicidade', 'de', 'fazer', 'alguma', 'cousa', 'a', 'contentoda', 'moca', 'e', 'satisfazer-lhe', 'as', 'fantasias', 'porque', 'nesse', 'caso', 'ela', 'elevava-lhe', 'a', 'cotacao', ',', 'assim', 'comoabaixava', 'a', 'daquele', 'que', 'a', 'contrariava', 'ou', 'incorria', 'em', 'seu', 'desagrado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Muito devia a cobiça embrutecer esses homens, ou cegá-los a paixão, para não verem o frio\n",
      "escárnio com que Aurélia os ludibriava nestes brincos ridículos, que eles tomavam por garridices\n",
      "de menina, e não eram senão ímpetos de uma irritação íntima e talvez mórbida'\n",
      "Tokens gerados: ['muito', 'devia', 'a', 'cobica', 'embrutecer', 'esses', 'homens', ',', 'ou', 'cega-los', 'a', 'paixao', ',', 'para', 'nao', 'verem', 'o', 'frioescarnio', 'com', 'que', 'aurelia', 'os', 'ludibriava', 'nestes', 'brincos', 'ridiculos', ',', 'que', 'eles', 'tomavam', 'por', 'garridicesde', 'menina', ',', 'e', 'nao', 'eram', 'senao', 'impetos', 'de', 'uma', 'irritacao', 'intima', 'e', 'talvez', 'morbida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A verdade é que todos porfiavam, às vezes colhidos por desânimo passageiro, mas logo restaurados\n",
      "por uma esperança obstinada, nenhum se resolvia a abandonar o campo; e muito menos o Alfredo\n",
      "Moreira que parecia figurar a cabeça do rol'\n",
      "Tokens gerados: ['a', 'verdade', 'e', 'que', 'todos', 'porfiavam', ',', 'as', 'vezes', 'colhidos', 'por', 'desanimo', 'passageiro', ',', 'mas', 'logo', 'restauradospor', 'uma', 'esperanca', 'obstinada', ',', 'nenhum', 'se', 'resolvia', 'a', 'abandonar', 'o', 'campo', 'e', 'muito', 'menos', 'o', 'alfredomoreira', 'que', 'parecia', 'figurar', 'a', 'cabeca', 'do', 'rol']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não acompanharei Aurélia em sua efêmera passagem pelos salões da Corte, onde viu, jungido a seu\n",
      "carro de triunfo, tudo que a nossa sociedade tinha de mais elevado e brilhante'\n",
      "Tokens gerados: ['nao', 'acompanharei', 'aurelia', 'em', 'sua', 'efemera', 'passagem', 'pelos', 'saloes', 'da', 'corte', ',', 'onde', 'viu', ',', 'jungido', 'a', 'seucarro', 'de', 'triunfo', ',', 'tudo', 'que', 'a', 'nossa', 'sociedade', 'tinha', 'de', 'mais', 'elevado', 'e', 'brilhante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Proponho-me unicamente a referir o drama íntimo e estranho que decidiu do destino dessa mulher\n",
      "singular'\n",
      "Tokens gerados: ['proponho-me', 'unicamente', 'a', 'referir', 'o', 'drama', 'intimo', 'e', 'estranho', 'que', 'decidiu', 'do', 'destino', 'dessa', 'mulhersingular']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_senhora_jose_de_alencar_cap_1.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Seriam nove horas do dia'\n",
      "Tokens gerados: ['seriam', 'nove', 'horas', 'do', 'dia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um sol ardente de março esbate-se nas venezianas que vestem as sacadas de uma sala, nas\n",
      "Laranjeiras'\n",
      "Tokens gerados: ['um', 'sol', 'ardente', 'de', 'marco', 'esbate-se', 'nas', 'venezianas', 'que', 'vestem', 'as', 'sacadas', 'de', 'uma', 'sala', ',', 'naslaranjeiras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A luz coada pelas verdes empanadas debuxa com a suavidade do nimbo o gracioso busto de Aurélia\n",
      "sobre o aveludado escarlate do papel que forra o gabinete'\n",
      "Tokens gerados: ['a', 'luz', 'coada', 'pelas', 'verdes', 'empanadas', 'debuxa', 'com', 'a', 'suavidade', 'do', 'nimbo', 'o', 'gracioso', 'busto', 'de', 'aureliasobre', 'o', 'aveludado', 'escarlate', 'do', 'papel', 'que', 'forra', 'o', 'gabinete']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Reclinada na conversadeira com os olhos a vagar pelo crepúsculo do aposento, a moça parece\n",
      "imersa em intensa cogitação'\n",
      "Tokens gerados: ['reclinada', 'na', 'conversadeira', 'com', 'os', 'olhos', 'a', 'vagar', 'pelo', 'crepusculo', 'do', 'aposento', ',', 'a', 'moca', 'pareceimersa', 'em', 'intensa', 'cogitacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O recolho apaga-lhe no semblante, como no porte, a reverberação\n",
      "mordaz que de ordinário ela desfere de si, como a chama sulfúrea de um relâmpago'\n",
      "Tokens gerados: ['o', 'recolho', 'apaga-lhe', 'no', 'semblante', ',', 'como', 'no', 'porte', ',', 'a', 'reverberacaomordaz', 'que', 'de', 'ordinario', 'ela', 'desfere', 'de', 'si', ',', 'como', 'a', 'chama', 'sulfurea', 'de', 'um', 'relampago']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas a serenidade que se derrama por toda a sua pessoa, se de alguma sorte desmaia a cintilação de\n",
      "sua beleza, a embebe de um fluido inefável de meiguice e carinho, que a torna irre-sistível'\n",
      "Tokens gerados: ['mas', 'a', 'serenidade', 'que', 'se', 'derrama', 'por', 'toda', 'a', 'sua', 'pessoa', ',', 'se', 'de', 'alguma', 'sorte', 'desmaia', 'a', 'cintilacao', 'desua', 'beleza', ',', 'a', 'embebe', 'de', 'um', 'fluido', 'inefavel', 'de', 'meiguice', 'e', 'carinho', ',', 'que', 'a', 'torna', 'irre-sistivel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seus olhos já não têm aqueles fulvos lampejos, que despedem nos salões, e que, a igual do\n",
      "mormaço crestam'\n",
      "Tokens gerados: ['seus', 'olhos', 'ja', 'nao', 'tem', 'aqueles', 'fulvos', 'lampejos', ',', 'que', 'despedem', 'nos', 'saloes', ',', 'e', 'que', ',', 'a', 'igual', 'domormaco', 'crestam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nos lá-bios, em vez do cáustico sorriso, borbulha agora a flor d’alma a rever os\n",
      "íntimos enlevos'\n",
      "Tokens gerados: ['nos', 'la-bios', ',', 'em', 'vez', 'do', 'caustico', 'sorriso', ',', 'borbulha', 'agora', 'a', 'flor', 'd', '’', 'alma', 'a', 'rever', 'osintimos', 'enlevos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sombreia o formoso semblante uma tinta de melancolia que não lhe é habitual desde certo tempo, e\n",
      "que não obstante se diria o matiz mais próprio das feições delicadas'\n",
      "Tokens gerados: ['sombreia', 'o', 'formoso', 'semblante', 'uma', 'tinta', 'de', 'melancolia', 'que', 'nao', 'lhe', 'e', 'habitual', 'desde', 'certo', 'tempo', ',', 'eque', 'nao', 'obstante', 'se', 'diria', 'o', 'matiz', 'mais', 'proprio', 'das', 'feicoes', 'delicadas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Há mulheres assim, a quem\n",
      "um perfume de tristeza idealiza'\n",
      "Tokens gerados: ['ha', 'mulheres', 'assim', ',', 'a', 'quemum', 'perfume', 'de', 'tristeza', 'idealiza']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As mais violentas paixões são inspiradas por esses anjos de exílio'\n",
      "Tokens gerados: ['as', 'mais', 'violentas', 'paixoes', 'sao', 'inspiradas', 'por', 'esses', 'anjos', 'de', 'exilio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aurélia concentra-se de todo dentro de si; ninguém ao ver essa gentil menina, na aparência tão\n",
      "calma e tranqüila, acreditaria que nesse momento ela agita e resolve o problema de sua existência; e\n",
      "prepara-se para sacrificar irremediavelmente todo o seu futuro'\n",
      "Tokens gerados: ['aurelia', 'concentra-se', 'de', 'todo', 'dentro', 'de', 'si', 'ninguem', 'ao', 'ver', 'essa', 'gentil', 'menina', ',', 'na', 'aparencia', 'taocalma', 'e', 'tranquila', ',', 'acreditaria', 'que', 'nesse', 'momento', 'ela', 'agita', 'e', 'resolve', 'o', 'problema', 'de', 'sua', 'existencia', 'eprepara-se', 'para', 'sacrificar', 'irremediavelmente', 'todo', 'o', 'seu', 'futuro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Alguém que entrava no gabinete veio arrancar a formosa pensativa à sua longa meditação'\n",
      "Tokens gerados: ['alguem', 'que', 'entrava', 'no', 'gabinete', 'veio', 'arrancar', 'a', 'formosa', 'pensativa', 'a', 'sua', 'longa', 'meditacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era D'\n",
      "Tokens gerados: ['era', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina Mascarenhas, a senhora que exercia junto de Aurélia o ofício de guarda-moça'\n",
      "Tokens gerados: ['firmina', 'mascarenhas', ',', 'a', 'senhora', 'que', 'exercia', 'junto', 'de', 'aurelia', 'o', 'oficio', 'de', 'guarda-moca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A viúva aproximou-se da conversadeira para estalar um beijo na face da menina, que só nessa\n",
      "ocasião acordou da profunda distração em que estava absorta'\n",
      "Tokens gerados: ['a', 'viuva', 'aproximou-se', 'da', 'conversadeira', 'para', 'estalar', 'um', 'beijo', 'na', 'face', 'da', 'menina', ',', 'que', 'so', 'nessaocasiao', 'acordou', 'da', 'profunda', 'distracao', 'em', 'que', 'estava', 'absorta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aurélia correu a vista surpresa pelo aposento; e interrogou uma miniatura de relógio presa à cintura\n",
      "por uma cadeia de ouro fosco'\n",
      "Tokens gerados: ['aurelia', 'correu', 'a', 'vista', 'surpresa', 'pelo', 'aposento', 'e', 'interrogou', 'uma', 'miniatura', 'de', 'relogio', 'presa', 'a', 'cinturapor', 'uma', 'cadeia', 'de', 'ouro', 'fosco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto D'\n",
      "Tokens gerados: ['entretanto', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina, acomodando a sua gordura semi-secular em uma das vastas cadeiras de\n",
      "braços que ficavam ao lado da conversadeira, dispunha-se esperar pelo almoço'\n",
      "Tokens gerados: ['firmina', ',', 'acomodando', 'a', 'sua', 'gordura', 'semi-secular', 'em', 'uma', 'das', 'vastas', 'cadeiras', 'debracos', 'que', 'ficavam', 'ao', 'lado', 'da', 'conversadeira', ',', 'dispunha-se', 'esperar', 'pelo', 'almoco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Está fatigada de ontem? perguntou a viúva com a expressão de afetada ternura que exigia o seu\n",
      "cargo'\n",
      "Tokens gerados: ['-', 'esta', 'fatigada', 'de', 'ontem', '?', 'perguntou', 'a', 'viuva', 'com', 'a', 'expressao', 'de', 'afetada', 'ternura', 'que', 'exigia', 'o', 'seucargo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Nem por isso; mas sinto-me lânguida; há de ser o calor - respondeu a moça para dar uma razão\n",
      "qualquer de sua atitude pensativa'\n",
      "Tokens gerados: ['-', 'nem', 'por', 'isso', 'mas', 'sinto-me', 'languida', 'ha', 'de', 'ser', 'o', 'calor', '-', 'respondeu', 'a', 'moca', 'para', 'dar', 'uma', 'razaoqualquer', 'de', 'sua', 'atitude', 'pensativa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Estes bailes que acabam tão tarde não podem ser bons para a saúde; por isso é que no Rio de\n",
      "Janeiro há tanta moça magra e amarela'\n",
      "Tokens gerados: ['-', 'estes', 'bailes', 'que', 'acabam', 'tao', 'tarde', 'nao', 'podem', 'ser', 'bons', 'para', 'a', 'saude', 'por', 'isso', 'e', 'que', 'no', 'rio', 'dejaneiro', 'ha', 'tanta', 'moca', 'magra', 'e', 'amarela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ora, ontem, quando serviram a ceia pouco faltava para\n",
      "tocar matinas em Santa Teresa'\n",
      "Tokens gerados: ['ora', ',', 'ontem', ',', 'quando', 'serviram', 'a', 'ceia', 'pouco', 'faltava', 'paratocar', 'matinas', 'em', 'santa', 'teresa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se a primeira quadrilha começou com o toque do Aragão!'\n",
      "Tokens gerados: ['se', 'a', 'primeira', 'quadrilha', 'comecou', 'com', 'o', 'toque', 'do', 'aragao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia\n",
      "muita confusão; o serviço não esteve mau, mas andou tão atrapalhado!'\n",
      "Tokens gerados: ['haviamuita', 'confusao', 'o', 'servico', 'nao', 'esteve', 'mau', ',', 'mas', 'andou', 'tao', 'atrapalhado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina continuou por aí além a descrever suas impressões do baile da véspera, sem tirar os olhos\n",
      "do semblante de Aurélia, onde espiava o efeito de suas palavras, pronta a desdizer-se de qualquer\n",
      "observação, ao menor indício de contrariedade'\n",
      "Tokens gerados: ['firmina', 'continuou', 'por', 'ai', 'alem', 'a', 'descrever', 'suas', 'impressoes', 'do', 'baile', 'da', 'vespera', ',', 'sem', 'tirar', 'os', 'olhosdo', 'semblante', 'de', 'aurelia', ',', 'onde', 'espiava', 'o', 'efeito', 'de', 'suas', 'palavras', ',', 'pronta', 'a', 'desdizer-se', 'de', 'qualquerobservacao', ',', 'ao', 'menor', 'indicio', 'de', 'contrariedade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deixou-a a moça falar, desejosa de desprender-se de suas preo-cupações e embalar-se ao rumor\n",
      "dessa voz que ouvia, sem compreender'\n",
      "Tokens gerados: ['deixou-a', 'a', 'moca', 'falar', ',', 'desejosa', 'de', 'desprender-se', 'de', 'suas', 'preo-cupacoes', 'e', 'embalar-se', 'ao', 'rumordessa', 'voz', 'que', 'ouvia', ',', 'sem', 'compreender']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sabia que a viúva conversava acerca do baile; mas não\n",
      "acompanhava o que ela dizia'\n",
      "Tokens gerados: ['sabia', 'que', 'a', 'viuva', 'conversava', 'acerca', 'do', 'baile', 'mas', 'naoacompanhava', 'o', 'que', 'ela', 'dizia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De repente, porém, interrompeu-a:\n",
      "- Que tal achou a Amaralzinha, D'\n",
      "Tokens gerados: ['de', 'repente', ',', 'porem', ',', 'interrompeu-a-', 'que', 'tal', 'achou', 'a', 'amaralzinha', ',', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina?\n",
      "A velha fez semblante de recordar-se'\n",
      "Tokens gerados: ['firmina', '?', 'a', 'velha', 'fez', 'semblante', 'de', 'recordar-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- A Amaralzinha?'\n",
      "Tokens gerados: ['-', 'a', 'amaralzinha', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É aquela moça toda de azul?\n",
      "- Com espigas de prata nos cabelos e nos apanhados da saia; simples e de muito bom gosto'\n",
      "Tokens gerados: ['e', 'aquela', 'moca', 'toda', 'de', 'azul', '?', '-', 'com', 'espigas', 'de', 'prata', 'nos', 'cabelos', 'e', 'nos', 'apanhados', 'da', 'saia', 'simples', 'e', 'de', 'muito', 'bom', 'gosto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Lembra-me'\n",
      "Tokens gerados: ['-', 'lembra-me']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É uma menina bem galante! afirmou a -viúva'\n",
      "Tokens gerados: ['e', 'uma', 'menina', 'bem', 'galante', 'afirmou', 'a', '-viuva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- E bem-educada'\n",
      "Tokens gerados: ['-', 'e', 'bem-educada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dizem que toca piano perfeitamente, e que tem uma voz muito agradável'\n",
      "Tokens gerados: ['dizem', 'que', 'toca', 'piano', 'perfeitamente', ',', 'e', 'que', 'tem', 'uma', 'voz', 'muito', 'agradavel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Mas não costuma aparecer na sociedade'\n",
      "Tokens gerados: ['-', 'mas', 'nao', 'costuma', 'aparecer', 'na', 'sociedade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É a primeira vez que a encontramos; não me lembro de a\n",
      "ter visto antes'\n",
      "Tokens gerados: ['e', 'a', 'primeira', 'vez', 'que', 'a', 'encontramos', 'nao', 'me', 'lembro', 'de', 'ater', 'visto', 'antes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Foi a primeira vez!\n",
      "Pronunciando estas palavras, a moça parecia de novo sentir sua alma refranger-se atraída\n",
      "imperiosamente por esse pensamento recôndito que a absorvia'\n",
      "Tokens gerados: ['-', 'foi', 'a', 'primeira', 'vezpronunciando', 'estas', 'palavras', ',', 'a', 'moca', 'parecia', 'de', 'novo', 'sentir', 'sua', 'alma', 'refranger-se', 'atraidaimperiosamente', 'por', 'esse', 'pensamento', 'recondito', 'que', 'a', 'absorvia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas reagiu contra essa preocupação; e dirigiu-se à viúva em tom vivo e instante:\n",
      "- Diga-me uma cousa, D'\n",
      "Tokens gerados: ['mas', 'reagiu', 'contra', 'essa', 'preocupacao', 'e', 'dirigiu-se', 'a', 'viuva', 'em', 'tom', 'vivo', 'e', 'instante-', 'diga-me', 'uma', 'cousa', ',', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina!\n",
      "- O que é, Aurélia?\n",
      "- Mas há de ser franca'\n",
      "Tokens gerados: ['firmina-', 'o', 'que', 'e', ',', 'aurelia', '?', '-', 'mas', 'ha', 'de', 'ser', 'franca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Promete-me?\n",
      "- Franca? Mais do que eu sou, menina? Se é este o meu defeito!'\n",
      "Tokens gerados: ['promete-me', '?', '-', 'franca', '?', 'mais', 'do', 'que', 'eu', 'sou', ',', 'menina', '?', 'se', 'e', 'este', 'o', 'meu', 'defeito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A moça hesitava'\n",
      "Tokens gerados: ['a', 'moca', 'hesitava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Experimente, Senhora!\n",
      "- Quem acha a senhora mais bonita, a Amaralzinha ou eu? disse afinal Aurélia, empalidecendo de\n",
      "leve'\n",
      "Tokens gerados: ['-', 'experimente', ',', 'senhora-', 'quem', 'acha', 'a', 'senhora', 'mais', 'bonita', ',', 'a', 'amaralzinha', 'ou', 'eu', '?', 'disse', 'afinal', 'aurelia', ',', 'empalidecendo', 'deleve']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Ora, ora! acudiu a viúva a rir'\n",
      "Tokens gerados: ['-', 'ora', ',', 'ora', 'acudiu', 'a', 'viuva', 'a', 'rir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Está zombando, Aurélia'\n",
      "Tokens gerados: ['esta', 'zombando', ',', 'aurelia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois, a Amaralzinha é para se comparar\n",
      "com você?\n",
      "- Seja sincera!\n",
      "- Outras muito mais bonitas que ela não chegam a seus pés'\n",
      "Tokens gerados: ['pois', ',', 'a', 'amaralzinha', 'e', 'para', 'se', 'compararcom', 'voce', '?', '-', 'seja', 'sincera-', 'outras', 'muito', 'mais', 'bonitas', 'que', 'ela', 'nao', 'chegam', 'a', 'seus', 'pes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A viúva citou quatro ou cinco nomes de moças que então andavam no galarim e dos quais não me\n",
      "recordo agora'\n",
      "Tokens gerados: ['a', 'viuva', 'citou', 'quatro', 'ou', 'cinco', 'nomes', 'de', 'mocas', 'que', 'entao', 'andavam', 'no', 'galarim', 'e', 'dos', 'quais', 'nao', 'merecordo', 'agora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- É tão elegante! disse Aurélia como se completasse uma reflexão íntima'\n",
      "Tokens gerados: ['-', 'e', 'tao', 'elegante', 'disse', 'aurelia', 'como', 'se', 'completasse', 'uma', 'reflexao', 'intima']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- São gostos!\n",
      "- Em todo o caso é mais bem-educada do que eu?\n",
      "- Do que você, Aurélia? Há de ser difícil que se encontre em todo o Rio de Janeiro outra moça que\n",
      "tenha sua educação'\n",
      "Tokens gerados: ['-', 'sao', 'gostos-', 'em', 'todo', 'o', 'caso', 'e', 'mais', 'bem-educada', 'do', 'que', 'eu', '?', '-', 'do', 'que', 'voce', ',', 'aurelia', '?', 'ha', 'de', 'ser', 'dificil', 'que', 'se', 'encontre', 'em', 'todo', 'o', 'rio', 'de', 'janeiro', 'outra', 'moca', 'quetenha', 'sua', 'educacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lá mesmo, por Paris, de que tanto se fala, duvido que haja'\n",
      "Tokens gerados: ['la', 'mesmo', ',', 'por', 'paris', ',', 'de', 'que', 'tanto', 'se', 'fala', ',', 'duvido', 'que', 'haja']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Obrigada! É esta a sua franqueza, D'\n",
      "Tokens gerados: ['-', 'obrigada', 'e', 'esta', 'a', 'sua', 'franqueza', ',', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina?\n",
      "- Sim, senhora; a minha franqueza está em dizer a verdade, e não em escondê-la'\n",
      "Tokens gerados: ['firmina', '?', '-', 'sim', ',', 'senhora', 'a', 'minha', 'franqueza', 'esta', 'em', 'dizer', 'a', 'verdade', ',', 'e', 'nao', 'em', 'esconde-la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Demais, isso é o\n",
      "que todos vêem e repetem'\n",
      "Tokens gerados: ['demais', ',', 'isso', 'e', 'oque', 'todos', 'veem', 'e', 'repetem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Você toca piano como o Arnaud, canta como uma prima-dona, e\n",
      "conversa na sala com os deputados e os diplomatas, que eles ficam todos enfeitiçados'\n",
      "Tokens gerados: ['voce', 'toca', 'piano', 'como', 'o', 'arnaud', ',', 'canta', 'como', 'uma', 'prima-dona', ',', 'econversa', 'na', 'sala', 'com', 'os', 'deputados', 'e', 'os', 'diplomatas', ',', 'que', 'eles', 'ficam', 'todos', 'enfeiticados']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E como não\n",
      "há de ser assim? Quando você quer, Aurélia, fala que parece uma novela'\n",
      "Tokens gerados: ['e', 'como', 'naoha', 'de', 'ser', 'assim', '?', 'quando', 'voce', 'quer', ',', 'aurelia', ',', 'fala', 'que', 'parece', 'uma', 'novela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Já vejo que a senhora não é nada lisonjeira'\n",
      "Tokens gerados: ['-', 'ja', 'vejo', 'que', 'a', 'senhora', 'nao', 'e', 'nada', 'lisonjeira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Está desmerecendo os meus dotes, acudiu a menina\n",
      "sublinhando a última palavra com um fino sorriso de ironia'\n",
      "Tokens gerados: ['esta', 'desmerecendo', 'os', 'meus', 'dotes', ',', 'acudiu', 'a', 'meninasublinhando', 'a', 'ultima', 'palavra', 'com', 'um', 'fino', 'sorriso', 'de', 'ironia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Então não sabe, D'\n",
      "Tokens gerados: ['entao', 'nao', 'sabe', ',', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina, que eu\n",
      "tenho um estilo de ouro, o mais sublime de todos os estilos, a cuja eloqüência arrebatadora não se\n",
      "resiste? As que falam como uma novela, em vil prosa, são essas moças românticas e pálidas que se\n",
      "andam evaporando em suspiros; eu falo como um poema: sou a poesia que brilha e deslumbra!\n",
      "- Entendo o que você quer dizer; o dinheiro faz do feio bonito, e dá tudo, até saúde'\n",
      "Tokens gerados: ['firmina', ',', 'que', 'eutenho', 'um', 'estilo', 'de', 'ouro', ',', 'o', 'mais', 'sublime', 'de', 'todos', 'os', 'estilos', ',', 'a', 'cuja', 'eloquencia', 'arrebatadora', 'nao', 'seresiste', '?', 'as', 'que', 'falam', 'como', 'uma', 'novela', ',', 'em', 'vil', 'prosa', ',', 'sao', 'essas', 'mocas', 'romanticas', 'e', 'palidas', 'que', 'seandam', 'evaporando', 'em', 'suspiros', 'eu', 'falo', 'como', 'um', 'poema', 'sou', 'a', 'poesia', 'que', 'brilha', 'e', 'deslumbra-', 'entendo', 'o', 'que', 'voce', 'quer', 'dizer', 'o', 'dinheiro', 'faz', 'do', 'feio', 'bonito', ',', 'e', 'da', 'tudo', ',', 'ate', 'saude']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas repare\n",
      "bem, os seus maiores admiradores são justamente aqueles que não podem pretender sua riqueza;\n",
      "uns casados, outros já velhos'\n",
      "Tokens gerados: ['mas', 'reparebem', ',', 'os', 'seus', 'maiores', 'admiradores', 'sao', 'justamente', 'aqueles', 'que', 'nao', 'podem', 'pretender', 'sua', 'riquezauns', 'casados', ',', 'outros', 'ja', 'velhos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Quando pela primeira vez fumaram perto da senhora, não sentiu alguma cousa, um\n",
      "atordoamento?'\n",
      "Tokens gerados: ['-', 'quando', 'pela', 'primeira', 'vez', 'fumaram', 'perto', 'da', 'senhora', ',', 'nao', 'sentiu', 'alguma', 'cousa', ',', 'umatordoamento', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois o ouro tem uma fumaça invisível, que embriaga ainda mais do que a do\n",
      "charuto de Havana, e até mesmo do que a desse nojento cigarro de papel, com que os rapazes de\n",
      "hoje se incensam'\n",
      "Tokens gerados: ['pois', 'o', 'ouro', 'tem', 'uma', 'fumaca', 'invisivel', ',', 'que', 'embriaga', 'ainda', 'mais', 'do', 'que', 'a', 'docharuto', 'de', 'havana', ',', 'e', 'ate', 'mesmo', 'do', 'que', 'a', 'desse', 'nojento', 'cigarro', 'de', 'papel', ',', 'com', 'que', 'os', 'rapazes', 'dehoje', 'se', 'incensam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Toda essa gente que rodeia um velho ricaço, ministros, senadores e fidalgos, de\n",
      "certo que não espera casar-se com a burra do sujeito; mas sofre a atração do dinheiro'\n",
      "Tokens gerados: ['toda', 'essa', 'gente', 'que', 'rodeia', 'um', 'velho', 'ricaco', ',', 'ministros', ',', 'senadores', 'e', 'fidalgos', ',', 'decerto', 'que', 'nao', 'espera', 'casar-se', 'com', 'a', 'burra', 'do', 'sujeito', 'mas', 'sofre', 'a', 'atracao', 'do', 'dinheiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Agora mesmo, Aurélia, está você me dando razão e mostrando sua instrução'\n",
      "Tokens gerados: ['-', 'agora', 'mesmo', ',', 'aurelia', ',', 'esta', 'voce', 'me', 'dando', 'razao', 'e', 'mostrando', 'sua', 'instrucao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quem há de dizer\n",
      "que uma menina de sua idade sabe mais de que muitos homens que aprenderam nas academias? E\n",
      "assim é bom; porque senão, com a riqueza que lhe deixou seu avô, sozinha no mundo, por força\n",
      "que havia de ser enganada'\n",
      "Tokens gerados: ['quem', 'ha', 'de', 'dizerque', 'uma', 'menina', 'de', 'sua', 'idade', 'sabe', 'mais', 'de', 'que', 'muitos', 'homens', 'que', 'aprenderam', 'nas', 'academias', '?', 'eassim', 'e', 'bom', 'porque', 'senao', ',', 'com', 'a', 'riqueza', 'que', 'lhe', 'deixou', 'seu', 'avo', ',', 'sozinha', 'no', 'mundo', ',', 'por', 'forcaque', 'havia', 'de', 'ser', 'enganada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Antes fosse! murmurou a moça recaindo em sua meditação'\n",
      "Tokens gerados: ['-', 'antes', 'fosse', 'murmurou', 'a', 'moca', 'recaindo', 'em', 'sua', 'meditacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'D'\n",
      "Tokens gerados: ['d']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina ainda proferiu algumas palavras em continuação da conversa; mas notou que a moça\n",
      "não lhe prestava a menor atenção, antes parecia esquivar-se a qualquer impressão exte-rior, para\n",
      "mais profundamente reconcentrar-se'\n",
      "Tokens gerados: ['firmina', 'ainda', 'proferiu', 'algumas', 'palavras', 'em', 'continuacao', 'da', 'conversa', 'mas', 'notou', 'que', 'a', 'mocanao', 'lhe', 'prestava', 'a', 'menor', 'atencao', ',', 'antes', 'parecia', 'esquivar-se', 'a', 'qualquer', 'impressao', 'exte-rior', ',', 'paramais', 'profundamente', 'reconcentrar-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Então com o tacto dessas almas feitas para a domesticidade moral, ergueu-se; e trocando alguns\n",
      "passos pela sala, disfarçou a reparar nas estatuetas de alabastro e vasos de porcelana colocados no\n",
      "mármore vermelho dos consolos'\n",
      "Tokens gerados: ['entao', 'com', 'o', 'tacto', 'dessas', 'almas', 'feitas', 'para', 'a', 'domesticidade', 'moral', ',', 'ergueu-se', 'e', 'trocando', 'algunspassos', 'pela', 'sala', ',', 'disfarcou', 'a', 'reparar', 'nas', 'estatuetas', 'de', 'alabastro', 'e', 'vasos', 'de', 'porcelana', 'colocados', 'nomarmore', 'vermelho', 'dos', 'consolos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Assim de costas para a conversadeira, mostrava-se desapercebida daquele enlevo de Aurélia, a\n",
      "quem de certo havia de contrariar, quando voltasse da distração à presença de uma pessoa a\n",
      "escrutar-lhe nos gestos o segredo dos pensamentos'\n",
      "Tokens gerados: ['assim', 'de', 'costas', 'para', 'a', 'conversadeira', ',', 'mostrava-se', 'desapercebida', 'daquele', 'enlevo', 'de', 'aurelia', ',', 'aquem', 'de', 'certo', 'havia', 'de', 'contrariar', ',', 'quando', 'voltasse', 'da', 'distracao', 'a', 'presenca', 'de', 'uma', 'pessoa', 'aescrutar-lhe', 'nos', 'gestos', 'o', 'segredo', 'dos', 'pensamentos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não teriam decorrido cinco minutos quando ouvia D'\n",
      "Tokens gerados: ['nao', 'teriam', 'decorrido', 'cinco', 'minutos', 'quando', 'ouvia', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina um som trépido e cristalino, que ela\n",
      "bem conhecia por tê-lo muitas vezes escutado'\n",
      "Tokens gerados: ['firmina', 'um', 'som', 'trepido', 'e', 'cristalino', ',', 'que', 'elabem', 'conhecia', 'por', 'te-lo', 'muitas', 'vezes', 'escutado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Voltou-se e viu Aurélia, cujos lábios de nácar\n",
      "vibravam ainda com o harpejo daquele ríspido sorriso'\n",
      "Tokens gerados: ['voltou-se', 'e', 'viu', 'aurelia', ',', 'cujos', 'labios', 'de', 'nacarvibravam', 'ainda', 'com', 'o', 'harpejo', 'daquele', 'rispido', 'sorriso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A gentil menina surgira de sua pensativa languidez, como uma estátua de cera que transmutando-se\n",
      "em jaspe de repente, se erigisse altiva e desdenhosa, desferindo de si os lívidos e fulvos reflexos do\n",
      "mármore polido'\n",
      "Tokens gerados: ['a', 'gentil', 'menina', 'surgira', 'de', 'sua', 'pensativa', 'languidez', ',', 'como', 'uma', 'estatua', 'de', 'cera', 'que', 'transmutando-seem', 'jaspe', 'de', 'repente', ',', 'se', 'erigisse', 'altiva', 'e', 'desdenhosa', ',', 'desferindo', 'de', 'si', 'os', 'lividos', 'e', 'fulvos', 'reflexos', 'domarmore', 'polido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ela caminhou para as janelas, e com petulância nervosa suspendeu impetuosamente as duas\n",
      "venezianas, que pareciam um peso excessivo para sua mão fina e mimosa'\n",
      "Tokens gerados: ['ela', 'caminhou', 'para', 'as', 'janelas', ',', 'e', 'com', 'petulancia', 'nervosa', 'suspendeu', 'impetuosamente', 'as', 'duasvenezianas', ',', 'que', 'pareciam', 'um', 'peso', 'excessivo', 'para', 'sua', 'mao', 'fina', 'e', 'mimosa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A torrente da luz precipitando-se pela abertura das janelas, encheu o aposento; e a moça adiantouse até a sacada, para banhar-se nessas cascatas de sol, que lhe borbotavam sobre a régia fronte\n",
      "coroada do diadema de cabelos castanhos, e desdobravam-se pelas formosas espáduas como uma\n",
      "túnica de ouro'\n",
      "Tokens gerados: ['a', 'torrente', 'da', 'luz', 'precipitando-se', 'pela', 'abertura', 'das', 'janelas', ',', 'encheu', 'o', 'aposento', 'e', 'a', 'moca', 'adiantouse', 'ate', 'a', 'sacada', ',', 'para', 'banhar-se', 'nessas', 'cascatas', 'de', 'sol', ',', 'que', 'lhe', 'borbotavam', 'sobre', 'a', 'regia', 'frontecoroada', 'do', 'diadema', 'de', 'cabelos', 'castanhos', ',', 'e', 'desdobravam-se', 'pelas', 'formosas', 'espaduas', 'como', 'umatunica', 'de', 'ouro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Embebia-se de luz'\n",
      "Tokens gerados: ['embebia-se', 'de', 'luz']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quem a visse nesse momento assim resplandecente, poderia acreditar que sob\n",
      "as pregas do roupão de cambraia estava a ondular voluptuosamente a ninfa das chamas, a lasciva\n",
      "salamandra, em que se transformara de chofre a fada encantada'\n",
      "Tokens gerados: ['quem', 'a', 'visse', 'nesse', 'momento', 'assim', 'resplandecente', ',', 'poderia', 'acreditar', 'que', 'sobas', 'pregas', 'do', 'roupao', 'de', 'cambraia', 'estava', 'a', 'ondular', 'voluptuosamente', 'a', 'ninfa', 'das', 'chamas', ',', 'a', 'lascivasalamandra', ',', 'em', 'que', 'se', 'transformara', 'de', 'chofre', 'a', 'fada', 'encantada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois de saturar-se de sol como a alva papoula, que se cora aos beijos de seu real amante, a moça\n",
      "dirigiu-se ao piano e estouvadamente o abriu'\n",
      "Tokens gerados: ['depois', 'de', 'saturar-se', 'de', 'sol', 'como', 'a', 'alva', 'papoula', ',', 'que', 'se', 'cora', 'aos', 'beijos', 'de', 'seu', 'real', 'amante', ',', 'a', 'mocadirigiu-se', 'ao', 'piano', 'e', 'estouvadamente', 'o', 'abriu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dos turbilhões da estrepitosa tempestade cromática,\n",
      "que revolvia o teclado, desprendeu-se afinal a sublime imprecação da Norma, quando rugindo\n",
      "ciúme, fulmina a perfídia de Polião'\n",
      "Tokens gerados: ['dos', 'turbilhoes', 'da', 'estrepitosa', 'tempestade', 'cromatica', ',', 'que', 'revolvia', 'o', 'teclado', ',', 'desprendeu-se', 'afinal', 'a', 'sublime', 'imprecacao', 'da', 'norma', ',', 'quando', 'rugindociume', ',', 'fulmina', 'a', 'perfidia', 'de', 'poliao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Moderando os arrojos dessa instrumentação vertiginosa, para fazer o acompanhamento, a moça\n",
      "começou a cantar; mas às primeiras notas, sentindo-se tolhida pela posição, abandonou o pia-no, e\n",
      "em pé, no meio da sala, roçagando a saia do roupão como se fosse a cauda do pálio gaulês, ela\n",
      "reproduziu com a voz e o gesto, aquela epopéia do coração traído, que tantas vezes tinha visto\n",
      "representada por Lagrange'\n",
      "Tokens gerados: ['moderando', 'os', 'arrojos', 'dessa', 'instrumentacao', 'vertiginosa', ',', 'para', 'fazer', 'o', 'acompanhamento', ',', 'a', 'mocacomecou', 'a', 'cantar', 'mas', 'as', 'primeiras', 'notas', ',', 'sentindo-se', 'tolhida', 'pela', 'posicao', ',', 'abandonou', 'o', 'pia-no', ',', 'eem', 'pe', ',', 'no', 'meio', 'da', 'sala', ',', 'rocagando', 'a', 'saia', 'do', 'roupao', 'como', 'se', 'fosse', 'a', 'cauda', 'do', 'palio', 'gaules', ',', 'elareproduziu', 'com', 'a', 'voz', 'e', 'o', 'gesto', ',', 'aquela', 'epopeia', 'do', 'coracao', 'traido', ',', 'que', 'tantas', 'vezes', 'tinha', 'vistorepresentada', 'por', 'lagrange']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A ferocidade da mulher enganada, sanha da leoa ferida, nunca teve para exprimi-la, nem mesmo na\n",
      "exímia cantora, uma voz mais bramida, um gesto mais sublime'\n",
      "Tokens gerados: ['a', 'ferocidade', 'da', 'mulher', 'enganada', ',', 'sanha', 'da', 'leoa', 'ferida', ',', 'nunca', 'teve', 'para', 'exprimi-la', ',', 'nem', 'mesmo', 'naeximia', 'cantora', ',', 'uma', 'voz', 'mais', 'bramida', ',', 'um', 'gesto', 'mais', 'sublime']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As notas que desatavam-se dos\n",
      "lábios de Aurélia, possantes de vigor e harmonia, deixavam após si um frêmito, que lembrava o\n",
      "silvo da serpente, sobretudo quando este braço mimoso e torneado distendia-se de repente com um\n",
      "movimento hirto para vibrar o supremo desprezo'\n",
      "Tokens gerados: ['as', 'notas', 'que', 'desatavam-se', 'doslabios', 'de', 'aurelia', ',', 'possantes', 'de', 'vigor', 'e', 'harmonia', ',', 'deixavam', 'apos', 'si', 'um', 'fremito', ',', 'que', 'lembrava', 'osilvo', 'da', 'serpente', ',', 'sobretudo', 'quando', 'este', 'braco', 'mimoso', 'e', 'torneado', 'distendia-se', 'de', 'repente', 'com', 'ummovimento', 'hirto', 'para', 'vibrar', 'o', 'supremo', 'desprezo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'D'\n",
      "Tokens gerados: ['d']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina, apesar de habituada desde muito ao caráter excêntrico de Aurélia, contemplava-a com\n",
      "surpresa nesse momento; e desconfiava que alguma cousa de extraordinário ocorrera na vida da\n",
      "moça, que a tornara a princípio tão pensativa, e produzia agora esse acesso sentimental'\n",
      "Tokens gerados: ['firmina', ',', 'apesar', 'de', 'habituada', 'desde', 'muito', 'ao', 'carater', 'excentrico', 'de', 'aurelia', ',', 'contemplava-a', 'comsurpresa', 'nesse', 'momento', 'e', 'desconfiava', 'que', 'alguma', 'cousa', 'de', 'extraordinario', 'ocorrera', 'na', 'vida', 'damoca', ',', 'que', 'a', 'tornara', 'a', 'principio', 'tao', 'pensativa', ',', 'e', 'produzia', 'agora', 'esse', 'acesso', 'sentimental']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto ela com a mesma volubilidade que a tomara ao erguer-se da conversadeira, correu para\n",
      "D'\n",
      "Tokens gerados: ['entretanto', 'ela', 'com', 'a', 'mesma', 'volubilidade', 'que', 'a', 'tomara', 'ao', 'erguer-se', 'da', 'conversadeira', ',', 'correu', 'parad']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina, travou-lhe do pulso fazendo-a de Polião, e deu imediatamente um jeito cômico à cena\n",
      "que terminou em risadas'\n",
      "Tokens gerados: ['firmina', ',', 'travou-lhe', 'do', 'pulso', 'fazendo-a', 'de', 'poliao', ',', 'e', 'deu', 'imediatamente', 'um', 'jeito', 'comico', 'a', 'cenaque', 'terminou', 'em', 'risadas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_senhora_jose_de_alencar_cap_2.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Era a hora do almoço'\n",
      "Tokens gerados: ['era', 'a', 'hora', 'do', 'almoco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As duas senhoras puseram-se à mesa'\n",
      "Tokens gerados: ['as', 'duas', 'senhoras', 'puseram-se', 'a', 'mesa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aurélia distinguia-se pela sobriedade,\n",
      "que era nela a conseqüên-cia de temperamento e educação'\n",
      "Tokens gerados: ['aurelia', 'distinguia-se', 'pela', 'sobriedade', ',', 'que', 'era', 'nela', 'a', 'consequen-cia', 'de', 'temperamento', 'e', 'educacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não quer isto dizer que fosse dessa\n",
      "espécie de moças papilionáceas que se alimentam do pólen das flores, e para quem o comer é um\n",
      "ato desgracioso e prosaico'\n",
      "Tokens gerados: ['nao', 'quer', 'isto', 'dizer', 'que', 'fosse', 'dessaespecie', 'de', 'mocas', 'papilionaceas', 'que', 'se', 'alimentam', 'do', 'polen', 'das', 'flores', ',', 'e', 'para', 'quem', 'o', 'comer', 'e', 'umato', 'desgracioso', 'e', 'prosaico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bem ao contrário, ela sabia que a nutrição dá a seiva de beleza, sem a qual as cores desmaiam nas\n",
      "faces e os sorrisos nos lábios, como as efêmeras e pálidas florações de uma roseira -ética'\n",
      "Tokens gerados: ['bem', 'ao', 'contrario', ',', 'ela', 'sabia', 'que', 'a', 'nutricao', 'da', 'a', 'seiva', 'de', 'beleza', ',', 'sem', 'a', 'qual', 'as', 'cores', 'desmaiam', 'nasfaces', 'e', 'os', 'sorrisos', 'nos', 'labios', ',', 'como', 'as', 'efemeras', 'e', 'palidas', 'floracoes', 'de', 'uma', 'roseira', '-etica']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Assim não tinha vergonha de comer; e sem vaidade acreditava que o esmalte de seus dentes não era\n",
      "menos gracioso quando eles se triscavam como a crepitação de um colar de pérolas; nem o matiz de\n",
      "seus lábios menos saboroso quando chupavam uma fruta, ou se entreabriam para receber o\n",
      "alimento'\n",
      "Tokens gerados: ['assim', 'nao', 'tinha', 'vergonha', 'de', 'comer', 'e', 'sem', 'vaidade', 'acreditava', 'que', 'o', 'esmalte', 'de', 'seus', 'dentes', 'nao', 'eramenos', 'gracioso', 'quando', 'eles', 'se', 'triscavam', 'como', 'a', 'crepitacao', 'de', 'um', 'colar', 'de', 'perolas', 'nem', 'o', 'matiz', 'deseus', 'labios', 'menos', 'saboroso', 'quando', 'chupavam', 'uma', 'fruta', ',', 'ou', 'se', 'entreabriam', 'para', 'receber', 'oalimento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nessa ocasião, a moça fez exceção a seus hábitos de sobrie-dade; ela que não gostava de\n",
      "especiarias, e só de longe em longe bebia algumas gotas de licor, quis experimentar quanto molho e\n",
      "condimento picante havia em casa; e para remate bebeu um cálice de Xerez'\n",
      "Tokens gerados: ['nessa', 'ocasiao', ',', 'a', 'moca', 'fez', 'excecao', 'a', 'seus', 'habitos', 'de', 'sobrie-dade', 'ela', 'que', 'nao', 'gostava', 'deespeciarias', ',', 'e', 'so', 'de', 'longe', 'em', 'longe', 'bebia', 'algumas', 'gotas', 'de', 'licor', ',', 'quis', 'experimentar', 'quanto', 'molho', 'econdimento', 'picante', 'havia', 'em', 'casa', 'e', 'para', 'remate', 'bebeu', 'um', 'calice', 'de', 'xerez']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'D'\n",
      "Tokens gerados: ['d']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina sem esquecer o almoço, continuava a observar de parte a menina, cada vez mais\n",
      "convencida da existência de um acontecimento importante que havia alterado a calma habi-tual da\n",
      "moça'\n",
      "Tokens gerados: ['firmina', 'sem', 'esquecer', 'o', 'almoco', ',', 'continuava', 'a', 'observar', 'de', 'parte', 'a', 'menina', ',', 'cada', 'vez', 'maisconvencida', 'da', 'existencia', 'de', 'um', 'acontecimento', 'importante', 'que', 'havia', 'alterado', 'a', 'calma', 'habi-tual', 'damoca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Esse acontecimento, na opinião da viúva, não podia ser outro senão aquele que tamanha influência\n",
      "exerce nas meninas de dezoito anos, sobretudo se não dependem de ninguém para dispor de si'\n",
      "Tokens gerados: ['esse', 'acontecimento', ',', 'na', 'opiniao', 'da', 'viuva', ',', 'nao', 'podia', 'ser', 'outro', 'senao', 'aquele', 'que', 'tamanha', 'influenciaexerce', 'nas', 'meninas', 'de', 'dezoito', 'anos', ',', 'sobretudo', 'se', 'nao', 'dependem', 'de', 'ninguem', 'para', 'dispor', 'de', 'si']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'D'\n",
      "Tokens gerados: ['d']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina tinha pois como certo que Aurélia, a desdenhosa, sentira afinal uma inclinação; e estava\n",
      "ansiosa a viúva, para conhecer o feliz que tivera o poder de cativar a altiva rainha dos salões, tão\n",
      "adorada, quanto fria e indiferente'\n",
      "Tokens gerados: ['firmina', 'tinha', 'pois', 'como', 'certo', 'que', 'aurelia', ',', 'a', 'desdenhosa', ',', 'sentira', 'afinal', 'uma', 'inclinacao', 'e', 'estavaansiosa', 'a', 'viuva', ',', 'para', 'conhecer', 'o', 'feliz', 'que', 'tivera', 'o', 'poder', 'de', 'cativar', 'a', 'altiva', 'rainha', 'dos', 'saloes', ',', 'taoadorada', ',', 'quanto', 'fria', 'e', 'indiferente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Revolvia na mente as recordações da noite anterior para certificar-se que não aparecera no baile\n",
      "nenhum moço desconhecido de quem Aurélia se pudesse apaixonar de súbito'\n",
      "Tokens gerados: ['revolvia', 'na', 'mente', 'as', 'recordacoes', 'da', 'noite', 'anterior', 'para', 'certificar-se', 'que', 'nao', 'aparecera', 'no', 'bailenenhum', 'moco', 'desconhecido', 'de', 'quem', 'aurelia', 'se', 'pudesse', 'apaixonar', 'de', 'subito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Devia ser pois\n",
      "qualquer dos antigos adoradores, dos que ela escarnecia, que por alguma circunstância inexplicável\n",
      "alcançara render-lhe enfim o coração'\n",
      "Tokens gerados: ['devia', 'ser', 'poisqualquer', 'dos', 'antigos', 'adoradores', ',', 'dos', 'que', 'ela', 'escarnecia', ',', 'que', 'por', 'alguma', 'circunstancia', 'inexplicavelalcancara', 'render-lhe', 'enfim', 'o', 'coracao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não se pôde conter a viúva; em risco de desagradar a menina, dirigiu-lhe uma indireta com que se\n",
      "propunha a entabular a conversa, e conforme a resposta dirigi-la para o ponto'\n",
      "Tokens gerados: ['nao', 'se', 'pode', 'conter', 'a', 'viuva', 'em', 'risco', 'de', 'desagradar', 'a', 'menina', ',', 'dirigiu-lhe', 'uma', 'indireta', 'com', 'que', 'sepropunha', 'a', 'entabular', 'a', 'conversa', ',', 'e', 'conforme', 'a', 'resposta', 'dirigi-la', 'para', 'o', 'ponto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Não sei que lhe acho hoje, Aurélia! Parece-me tão contente, e até mais bonita, se é possível, do\n",
      "que de costume!\n",
      "- Deveras!\n",
      "- Não é exageração, não'\n",
      "Tokens gerados: ['-', 'nao', 'sei', 'que', 'lhe', 'acho', 'hoje', ',', 'aurelia', 'parece-me', 'tao', 'contente', ',', 'e', 'ate', 'mais', 'bonita', ',', 'se', 'e', 'possivel', ',', 'doque', 'de', 'costume-', 'deveras-', 'nao', 'e', 'exageracao', ',', 'nao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Olhe? As moças quando se vestem para um baile onde esperam encontrar\n",
      "alguém, ficam mais bonitas do que são'\n",
      "Tokens gerados: ['olhe', '?', 'as', 'mocas', 'quando', 'se', 'vestem', 'para', 'um', 'baile', 'onde', 'esperam', 'encontraralguem', ',', 'ficam', 'mais', 'bonitas', 'do', 'que', 'sao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas você está hoje ainda mais bonita do que nos bailes'\n",
      "Tokens gerados: ['mas', 'voce', 'esta', 'hoje', 'ainda', 'mais', 'bonita', 'do', 'que', 'nos', 'bailes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nunca lhe vi assim'\n",
      "Tokens gerados: ['nunca', 'lhe', 'vi', 'assim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aqui anda volta de algum segredinho!\n",
      "- Quer saber qual é? perguntou Aurélia com um sorriso'\n",
      "Tokens gerados: ['aqui', 'anda', 'volta', 'de', 'algum', 'segredinho-', 'quer', 'saber', 'qual', 'e', '?', 'perguntou', 'aurelia', 'com', 'um', 'sorriso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Não sou curiosa, replicou a viúva sentindo o pungir daquele sorriso'\n",
      "Tokens gerados: ['-', 'nao', 'sou', 'curiosa', ',', 'replicou', 'a', 'viuva', 'sentindo', 'o', 'pungir', 'daquele', 'sorriso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Resolvi ser freira!\n",
      "- Está bom!\n",
      "- Mas o meu convento há de ser este mesmo mundo em que vivemos, que nenhum outro teria mais\n",
      "penitências e mortificações para mim'\n",
      "Tokens gerados: ['-', 'resolvi', 'ser', 'freira-', 'esta', 'bom-', 'mas', 'o', 'meu', 'convento', 'ha', 'de', 'ser', 'este', 'mesmo', 'mundo', 'em', 'que', 'vivemos', ',', 'que', 'nenhum', 'outro', 'teria', 'maispenitencias', 'e', 'mortificacoes', 'para', 'mim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desmentindo logo após a gravidade destas palavras com uma risada galhofeira, Aurélia deixou na\n",
      "sala de jantar D'\n",
      "Tokens gerados: ['desmentindo', 'logo', 'apos', 'a', 'gravidade', 'destas', 'palavras', 'com', 'uma', 'risada', 'galhofeira', ',', 'aurelia', 'deixou', 'nasala', 'de', 'jantar', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina, espantada de que uma menina imensamente rica e formosa, desejada por\n",
      "todos, pudesse ter semelhantes pensamentos, ainda mesmo por gracejo'\n",
      "Tokens gerados: ['firmina', ',', 'espantada', 'de', 'que', 'uma', 'menina', 'imensamente', 'rica', 'e', 'formosa', ',', 'desejada', 'portodos', ',', 'pudesse', 'ter', 'semelhantes', 'pensamentos', ',', 'ainda', 'mesmo', 'por', 'gracejo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aurélia que se dirigira ao seu toucador, sentou-se a uma escrivaninha de araribá guarnecido de\n",
      "relevos de bronze dourado e escreveu uma carta de poucas linhas'\n",
      "Tokens gerados: ['aurelia', 'que', 'se', 'dirigira', 'ao', 'seu', 'toucador', ',', 'sentou-se', 'a', 'uma', 'escrivaninha', 'de', 'arariba', 'guarnecido', 'derelevos', 'de', 'bronze', 'dourado', 'e', 'escreveu', 'uma', 'carta', 'de', 'poucas', 'linhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A todos os pormenores dessa comezinha operação, no dobrar a folha de papel, encerrá-la na capa,\n",
      "derreter o lacre e imprimir o sinete, a moça deliberadamente aplicava a maior atenção e esmero'\n",
      "Tokens gerados: ['a', 'todos', 'os', 'pormenores', 'dessa', 'comezinha', 'operacao', ',', 'no', 'dobrar', 'a', 'folha', 'de', 'papel', ',', 'encerra-la', 'na', 'capa', ',', 'derreter', 'o', 'lacre', 'e', 'imprimir', 'o', 'sinete', ',', 'a', 'moca', 'deliberadamente', 'aplicava', 'a', 'maior', 'atencao', 'e', 'esmero']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ou essa carta era destinada a quem tudo lhe merecia, ou nesse apuro e cuidado buscava Aurélia\n",
      "disfarçar a hesitação que a surpreendera no momento de realizar uma idéia anteriormente assentada'\n",
      "Tokens gerados: ['ou', 'essa', 'carta', 'era', 'destinada', 'a', 'quem', 'tudo', 'lhe', 'merecia', ',', 'ou', 'nesse', 'apuro', 'e', 'cuidado', 'buscava', 'aureliadisfarcar', 'a', 'hesitacao', 'que', 'a', 'surpreendera', 'no', 'momento', 'de', 'realizar', 'uma', 'ideia', 'anteriormente', 'assentada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois de sobrescrita a carta, a moça tirou do segredo da secretária um cofre de sândalo embutido\n",
      "de marfim'\n",
      "Tokens gerados: ['depois', 'de', 'sobrescrita', 'a', 'carta', ',', 'a', 'moca', 'tirou', 'do', 'segredo', 'da', 'secretaria', 'um', 'cofre', 'de', 'sandalo', 'embutidode', 'marfim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia ali entre cartas e flores murchas um cartão de visita, já amarelo, que ela escondeu no bolso\n",
      "do roupão, depois de guardado na sua carteirinha de veludo'\n",
      "Tokens gerados: ['havia', 'ali', 'entre', 'cartas', 'e', 'flores', 'murchas', 'um', 'cartao', 'de', 'visita', ',', 'ja', 'amarelo', ',', 'que', 'ela', 'escondeu', 'no', 'bolsodo', 'roupao', ',', 'depois', 'de', 'guardado', 'na', 'sua', 'carteirinha', 'de', 'veludo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao som do tímpano apareceu um criado'\n",
      "Tokens gerados: ['ao', 'som', 'do', 'timpano', 'apareceu', 'um', 'criado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aurélia entregou-lhe a carta com um gesto vivo e a voz\n",
      "breve, como receosa de súbito arrependimento'\n",
      "Tokens gerados: ['aurelia', 'entregou-lhe', 'a', 'carta', 'com', 'um', 'gesto', 'vivo', 'e', 'a', 'vozbreve', ',', 'como', 'receosa', 'de', 'subito', 'arrependimento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Para o Sr'\n",
      "Tokens gerados: ['-', 'para', 'o', 'sr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lemos! Depressa!\n",
      "Sentiu então Aurélia essa quietude que sucede às lutas do coração'\n",
      "Tokens gerados: ['lemos', 'depressasentiu', 'entao', 'aurelia', 'essa', 'quietude', 'que', 'sucede', 'as', 'lutas', 'do', 'coracao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ela tinha afinal resolvido o\n",
      "problema inextricável de sua vida; e em vez de abandonar-se ao acaso e deixar-se levar pelo\n",
      "turbilhão do mundo, achara em sua alma a força precisa para dirigir os acontecimentos e dominar o\n",
      "futuro'\n",
      "Tokens gerados: ['ela', 'tinha', 'afinal', 'resolvido', 'oproblema', 'inextricavel', 'de', 'sua', 'vida', 'e', 'em', 'vez', 'de', 'abandonar-se', 'ao', 'acaso', 'e', 'deixar-se', 'levar', 'peloturbilhao', 'do', 'mundo', ',', 'achara', 'em', 'sua', 'alma', 'a', 'forca', 'precisa', 'para', 'dirigir', 'os', 'acontecimentos', 'e', 'dominar', 'ofuturo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Daí provinha a calma de que revestia-se ao deixar o toucador e que outra vez imprimia à sua beleza\n",
      "uma doce expressão de melancolia e resignação'\n",
      "Tokens gerados: ['dai', 'provinha', 'a', 'calma', 'de', 'que', 'revestia-se', 'ao', 'deixar', 'o', 'toucador', 'e', 'que', 'outra', 'vez', 'imprimia', 'a', 'sua', 'belezauma', 'doce', 'expressao', 'de', 'melancolia', 'e', 'resignacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'D'\n",
      "Tokens gerados: ['d']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina como de costume, esperava que Aurélia dispusesse a maneira por que passariam a\n",
      "manhã, pois a viúva não tinha outra ocupação que não fosse agradar à menina, fazer-lhe companhia\n",
      "e prestar-se a todas as suas vontades e caprichos'\n",
      "Tokens gerados: ['firmina', 'como', 'de', 'costume', ',', 'esperava', 'que', 'aurelia', 'dispusesse', 'a', 'maneira', 'por', 'que', 'passariam', 'amanha', ',', 'pois', 'a', 'viuva', 'nao', 'tinha', 'outra', 'ocupacao', 'que', 'nao', 'fosse', 'agradar', 'a', 'menina', ',', 'fazer-lhe', 'companhiae', 'prestar-se', 'a', 'todas', 'as', 'suas', 'vontades', 'e', 'caprichos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Para isto recebia além do tratamento uma boa mesada que ia acumulando para os tempos difíceis,\n",
      "como já os havia passado logo depois da perda do marido'\n",
      "Tokens gerados: ['para', 'isto', 'recebia', 'alem', 'do', 'tratamento', 'uma', 'boa', 'mesada', 'que', 'ia', 'acumulando', 'para', 'os', 'tempos', 'dificeis', ',', 'como', 'ja', 'os', 'havia', 'passado', 'logo', 'depois', 'da', 'perda', 'do', 'marido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Você não sai hoje, Aurélia?\n",
      "- Pode ser'\n",
      "Tokens gerados: ['-', 'voce', 'nao', 'sai', 'hoje', ',', 'aurelia', '?', '-', 'pode', 'ser']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas não se constranja por meu respeito'\n",
      "Tokens gerados: ['mas', 'nao', 'se', 'constranja', 'por', 'meu', 'respeito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Há de ficar sozinha?\n",
      "- Tenho em que empregar o tempo'\n",
      "Tokens gerados: ['-', 'ha', 'de', 'ficar', 'sozinha', '?', '-', 'tenho', 'em', 'que', 'empregar', 'o', 'tempo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um negócio grave! tornou a menina sorrindo'\n",
      "Tokens gerados: ['um', 'negocio', 'grave', 'tornou', 'a', 'menina', 'sorrindo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- É já alguma penitenciazinha?\n",
      "- Ainda não; é a profissão de noviça'\n",
      "Tokens gerados: ['-', 'e', 'ja', 'alguma', 'penitenciazinha', '?', '-', 'ainda', 'nao', 'e', 'a', 'profissao', 'de', 'novica']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nessa ocasião e no meio das risadas da menina, anunciaram o Sr'\n",
      "Tokens gerados: ['nessa', 'ocasiao', 'e', 'no', 'meio', 'das', 'risadas', 'da', 'menina', ',', 'anunciaram', 'o', 'sr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lemos, que foi imediatamente\n",
      "introduzido na sala'\n",
      "Tokens gerados: ['lemos', ',', 'que', 'foi', 'imediatamenteintroduzido', 'na', 'sala']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Recebi a sua carta em caminho; ia ao Botafogo: o José encontrou-me no Largo do Machado'\n",
      "Tokens gerados: ['-', 'recebi', 'a', 'sua', 'carta', 'em', 'caminho', 'ia', 'ao', 'botafogo', 'o', 'jose', 'encontrou-me', 'no', 'largo', 'do', 'machado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estou às suas ordens, Aurélia'\n",
      "Tokens gerados: ['estou', 'as', 'suas', 'ordens', ',', 'aurelia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era o Sr'\n",
      "Tokens gerados: ['era', 'o', 'sr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lemos um velho de pequena estatura, não muito gordo, mas rolho e bojudo como um\n",
      "vaso chinês'\n",
      "Tokens gerados: ['lemos', 'um', 'velho', 'de', 'pequena', 'estatura', ',', 'nao', 'muito', 'gordo', ',', 'mas', 'rolho', 'e', 'bojudo', 'como', 'umvaso', 'chines']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Apesar de seu corpo rechonchudo tinha certa vivacidade buliçosa e saltitante que lhe\n",
      "dava petulância de rapaz, e casava perfeitamente com os olhinhos de azougue'\n",
      "Tokens gerados: ['apesar', 'de', 'seu', 'corpo', 'rechonchudo', 'tinha', 'certa', 'vivacidade', 'bulicosa', 'e', 'saltitante', 'que', 'lhedava', 'petulancia', 'de', 'rapaz', ',', 'e', 'casava', 'perfeitamente', 'com', 'os', 'olhinhos', 'de', 'azougue']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Logo à primeira apresentação reconhecia-se o tipo desses folgazões que trazem sempre um\n",
      "provimento de boas risadas com que se festejam a si mesmos'\n",
      "Tokens gerados: ['logo', 'a', 'primeira', 'apresentacao', 'reconhecia-se', 'o', 'tipo', 'desses', 'folgazoes', 'que', 'trazem', 'sempre', 'umprovimento', 'de', 'boas', 'risadas', 'com', 'que', 'se', 'festejam', 'a', 'si', 'mesmos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando o Lemos na qualidade de tio fora pelo juiz de órfãos encarregado da tutela de Aurélia, deuse um incidente que desde logo determinou a natureza das relações entre o tutor e sua pupila'\n",
      "Tokens gerados: ['quando', 'o', 'lemos', 'na', 'qualidade', 'de', 'tio', 'fora', 'pelo', 'juiz', 'de', 'orfaos', 'encarregado', 'da', 'tutela', 'de', 'aurelia', ',', 'deuse', 'um', 'incidente', 'que', 'desde', 'logo', 'determinou', 'a', 'natureza', 'das', 'relacoes', 'entre', 'o', 'tutor', 'e', 'sua', 'pupila']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pretendia o velho levar a menina para a companhia de sua família'\n",
      "Tokens gerados: ['pretendia', 'o', 'velho', 'levar', 'a', 'menina', 'para', 'a', 'companhia', 'de', 'sua', 'familia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Opôs-se formalmente Aurélia, e declarou que era sua intenção viver em casa própria, na companhia\n",
      "de D'\n",
      "Tokens gerados: ['opos-se', 'formalmente', 'aurelia', ',', 'e', 'declarou', 'que', 'era', 'sua', 'intencao', 'viver', 'em', 'casa', 'propria', ',', 'na', 'companhiade', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina Mascarenhas'\n",
      "Tokens gerados: ['firmina', 'mascarenhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Mas atenda, minha menina, que ainda é menor'\n",
      "Tokens gerados: ['-', 'mas', 'atenda', ',', 'minha', 'menina', ',', 'que', 'ainda', 'e', 'menor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Tenho dezoito anos'\n",
      "Tokens gerados: ['-', 'tenho', 'dezoito', 'anos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Só aos vinte e um é que poderá viver sobre si e governar-se'\n",
      "Tokens gerados: ['-', 'so', 'aos', 'vinte', 'e', 'um', 'e', 'que', 'podera', 'viver', 'sobre', 'si', 'e', 'governar-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- É a sua opinião? Vou pedir ao juiz que me dê outro tutor mais condescendente'\n",
      "Tokens gerados: ['-', 'e', 'a', 'sua', 'opiniao', '?', 'vou', 'pedir', 'ao', 'juiz', 'que', 'me', 'de', 'outro', 'tutor', 'mais', 'condescendente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Como diz?\n",
      "- E tais argumentos lhe apresentarei, que ele há de atender-me'\n",
      "Tokens gerados: ['-', 'como', 'diz', '?', '-', 'e', 'tais', 'argumentos', 'lhe', 'apresentarei', ',', 'que', 'ele', 'ha', 'de', 'atender-me']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'À vista desse tom positivo, o Lemos refletiu, e julgou mais prudente não contrariar a vontade da\n",
      "menina'\n",
      "Tokens gerados: ['a', 'vista', 'desse', 'tom', 'positivo', ',', 'o', 'lemos', 'refletiu', ',', 'e', 'julgou', 'mais', 'prudente', 'nao', 'contrariar', 'a', 'vontade', 'damenina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aquela idéia do pedido ao juiz para remoção da tutela não lhe agradara'\n",
      "Tokens gerados: ['aquela', 'ideia', 'do', 'pedido', 'ao', 'juiz', 'para', 'remocao', 'da', 'tutela', 'nao', 'lhe', 'agradara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pensava ele que às\n",
      "mulheres ricas e bonitas não faltam protetores de influência'\n",
      "Tokens gerados: ['pensava', 'ele', 'que', 'asmulheres', 'ricas', 'e', 'bonitas', 'nao', 'faltam', 'protetores', 'de', 'influencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Logo depois dos cumprimentos, D'\n",
      "Tokens gerados: ['logo', 'depois', 'dos', 'cumprimentos', ',', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Firmina retirou-se para deixar a moça em liberdade'\n",
      "Tokens gerados: ['firmina', 'retirou-se', 'para', 'deixar', 'a', 'moca', 'em', 'liberdade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bem\n",
      "desejos tinha a viúva de assistir a essas conferências que o Lemos costumava ter de vez em quando\n",
      "com a pupila acerca de contas da tutela; mas neste ponto Aurélia era de extrema reserva e não\n",
      "gostava que ninguém entendesse com o que ela chamava seus negócios'\n",
      "Tokens gerados: ['bemdesejos', 'tinha', 'a', 'viuva', 'de', 'assistir', 'a', 'essas', 'conferencias', 'que', 'o', 'lemos', 'costumava', 'ter', 'de', 'vez', 'em', 'quandocom', 'a', 'pupila', 'acerca', 'de', 'contas', 'da', 'tutela', 'mas', 'neste', 'ponto', 'aurelia', 'era', 'de', 'extrema', 'reserva', 'e', 'naogostava', 'que', 'ninguem', 'entendesse', 'com', 'o', 'que', 'ela', 'chamava', 'seus', 'negocios']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Faça favor, meu tio! disse a moça abrindo uma porta lateral'\n",
      "Tokens gerados: ['-', 'faca', 'favor', ',', 'meu', 'tio', 'disse', 'a', 'moca', 'abrindo', 'uma', 'porta', 'lateral']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Essa porta dava para um gabinete elegantemente mobiliado; o centro era ocupado por uma banca\n",
      "oval, como o resto dos trastes de érable e coberta com um pano azul de franjas escarlates'\n",
      "Tokens gerados: ['essa', 'porta', 'dava', 'para', 'um', 'gabinete', 'elegantemente', 'mobiliado', 'o', 'centro', 'era', 'ocupado', 'por', 'uma', 'bancaoval', ',', 'como', 'o', 'resto', 'dos', 'trastes', 'de', 'erable', 'e', 'coberta', 'com', 'um', 'pano', 'azul', 'de', 'franjas', 'escarlates']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sobre a\n",
      "mesa, em salva de prata, havia o tinteiro e mais preparos de escrever'\n",
      "Tokens gerados: ['sobre', 'amesa', ',', 'em', 'salva', 'de', 'prata', ',', 'havia', 'o', 'tinteiro', 'e', 'mais', 'preparos', 'de', 'escrever']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No momento em que Aurélia, depois de passar o Lemos, ia por sua vez entrar no gabinete, apareceu\n",
      "à porta da saleta a Bernardina, velha a quem a menina protegia com esmolas'\n",
      "Tokens gerados: ['no', 'momento', 'em', 'que', 'aurelia', ',', 'depois', 'de', 'passar', 'o', 'lemos', ',', 'ia', 'por', 'sua', 'vez', 'entrar', 'no', 'gabinete', ',', 'apareceua', 'porta', 'da', 'saleta', 'a', 'bernardina', ',', 'velha', 'a', 'quem', 'a', 'menina', 'protegia', 'com', 'esmolas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A sujeita parara com\n",
      "um modo tímido, esperando permissão para adiantar-se'\n",
      "Tokens gerados: ['a', 'sujeita', 'parara', 'comum', 'modo', 'timido', ',', 'esperando', 'permissao', 'para', 'adiantar-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aurélia aproximou-se dela com um gesto de interrogação'\n",
      "Tokens gerados: ['aurelia', 'aproximou-se', 'dela', 'com', 'um', 'gesto', 'de', 'interrogacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Quis vir ontem, segredou a Bernardina; mas não pude, que atacou-me o reumatismo'\n",
      "Tokens gerados: ['-', 'quis', 'vir', 'ontem', ',', 'segredou', 'a', 'bernardina', 'mas', 'nao', 'pude', ',', 'que', 'atacou-me', 'o', 'reumatismo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era para\n",
      "dizer que ele chegou'\n",
      "Tokens gerados: ['era', 'paradizer', 'que', 'ele', 'chegou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Já sabia!\n",
      "- Ah! quem lhe contou? Pois foi ontem, havia de ser mais de meio-dia'\n",
      "Tokens gerados: ['-', 'ja', 'sabia-', 'ah', 'quem', 'lhe', 'contou', '?', 'pois', 'foi', 'ontem', ',', 'havia', 'de', 'ser', 'mais', 'de', 'meio-dia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Entre!\n",
      "Aurélia cortou o diálogo, indicando à velha o corredor que levava para o interior; e passando ao\n",
      "gabinete cerrou a porta sobre si'\n",
      "Tokens gerados: ['-', 'entreaurelia', 'cortou', 'o', 'dialogo', ',', 'indicando', 'a', 'velha', 'o', 'corredor', 'que', 'levava', 'para', 'o', 'interior', 'e', 'passando', 'aogabinete', 'cerrou', 'a', 'porta', 'sobre', 'si']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não escapou este pormenor ao Lemos, que pela solenidade da conferência avaliava de sua\n",
      "importância'\n",
      "Tokens gerados: ['nao', 'escapou', 'este', 'pormenor', 'ao', 'lemos', ',', 'que', 'pela', 'solenidade', 'da', 'conferencia', 'avaliava', 'de', 'suaimportancia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Com que história virá ela hoje? dizia entre si o alegre velhinho'\n",
      "Tokens gerados: ['-', 'com', 'que', 'historia', 'vira', 'ela', 'hoje', '?', 'dizia', 'entre', 'si', 'o', 'alegre', 'velhinho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aurélia sentou-se à mesa de érable, convidando o tutor a ocupar a poltrona que lhe ficava defronte'\n",
      "Tokens gerados: ['aurelia', 'sentou-se', 'a', 'mesa', 'de', 'erable', ',', 'convidando', 'o', 'tutor', 'a', 'ocupar', 'a', 'poltrona', 'que', 'lhe', 'ficava', 'defronte']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_senhora_jose_de_alencar_cap_3.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Quem observasse Aurélia naquele momento, não deixaria de notar a nova fisionomia que tomara o\n",
      "seu belo semblante e que influía em toda a sua pessoa'\n",
      "Tokens gerados: ['quem', 'observasse', 'aurelia', 'naquele', 'momento', ',', 'nao', 'deixaria', 'de', 'notar', 'a', 'nova', 'fisionomia', 'que', 'tomara', 'oseu', 'belo', 'semblante', 'e', 'que', 'influia', 'em', 'toda', 'a', 'sua', 'pessoa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era uma expressão fria, pausada, inflexível, que jaspeava sua beleza, dando-lhe quase a gelidez da\n",
      "estátua'\n",
      "Tokens gerados: ['era', 'uma', 'expressao', 'fria', ',', 'pausada', ',', 'inflexivel', ',', 'que', 'jaspeava', 'sua', 'beleza', ',', 'dando-lhe', 'quase', 'a', 'gelidez', 'daestatua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas no lampejo de seus grandes olhos pardos brilhavam as irradiações da inteligência'\n",
      "Tokens gerados: ['mas', 'no', 'lampejo', 'de', 'seus', 'grandes', 'olhos', 'pardos', 'brilhavam', 'as', 'irradiacoes', 'da', 'inteligencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Operava-se nela uma revolução'\n",
      "Tokens gerados: ['operava-se', 'nela', 'uma', 'revolucao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O princípio vital da mulher abandonava seu foco natural, o\n",
      "coração, para concentrar-se no cérebro, onde residem as faculdades especulativas do homem'\n",
      "Tokens gerados: ['o', 'principio', 'vital', 'da', 'mulher', 'abandonava', 'seu', 'foco', 'natural', ',', 'ocoracao', ',', 'para', 'concentrar-se', 'no', 'cerebro', ',', 'onde', 'residem', 'as', 'faculdades', 'especulativas', 'do', 'homem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nessas ocasiões seu espírito adquiria tal lucidez que fazia correr um calafrio pela medula do\n",
      "Lemos, apesar do lombo maciço de que a natureza havia forrado no roliço velhinho o tronco do\n",
      "sistema nervoso'\n",
      "Tokens gerados: ['nessas', 'ocasioes', 'seu', 'espirito', 'adquiria', 'tal', 'lucidez', 'que', 'fazia', 'correr', 'um', 'calafrio', 'pela', 'medula', 'dolemos', ',', 'apesar', 'do', 'lombo', 'macico', 'de', 'que', 'a', 'natureza', 'havia', 'forrado', 'no', 'rolico', 'velhinho', 'o', 'tronco', 'dosistema', 'nervoso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era realmente para causar pasmo aos estranhos e susto a um tutor, a perspicácia com que essa moça\n",
      "de dezoito anos aprecia-va as questões mais complicadas; o perfeito conhecimento que mostrava\n",
      "dos negócios, e a facilidade com que fazia, muitas vezes de memória, qualquer operação aritmética\n",
      "por muito difícil e intrincada que fosse'\n",
      "Tokens gerados: ['era', 'realmente', 'para', 'causar', 'pasmo', 'aos', 'estranhos', 'e', 'susto', 'a', 'um', 'tutor', ',', 'a', 'perspicacia', 'com', 'que', 'essa', 'mocade', 'dezoito', 'anos', 'aprecia-va', 'as', 'questoes', 'mais', 'complicadas', 'o', 'perfeito', 'conhecimento', 'que', 'mostravados', 'negocios', ',', 'e', 'a', 'facilidade', 'com', 'que', 'fazia', ',', 'muitas', 'vezes', 'de', 'memoria', ',', 'qualquer', 'operacao', 'aritmeticapor', 'muito', 'dificil', 'e', 'intrincada', 'que', 'fosse']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não havia porém em Aurélia nem sombra do ridículo pedantismo de certas moças que, tendo\n",
      "colhido em leituras superficiais algumas noções vagas, se metem a tagarelar de tudo'\n",
      "Tokens gerados: ['nao', 'havia', 'porem', 'em', 'aurelia', 'nem', 'sombra', 'do', 'ridiculo', 'pedantismo', 'de', 'certas', 'mocas', 'que', ',', 'tendocolhido', 'em', 'leituras', 'superficiais', 'algumas', 'nocoes', 'vagas', ',', 'se', 'metem', 'a', 'tagarelar', 'de', 'tudo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bem ao contrário, ela recatava sua experiência, de que só fazia uso, quando o exigiam seus próprios\n",
      "interesses'\n",
      "Tokens gerados: ['bem', 'ao', 'contrario', ',', 'ela', 'recatava', 'sua', 'experiencia', ',', 'de', 'que', 'so', 'fazia', 'uso', ',', 'quando', 'o', 'exigiam', 'seus', 'propriosinteresses']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fora daí ninguém lhe ouvia falar de negócios e emitir opinião acerca de cousas que não\n",
      "pertencessem à sua especialidade de moça solteira'\n",
      "Tokens gerados: ['fora', 'dai', 'ninguem', 'lhe', 'ouvia', 'falar', 'de', 'negocios', 'e', 'emitir', 'opiniao', 'acerca', 'de', 'cousas', 'que', 'naopertencessem', 'a', 'sua', 'especialidade', 'de', 'moca', 'solteira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Lemos não estava a gosto; tinha perdido aquela jovialidade saltitante, que lhe dava um gracioso\n",
      "ar de pipoca'\n",
      "Tokens gerados: ['o', 'lemos', 'nao', 'estava', 'a', 'gosto', 'tinha', 'perdido', 'aquela', 'jovialidade', 'saltitante', ',', 'que', 'lhe', 'dava', 'um', 'graciosoar', 'de', 'pipoca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na gravidade desusada dessa conferência, ele, homem experiente e sagaz, entrevia\n",
      "sérias complicações'\n",
      "Tokens gerados: ['na', 'gravidade', 'desusada', 'dessa', 'conferencia', ',', 'ele', ',', 'homem', 'experiente', 'e', 'sagaz', ',', 'entreviaserias', 'complicacoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Assim era todo ouvidos, atento às palavras da moça'\n",
      "Tokens gerados: ['assim', 'era', 'todo', 'ouvidos', ',', 'atento', 'as', 'palavras', 'da', 'moca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Tomei a liberdade de incomodá-lo, meu tio, para falar-lhe de objeto muito importante para mim'\n",
      "Tokens gerados: ['-', 'tomei', 'a', 'liberdade', 'de', 'incomoda-lo', ',', 'meu', 'tio', ',', 'para', 'falar-lhe', 'de', 'objeto', 'muito', 'importante', 'para', 'mim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Ah! muito importante?'\n",
      "Tokens gerados: ['-', 'ah', 'muito', 'importante', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'repetiu o velho batendo a cabeça'\n",
      "Tokens gerados: ['repetiu', 'o', 'velho', 'batendo', 'a', 'cabeca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- De meu casamento! disse Aurélia com a maior frieza e serenidade'\n",
      "Tokens gerados: ['-', 'de', 'meu', 'casamento', 'disse', 'aurelia', 'com', 'a', 'maior', 'frieza', 'e', 'serenidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O velhinho saltou na cadeira como um balão elástico'\n",
      "Tokens gerados: ['o', 'velhinho', 'saltou', 'na', 'cadeira', 'como', 'um', 'balao', 'elastico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Para disfarçar sua comoção esfregou as mãos\n",
      "rapidamente uma na outra, gesto que indicava nele grande agitação'\n",
      "Tokens gerados: ['para', 'disfarcar', 'sua', 'comocao', 'esfregou', 'as', 'maosrapidamente', 'uma', 'na', 'outra', ',', 'gesto', 'que', 'indicava', 'nele', 'grande', 'agitacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Não acha que já estou em idade de pensar nisso? perguntou a moça'\n",
      "Tokens gerados: ['-', 'nao', 'acha', 'que', 'ja', 'estou', 'em', 'idade', 'de', 'pensar', 'nisso', '?', 'perguntou', 'a', 'moca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Certamente! Dezoito anos'\n",
      "Tokens gerados: ['-', 'certamente', 'dezoito', 'anos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Dezenove'\n",
      "Tokens gerados: ['-', 'dezenove']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Dezenove? Cuidei que ainda não os tinha feito!'\n",
      "Tokens gerados: ['-', 'dezenove', '?', 'cuidei', 'que', 'ainda', 'nao', 'os', 'tinha', 'feito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Muitas casam-se desta idade, e até mais moças;\n",
      "porém é quando têm o paizinho ou a mãezinha para escolher um bom noivo e arredar certos\n",
      "espertalhões'\n",
      "Tokens gerados: ['muitas', 'casam-se', 'desta', 'idade', ',', 'e', 'ate', 'mais', 'mocasporem', 'e', 'quando', 'tem', 'o', 'paizinho', 'ou', 'a', 'maezinha', 'para', 'escolher', 'um', 'bom', 'noivo', 'e', 'arredar', 'certosespertalhoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma menina órfã, inexperiente, eu não lhe aconselharia que se casasse senão depois\n",
      "da maioridade, quando conhecesse bem o mundo'\n",
      "Tokens gerados: ['uma', 'menina', 'orfa', ',', 'inexperiente', ',', 'eu', 'nao', 'lhe', 'aconselharia', 'que', 'se', 'casasse', 'senao', 'depoisda', 'maioridade', ',', 'quando', 'conhecesse', 'bem', 'o', 'mundo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Já o conheço demais, tornou a moça com o mesmo tom sério'\n",
      "Tokens gerados: ['-', 'ja', 'o', 'conheco', 'demais', ',', 'tornou', 'a', 'moca', 'com', 'o', 'mesmo', 'tom', 'serio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Então está decidida?\n",
      "- Tão decidida que lhe pedi esta conferência'\n",
      "Tokens gerados: ['-', 'entao', 'esta', 'decidida', '?', '-', 'tao', 'decidida', 'que', 'lhe', 'pedi', 'esta', 'conferencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Já sei! Deseja que eu aponte alguém'\n",
      "Tokens gerados: ['-', 'ja', 'sei', 'deseja', 'que', 'eu', 'aponte', 'alguem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que eu lhe procure um noivo nas condições precisas'\n",
      "Tokens gerados: ['que', 'eu', 'lhe', 'procure', 'um', 'noivo', 'nas', 'condicoes', 'precisas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Hã!'\n",
      "Tokens gerados: ['ha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É difícil'\n",
      "Tokens gerados: ['e', 'dificil']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'um sujeito no caso de pretender uma moça como você, Aurélia? Enfim há de se\n",
      "fazer a diligência!\n",
      "- Não precisa, meu tio'\n",
      "Tokens gerados: ['um', 'sujeito', 'no', 'caso', 'de', 'pretender', 'uma', 'moca', 'como', 'voce', ',', 'aurelia', '?', 'enfim', 'ha', 'de', 'sefazer', 'a', 'diligencia-', 'nao', 'precisa', ',', 'meu', 'tio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Já o achei!\n",
      "Teve o Lemos outro sobressalto que o fez de novo pular na cadeira'\n",
      "Tokens gerados: ['ja', 'o', 'acheiteve', 'o', 'lemos', 'outro', 'sobressalto', 'que', 'o', 'fez', 'de', 'novo', 'pular', 'na', 'cadeira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Como?'\n",
      "Tokens gerados: ['-', 'como', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tem alguém de olho?\n",
      "- Perdão, meu tio, não entendo sua linguagem figurada'\n",
      "Tokens gerados: ['tem', 'alguem', 'de', 'olho', '?', '-', 'perdao', ',', 'meu', 'tio', ',', 'nao', 'entendo', 'sua', 'linguagem', 'figurada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Digo-lhe que escolhi o homem com quem\n",
      "me hei de casar'\n",
      "Tokens gerados: ['digo-lhe', 'que', 'escolhi', 'o', 'homem', 'com', 'quemme', 'hei', 'de', 'casar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Já compreendo'\n",
      "Tokens gerados: ['-', 'ja', 'compreendo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas bem vê!'\n",
      "Tokens gerados: ['mas', 'bem', 've']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Como tutor, tenho de dar a minha aprovação'\n",
      "Tokens gerados: ['como', 'tutor', ',', 'tenho', 'de', 'dar', 'a', 'minha', 'aprovacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- De certo, meu tutor; mas essa aprovação o senhor não há de ser tão cruel que a negue'\n",
      "Tokens gerados: ['-', 'de', 'certo', ',', 'meu', 'tutor', 'mas', 'essa', 'aprovacao', 'o', 'senhor', 'nao', 'ha', 'de', 'ser', 'tao', 'cruel', 'que', 'a', 'negue']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se o fizer,\n",
      "o que eu não espero, o juiz de órfãos a suprirá'\n",
      "Tokens gerados: ['se', 'o', 'fizer', ',', 'o', 'que', 'eu', 'nao', 'espero', ',', 'o', 'juiz', 'de', 'orfaos', 'a', 'suprira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- O juiz?'\n",
      "Tokens gerados: ['-', 'o', 'juiz', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que histórias são essas que lhe andam metendo na cabeça, Aurélia?\n",
      "- Sr'\n",
      "Tokens gerados: ['que', 'historias', 'sao', 'essas', 'que', 'lhe', 'andam', 'metendo', 'na', 'cabeca', ',', 'aurelia', '?', '-', 'sr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lemos, disse a moça pausadamente e traspassando com um olhar frio a vista perplexa do\n",
      "velho, completei dezenove anos; posso requerer um suplemento de idade mostrando que tenho\n",
      "capacidade para reger minha pessoa e bens; com maioria de razão obterei do juiz de órfãos, apesar\n",
      "de sua oposição, um alvará de licença para casar-me com quem eu quiser'\n",
      "Tokens gerados: ['lemos', ',', 'disse', 'a', 'moca', 'pausadamente', 'e', 'traspassando', 'com', 'um', 'olhar', 'frio', 'a', 'vista', 'perplexa', 'dovelho', ',', 'completei', 'dezenove', 'anos', 'posso', 'requerer', 'um', 'suplemento', 'de', 'idade', 'mostrando', 'que', 'tenhocapacidade', 'para', 'reger', 'minha', 'pessoa', 'e', 'bens', 'com', 'maioria', 'de', 'razao', 'obterei', 'do', 'juiz', 'de', 'orfaos', ',', 'apesarde', 'sua', 'oposicao', ',', 'um', 'alvara', 'de', 'licenca', 'para', 'casar-me', 'com', 'quem', 'eu', 'quiser']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se estes argumentos\n",
      "jurídicos não lhe satisfazem, apresentar-lhe-ei um que me é pessoal'\n",
      "Tokens gerados: ['se', 'estes', 'argumentosjuridicos', 'nao', 'lhe', 'satisfazem', ',', 'apresentar-lhe-ei', 'um', 'que', 'me', 'e', 'pessoal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Vamos a ver! acudiu o velho para quebrar o silêncio'\n",
      "Tokens gerados: ['-', 'vamos', 'a', 'ver', 'acudiu', 'o', 'velho', 'para', 'quebrar', 'o', 'silencio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- É a minha vontade'\n",
      "Tokens gerados: ['-', 'e', 'a', 'minha', 'vontade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O senhor não sabe o que ela vale, mas juro-lhe que para a levar a efeito não se\n",
      "me dará de sacrificar a herança de meu avô'\n",
      "Tokens gerados: ['o', 'senhor', 'nao', 'sabe', 'o', 'que', 'ela', 'vale', ',', 'mas', 'juro-lhe', 'que', 'para', 'a', 'levar', 'a', 'efeito', 'nao', 'seme', 'dara', 'de', 'sacrificar', 'a', 'heranca', 'de', 'meu', 'avo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- É próprio da idade! São idéias que somente se têm aos dezenove anos; e isso mesmo já vai sendo\n",
      "raro'\n",
      "Tokens gerados: ['-', 'e', 'proprio', 'da', 'idade', 'sao', 'ideias', 'que', 'somente', 'se', 'tem', 'aos', 'dezenove', 'anos', 'e', 'isso', 'mesmo', 'ja', 'vai', 'sendoraro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Esquece que desses dezenove anos, dezoito os vivi na extrema pobreza e um no seio da riqueza\n",
      "para onde fui transportada de repente'\n",
      "Tokens gerados: ['-', 'esquece', 'que', 'desses', 'dezenove', 'anos', ',', 'dezoito', 'os', 'vivi', 'na', 'extrema', 'pobreza', 'e', 'um', 'no', 'seio', 'da', 'riquezapara', 'onde', 'fui', 'transportada', 'de', 'repente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tenho as duas grandes lições do mundo: a da miséria e a da\n",
      "opulência'\n",
      "Tokens gerados: ['tenho', 'as', 'duas', 'grandes', 'licoes', 'do', 'mundo', 'a', 'da', 'miseria', 'e', 'a', 'daopulencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Conheci outrora o dinheiro como um tirano; hoje o conheço como um cativo submisso'\n",
      "Tokens gerados: ['conheci', 'outrora', 'o', 'dinheiro', 'como', 'um', 'tirano', 'hoje', 'o', 'conheco', 'como', 'um', 'cativo', 'submisso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por conseguinte devo ser mais velha do que o senhor que nunca foi nem tão pobre, como eu fui,\n",
      "nem tão rico, como eu sou'\n",
      "Tokens gerados: ['por', 'conseguinte', 'devo', 'ser', 'mais', 'velha', 'do', 'que', 'o', 'senhor', 'que', 'nunca', 'foi', 'nem', 'tao', 'pobre', ',', 'como', 'eu', 'fui', ',', 'nem', 'tao', 'rico', ',', 'como', 'eu', 'sou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Lemos olhava com pasmo essa moça que lhe falava com tão profunda lição do mundo e uma\n",
      "filosofia para ele desconhecida'\n",
      "Tokens gerados: ['o', 'lemos', 'olhava', 'com', 'pasmo', 'essa', 'moca', 'que', 'lhe', 'falava', 'com', 'tao', 'profunda', 'licao', 'do', 'mundo', 'e', 'umafilosofia', 'para', 'ele', 'desconhecida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '-\n",
      "- Não valia a pena ter tanto dinheiro, continuou Aurélia, se ele não servisse para casar-me a meu\n",
      "gosto; ainda que para isto seja necessário gastar alguns miseráveis contos de réis'\n",
      "Tokens gerados: ['--', 'nao', 'valia', 'a', 'pena', 'ter', 'tanto', 'dinheiro', ',', 'continuou', 'aurelia', ',', 'se', 'ele', 'nao', 'servisse', 'para', 'casar-me', 'a', 'meugosto', 'ainda', 'que', 'para', 'isto', 'seja', 'necessario', 'gastar', 'alguns', 'miseraveis', 'contos', 'de', 'reis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Aí é que está a dificuldade, acudiu o Lemos, que desde muito espreitava uma objeção'\n",
      "Tokens gerados: ['-', 'ai', 'e', 'que', 'esta', 'a', 'dificuldade', ',', 'acudiu', 'o', 'lemos', ',', 'que', 'desde', 'muito', 'espreitava', 'uma', 'objecao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bem sabe\n",
      "Aurélia, que eu como tutor não posso despender um vintém sem autorização do juiz'\n",
      "Tokens gerados: ['bem', 'sabeaurelia', ',', 'que', 'eu', 'como', 'tutor', 'nao', 'posso', 'despender', 'um', 'vintem', 'sem', 'autorizacao', 'do', 'juiz']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- O senhor não me quer entender, meu tutor, replicou a moça com um tênue assomo de\n",
      "impaciência'\n",
      "Tokens gerados: ['-', 'o', 'senhor', 'nao', 'me', 'quer', 'entender', ',', 'meu', 'tutor', ',', 'replicou', 'a', 'moca', 'com', 'um', 'tenue', 'assomo', 'deimpaciencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sei disso, e sei também muitas cousas que ninguém imagina'\n",
      "Tokens gerados: ['sei', 'disso', ',', 'e', 'sei', 'tambem', 'muitas', 'cousas', 'que', 'ninguem', 'imagina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por exemplo: sei o\n",
      "dividendo das apólices, a taxa do juro, as cotações da praça, sei que faço uma conta de prêmios\n",
      "compostos com a justeza e exatidão de uma tábua de câmbio'\n",
      "Tokens gerados: ['por', 'exemplo', 'sei', 'odividendo', 'das', 'apolices', ',', 'a', 'taxa', 'do', 'juro', ',', 'as', 'cotacoes', 'da', 'praca', ',', 'sei', 'que', 'faco', 'uma', 'conta', 'de', 'premioscompostos', 'com', 'a', 'justeza', 'e', 'exatidao', 'de', 'uma', 'tabua', 'de', 'cambio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Lemos estava tonto'\n",
      "Tokens gerados: ['o', 'lemos', 'estava', 'tonto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- E por último sei que tenho uma relação de tudo quanto possuía meu avô, escrita por seu próprio\n",
      "punho e que me foi dada por ele mesmo'\n",
      "Tokens gerados: ['-', 'e', 'por', 'ultimo', 'sei', 'que', 'tenho', 'uma', 'relacao', 'de', 'tudo', 'quanto', 'possuia', 'meu', 'avo', ',', 'escrita', 'por', 'seu', 'propriopunho', 'e', 'que', 'me', 'foi', 'dada', 'por', 'ele', 'mesmo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desta vez o purpurino velhinho empalideceu, sintoma assustador de tão completa e maciça\n",
      "carnadura, como a que lhe acolchoava as calcinhas emigradas e o fraque preto'\n",
      "Tokens gerados: ['desta', 'vez', 'o', 'purpurino', 'velhinho', 'empalideceu', ',', 'sintoma', 'assustador', 'de', 'tao', 'completa', 'e', 'macicacarnadura', ',', 'como', 'a', 'que', 'lhe', 'acolchoava', 'as', 'calcinhas', 'emigradas', 'e', 'o', 'fraque', 'preto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Isto quer dizer que se eu tivesse um tutor que me contrariasse e caísse em meu desagrado, ao\n",
      "chegar à minha maioridade não lhe daria quitação, sem primeiro passar um exame nas contas de\n",
      "sua administração para o que felizmente não careço de advogado nem de guarda-livros'\n",
      "Tokens gerados: ['-', 'isto', 'quer', 'dizer', 'que', 'se', 'eu', 'tivesse', 'um', 'tutor', 'que', 'me', 'contrariasse', 'e', 'caisse', 'em', 'meu', 'desagrado', ',', 'aochegar', 'a', 'minha', 'maioridade', 'nao', 'lhe', 'daria', 'quitacao', ',', 'sem', 'primeiro', 'passar', 'um', 'exame', 'nas', 'contas', 'desua', 'administracao', 'para', 'o', 'que', 'felizmente', 'nao', 'careco', 'de', 'advogado', 'nem', 'de', 'guarda-livros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Sim, senhora; está em seu direito, tornou o velho contrito'\n",
      "Tokens gerados: ['-', 'sim', ',', 'senhora', 'esta', 'em', 'seu', 'direito', ',', 'tornou', 'o', 'velho', 'contrito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Cabendo-me porém a fortuna de ter um tutor meu amigo, que me faz todas as vontades, como o\n",
      "senhor, meu tio'\n",
      "Tokens gerados: ['-', 'cabendo-me', 'porem', 'a', 'fortuna', 'de', 'ter', 'um', 'tutor', 'meu', 'amigo', ',', 'que', 'me', 'faz', 'todas', 'as', 'vontades', ',', 'como', 'osenhor', ',', 'meu', 'tio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Lá isso é verdade!\n",
      "- Neste caso, em vez de matar a paciência e aborrecer-me com autos e contas, dou tudo por bemfeito'\n",
      "Tokens gerados: ['-', 'la', 'isso', 'e', 'verdade-', 'neste', 'caso', ',', 'em', 'vez', 'de', 'matar', 'a', 'paciencia', 'e', 'aborrecer-me', 'com', 'autos', 'e', 'contas', ',', 'dou', 'tudo', 'por', 'bemfeito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ainda mais, sei que a tutela é gratuita, mas assim não deve ser quando os órfãos têm de sobra\n",
      "com que recompensar o trabalho que dão'\n",
      "Tokens gerados: ['ainda', 'mais', ',', 'sei', 'que', 'a', 'tutela', 'e', 'gratuita', ',', 'mas', 'assim', 'nao', 'deve', 'ser', 'quando', 'os', 'orfaos', 'tem', 'de', 'sobracom', 'que', 'recompensar', 'o', 'trabalho', 'que', 'dao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Lá isso não, Aurélia'\n",
      "Tokens gerados: ['-', 'la', 'isso', 'nao', ',', 'aurelia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Este encargo é uma dívida sagrada, que pago à memória de sua mãe, a\n",
      "minha boa e sempre chorada irmã!'\n",
      "Tokens gerados: ['este', 'encargo', 'e', 'uma', 'divida', 'sagrada', ',', 'que', 'pago', 'a', 'memoria', 'de', 'sua', 'mae', ',', 'aminha', 'boa', 'e', 'sempre', 'chorada', 'irma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Lemos enxugou no canto do olho uma lágrima que ele conseguira espremer, se é que não a tinha\n",
      "inventado como parece mais provável'\n",
      "Tokens gerados: ['o', 'lemos', 'enxugou', 'no', 'canto', 'do', 'olho', 'uma', 'lagrima', 'que', 'ele', 'conseguira', 'espremer', ',', 'se', 'e', 'que', 'nao', 'a', 'tinhainventado', 'como', 'parece', 'mais', 'provavel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E a moça em tributo à memória de sua mãe evocada pelo\n",
      "velho, ergueu-se um instante a pretexto de olhar pela janela'\n",
      "Tokens gerados: ['e', 'a', 'moca', 'em', 'tributo', 'a', 'memoria', 'de', 'sua', 'mae', 'evocada', 'pelovelho', ',', 'ergueu-se', 'um', 'instante', 'a', 'pretexto', 'de', 'olhar', 'pela', 'janela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando voltou a seu lugar, o Lemos estava de todo restabelecido dos choques por que havia\n",
      "passado; e mostrava-se ao natural, fresco, titilante e risonho'\n",
      "Tokens gerados: ['quando', 'voltou', 'a', 'seu', 'lugar', ',', 'o', 'lemos', 'estava', 'de', 'todo', 'restabelecido', 'dos', 'choques', 'por', 'que', 'haviapassado', 'e', 'mostrava-se', 'ao', 'natural', ',', 'fresco', ',', 'titilante', 'e', 'risonho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Estamos entendidos? perguntou a menina com a sisudez que não deixara em todo este diálogo'\n",
      "Tokens gerados: ['-', 'estamos', 'entendidos', '?', 'perguntou', 'a', 'menina', 'com', 'a', 'sisudez', 'que', 'nao', 'deixara', 'em', 'todo', 'este', 'dialogo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Você é uma feiticeirazinha, Aurélia; faz de mim o que quer'\n",
      "Tokens gerados: ['-', 'voce', 'e', 'uma', 'feiticeirazinha', ',', 'aurelia', 'faz', 'de', 'mim', 'o', 'que', 'quer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Reflita bem, meu tio'\n",
      "Tokens gerados: ['-', 'reflita', 'bem', ',', 'meu', 'tio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vou confiar-lhe meu segredo, um segredo que a ninguém neste mundo foi\n",
      "revelado, e que só Deus sabe'\n",
      "Tokens gerados: ['vou', 'confiar-lhe', 'meu', 'segredo', ',', 'um', 'segredo', 'que', 'a', 'ninguem', 'neste', 'mundo', 'foirevelado', ',', 'e', 'que', 'so', 'deus', 'sabe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se depois de conhecê-lo, o senhor não me quiser servir, ou não\n",
      "souber, eu jamais lhe perdoarei'\n",
      "Tokens gerados: ['se', 'depois', 'de', 'conhece-lo', ',', 'o', 'senhor', 'nao', 'me', 'quiser', 'servir', ',', 'ou', 'naosouber', ',', 'eu', 'jamais', 'lhe', 'perdoarei']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Pode confiar em mim sem susto o seu segredo, Aurélia, que mostrar-me-ei digno dessa confiança'\n",
      "Tokens gerados: ['-', 'pode', 'confiar', 'em', 'mim', 'sem', 'susto', 'o', 'seu', 'segredo', ',', 'aurelia', ',', 'que', 'mostrar-me-ei', 'digno', 'dessa', 'confianca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Creio, Sr'\n",
      "Tokens gerados: ['-', 'creio', ',', 'sr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lemos, e para tirar-lhe qualquer escrúpulo que por acaso o assalte, lhe juro pela\n",
      "memória de minha mãe, que se há para mim felicidade neste mundo, é somente esta que o senhor\n",
      "me pode dar'\n",
      "Tokens gerados: ['lemos', ',', 'e', 'para', 'tirar-lhe', 'qualquer', 'escrupulo', 'que', 'por', 'acaso', 'o', 'assalte', ',', 'lhe', 'juro', 'pelamemoria', 'de', 'minha', 'mae', ',', 'que', 'se', 'ha', 'para', 'mim', 'felicidade', 'neste', 'mundo', ',', 'e', 'somente', 'esta', 'que', 'o', 'senhorme', 'pode', 'dar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Disponha de mim'\n",
      "Tokens gerados: ['-', 'disponha', 'de', 'mim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aurélia parou um instante'\n",
      "Tokens gerados: ['aurelia', 'parou', 'um', 'instante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Conhece o Amaral?\n",
      "- Qual deles? perguntou o velho um tanto acanhado'\n",
      "Tokens gerados: ['-', 'conhece', 'o', 'amaral', '?', '-', 'qual', 'deles', '?', 'perguntou', 'o', 'velho', 'um', 'tanto', 'acanhado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Manuel Tavares do Amaral, empregado da alfândega; disse a moça consultando sua carteirinha'\n",
      "Tokens gerados: ['-', 'manuel', 'tavares', 'do', 'amaral', ',', 'empregado', 'da', 'alfandega', 'disse', 'a', 'moca', 'consultando', 'sua', 'carteirinha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tenha a bondade de tomar nota'\n",
      "Tokens gerados: ['tenha', 'a', 'bondade', 'de', 'tomar', 'nota']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não é rico, mas possui alguma cousa; ajustou o casamento da filha\n",
      "Adelaide com um moço que esteve ausente do Rio de Janeiro, e a quem ele ofereceu de dote trinta\n",
      "contos de réis'\n",
      "Tokens gerados: ['nao', 'e', 'rico', ',', 'mas', 'possui', 'alguma', 'cousa', 'ajustou', 'o', 'casamento', 'da', 'filhaadelaide', 'com', 'um', 'moco', 'que', 'esteve', 'ausente', 'do', 'rio', 'de', 'janeiro', ',', 'e', 'a', 'quem', 'ele', 'ofereceu', 'de', 'dote', 'trintacontos', 'de', 'reis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao proferir estas palavras sentiu-se um fugaz tremor na voz sempre tão límpida da moça, que logo\n",
      "após tomou um timbre ríspido'\n",
      "Tokens gerados: ['ao', 'proferir', 'estas', 'palavras', 'sentiu-se', 'um', 'fugaz', 'tremor', 'na', 'voz', 'sempre', 'tao', 'limpida', 'da', 'moca', ',', 'que', 'logoapos', 'tomou', 'um', 'timbre', 'rispido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Lemos ficara roxo de vermelho que já era; e para disfarçar o seu vexame remexia a cabeça mui\n",
      "desinquieto, com o dedo a repuxar e alargar o colarinho, como se este o sufocasse'\n",
      "Tokens gerados: ['o', 'lemos', 'ficara', 'roxo', 'de', 'vermelho', 'que', 'ja', 'era', 'e', 'para', 'disfarcar', 'o', 'seu', 'vexame', 'remexia', 'a', 'cabeca', 'muidesinquieto', ',', 'com', 'o', 'dedo', 'a', 'repuxar', 'e', 'alargar', 'o', 'colarinho', ',', 'como', 'se', 'este', 'o', 'sufocasse']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aurélia demorou um instante o seu frio olhar no semblante do velho; depois desviando com\n",
      "placidez a vista para fitá-la na página aberta de sua carteirinha, deu tempo ao tio de reportar-se, o\n",
      "que foi breve'\n",
      "Tokens gerados: ['aurelia', 'demorou', 'um', 'instante', 'o', 'seu', 'frio', 'olhar', 'no', 'semblante', 'do', 'velho', 'depois', 'desviando', 'complacidez', 'a', 'vista', 'para', 'fita-la', 'na', 'pagina', 'aberta', 'de', 'sua', 'carteirinha', ',', 'deu', 'tempo', 'ao', 'tio', 'de', 'reportar-se', ',', 'oque', 'foi', 'breve']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Lemos tinha o traquejo do mundo'\n",
      "Tokens gerados: ['o', 'lemos', 'tinha', 'o', 'traquejo', 'do', 'mundo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Trinta contos?'\n",
      "Tokens gerados: ['-', 'trinta', 'contos', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'observou ele'\n",
      "Tokens gerados: ['observou', 'ele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Já não é mau começo!\n",
      "Aurélia continuou:\n",
      "- É preciso quanto antes desmanchar este casamento'\n",
      "Tokens gerados: ['ja', 'nao', 'e', 'mau', 'comecoaurelia', 'continuou-', 'e', 'preciso', 'quanto', 'antes', 'desmanchar', 'este', 'casamento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A Adelaide deve casar com o Dr'\n",
      "Tokens gerados: ['a', 'adelaide', 'deve', 'casar', 'com', 'o', 'dr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Torquato\n",
      "Ribeiro de quem ela gosta'\n",
      "Tokens gerados: ['torquatoribeiro', 'de', 'quem', 'ela', 'gosta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ele é pobre; e por isso o pai o tem rejeitado, mas se o senhor\n",
      "assegurasse ao Amaral que esse moço tem de seu uns cinqüenta contos de réis, acha que ele\n",
      "recusaria?\n",
      "- Suponha que eu assegurasse isso'\n",
      "Tokens gerados: ['ele', 'e', 'pobre', 'e', 'por', 'isso', 'o', 'pai', 'o', 'tem', 'rejeitado', ',', 'mas', 'se', 'o', 'senhorassegurasse', 'ao', 'amaral', 'que', 'esse', 'moco', 'tem', 'de', 'seu', 'uns', 'cinquenta', 'contos', 'de', 'reis', ',', 'acha', 'que', 'elerecusaria', '?', '-', 'suponha', 'que', 'eu', 'assegurasse', 'isso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Donde sairia esse dinheiro?\n",
      "- Eu o darei com o maior prazer'\n",
      "Tokens gerados: ['donde', 'sairia', 'esse', 'dinheiro', '?', '-', 'eu', 'o', 'darei', 'com', 'o', 'maior', 'prazer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Mas, minha menina, para que nos vamos nós intrometer nos negócios alheios?\n",
      "- O senhor é bastante perspicaz para perceber aquilo que debalde lhe procuraria ocultar'\n",
      "Tokens gerados: ['-', 'mas', ',', 'minha', 'menina', ',', 'para', 'que', 'nos', 'vamos', 'nos', 'intrometer', 'nos', 'negocios', 'alheios', '?', '-', 'o', 'senhor', 'e', 'bastante', 'perspicaz', 'para', 'perceber', 'aquilo', 'que', 'debalde', 'lhe', 'procuraria', 'ocultar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prefiro\n",
      "confiar-me sem reservas à sua lealdade'\n",
      "Tokens gerados: ['prefiroconfiar-me', 'sem', 'reservas', 'a', 'sua', 'lealdade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A moça fez um esforço'\n",
      "Tokens gerados: ['a', 'moca', 'fez', 'um', 'esforco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Esse moço, que está justo com a Adelaide Amaral, é o homem a quem eu escolhi para meu\n",
      "marido'\n",
      "Tokens gerados: ['-', 'esse', 'moco', ',', 'que', 'esta', 'justo', 'com', 'a', 'adelaide', 'amaral', ',', 'e', 'o', 'homem', 'a', 'quem', 'eu', 'escolhi', 'para', 'meumarido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Já se vê que, não podendo pertencer a duas, é necessário que eu o dispute'\n",
      "Tokens gerados: ['ja', 'se', 've', 'que', ',', 'nao', 'podendo', 'pertencer', 'a', 'duas', ',', 'e', 'necessario', 'que', 'eu', 'o', 'dispute']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Conte comigo! acudiu o velho esfregando as mãos, como quem entrevia os benefícios que essa\n",
      "paixão prometia a um tutor hábil'\n",
      "Tokens gerados: ['-', 'conte', 'comigo', 'acudiu', 'o', 'velho', 'esfregando', 'as', 'maos', ',', 'como', 'quem', 'entrevia', 'os', 'beneficios', 'que', 'essapaixao', 'prometia', 'a', 'um', 'tutor', 'habil']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Esse moço'\n",
      "Tokens gerados: ['-', 'esse', 'moco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- O nome? perguntou o velho molhando a pena'\n",
      "Tokens gerados: ['-', 'o', 'nome', '?', 'perguntou', 'o', 'velho', 'molhando', 'a', 'pena']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aurélia fez um aceno de espera'\n",
      "Tokens gerados: ['aurelia', 'fez', 'um', 'aceno', 'de', 'espera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Este moço chegou ontem; é natural que trate agora dos preparativos para o casamento que está\n",
      "justo há perto de um ano'\n",
      "Tokens gerados: ['-', 'este', 'moco', 'chegou', 'ontem', 'e', 'natural', 'que', 'trate', 'agora', 'dos', 'preparativos', 'para', 'o', 'casamento', 'que', 'estajusto', 'ha', 'perto', 'de', 'um', 'ano']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O senhor deve procurá-lo quanto antes'\n",
      "Tokens gerados: ['o', 'senhor', 'deve', 'procura-lo', 'quanto', 'antes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Hoje mesmo'\n",
      "Tokens gerados: ['-', 'hoje', 'mesmo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- E fazer-lhe sua proposta'\n",
      "Tokens gerados: ['-', 'e', 'fazer-lhe', 'sua', 'proposta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estes arranjos são muito comuns no Rio de Janeiro'\n",
      "Tokens gerados: ['estes', 'arranjos', 'sao', 'muito', 'comuns', 'no', 'rio', 'de', 'janeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Estão-se fazendo todos os dias'\n",
      "Tokens gerados: ['-', 'estao-se', 'fazendo', 'todos', 'os', 'dias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- O senhor sabe melhor do que eu como se aviam estas encomendas de noivos'\n",
      "Tokens gerados: ['-', 'o', 'senhor', 'sabe', 'melhor', 'do', 'que', 'eu', 'como', 'se', 'aviam', 'estas', 'encomendas', 'de', 'noivos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Ora, ora!\n",
      "- Previno-o de que meu nome não deve figurar em tudo isto'\n",
      "Tokens gerados: ['-', 'ora', ',', 'ora-', 'previno-o', 'de', 'que', 'meu', 'nome', 'nao', 'deve', 'figurar', 'em', 'tudo', 'isto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Ah! quer conservar o incógnito?\n",
      "- Até o momento da apresentação'\n",
      "Tokens gerados: ['-', 'ah', 'quer', 'conservar', 'o', 'incognito', '?', '-', 'ate', 'o', 'momento', 'da', 'apresentacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto pode dizer quanto baste para que não suponham que\n",
      "se trata de alguma velha ou aleijada'\n",
      "Tokens gerados: ['entretanto', 'pode', 'dizer', 'quanto', 'baste', 'para', 'que', 'nao', 'suponham', 'quese', 'trata', 'de', 'alguma', 'velha', 'ou', 'aleijada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Percebo! exclamou o velho rindo'\n",
      "Tokens gerados: ['-', 'percebo', 'exclamou', 'o', 'velho', 'rindo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um casamento romântico'\n",
      "Tokens gerados: ['um', 'casamento', 'romantico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Não, senhor; nada de exagerações'\n",
      "Tokens gerados: ['-', 'nao', ',', 'senhor', 'nada', 'de', 'exageracoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Só tem licença para afirmar que a noiva não é velha nem feia'\n",
      "Tokens gerados: ['so', 'tem', 'licenca', 'para', 'afirmar', 'que', 'a', 'noiva', 'nao', 'e', 'velha', 'nem', 'feia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Quer preparar a surpresa?\n",
      "- Talvez'\n",
      "Tokens gerados: ['-', 'quer', 'preparar', 'a', 'surpresa', '?', '-', 'talvez']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os termos da proposta'\n",
      "Tokens gerados: ['os', 'termos', 'da', 'proposta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Com licença! Desde que deseja conservar o incógnito, não devo aparecer?\n",
      "Aurélia refletiu um instante:\n",
      "- Não quero que isto passe do senhor'\n",
      "Tokens gerados: ['-', 'com', 'licenca', 'desde', 'que', 'deseja', 'conservar', 'o', 'incognito', ',', 'nao', 'devo', 'aparecer', '?', 'aurelia', 'refletiu', 'um', 'instante-', 'nao', 'quero', 'que', 'isto', 'passe', 'do', 'senhor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Caso ele o reconheça como meu tio e tutor, não poderia o\n",
      "senhor convencê-lo que eu não tenho nisso a mínima parte? que é um negócio da família ou dos\n",
      "parentes?\n",
      "- Bem lembrado! Eu cá me arranjo; não tenha cuidado'\n",
      "Tokens gerados: ['caso', 'ele', 'o', 'reconheca', 'como', 'meu', 'tio', 'e', 'tutor', ',', 'nao', 'poderia', 'osenhor', 'convence-lo', 'que', 'eu', 'nao', 'tenho', 'nisso', 'a', 'minima', 'parte', '?', 'que', 'e', 'um', 'negocio', 'da', 'familia', 'ou', 'dosparentes', '?', '-', 'bem', 'lembrado', 'eu', 'ca', 'me', 'arranjo', 'nao', 'tenha', 'cuidado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Os termos da proposta devem ser estes; atenda bem'\n",
      "Tokens gerados: ['-', 'os', 'termos', 'da', 'proposta', 'devem', 'ser', 'estes', 'atenda', 'bem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A família da tal moça misteriosa deseja casála com separação de bens, dando ao noivo a quantia de cem contos de réis de dote'\n",
      "Tokens gerados: ['a', 'familia', 'da', 'tal', 'moca', 'misteriosa', 'deseja', 'casala', 'com', 'separacao', 'de', 'bens', ',', 'dando', 'ao', 'noivo', 'a', 'quantia', 'de', 'cem', 'contos', 'de', 'reis', 'de', 'dote']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se não bastarem\n",
      "cem e ele exigir mais, será o dote de du--zentos'\n",
      "Tokens gerados: ['se', 'nao', 'bastaremcem', 'e', 'ele', 'exigir', 'mais', ',', 'sera', 'o', 'dote', 'de', 'du', '--', 'zentos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Hão de bastar'\n",
      "Tokens gerados: ['-', 'hao', 'de', 'bastar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não tenha dúvida'\n",
      "Tokens gerados: ['nao', 'tenha', 'duvida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Em todo o caso quero que o senhor compreenda bem o meu pensamento'\n",
      "Tokens gerados: ['-', 'em', 'todo', 'o', 'caso', 'quero', 'que', 'o', 'senhor', 'compreenda', 'bem', 'o', 'meu', 'pensamento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desejo, como é natural,\n",
      "obter o que pretendo, o mais barato possível; mas o essencial é obter; e portanto até metade do que\n",
      "possuo, não faço questão de preço'\n",
      "Tokens gerados: ['desejo', ',', 'como', 'e', 'natural', ',', 'obter', 'o', 'que', 'pretendo', ',', 'o', 'mais', 'barato', 'possivel', 'mas', 'o', 'essencial', 'e', 'obter', 'e', 'portanto', 'ate', 'metade', 'do', 'quepossuo', ',', 'nao', 'faco', 'questao', 'de', 'preco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É a minha felicidade que vou comprar'\n",
      "Tokens gerados: ['e', 'a', 'minha', 'felicidade', 'que', 'vou', 'comprar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estas últimas palavras, a moça proferiu-as com uma inde-finível expressão'\n",
      "Tokens gerados: ['estas', 'ultimas', 'palavras', ',', 'a', 'moca', 'proferiu-as', 'com', 'uma', 'inde-finivel', 'expressao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Não será caro?\n",
      "- Oh! exclamou Aurélia, eu daria por ela toda a minha riqueza'\n",
      "Tokens gerados: ['-', 'nao', 'sera', 'caro', '?', '-', 'oh', 'exclamou', 'aurelia', ',', 'eu', 'daria', 'por', 'ela', 'toda', 'a', 'minha', 'riqueza']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Outras a têm de graça, que lhes vem\n",
      "diretamente do céu'\n",
      "Tokens gerados: ['outras', 'a', 'tem', 'de', 'graca', ',', 'que', 'lhes', 'vemdiretamente', 'do', 'ceu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas não me posso queixar, pois negando-me esse bem, Deus compadeceu-se\n",
      "de mim, e enviou-me quando menos esperava tamanha herança para que eu possa realizar a\n",
      "aspiração de minha vida'\n",
      "Tokens gerados: ['mas', 'nao', 'me', 'posso', 'queixar', ',', 'pois', 'negando-me', 'esse', 'bem', ',', 'deus', 'compadeceu-sede', 'mim', ',', 'e', 'enviou-me', 'quando', 'menos', 'esperava', 'tamanha', 'heranca', 'para', 'que', 'eu', 'possa', 'realizar', 'aaspiracao', 'de', 'minha', 'vida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não dizem que o dinheiro traz todas as venturas?\n",
      "- A maior ventura que dá o dinheiro é possuí-lo; as outras são secundárias, disse o Lemos como\n",
      "entendido na matéria'\n",
      "Tokens gerados: ['nao', 'dizem', 'que', 'o', 'dinheiro', 'traz', 'todas', 'as', 'venturas', '?', '-', 'a', 'maior', 'ventura', 'que', 'da', 'o', 'dinheiro', 'e', 'possui-lo', 'as', 'outras', 'sao', 'secundarias', ',', 'disse', 'o', 'lemos', 'comoentendido', 'na', 'materia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aurélia, que um instante se deixara arrebatar pelo sentimento, voltava ao tom frio e refletido com\n",
      "que havia discutido até ali a questão de seu futuro'\n",
      "Tokens gerados: ['aurelia', ',', 'que', 'um', 'instante', 'se', 'deixara', 'arrebatar', 'pelo', 'sentimento', ',', 'voltava', 'ao', 'tom', 'frio', 'e', 'refletido', 'comque', 'havia', 'discutido', 'ate', 'ali', 'a', 'questao', 'de', 'seu', 'futuro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Falta-me ainda, meu tio, recomendar-lhe um ponto'\n",
      "Tokens gerados: ['-', 'falta-me', 'ainda', ',', 'meu', 'tio', ',', 'recomendar-lhe', 'um', 'ponto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A palavra, além de esquecer, está sujeita a\n",
      "equívocos'\n",
      "Tokens gerados: ['a', 'palavra', ',', 'alem', 'de', 'esquecer', ',', 'esta', 'sujeita', 'aequivocos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não seria possível tratar este negócio por escrito?\n",
      "- Passar o sujeito um papel?'\n",
      "Tokens gerados: ['nao', 'seria', 'possivel', 'tratar', 'este', 'negocio', 'por', 'escrito', '?', '-', 'passar', 'o', 'sujeito', 'um', 'papel', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Certamente, mas se ele roer a corda, não há meio de obrigá-lo a\n",
      "casar'\n",
      "Tokens gerados: ['certamente', ',', 'mas', 'se', 'ele', 'roer', 'a', 'corda', ',', 'nao', 'ha', 'meio', 'de', 'obriga-lo', 'acasar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Não importa'\n",
      "Tokens gerados: ['-', 'nao', 'importa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eu prefiro confiar-me à honra dessa pessoa, antes do que aos tribunais'\n",
      "Tokens gerados: ['eu', 'prefiro', 'confiar-me', 'a', 'honra', 'dessa', 'pessoa', ',', 'antes', 'do', 'que', 'aos', 'tribunais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Com uma\n",
      "obrigação em que ele empenhe sua palavra ficarei tranqüila'\n",
      "Tokens gerados: ['com', 'umaobrigacao', 'em', 'que', 'ele', 'empenhe', 'sua', 'palavra', 'ficarei', 'tranquila']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Há de se arranjar'\n",
      "Tokens gerados: ['-', 'ha', 'de', 'se', 'arranjar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Eis o que espero de sua amizade, meu tio'\n",
      "Tokens gerados: ['-', 'eis', 'o', 'que', 'espero', 'de', 'sua', 'amizade', ',', 'meu', 'tio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Lemos deixou passar a ironia que acentuara a palavra amizade, e esticou a prumo diante dos\n",
      "olhos e contra a luz, a folha de papel em que tomara suas notas'\n",
      "Tokens gerados: ['o', 'lemos', 'deixou', 'passar', 'a', 'ironia', 'que', 'acentuara', 'a', 'palavra', 'amizade', ',', 'e', 'esticou', 'a', 'prumo', 'diante', 'dosolhos', 'e', 'contra', 'a', 'luz', ',', 'a', 'folha', 'de', 'papel', 'em', 'que', 'tomara', 'suas', 'notas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Vejamos!'\n",
      "Tokens gerados: ['-', 'vejamos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tavares do Amaral, empregado da alfândega'\n",
      "Tokens gerados: ['tavares', 'do', 'amaral', ',', 'empregado', 'da', 'alfandega']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'a filha D'\n",
      "Tokens gerados: ['a', 'filha', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Adelaide, trinta contos de\n",
      "réis'\n",
      "Tokens gerados: ['adelaide', ',', 'trinta', 'contos', 'dereis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Dr'\n",
      "Tokens gerados: ['o', 'dr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Torquato Ribeiro'\n",
      "Tokens gerados: ['torquato', 'ribeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'garantir cinqüenta'\n",
      "Tokens gerados: ['garantir', 'cinquenta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O outro'\n",
      "Tokens gerados: ['o', 'outro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'de cem até duzentos'\n",
      "Tokens gerados: ['de', 'cem', 'ate', 'duzentos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Só me falta o\n",
      "nome'\n",
      "Tokens gerados: ['so', 'me', 'falta', 'onome']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aurélia tirou da carteirinha o bilhete de visita e apresentou-o ao tutor'\n",
      "Tokens gerados: ['aurelia', 'tirou', 'da', 'carteirinha', 'o', 'bilhete', 'de', 'visita', 'e', 'apresentou-o', 'ao', 'tutor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Como este se preparasse para\n",
      "repetir em alta voz o nome, ela o atalhou com a palavra breve e imperativa que às vezes lhe\n",
      "crispava os lábios'\n",
      "Tokens gerados: ['como', 'este', 'se', 'preparasse', 'pararepetir', 'em', 'alta', 'voz', 'o', 'nome', ',', 'ela', 'o', 'atalhou', 'com', 'a', 'palavra', 'breve', 'e', 'imperativa', 'que', 'as', 'vezes', 'lhecrispava', 'os', 'labios']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Escreva!\n",
      "O velhinho copiou as indicações que havia no cartão e o restituiu'\n",
      "Tokens gerados: ['-', 'escrevao', 'velhinho', 'copiou', 'as', 'indicacoes', 'que', 'havia', 'no', 'cartao', 'e', 'o', 'restituiu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Nada mais?\n",
      "- Nada, senão repetir-lhe ainda uma vez que entreguei em suas mãos a única felicidade que Deus\n",
      "me reserva neste mundo'\n",
      "Tokens gerados: ['-', 'nada', 'mais', '?', '-', 'nada', ',', 'senao', 'repetir-lhe', 'ainda', 'uma', 'vez', 'que', 'entreguei', 'em', 'suas', 'maos', 'a', 'unica', 'felicidade', 'que', 'deusme', 'reserva', 'neste', 'mundo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A moça proferiu estas palavras com um tom de profunda convicção que penetrou o bonacho\n",
      "ceticismo do velho'\n",
      "Tokens gerados: ['a', 'moca', 'proferiu', 'estas', 'palavras', 'com', 'um', 'tom', 'de', 'profunda', 'conviccao', 'que', 'penetrou', 'o', 'bonachoceticismo', 'do', 'velho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Há de ser muito feliz, eu lhe garanto'\n",
      "Tokens gerados: ['-', 'ha', 'de', 'ser', 'muito', 'feliz', ',', 'eu', 'lhe', 'garanto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Dê-me esta felicidade, que eu tanto invejo; eu lhe darei da que me sobra'\n",
      "Tokens gerados: ['-', 'de-me', 'esta', 'felicidade', ',', 'que', 'eu', 'tanto', 'invejo', 'eu', 'lhe', 'darei', 'da', 'que', 'me', 'sobra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '- Conte comigo, Aurélia'\n",
      "Tokens gerados: ['-', 'conte', 'comigo', ',', 'aurelia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O velhinho apertou a mão da moça, que lhe tocara o coração com a última promessa e retirou-se'\n",
      "Tokens gerados: ['o', 'velhinho', 'apertou', 'a', 'mao', 'da', 'moca', ',', 'que', 'lhe', 'tocara', 'o', 'coracao', 'com', 'a', 'ultima', 'promessa', 'e', 'retirou-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando chegou a casa, ainda o Lemos não estava de todo restabelecido do atordoamento que\n",
      "sofrera'\n",
      "Tokens gerados: ['quando', 'chegou', 'a', 'casa', ',', 'ainda', 'o', 'lemos', 'nao', 'estava', 'de', 'todo', 'restabelecido', 'do', 'atordoamento', 'quesofrera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_senhora_jose_de_alencar_cap_4.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Caminhos\n",
    "caminho_txts = \"data/caps\"\n",
    "caminho_saida_pasta = \"data/caps_processados\"\n",
    "os.makedirs(caminho_saida_pasta, exist_ok=True)\n",
    "\n",
    "for arquivo_nome in os.listdir(caminho_txts):\n",
    "    dumped_dict = {}\n",
    "    dumped_dict['titulo'], dumped_dict['autor'] = extrair_titulo_autor(arquivo_nome)\n",
    "    dumped_dict['tokens'] = [] # Esta será uma lista de listas de tokens\n",
    "    caminho_arquivo = os.path.join(caminho_txts, arquivo_nome)\n",
    "    if os.path.isfile(caminho_arquivo):\n",
    "        with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo:\n",
    "            texto_pre = arquivo.read()\n",
    "            frases = texto_pre.split('.') # Divide o texto em segmentos baseados no ponto final\n",
    "\n",
    "            for frase_segmento in frases: # Renomeei para clareza\n",
    "                frase_limpa_para_tokenizar = frase_segmento.strip()\n",
    "                if frase_limpa_para_tokenizar: # Pula segmentos vazios\n",
    "                    print(f\"\\nProcessando frase original (após split e strip): '{frase_limpa_para_tokenizar}'\") # DEBUG\n",
    "\n",
    "                    # A função limpar_e_tokenizar_texto deve retornar uma lista plana de tokens\n",
    "                    tokens_da_frase = limpar_e_tokenizar_texto(frase_limpa_para_tokenizar)\n",
    "\n",
    "                    print(f\"Tokens gerados: {tokens_da_frase}\") # DEBUG\n",
    "                    print(f\"Tipo dos tokens gerados: {type(tokens_da_frase)}\") # DEBUG\n",
    "                    if tokens_da_frase and isinstance(tokens_da_frase[0], list):\n",
    "                        print(f\"ALERTA: Parece que os tokens estão aninhados desnecessariamente: {tokens_da_frase}\") # DEBUG\n",
    "\n",
    "                    # Adiciona a lista de tokens (que deve ser plana) à lista principal\n",
    "                    dumped_dict['tokens'].append(tokens_da_frase)\n",
    "\n",
    "            caminho_saida = os.path.join(caminho_saida_pasta, \"preprocessado_\" + arquivo_nome.replace('.txt', '.json'))\n",
    "            with open(caminho_saida, 'w', encoding='utf-8') as arquivo_saida:\n",
    "                json.dump(dumped_dict, arquivo_saida, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"\\nTokens estruturados salvos em: {caminho_saida}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964421f0",
   "metadata": {},
   "source": [
    "## Extração de features\n",
    "\n",
    "Após a etapa de pré processamento, iniciaremos a extração de features. Definimos alguns grupos de features que acreditamos serem úteis para o nosso projeto, sendo elas: \n",
    "#### Linguísticas (POS, gramática, estilo):\n",
    "- Frequência relativa de substantivos, verbos, adjetivos, advérbios\n",
    "- Frequência de tempos verbais (passado, presente, futuro)\n",
    "- Frequência de pronomes pessoais (\"eu\", \"nós\", etc.)\n",
    "- Frequência de artigos definidos (\"o\", \"a\") e indefinidos (\"um\", \"uma\")\n",
    "- Frequência de conjunções (\"e\", \"mas\", \"porque\")\n",
    "- Número médio de palavras por frase\n",
    "- Comprimento médio das palavras\n",
    "- Variabilidade de comprimento de palavras (desvio padrão)\n",
    "- Número de frases curtas vs. frases longas\n",
    "- Uso de voz passiva (frases com \"foi feito\", \"era conhecido\", etc.)\n",
    "- Proporção de substantivos abstratos vs. concretos (se você quiser ser avançado)\n",
    "\n",
    "#### Lexicais (vocabulário):\n",
    "- Número total de palavras (tokens)\n",
    "- Número de palavras únicas (tipos)\n",
    "- Índice de riqueza lexical: tipos / tokens\n",
    "- Frequência de palavras raras (pouco frequentes em um corpus comum)\n",
    "- Frequência de palavras comuns (ex: palavras do top-1000 do português)\n",
    "- Uso de palavras sofisticadas (frequência de palavras acima de certo número de sílabas)\n",
    "\n",
    "#### Semânticas (significado):\n",
    "- Similaridade semântica média entre frases (usando embeddings como Word2Vec, BERT, etc.)\n",
    "- Distância semântica entre parágrafos\n",
    "- Frequência de negação (\"não\", \"nunca\", \"jamais\")\n",
    "- Sentimento médio do texto (positivo/negativo/neutro)\n",
    "- Frequência de emoções específicas (raiva, alegria, tristeza, surpresa)\n",
    "\n",
    "#### Estilísticas:\n",
    "- Uso de metáforas, hipérboles (difícil, mas dá pra tentar detectar por padrões)\n",
    "- Uso de citações diretas (\"...\")\n",
    "- Frequência de perguntas feitas (\"?\")\n",
    "- Uso de primeira pessoa (\"eu\", \"meu\") vs. terceira pessoa (\"ele\", \"ela\")\n",
    "\n",
    "#### Estatísticas avançadas:\n",
    "- TF-IDF de palavras ou n-grams (unigrams, bigrams, trigrams)\n",
    "- Topic Modeling (LDA) — para ver quais temas o autor tende a abordar\n",
    "- Frequência de erros ortográficos (se tiver corpus sujo)\n",
    "- Medidas de entropia do texto (quanto o texto é previsível)\n",
    "Abaixo vamos buscar extrair features lexicais e semânticas dos textos. A partir disso, podemos comparar cada texto buscando entender melhor o que define os padrões na escrita de um autor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8c088c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho para os textos processados\n",
    "caminho_textos_processados = \"data/caps_processados\"\n",
    "\n",
    "def normalizar(valor, minimo, maximo, escala_min=1, escala_max=5):\n",
    "    valor_normalizado = (valor - minimo) / (maximo - minimo)\n",
    "    valor_normalizado = max(0, min(1, valor_normalizado)) \n",
    "    return escala_min + valor_normalizado * (escala_max - escala_min)\n",
    "\n",
    "def diversidade_lexical(tokens):\n",
    "    tokens_alfabeticos = [t.lower() for t in tokens if re.match(r'[a-zA-ZáàâãéèêíìóòôõúùûçÁÀÂÃÉÈÊÍÌÓÒÔÕÚÙÛÇ]+$', t)]\n",
    "    num_tokens = len(tokens_alfabeticos)\n",
    "    num_types = len(set(tokens_alfabeticos))\n",
    "\n",
    "    # Inicializa todas as variáveis com valor padrão\n",
    "    ttr = 0\n",
    "    hapax_legomena = 0\n",
    "    proporcao_hapax = 0\n",
    "    score_ttr = 0\n",
    "    score_hapax = 0\n",
    "    score_diversidade = 1\n",
    "    comprimento_medio_sentencas = 0\n",
    "    comprimento_medio_palavras = 0\n",
    "    media_stopwords = 0\n",
    "\n",
    "    if num_tokens > 0:\n",
    "        ttr = num_types / num_tokens\n",
    "        contador = Counter(tokens_alfabeticos)\n",
    "        hapax_legomena = sum(1 for palavra, freq in contador.items() if freq == 1)\n",
    "        proporcao_hapax = hapax_legomena / num_tokens\n",
    "        score_ttr = normalizar(ttr, minimo=0.0, maximo=1.0)\n",
    "        score_hapax = normalizar(proporcao_hapax, minimo=0.0, maximo=1.0)\n",
    "        score_diversidade = (score_ttr + score_hapax) / 2\n",
    "        comprimento_medio_sentencas = sum(len(token) for token in tokens_alfabeticos) / num_tokens\n",
    "        comprimento_medio_palavras = sum(len(token) for token in tokens_alfabeticos) / num_tokens\n",
    "        stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "        media_stopwords = sum(1 for token in tokens_alfabeticos if token in stopwords) / num_tokens\n",
    "\n",
    "    return {\n",
    "        'ttr': ttr,\n",
    "        'score_ttr': score_ttr,\n",
    "        'comprimento_medio_sentencas': comprimento_medio_sentencas,\n",
    "        'comprimento_medio_palavras': comprimento_medio_palavras,\n",
    "        'media_stopwords': media_stopwords,\n",
    "        'hapax_legomena': hapax_legomena,\n",
    "        'proporcao_hapax': proporcao_hapax,\n",
    "        'score_diversidade': score_diversidade,\n",
    "        'score_palavras_unicas': score_hapax\n",
    "    }\n",
    "\n",
    "# Iterar sobre os textos processados e calcular a diversidade lexical\n",
    "resultados_por_texto = []\n",
    "for arquivo_nome in os.listdir(caminho_textos_processados):\n",
    "    caminho_arquivo = os.path.join(caminho_textos_processados, arquivo_nome)\n",
    "    if os.path.isfile(caminho_arquivo) and arquivo_nome.endswith('.json'):\n",
    "        with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo:\n",
    "            dados = json.load(arquivo)\n",
    "            tokens = [token for sublist in dados['tokens'] for token in sublist]\n",
    "            resultado = diversidade_lexical(tokens)\n",
    "            resultado['titulo'] = dados.get('titulo', 'Desconhecido')\n",
    "            resultados_por_texto.append(resultado)\n",
    "\n",
    "# Plotar os resultados\n",
    "titulos = [r['titulo'] for r in resultados_por_texto]\n",
    "scores_diversidade = [r['score_diversidade'] for r in resultados_por_texto]\n",
    "ttrs = [r['ttr'] for r in resultados_por_texto]\n",
    "proporcoes_hapax = [r['proporcao_hapax'] for r in resultados_por_texto]\n",
    "\n",
    "# Criar um DataFrame com os resultados\n",
    "df_resultados = pd.DataFrame(resultados_por_texto)\n",
    "\n",
    "# Reordenar as colunas para que o título seja o primeiro\n",
    "colunas_reordenadas = ['titulo'] + [coluna for coluna in df_resultados.columns if coluna != 'titulo']\n",
    "df_resultados = df_resultados[colunas_reordenadas]\n",
    "\n",
    "# Salvar o DataFrame em um arquivo CSV\n",
    "caminho_csv = os.path.join(caminho_textos_processados, 'resultados.csv')\n",
    "df_resultados.to_csv(caminho_csv, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6f2990f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Tokenized sentence 0: tensor([[  101,  3179,   596, 18540,   593, 22283,   176, 12444,  8925,  3769,\n",
      "         14876,   562,   423,   905,   247,   291,   423,  1338,   117,  3413,\n",
      "           122,   117,   176,   240,   151,  4276,  3198,   458,  1247,   146,\n",
      "          7343,  5774,   291,   123,  7122,  1386,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101, 15581,   146,  1700, 21928,  1547,   662,   934,   423,  5774,\n",
      "           117,   924,   996,   303,   143,   311,  6777,   123, 14396,  3575,\n",
      "         11637, 22280,   123,   681,   122,   179,  2779,   229, 22280,  7206,\n",
      "         12762,   389,  1368,   975,  5242, 22280,   117,   449,   222,   975,\n",
      "          5242, 22280,  1368,   117,   221,  1977,   123,  5036, 22278,   262,\n",
      "          1342, 10420,   303,   123,  1448,   122,   179,   146,  2685,  1282,\n",
      "           322,  1016,   325, 22231,   175,   122,   325,  1160,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   390,  7360,   117,   179, 14619,   210,  5222,   123,   327,\n",
      "          1386,   117,   229, 22280,   123,   429,   202,  2655,   373,   117,\n",
      "           449,   202,  6365,   990,  2028, 12357,   420,   860,  1722,   122,\n",
      "           146,  2377,   154,  4432,   303,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,  6775,  3413,   117,  2468,  1235, 22283,   260,   924,  2856,\n",
      "           180,  1373,   125,   230,  7250,   118, 14258,   171,   454,   125,\n",
      "          1600,   125,  3100, 22315,   117,   229,  7122, 11791,  1690,  5105,\n",
      "           125,   329, 10848,  3301,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   978,  7226, 14958,   122,  1256,   481,   117,  1984,  3103,\n",
      "           122,  4041, 11097, 22281,   117,   495,   969,  3120,   117,  1776,\n",
      "         22278,  1384,   125,  9967,   850,  9342,   122,   572, 22283,  8582,\n",
      "           320,  2812,  2550,   247,   240,  7981,  3667,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,  7981,  3667,  3295,   122,   179,   229, 22280,  3363,  6536,\n",
      "          2798,  2043,   128,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,  5103,   289,   179,  5334,   619,  2010, 11092,   364,   256,\n",
      "          2010,   230, 18313,   508, 14689,  6915,   117, 12448,   122,  4060,\n",
      "           117,   316, 22280,  4060,   122,   316, 22280, 12448,   117,   179,\n",
      "          2628,   222, 13340, 10692,   244, 22281,   180,   169,  8388,  5314,\n",
      "           123, 14326,   840,  4283, 22278, 21613, 22278,  3138,   202,  6789,\n",
      "           179, 17782,   456,   123, 12241,   125,  7122,   144,   256,  2010,\n",
      "          1112,   962, 22281,   117,   179,   146,  7422,  7485,   143,   117,\n",
      "         17080, 14415,   117,   962, 22281,   706,   145, 20222,   140,   170,\n",
      "          1039,   179,   123,  3402,  4048,  2765,  5334,  2537,   123,  5112,\n",
      "         10984,  1266,   415,   125,   222,   298,   325, 15152, 22281, 12962,\n",
      "           179,   376,  6320,   243,   123,  6021, 21948,  1076,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   860,   388, 10978,   342,   117,  3769,  3746,   470,   171,\n",
      "          2992, 22288,   117, 11665, 15553, 15357, 22281,   179,  9312, 22287,\n",
      "           146,  5580,   271,   222,  3960,   269, 20675, 22279, 22280,   117,\n",
      "          2745,  2291,   122,   123,  4678,  4793, 22278,   122,   223,   179,\n",
      "          2036,   577, 22283,   123,  3402,   260,   325, 14353,   138,  3837,\n",
      "         13808, 22281,  2745,  1257,   122,   222, 18563,  1608,  7406,  2248,\n",
      "           320,  7275, 20676,  3192,  1862,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101, 22354,  4062,   122, 11293,  3695,   229, 22280,   117,   229,\n",
      "         22280,   311,  3456,  2040, 22280,   366,  4698,  1178, 17377, 22281,\n",
      "           179,  2036,  1114,   244,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   122,   262,  1016,   179,  1120,  3897,   151,  4485,   249,\n",
      "           565,   298, 17080,  1564,   262,  1016,   179,   311, 16924,   244,\n",
      "           221,   146, 19933,  1688,  9997, 22284, 20784,   125,   607, 22287,\n",
      "          4362,   117,   834, 14114,   255,   562,  2798,   260,   623,  3593,\n",
      "           171,   390,   303,   905, 22279,   117,   449, 18827,   243,   122,\n",
      "          4525, 22279,   339,   117,   271,  1977,   176, 20813,  1373,   171,\n",
      "           734,   154,  2177,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,  1373,   122, 10363, 19356,   286,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101, 12061,   118,   311,   925, 11368,  4167,   291,  1027,  1101,\n",
      "           117,   420,  2859,  1510, 22281, 17704, 22281,   117,  7122,   925,\n",
      "           148,  6459,   387,   117, 12222,   170,   146,   144,  2577, 22287,\n",
      "           117,  2010,   123,  2267,   117,   222, 19449,   342,   118,   171,\n",
      "           118,  5488,   117,  2010,   122,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  5900,  4500,  3292,   180,  8156,   123,  1695,  7707,   641,\n",
      "          3533, 22287,   495,   123,  2914, 17704,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,  1519,   175, 22287,   118,   176,   125,  4945,   179,  1921,\n",
      "           622,  4029,   148,   117,   744,   179,   229, 22280,  1517,   539,\n",
      "           117,  1852, 15502,  2332,   171,   179,   260,  1517,  1922,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   122,  3295,   117,  1852, 15502,   325,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   229, 22280,  2826, 22280,   179,   176,   505, 17573,   236,\n",
      "           117,   229, 22280,  2826, 22280,   179,   176,  5308, 22281, 11448,\n",
      "         10432,   423,  1690, 22280,   117,  5572,  4617, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[ 101, 2798,  146, 7343,  463,  373,  495, 5664, 7532, 5866,  478,  102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,   222,   969,  7700, 22280,   179,  2468,   364, 13793, 22281,\n",
      "         14958,   122,  1256,   481,   117,   229, 22280,  4048,   179,  2259,\n",
      "         22278,   173,   898,   944,   259,  2769,   125,   230, 12424, 19181,\n",
      "         22278,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[ 101,  122, 3433, 5790,  314,  117,  146,  179, 1528, 5572,  508,  123,\n",
      "         1921,  622, 4029,  148,  495, 1183,  539,  118, 1340,  102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   125,   766,   117,   123,  3049,   289,   364,   180,  9461,\n",
      "           117,   170,   259,  5708,   687,  9412,   442,   117,   123,  9463,\n",
      "           420, 12764,  5013,   117,   123, 12448, 17704,  3002,  4207,  3960,\n",
      "         22282,   229,  7122,  6386,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,  2010,  4807,  4807, 10355, 11631,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,   122,   123, 12223,  3391, 13793,  3914,   117,   271,   260,\n",
      "          2992, 12268, 22281,   179,   222, 20676,   130,  8022,   175,  4970,\n",
      "          5510, 19927,   684,   146,  5557,  1065,   146,  1330,  2291,   138,\n",
      "         13436,   138, 21294,   117,   834,  4565,   339,   366,  1315,   986,\n",
      "           122,   298,  3492,   117,  2010,   123, 12223,  3391, 13793,  2990,\n",
      "         17704, 14619,   210,  5557,  1499,   141,   498,   259, 16863,   942,\n",
      "          4400,  2684,   260, 13436,   138,   125,   230,  5566, 22278, 14134,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[ 101, 5308,  118, 1084,  925, 1084,  925, 5835,  325, 1373,  266,  925,\n",
      "         5835,  625, 2779,  311,  398,  569,  307,  712, 1867,  481,  102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  2535,   117, 18691,  8111, 20885,  3514,   117, 11637,  1780,\n",
      "           117,  8362,   214,   259,  7078,   942,   366, 22186, 22281,   117,\n",
      "           260,  3887, 22281,  7086,   298,  2217,   117,   123,  9856,   179,\n",
      "         14619,  2758,   266,   529,  7438,   125,   964, 21102,   124,  1657,\n",
      "         22278,  1690,  5105,   117,   122,   146,  4081, 15777,   379,   326,\n",
      "           125,   230,  9324,   252,   179,   222, 14283,  5872, 22282,   418,\n",
      "           870, 17598,  1084,  1796,   117,   123,  4303,   125,   222, 16165,\n",
      "           458,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,  3436, 22280,   118,  7707,   179,  1921,  7816,   180,  1386,\n",
      "           262,   785,  1528, 12448,   171,   179,  4207, 11639,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   125,  4863, 19038,   173,  4271,  2080,   123,   333, 15020,\n",
      "          6044,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   123,  1069,  1048,   694,  2034,   256,   118,   311,   202,\n",
      "         11979,   117,   170,  7226,  6554,   382,   125,  5926,   956,   508,\n",
      "           117,   188,   256,   151,   118,   176,   118,   311,   123,  5594,\n",
      "          3292,   117,  2779,   273,   351,   123, 13175,   292, 15578,   304,\n",
      "           122,  6496,   117,   122,   146,  1831,  5057,   118,   176,   118,\n",
      "           311, 14055,   154,   117,   122,  5028,   117,   122,  1340,   243,\n",
      "           117,   122,  5664,  3963,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,  1623,   172,   125,   230, 21484,  7595,   449,   176,  2036,\n",
      "          1996, 22282,   179,   262,  1528,   123, 21484,  7595,   117,   171,\n",
      "           179,   230,  3138, 16318,  6044, 22279,   960,   117,   123,  2318,\n",
      "           180,  7122,  1386,   117,   122,   668,  4812,   179,   146, 14624,\n",
      "           311,   229, 22280,  3960,   151,   117,   122, 17226,   122,  3295,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101, 17891,  5924,   118,  7707,   809,  4135,   146,  1652,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[ 101, 4031, 1537,  118,  146,  240,  898,  653,  102]])\n",
      "DEBUG ======================\n",
      " len vetores 30 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.02510460247544686\n",
      " Coesão Score Final: 0.5125523012377234\n",
      " Conectivos encontrados: ['e', 'mas', 'assim', 'quando', 'se', 'caso', 'isto e', 'todavia', 'ainda que', 'nem', 'ou', 'ora', 'quer', 'seja', 'como', 'para que', 'assim que', 'nao so']\n",
      " Número de conectivos: 18\n",
      " Número de sentenças: 30\n",
      "======================\n",
      "Resultados para preprocessado_bras_cubas_machado_cap_1.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'assim', 'quando', 'se', 'caso', 'isto e', 'todavia', 'ainda que', 'nem', 'ou', 'ora', 'quer', 'seja', 'como', 'para que', 'assim que', 'nao so'], 'num_conectivos': 18, 'proporcao_conectivos': 0.025, 'similaridade_media': np.float64(1.0), 'num_sentencas': 30}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,   170,  3901,   117,   222,   644,   125,  1062,   252,   117,\n",
      "          6743,   123,  7282,   159,   229,  1690,  5105,   117, 15231, 18417,\n",
      "           118,   176,   118,   311,   230,  3138,   202,   338, 10651,   247,\n",
      "           179,  2779,   978,   202,   943,  2152,   157,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   230,   576, 15231, 21794,   117,  3033,   123,  4332,   289,\n",
      "          4765,   117,   123,   337, 13847, 22282,   117,   123,  1434,   260,\n",
      "           325,  7096, 13199,  5351,   342,   966,   125, 13898,  2762,   117,\n",
      "           179,   122,   668,  4812,  3960, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[ 101, 2779, 1114,  244,  118,  311, 2765,  123, 4108,  266,  118, 1084,\n",
      "          102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   695,   373,   117,  2002,   222,   739, 11602,   117, 16621,\n",
      "           259,  4332,   942,   122,   260, 11351,   117,  2684,  5357,   123,\n",
      "           547,   125,   222,  3043,   272, 17452,   124,   118,   311,   291,\n",
      "          7427,   157,   118,   437,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,  1921,  3138,   495,  3874,  1528,   179,   123,  2401,   194,\n",
      "           304, 22280,   125,   222, 10531,   310, 18563,  1608,   117,   222,\n",
      "          4276,   966,   183,  6267, 15604,   451,   181, 22284,   322,   303,\n",
      "           117, 10243,   123, 19094,  2048,   123,  7308,   949,  1601,  8551,\n",
      "           304,  8676,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   229,   766,   478, 22280,   125, 15627, 22280,   179,   318,\n",
      "         13793,  1668, 10009,   117,   849,   244,   123, 12171,   304, 22280,\n",
      "           171,  1161,   221,  1966,  2453,   117, 19033,  7045, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101, 17226,   117,   229, 22280,  2128,  3897,   151,   128,  3667,\n",
      "           260, 12816, 11977,  2264,  1313,   179, 21494, 17903,   180,  2289,\n",
      "           232, 22280,   125,   222,  3576,   125, 18577,  4246, 22280, 12186,\n",
      "         22281,  3997,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,  2535,   117,   240,   210,   117,   179, 12044,   329,   171,\n",
      "          1342,  1341,   180,  1069,   117, 21174, 15982,   159,  2745,   146,\n",
      "           179,   311, 14451,   411,   456,  1953,   262,   146, 10303,   125,\n",
      "           792, 20615, 22281,   538,  6349,   117,  9251,   207,   117, 20105,\n",
      "           382,   117, 21734, 22281,   117,   122,   194,  7439,   529, 11314,\n",
      "         14271, 13808, 22281,   171, 11935,   635,   117,  3769,  1510, 22281,\n",
      "          3724,  4276,   966,   183,  1010, 13435,   138,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   221,   179,  4394,   118,  1340,   136,  2779,   978, 22278,\n",
      "          1568,  2037, 22280,   171, 12825,   286,   117,   171, 21121,   117,\n",
      "           171, 16805,   125,  1084, 20739,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,  5787,   259, 18913,   382,   311,  3828,   228,  1966,  2455,\n",
      "           373, 13831,   117,   240,   210,   117,   179,  1966,  9861,   311,\n",
      "           607, 22280,   125, 11413,   259,  8243,   244, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,  1016,   117,   123,  7122,  3138, 16555,   924, 21347,   117,\n",
      "           271,   260, 10909,   117,   230, 12939,   221,   146,  1336, 22280,\n",
      "           117,  1858,   221,  9726,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   125,   222,  1341,   117,   450,   938,  7755,   151,   122,\n",
      "         14699,   125,  1342,  1341,   117,  2496,   125, 10400,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  2826,  3558,  2010,  3165,   180, 20838,   151,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   222,  7392,  7343,   117,   317, 22279,   339,   125,   466,\n",
      "          9228,   285,  6650,   117, 17003,  4640,   179,   146,  3165,   180,\n",
      "         20838,   151, 15787,   495,   123,  4363,   232, 22280,   366, 18544,\n",
      "           117,   179,   331,   272,  2158,  8087, 18775,   123, 20838,   151,\n",
      "         19731,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   320,   179,  1810,  1936,  1342,  7392,   117,  1538,   125,\n",
      "           222,   298,  3845,  1496,   128,   125,  9719,   117,   179,   146,\n",
      "          3165,   180, 20838,   151,   371,   123,  5664,   325, 19033,  4570,\n",
      "           179,   607,   202,  2397,   117,   122,   117,  1104,  1017,   246,\n",
      "           117,   123,   327,   325,  1677,  4838,   324,  2996,   304, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,  2121, 22278,   146, 14624,   420,   146,  2260,   122,   146,\n",
      "           317, 22279,   339,  2779,   781,   183,   320,  4276,   966,   183,\n",
      "           102]])\n",
      "DEBUG ======================\n",
      " len vetores 16 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.034883720828826396\n",
      " Coesão Score Final: 0.5174418604144132\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'assim', 'se', 'todavia', 'ou', 'ora', 'como', 'para que', 'com efeito', 'principalmente']\n",
      " Número de conectivos: 12\n",
      " Número de sentenças: 16\n",
      "======================\n",
      "Resultados para preprocessado_bras_cubas_machado_cap_2.json:\n",
      "{'coesao_score': np.float64(0.52), 'conectivos_encontrados': ['e', 'mas', 'porem', 'assim', 'se', 'todavia', 'ou', 'ora', 'como', 'para que', 'com efeito', 'principalmente'], 'num_conectivos': 12, 'proporcao_conectivos': 0.035, 'similaridade_media': np.float64(1.0), 'num_sentencas': 16}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,   449,   117,  1941,   179,  4763, 22283,   538, 17080,   682,\n",
      "          7392, 22281,   117,  5308,   118,   311,  1434,  5863,   222,  6995,\n",
      "         14999,   303, 20400,   439,   319,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   146,  5712,   125,  7122, 20027,   262,   222,  4863,   180,\n",
      "          4322, 22280, 13435,   138,   117,   179,  2968,   685,   229,   681,\n",
      "          3437, 19189,  2177,  3043,   419,  8561,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   495,  6736, 22280,   458,   125, 13833,   247,   117,  2711,\n",
      "           171,  2187,   125,  1543,   117,   582,  2471, 14775,   229,  2377,\n",
      "         10866, 22278,   122,   229, 12126, 22281, 16055,   292,   117,   176,\n",
      "          2529, 16996, 22281,   236,   123,  6736,  3632,   322,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   449,   229, 22280,  1191,   118,   176, 16982,  7028,   117,\n",
      "         16733,   203,   117, 13301, 13665,   117,  1303,  5567, 22288,   259,\n",
      "         13665,  3576,   240,  7799,   122,  6320,   591, 19271,  1149,   117,\n",
      "          2684,   179,  2931,   117,  4513, 15618,   293,  3049, 11437,   123,\n",
      "           222,  1417,   117,   146, 12150,   243,   411,   145, 13435,   138,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,  3102, 13254,   122,   179, 19033,   662,   304,   123,   333,\n",
      "           794,   125, 17080,  1938,   128,  2010,   298,  1938,   128,   179,\n",
      "           123,  7122, 14735,  4146,  1684, 15982,   203,  2010,   117,  2113,\n",
      "           146,   180,  4322, 22280, 13435,   138,   495,   870,   509,   125,\n",
      "         11169,   222,  6736, 22280,   458,   117,   122,  5787, 11775, 18924,\n",
      "           458,   117,   320,  6793,   179,   146,   599,   145, 13435,   138,\n",
      "          6000,   173,   144,  6562,   117,  7365, 22288,   202,  1177,   117,\n",
      "           122,   262,   222,   298,  3667,  1329,   393,  2033,   171,  2700,\n",
      "           118,  1754,  4917,   180,  8981,   252,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   271,   860,  9518,   125, 13435,   138,  2036,   765,  1402,\n",
      "           236, 21296,   246,   123,  6736,  3632,   322,   117, 20165,   256,\n",
      "          7343,  1568,   117,  7949, 12915,   171,  9398,   151, 22280,   117,\n",
      "           179,   146,  6775,  9518,  1796,  3433,   123,   222, 12810,   117,\n",
      "         11593, 22283,   529, 10280, 22281,   180,  5566, 22278,   117,   173,\n",
      "         11050, 22280,   180,  1165,  2733,   252,   179,  6011,   203,  3456,\n",
      "          4776,   348,  9967,  1922, 13435,   138,   712,   390,  4418,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,  7343,  1568,   495,  2397,   125, 12223,  3391, 13793,  3656,\n",
      "          7904,   203,   123,  6736,  3632,   322,   529, 10180,   125,   222,\n",
      "          9599,  7052,   365,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   495,   222,  4062,  1354,   367,   117,  7343,  1568,   117,\n",
      "         17935, 22280, 12856, 22280,   122, 20065,   271,  9390,   942,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   978,   117,   122,  3295,   117,  7226,   572,   793,   125,\n",
      "         12164, 15203,  1156,   449,  1977,   229, 22280,   122,   222,  1695,\n",
      "         12164, 15203, 22278,  3876,  1147,   136,  3560,   256, 14623,   179,\n",
      "           368,   229, 22280,  7253, 22288,   123,  7029,  1319,  3133, 13793,\n",
      "           700,   125, 21066,   123, 18612,  1268,   304, 22280,  9817,   117,\n",
      "         18450,  3048, 22288,   118,   176,   229, 20027,  4566,  7343,  4828,\n",
      "         11002,  4029,   283,   117,   146,  9937, 22280,   118,  1623,  1010,\n",
      "         13435,   138,   117,   179,  6043,   123,  4411,   272,   629, 22280,\n",
      "          2700,   175,   117,   582,  2931,   173, 12693, 22313,   117,   122,\n",
      "           240,  1966,  5607,   122,   179,   311,  2002,   146,   655,   125,\n",
      "          1010,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101, 13638,   118,   176,   118,  2036,   117,   240,   210,   117,\n",
      "           123, 20027,   171,  9937, 22280,   118,  1623,   117,   122,   262,\n",
      "           318, 13793,   179,   368, 12223,   203,   260,  9967,  1922, 13435,\n",
      "           138,   390, 10866, 22281,  1149,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,  7873,   744,  1089,  1823,   180,  7122, 20027,   117,  7122,\n",
      "         17181,  1214,  1601,   151,   117,   240,  1416,   117,   146, 19449,\n",
      "           342,   118,   171,   810, 22279,   117,   179,   122,   123, 12252,\n",
      "           366, 22186, 22281,   171,   347,   596,  3390,   146,  1568,   117,\n",
      "           146,   144,  2577, 22287,   117,   222, 10738,   179,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   449,   229, 22280, 16033,  5835,   128,  7780,  1642,  5835,\n",
      "           125,   230,   576,   271,  7275,  4276,   966,   183,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 12 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.047222222091049385\n",
      " Coesão Score Final: 0.5236111110455247\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'pois', 'porque', 'ja que', 'se', 'ou', 'ora', 'senao', 'como', 'ao passo que', 'porque', 'ja que', 'por exemplo', 'por esse motivo', 'por esse motivo']\n",
      " Número de conectivos: 17\n",
      " Número de sentenças: 12\n",
      "======================\n",
      "Resultados para preprocessado_bras_cubas_machado_cap_3.json:\n",
      "{'coesao_score': np.float64(0.52), 'conectivos_encontrados': ['e', 'mas', 'porem', 'pois', 'porque', 'ja que', 'se', 'ou', 'ora', 'senao', 'como', 'ao passo que', 'porque', 'ja que', 'por exemplo', 'por esse motivo', 'por esse motivo'], 'num_conectivos': 17, 'proporcao_conectivos': 0.047, 'similaridade_media': np.float64(1.0), 'num_sentencas': 12}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,   123,  7122,  3138,   117,   700,   125, 17783,  5351,   342,\n",
      "           966,   117, 16977, 22278,   118,   176,  3138, 11333,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,  4023,   437,  3039,   117, 14624,   117,   125,   230,  3138,\n",
      "         11429,  2037,  1075,   222,  3828,   458,   117,  1075,   230, 12682,\n",
      "         22279,   202,  9834,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   873,   146,   329,  5458, 22282,   262,   123,  3138, 11333,\n",
      "           180,  3812,  6599,   179, 22280, 10077,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,  3295,   122,   179,  7949, 21596,   229, 22280,  2931,   449,\n",
      "          4395,   130, 11760,   779,   179,   123,  3402,   122,   230,   739,\n",
      "          8193,  6522,  1337,   122,   123,  4131, 22278,   230, 19731,  7406,\n",
      "          1272,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   240,  1416,   117,  9160,   897,   247,  2002,   118,   538,\n",
      "           222,  4485,  8631,   117,   179,   495,   222,  8512,  2758, 22280,\n",
      "           117,  2010,   291,  1112,   230,   472, 12126,  1516, 22354,   271,\n",
      "          2036, 13384,   203,  3133,   383, 22278,   117,   122,   222,  4222,\n",
      "         22280,   117,   179,  9607,   685,   333,   260, 15020,   562,   125,\n",
      "          2415,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,  3429,  5414,   246,   222,  2917,   122,  4645,   203,  1423,\n",
      "           125, 13409,   179,   298,   682,  2992,  8889,   143,   117,   146,\n",
      "         15020,  5092,   117,   146, 19033, 15020,  5092,   117,   262,   146,\n",
      "          1112,   472, 12126,  1516, 22354,   125,  3133,   383, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   122,  5023,   117,  5836, 13806,   599,  1128,   351,   117,\n",
      "         12252,   298, 10527,  7867,   117,   176,   222,  6447,   437, 12590,\n",
      "         22288,   271,   123,   454, 10557,   387,   329, 12754,   232,   117,\n",
      "          4169,   222,  5575, 22024,  6429, 15856,   379,   326,   179,   437,\n",
      "         17896,   203,   785,  1921,  3322,   117,   122,   117,   176,   229,\n",
      "         22280,  7762,  7485, 22279,   123, 19449,   342,   117, 14619,   210,\n",
      "           324, 22280,  1968,  7485, 22279,  9196, 18924,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,  2779,  1114, 22280,   118,   311,  2765,   420,   146,  6447,\n",
      "           122,   146,  6459,   247,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101, 12043,  1502,   123,  4131, 22278,   117,   123,   781, 22288,\n",
      "           415,  4131, 22278,   179,   180,   221,  2745,   122,   117, 12086,\n",
      "           123,  3138, 11333,   117,   641, 22283,   179,   122,   740,   123,\n",
      "           179,   659,   259,  8525, 22280,   143,  5248, 22279,   259,   171,\n",
      "           474,   123,  3138,  8556,   117,  5926,   291, 16463,   154,   118,\n",
      "           549,   122,   123,   179,   659,   259,  4485,  8631, 22281,   117,\n",
      "          2010,  1454,  9160,   897,   247,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   495, 11333,   123,  7122,  3138,   117, 11333,   271,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   229, 22280,   311,  1366,  3874,   179,  1547,   502, 11837,\n",
      "         14654,  3876,  1147,  5787,  5231, 22278,   117,  5787,   260,  8685,\n",
      "         16932,   143,   171,  7222,   373,   117,  5787,   123, 16907,   285,\n",
      "         14559, 10877,  7839,   304,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   873,   524,   146, 14624,   123,  3286,   304, 22280,  1977,\n",
      "          1215, 22282,  2036, 14142, 22282,   117,   873,   524,   118,   123,\n",
      "           122,   229, 22280, 10726,   180, 22283,   123,  8861, 22282,   118,\n",
      "           311,   146, 17749,   117,   331,  2113,   744,   229, 22280, 10142,\n",
      "           128,   123,   670, 10656,  8865,  5809, 14876,   562,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[ 101, 1084,  925, 5835,  102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,  3960,   247,   179, 13521,   123, 15631,   243,   154,   123,\n",
      "         10857, 13793,   117,   271,   259,   736, 12427,   117,   532,   915,\n",
      "          8008, 22281,   117,   122, 17386,   179,   659,   785,  1004,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[ 101, 1502, 1084,  925, 5835,  102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101, 17226,   117,  1168,  4640,   179,   860,  1722,   122,  2685,\n",
      "          1988, 12164,   914,   124,   117,   170,   123, 12164,   914,   124,\n",
      "           125,   222,  2397,  1941,  5452,  1885,   487,   180,  4312, 12683,\n",
      "           171,   176,  2177,   117,  1706, 10262, 13611,  3252,  1212,  1282,\n",
      "           117,   125,   230,  5422, 12353,   162,   117,  2535,  5382,  4623,\n",
      "           117,  2044,  5911,  2848,  9811, 22278,   117,  5664,   179,   229,\n",
      "         22280,  2597,   232,  2798, 16863, 22283,   117,   229, 22280,   851,\n",
      "          4839,  2798,   494,   747,   117,   122,   122, 17226,   325,   171,\n",
      "           179,  1425, 10730,   122,  1528,   171,   179,  4230, 18397,   243,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101, 19821,  1084,  2582, 11429,   455,   146,   347, 17749,   117,\n",
      "           122,   745,  5835,   320,  4276,   966,   183,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,  1114,  5835,   123,  4131, 22278,   170,   259,   532,   853,\n",
      "          6522,   128,   272, 22186, 20882,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,  3484,   125,   538,  6614,  5254,   123,  2971,   125,  4767,\n",
      "          1817,   117,  3484,  2694,   123, 17779,   375, 22280,   125,  1382,\n",
      "          6891,  4609,   412,  7122,   670,   117,   176,  3933,   576,   311,\n",
      "          5069,   157,   125,  6010, 22287,  6244,   117,   122,   331,   412,\n",
      "          3138,   125,   179,   327,  2833,   852,   117,   170,   123,  1589,\n",
      "           223, 22280,   179,  8196,  5105,   146,  9503,   117,  2471, 12366,\n",
      "           712,  8563,   146,  4276,   966,   183,  1010, 13435,   138,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   229,  6889, 18419, 22287,  2990, 10673,   151,  2508,   180,\n",
      "         14951,   151,   122,   171, 19843,  2148,   714,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,  1977,   229, 22280,  4178,   179,   320,   766,   125,  1078,\n",
      "          5486, 18042,   117, 10033,   117, 13944,   255,  1319,   117,   607,\n",
      "          1615,  1176,  2421, 22281,  1028, 15063, 18913,  1121,  9094,   117,\n",
      "           179,   176,   842, 14036, 22287,   122, 19195, 22287,   123, 15419,\n",
      "          7970,   117,   122,   229, 22280,  6387,  1176,  2036, 20209, 22287,\n",
      "           136,  3002,  3286,   214,   117,   122,   271,   123,   159,   124,\n",
      "           151,   118, 14689,  6915,   117,   179,   176, 19903,   151,   123,\n",
      "         15419,   171,  5001,   118,  1154, 16660,  6600,   860,   122,   123,\n",
      "         18142,   151,  1767,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,  3295,   122,   179,   176,  1191,   522,  6915,   122,  9232,\n",
      "         22278,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   229, 22280,   117,   123,  3286,   304, 22280,   229, 22280,\n",
      "           543,   154,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 23 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.0285714285170068\n",
      " Coesão Score Final: 0.5142857142585034\n",
      " Conectivos encontrados: ['e', 'mas', 'logo', 'pois', 'porque', 'se', 'todavia', 'nem', 'ou', 'ora', 'seja', 'como', 'porque', 'por exemplo', 'se nao']\n",
      " Número de conectivos: 15\n",
      " Número de sentenças: 23\n",
      "======================\n",
      "Resultados para preprocessado_bras_cubas_machado_cap_4.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'logo', 'pois', 'porque', 'se', 'todavia', 'nem', 'ou', 'ora', 'seja', 'como', 'porque', 'por exemplo', 'se nao'], 'num_conectivos': 15, 'proporcao_conectivos': 0.029, 'similaridade_media': np.float64(1.0), 'num_sentencas': 23}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,   230,  2954,  5809,   117,  6661,   180,   651,   221,   146,\n",
      "          3815, 22280,  1160,   117,   778,  8393,  1362,  8574,   180,  2692,\n",
      "           222, 13254,  5863,   171,  2907,   117,   179,  2779,   818, 22280,\n",
      "           125,  3122,   122,   125,  1690,  7817,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101, 15322, 22288,   118,   311,   117,  1636,   203,   118,   176,\n",
      "           320,   766,   125,  9726,   117, 11234,   180, 13943,   122,   298,\n",
      "         12058,   117,   122,  2467,  8825,  1552,   118,   311, 11454,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   123,  4036,   495,  6003,   117,   122,   259, 11454,   706,\n",
      "           333,   179,   229, 22280,  5327,  9192, 14697,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101, 12519,   117,   240,   210,   117,   179,   117,   271,  2779,\n",
      "          1011,   822, 13550,   117,  3030,  9193,   259,  5708,  1510, 22281,\n",
      "           291,  1256,  1176,  1971,  1587,   654,   221,   179,   368,  8082,\n",
      "          1249,   123,  8092,   122,  3285,  1249,   259, 11454,   202,  4102,\n",
      "           293,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   118,   118,  1236, 22279,   117,  1996,  2779, 19994,   214,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   118,   118,  1941,  1642,   244,   117,   362, 22282,  4643,\n",
      "           748,   368,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   118,   118,   629, 22280,   785, 22003, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,  1976,   118,  2036,  1434,   222, 22000,   221,  4551,   118,\n",
      "         15887,  1858,   576,   171,  4102,   293,   117,   449,   229, 22280,\n",
      "          1367,   171, 22000,  1011,  1052,  3611,   243,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   202,   644,  1457,  3033,   123,  4640,   125,  9726,  3360,\n",
      "          2996,   128,   117,   122,  2467, 20025,   214,   118,   311,  2350,\n",
      "           504,  4643, 22282,   157,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   259,  9000,   117,   179,   229, 22280,  5971, 22287,   298,\n",
      "         17080, 19877,   128,   570,  1435,   128,   122,  1945,   308,   117,\n",
      "          7320,  3418,   123, 20025,   117,   179,   870,   509, 16436,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,  2798,   240,  1257,   311, 11510,   833,  3897, 22283,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,  1519, 22283,   123, 15631,   243,   154,   712,  3667,   180,\n",
      "           651,   117,   122,  1061,   117,   240,   416,   304,   117, 17221,\n",
      "           118,   311,  1016,   117,  1089,   173,  7645,   555, 22443, 22443,\n",
      "          2350,   504,  4643, 22282,   157,   117, 11388, 17891, 14890,   170,\n",
      "          2354, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101, 22443, 22443,   118,   118,   112,   112, 17891,   221, 17249,\n",
      "         10213,   117,  2350,   504,  4643, 22282,   157,   123,  1105,   122,\n",
      "           123,  1589,   180,  5302,  6156,   873,   176,  5308, 22281,  1921,\n",
      "         18147,   171,  3815, 22280,  1160,   117,   122,  2541,  1084,  3852,\n",
      "          7226,  8184,  1564,   170,  1039,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101, 22443, 22443,   118,   118,   112,   112,  7343, 15045,  2350,\n",
      "           504,  4643, 22282,   157,   117,   229, 22280,  4450, 22279,   179,\n",
      "           146, 12119, 22280,   171,  3822, 22032,   252,  1214,   252,   122,\n",
      "         18165, 22278,  5863,   229,   651,  3687,   118,  2036,  9461,   157,\n",
      "           185,   117,  3687,   118,  2036,  1690,   117,  3687,   118,  2036,\n",
      "          9461,   331,   229, 22280,  2036,  3687,   390,   304,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101, 22443, 22443,   229, 22280,  5305,   555,   434,  9223,   501,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   504,  4643, 22282,   157,   229, 22280,   418,  5863,   202,\n",
      "          3288,   179,  1061,  2036,   180, 22280,   117,   449,   202,   179,\n",
      "          2036,   429,   146,  5184,   339,   125,  2397,  1945,   201,   122,\n",
      "          3285,   286, 11631,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,  2350,  3429,   240, 18447,   151,   117,   221, 16266, 22282,\n",
      "           118,   311,   572,   793,   125, 10692, 11437,   339,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,  2745,   240,  2765,   144,  5689,  8409, 14619,   210,   229,\n",
      "         22280,   417,  9193,  1407,  4222,   452,   221,   123,  7122,  5233,\n",
      "           304, 22280,   118,   176,   229, 22280, 18424,  1342,   180,  8156,\n",
      "          2684,   320,  1338,   171,  1722,   117,  2541,   860,   653,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,   146,  7343,  6447,   171,  8574,  4412, 22278, 14672,   179,\n",
      "           229, 22280,  2036,  8098, 22280, 21568,   141,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   122,   170,  3265,  3803,   303,   117,   660,   146,  4222,\n",
      "           452,   347,   117,   926, 22278, 13917,   179,   123,  1706,   122,\n",
      "           327,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   607,  2978,   179,   820,  8743, 22280,  1257,   298,   532,\n",
      "          4761,  1089,  2798,  1971,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 21 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.03183023864236011\n",
      " Coesão Score Final: 0.5159151193211801\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'assim', 'se', 'nem', 'ou', 'como', 'para que', 'tanto', 'se nao', 'por isso']\n",
      " Número de conectivos: 12\n",
      " Número de sentenças: 21\n",
      "======================\n",
      "Resultados para preprocessado_dom_casmurro_machado_cap_1.json:\n",
      "{'coesao_score': np.float64(0.52), 'conectivos_encontrados': ['e', 'mas', 'porem', 'assim', 'se', 'nem', 'ou', 'como', 'para que', 'tanto', 'se nao', 'por isso'], 'num_conectivos': 12, 'proporcao_conectivos': 0.032, 'similaridade_media': np.float64(1.0), 'num_sentencas': 21}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  2535,   179,  2078,  3533, 22283,   146,  4222,   452,   117,\n",
      "          6793,   123,  4766,   146,  1722,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,  1075,  1659,   117,   240,   210,   117,  2826,  3558,   259,\n",
      "          6593,   179,   311,  6792, 22287,   123,  7482,   229,   223, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[ 101, 3361,  331,  117,  170,  222, 2724,  102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   123,  1105,   173,   179,  1623, 22280,   122,  2004, 22278,\n",
      "         10692,   118,  1084,  4902,   125, 19284,   373,   117,  6751,   125,\n",
      "           222,  6532,   316, 22280,  2754,   179,   311,   873,  2037,   525,\n",
      "          3198, 22283,   118,  1340,   117,   449,  1447,  1084,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[101, 222, 644, 102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   607,  2780, 22281,   481,   117,  5069,   748,   118,   311,\n",
      "         20927,   202,  3815, 22280,  1160,   123,  1105,   173,   179,   311,\n",
      "           786,   244,   229,  2557,  4768,   125,  6629,   118,  9701,   117,\n",
      "          3951,   118,  2036,   146,   653,  7316,   122,  3338,  7970,  1858,\n",
      "           117,   179, 14778,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   893,   428,   122,  7090,  9050,   228,  1004,   260,  4837,\n",
      "           303,   143,   179,  7707,  3283,   122,   146,   653,  9078,   247,\n",
      "          1990,  1609,  6859,   117,  1510, 22281,  9751,   125,  2375,   117,\n",
      "         17935,   404,   320,  4707,   117,   260,  7477, 15460,  1671,   122,\n",
      "          7289,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   229,  1310,  5809,   117,   123,  4501,   171,  1835,   183,\n",
      "           122,   366,  7367,   122,   325,   291,  1528,  4209,   117, 11368,\n",
      "          9247,  7134,   591,   125,  2968, 14689,  6915, 22281,   122,  1491,\n",
      "          3852,   128,   179,   260,  5267, 22287,   538,  9786,   117,   125,\n",
      "          1632,   303,   123,  1632,   303,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   538,  1256, 18694,   171,  1835,   183,   260,  5969,   366,\n",
      "           418,   303,   143,   117,   122,   320,  1997,   366,  7367,   259,\n",
      "         21496, 22280,   143,   125,  2992,  8889,   117,   527,  3209,   183,\n",
      "           117, 21310, 22280,   122,  8050,  2949, 22281,   375,   117,   170,\n",
      "           259,  3360,   240,  3378,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   229, 22280, 20482,  3048,   123,  3083, 13793,   125,  2571,\n",
      "          2641,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[ 101,  625,  227,  793,  221,  123, 1105,  125, 6629,  118, 9701,  117,\n",
      "         1941,  740, 1011, 1016, 3916,  671, 8940,  171,  693,  194,  247, 2095,\n",
      "          102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101, 12931,   495, 10303,   171,   596,  3285,   140, 15107,  1548,\n",
      "           319,   122,  5969,  5227,   173,  6807, 11190,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,   146,   325,   122, 14619,   210,  3469,   763,   122, 16408,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101, 15212,  1690,   934,   508,   117,  2968,   117,  2653,  2766,\n",
      "           117,   230,   504, 21304,   387,   117,   222,   302,   303,   122,\n",
      "         11348, 17351,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[ 101, 1700, 7406,  304, 7492,  122, 8556,  151, 7492,  102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101, 17878,   117,  2535,   117,   271, 19888,   117,   607,  5863,\n",
      "           146,   653,  9266,   180,  1069,  2699,   117,   179,   122, 12164,\n",
      "          7137,   117,   170,   123,  5169,   117,   179,   122,  1315,   474,\n",
      "         22278,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   146,  7343,  1338, 10790,   495,  1316, 22282,   260,   924,\n",
      "         19576,   180,  1069,   117,   122, 15334,   229,  2189,  4095,   289,\n",
      "           123,  6945,  3292,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,  1502,   117,  7258,   117,   229, 22280,  1104, 22283,  9126,\n",
      "           141,   146,   179,   262,  2798,   146,   179,   572, 22283,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,   173,  2745,   117,   176,   146,  9169,   122,  4209,   117,\n",
      "           123, 15578,  4275,  4322,   122,  3575,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   176,   331,   311,  3207, 22281,  6867,   259,   736,   117,\n",
      "          1447,   222,  2397,  5411, 22278,   118,   176,   325,   291,  1528,\n",
      "           366,  1101,   179,  2243,   325,   988,   183,  2779,   653,   117,\n",
      "           122,   418, 15530,  2666,   122,  2745,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   146,   179,  5863,   418,   122,   117,  3002,  3286,   214,\n",
      "           117,  4327,   123,  4501,   179,   176,  6792,   229, 17897,   122,\n",
      "           538, 13841,   117,   122,   179,   820, 14223,   146, 19877, 22280,\n",
      "         11792,   117,   271,   176,  1331,   529,  1846,  2742,   562,   146,\n",
      "          8280,   229, 22280,  6205,   796, 17681,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,   230,   738,  3679, 22280,   179,   311,  2811,  4698,   481,\n",
      "           125,  2169,  2686, 13441,   159,   259, 14848, 22281,   117,   271,\n",
      "           944,   259,  5590, 15740, 22281,   117,   449,   229, 22280,   123,\n",
      "          9726,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   259,  3667,   179,   311,  2226, 22287,   629, 22280,   125,\n",
      "          2788,  6240,   944,   259,  3845,   506,  5964,   123, 15488,  4984,\n",
      "           298,  5097,   118, 14125,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  2249,   260, 19763,   117,  1450, 17032,   125,  8184,   481,\n",
      "           117,  1028,   125,  1528,   117,   122,  1821,  1485,  3960,   210,\n",
      "           229,   390,  2420,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,   924,   291,  1510, 22281, 11610, 22287,  3960, 22282,  9918,\n",
      "           712,   736,   117,   449,   123,  3182, 22278,   179, 14936, 18237,\n",
      "          5747,   576,   123, 16700, 22282,   259,   434,  9223,   501,   117,\n",
      "           122,  1815,  1864,  3292,   122,   822,   375,   567,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,  5147,   117,  1069,  3575,   229, 22280,  3189,  4640,  1069,\n",
      "          8562,   117,   122,  1858,   144,  5424,   123,  8128,  3953, 22281,\n",
      "           117,  7583,  1069,  2557,  3379,   118,   311, 12989,   328,   125,\n",
      "          1415, 16473,   128,   179,  2036,   417,  9193,   449,   122, 14619,\n",
      "           210, 20811,   179,  3763,   785,   730,   994,   179,   123,  1191,\n",
      "          3848,  1009,   117,   122,   117,   125, 14876,   151,   117,  5249,\n",
      "         22280,  3933, 18127,   304, 22280, 11152,   122, 18190,   289,   364,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   173,  3295,   117,  1695,  1183,   303,   122,  1528,   988,\n",
      "         22280,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101, 22238,   303,   143, 16099,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,   146,   325,   171,   596,   122, 20639,   173,  3428,   578,\n",
      "           117,  1941, 18304,  2623,   122,  9784,   271,  1004,   122,   229,\n",
      "         22280,   623, 22282,   283,  3002,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101, 11032,   117,   271,  2745,   822,   375,   117,   418, 10537,\n",
      "           897,   151,  2467,   240,  3899, 10866, 22282,   118,   311, 14619,\n",
      "           210,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,  8204, 15611,   117,   122,  5069,   748,   118,   311,  4766,\n",
      "           222,  1722,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101, 11255, 12999,  3992,   340,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,  5422,   122, 12067,   232,   417,  1797,  1429,   118,   311,\n",
      "           117,   449,   229, 22280,   311,   417,  1797,  1429,   260,   344,\n",
      "          1149,  1395, 14348,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   700,   117,  4174,   244,   173,  1434,   230, 22443, 22443,\n",
      "          4131, 22278,   298,   695, 11364,   501,   112,   112,  1528,  8742,\n",
      "           179,   260, 14876,   562,   171,  7148,   599,   145,  3746,  2028,\n",
      "         22290,   933,   298, 14125, 17233,   123,   651,   495,  1706, 18913,\n",
      "           154,   117,   449, 19764,  5590,   122, 10995,   271, 21941,   143,\n",
      "           117,  2745,   388,   286,   122,  1639,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,   262,   318, 13793,   179,   259,  2231,   382, 20367, 22281,\n",
      "           529,  7367,  8880,   123,  5961,   118,   311,   122,   123,  4640,\n",
      "           118,   311,   179,   117,   230,   576,   179,  1061,   229, 22280,\n",
      "         20482,  2028,   692, 19252,   569,   307,   118,   311,   259,  3492,\n",
      "         10788,   117, 15854, 22281,   236,   180,  7482,   122, 11169,   236,\n",
      "          1089,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,  5787,   123,  5233,   304, 22280,   311,  2811,   123, 14306,\n",
      "         13793,   117,   122,   260, 15419, 22281,  7762, 22281,  6867,   337,\n",
      "         15394, 22282, 10032, 22281,   117,   271,   320,  6447,   117,   229,\n",
      "         22280,   146,   171,  8574,   117,   449,   146,   171,   395, 11011,\n",
      "           123, 22283,  4778,  1111,  1858,   576,   117, 11233, 10359, 22281,\n",
      "         15419, 22281,   136,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101, 10692, 19623,   316, 22280, 20073,   170,   418,  3138,   117,\n",
      "           179,   744,  2535,   311,  8574, 22279,   123,  7482,   229,   223,\n",
      "         22280,   102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,  1141,   117, 21310, 22280,   117,   527,  3209,   183,   117,\n",
      "          8050,  2949, 22281,   375,   117,   122,  5023,   117,   739,  2992,\n",
      "          8889,   117,   179,   311,   563,  1025,   123,  1434,   259, 17080,\n",
      "         13052,   501,   117, 13406,   303,   118,   962, 22281,   146,  6865,\n",
      "           117,   122, 17891, 20933,   578,   320,  1798,   260, 10200,  2949,\n",
      "         22281, 12773,   784,   179,   311,  7762,   684,  6661,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,  2166,  2277,   117,  5212,   244,   146,   179,  2569, 22283,\n",
      "           117,   122,  3791,  2443,   244,   123,   223, 22280,   221,  3933,\n",
      "          1706,   125,   636,  5902, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,   122,   151,   117,   662,  2812,   128,   123, 16117,   304,\n",
      "           304, 22280,   240,   230, 13783, 22279,  1373,   125,  1617,   117,\n",
      "           179,  2364,   311,  6969,   685,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,  4435, 22279,  1028,  1615,   117,  2980,   117,   122, 18967,\n",
      "           117,   449,  7583,  2364,   176,   311, 17896,   203,   171,  5791,\n",
      "           183,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,   122,   146,   179,  2541, 22281,  9050,   117, 20601, 22280,\n",
      "           102]])\n",
      "DEBUG ======================\n",
      " len vetores 42 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.023284313696955498\n",
      " Coesão Score Final: 0.5116421568484778\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'entretanto', 'assim', 'logo', 'pois', 'uma vez que', 'quando', 'se', 'nem', 'ou', 'ora', 'quer', 'como', 'quanto', 'uma vez que', 'tanto', 'quanto']\n",
      " Número de conectivos: 19\n",
      " Número de sentenças: 42\n",
      "======================\n",
      "Resultados para preprocessado_dom_casmurro_machado_cap_2.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'porem', 'entretanto', 'assim', 'logo', 'pois', 'uma vez que', 'quando', 'se', 'nem', 'ou', 'ora', 'quer', 'como', 'quanto', 'uma vez que', 'tanto', 'quanto'], 'num_conectivos': 19, 'proporcao_conectivos': 0.023, 'similaridade_media': np.float64(1.0), 'num_sentencas': 42}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  8544,  4270,   229,  4767,   125, 12338,   117,   625,  8362,\n",
      "         17782,   307,   146,  7343,   655,   122,  3235, 10773,   118,   311,\n",
      "          7521,   180,  4303,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   123,  1105,   495,   123,   180,  4768,   125,  6629,   118,\n",
      "          9701,   117,   146,   454,  1617,   117,   146,   622,   122,   179,\n",
      "           122,   222,  1971,  2873,   183,   117,   449,  2779,   229, 22280,\n",
      "          2678, 22283,   125, 16326,   260, 10995,   123,  7122,  1069,   331,\n",
      "           221, 15397,   159,   260,  1101,   179,   229, 22280,  3330, 22287,\n",
      "          4131,   138, 20991,   146,   622,   495,   125,  3692, 22337,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101, 20838,   151,   117,   123, 17704,  4588,   544,   229,  3138,\n",
      "           125,  3285,   140,   146,  7275,  7489,  5321,   202, 11504, 17551,\n",
      "           136,   122,   325,   179,   596,   117,   122,  1941,  2535,   706,\n",
      "          5110,   230,  7614,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[ 101,  118,  118,  179, 7614,  136,  118,  118,  230,  739, 7614,  102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,  7122,   223, 22279,  8204,  4945,   146,   179,   495,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,  4141,   236,  1564,   117,   700,   125,  1089, 16423,   143,\n",
      "           125, 14777,   304, 22280,   117,  3429,   792,   176,  1021,   614,\n",
      "           210,   202, 14148,   229, 22280,  2002,   240,  9726,   117,  2927,\n",
      "           122,   117,  3508, 12190,   243,   123,  4410,   117,  1996,   179,\n",
      "           123,  7614,  1011,   229,  1105,   320,   766,   117,   123,  9349,\n",
      "           171,  1852,  3611,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   118,   118,   123,  9349,   171,  1852,  3611,   136,   118,\n",
      "           118,   607,  3179,   596, 12044,   221,  2036,  4640,  3413,   117,\n",
      "           449,   229, 22280,   311,   361,   130,   619,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   229, 22280,   311,  4048, 22003,   179,   146,  7275,  7489,\n",
      "          5321,  1961, 22279,  3285,   286,   538, 18694,   170,   123,  2267,\n",
      "           171, 19889,   421,   117,   122,   418,   122,   123,  7614,   117,\n",
      "          2113,   176,  1061, 15854, 22287,   125, 20401,   117,   123, 17704,\n",
      "          8743,   785,   179,  6680,   221,  2531,   118, 15887,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   118,   118,   229, 22280, 17386,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,  3285,   474,   538, 18694,   136,   118,   118,   122,   222,\n",
      "          2277,   125,  5961,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[ 101,  173, 7513, 6436,  683,  117, 1684, 5062,  102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  7489,  5321,  1821,   179,   229, 22280,  8625,   125,  1084,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   123,  3113,   122,   230,   273, 22259, 15770,   146,  1568,\n",
      "           659,   179,   229, 22280,   873,  5357, 22278,   368,   179,   260,\n",
      "           144,  5424, 22281,  1820, 22281,  6867,   125,  2821,   117,   179,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,  4620,   214,   146,   347, 22000,   123, 17704,   229, 22280,\n",
      "          3960,   173,  2571, 10315,   128,   117,  4048,   118,  2036,   179,\n",
      "           944,   376,   123,  8410,  2452,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   118,   118,   449,   117,   139, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,  4141,   236,  1564,   117, 15212,  3382,   259,  4385,  5911,\n",
      "          2746,   117,   122,  2364,  1976,  3874,   179, 21719, 18224, 22282,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[ 101, 1587,  154,  123, 2169, 7489, 5321, 3002,  376, 8184,  481,  102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101, 12109, 22288,  1191, 17279,   123,  2767, 19023,   629, 22280,\n",
      "          3687, 22281,   854,  3048,   966,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   229, 22280,   176,  6969,   304,   179,   506,  7329,  5062,\n",
      "           117,  1065,  7583,   739, 16386,   403,   117,   607,  1027,   481,\n",
      "           117,   173,   179,   123, 20027,  1852,  3611,  3763, 19020,   144,\n",
      "          5424,   180, 22283,  8198,   260, 16594,   689,   303,   143,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,  1502,  2779,  2678, 22283,   125,  3960, 22282,   136,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101, 10018, 13636, 22279,   117,  2354, 22279,   179,  7093,   136,\n",
      "          7392, 13636, 22279,  9396,   170,   222, 22443, 22443, 11032,   112,\n",
      "           112,   179,   117, 13078,   173, 21928,   117,  4750,  4640, 22443,\n",
      "         22443,   629, 22280, 12223,  3391, 22280,   143,   171,  4141,   236,\n",
      "          1564,   259,  4385,  7465,  2097,   118,   176,   117,  2779,   901,\n",
      "           307,   183,   118,   311,   582,   418,   146, 11240, 22280,   136,\n",
      "           112,   112,   118,   118,  1141,   117,  3960,   247,   179,   146,\n",
      "          7258,   418, 13441,   201,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   118,   118,   706,   333,  7122, 17704,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  6368,   840,  5900,  3083, 13793,   449,  3960,   151,   179,\n",
      "           229, 22280,  4763, 22283,  3133, 13793,   700,   125,   785, 11367,\n",
      "          2623,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,   118,   118,   173,  1364,  1652,   117,  2541,   660,   596,\n",
      "           117,  8082, 13665,  7122,   223, 22279, 17891,  8364,   125,  3285,\n",
      "         22279,   118,  1340,   202, 11504, 17551,  2249,  1075,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   118,   118,  1004,   117,   230,   576,   179,   229, 22280,\n",
      "          3763,   123,  3138,   125,   146,  1434,  7148,   117,   376,   118,\n",
      "           176, 12650,   146,  1310,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,  7489,  5321,   607,   125, 21619,   259, 16617,   125,   327,\n",
      "           223, 22279,   122,   700,   123,  2567,  2509,   376,  7188, 18261,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,   229, 22280,  6969,  2227,   128,   179,   222,  4864, 19080,\n",
      "           123,  8883,   175,   117,   122,   179,   146,  7148,  2996,  1286,\n",
      "         15705,   146, 16421,   247,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,   118,   118,  1161,   271,   123,  1354,  2461,  1316, 22290,\n",
      "          2598,  7392, 13636, 22279,   117,  9262,  1825,   123,  3845, 21568,\n",
      "          2841, 12067,   560,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,   118,   118,  5112, 22280,   117, 14914,   117,   229, 22280,\n",
      "         12044, 14530, 16241,  1537, 22287,   117, 12044, 17860,   146,   179,\n",
      "          2779, 18691,   122,  4640,   179,   146, 14445,   744,   376,   739,\n",
      "          1798,   202,  1010,   215,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,   118,   118,  2354, 22279,   146,   179,  3189,   122,   222,\n",
      "           853,  3356, 22279,  1961, 22279,   117,  1447, 10467,   146, 11240,\n",
      "         22280,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101,  2249,   320,  3265,   117,   176,   376,   125,   333,  7148,\n",
      "           117,  5034,   122,  1407,   179,   229, 22280,   662,   289,   123,\n",
      "          4640, 13544,  7521,   366,  6929,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,   449,   117,  2389,   296,   329,   117,  1062, 22278, 20838,\n",
      "           151,   117,   607,   653,  4096,   125,  5637,   118,  1340,  7148,\n",
      "           136,   118,   118,   122, 13246,   117,   607,   125, 10241,   118,\n",
      "           176,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   118,   118, 18661,   179,  2354, 22279,  1191, 13246,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,   449,   230, 13246,  1016,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,   229, 22280, 18661,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[ 101, 3960,  247,  179,  117, 1004, 4047,  243,  102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,  2354, 22279,   179,  7093,   117,  5495, 13012,   324,   136,\n",
      "           118,   118,  2779,   136,   118,   118,  3295,   122,   179,  1078,\n",
      "           222,  4178,  1407,   125,   898,   117,  3449,  7392, 13636, 22279,\n",
      "           118,  4023,   122,   179,  4178,   125,   944,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,  6033,   117,   230, 13246,   125, 14730,   481,   102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,   449,   117,   179,   122,  1257,   117,  1062, 22278, 20838,\n",
      "           151,   136,   418,  5334,  2537,   136, 11032,   418,  1502,  3413,\n",
      "           122,   144,  5424,   125,  1084, 20739,   136,  7122,   223, 22279,\n",
      "          1990,   203,   118,   176,   834, 12304,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,  5495, 13012,   324,  3960,   247,   179,   176, 17760,   122,\n",
      "           262,   370,   170,   740,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,  5793,   118,   176,   222,  2979, 22243, 22280,   117,   726,\n",
      "           146,   615,  7932, 22279,   123, 13779,   455,   125,  4270,   229,\n",
      "          4767,   117,   449,  1858,   344,   304,   636,   117,  1858,  5088,\n",
      "           304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,   229, 22280, 10340, 22279,  9226,   260,  3724,   179,  7392,\n",
      "         13636, 22279,  3033,   123,  4640,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101,  5495, 13012,   324,   294,   890,   256, 22443, 22443,  5495,\n",
      "         20838,   151,  5495, 20838,   151,   112,   112,  4141,   236,  1564,\n",
      "         13201,   321,   256,   118,   176, 22443, 22443,   176, 13256, 22281,\n",
      "           236,   117,   229, 22280,  2471, 14412,   117,   449,  4763, 22283,\n",
      "           412,  1214,   371,   304, 22280,   117,   412,  5948,   117,   423,\n",
      "         14477, 22280,   117,   221, 10241,   222,  2643,  8650,   339,   117,\n",
      "           222,  2643,  8650,   613,  1211,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 43 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.029062087147870428\n",
      " Coesão Score Final: 0.5145310435739352\n",
      " Conectivos encontrados: ['e', 'mas', 'contudo', 'assim', 'pois', 'porque', 'uma vez que', 'quando', 'se', 'caso', 'isto e', 'ou', 'ora', 'quer', 'senao', 'como', 'quanto', 'porque', 'uma vez que', 'realmente', 'tanto', 'quanto']\n",
      " Número de conectivos: 22\n",
      " Número de sentenças: 45\n",
      "======================\n",
      "Resultados para preprocessado_dom_casmurro_machado_cap_3.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'contudo', 'assim', 'pois', 'porque', 'uma vez que', 'quando', 'se', 'caso', 'isto e', 'ou', 'ora', 'quer', 'senao', 'como', 'quanto', 'porque', 'uma vez que', 'realmente', 'tanto', 'quanto'], 'num_conectivos': 22, 'proporcao_conectivos': 0.029, 'similaridade_media': np.float64(1.0), 'num_sentencas': 45}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[ 101, 4141,  236, 1564, 3330,  256,  259, 1229,  266, 1100,  102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   495,   222,  2277,   125,  2822,  2996,   304, 22280, 19265,\n",
      "           260,  5365,   229, 22280,   260, 11020,   117, 15724,   123, 14051,\n",
      "           159,   260, 16264,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101, 17760,   118,   176,   221,   925, 10467,   146, 11240, 22280,\n",
      "           117,   179,  1011,   202,  2699,   180,  1105,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,  2252, 22283,   118,   311,   785,   123,  8130,   117,   122,\n",
      "          1976,   118,   146,  3852,   170,   260,   675, 14790,   138, 11563,\n",
      "          2787,   703,   401,   117,   543, 10661,   117, 10849,   455,   122,\n",
      "         14967,   154,   125,  3848, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   262,   298,   169, 14116, 22281,   179, 14827,   543, 10661,\n",
      "           202,  2187,   125,  1543,   117,   122,  5787,  3102,  1147,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101, 16555,   260, 14790,   138, 10647,   221,   179,  2036,  1968,\n",
      "         22281,  6867,  1004,  1241,  4851,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   123, 14967,   154,   125,  2992,  2762,  7967,   117,   170,\n",
      "           222,  7055,   125,  1169,   240,  1839,   117, 13175, 16754,   118,\n",
      "          2036,   146, 13227,   303,   495,   318, 13793,  7800,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   146, 10849,   455,   125,  1224,   343,   117,  4677, 22279,\n",
      "           504,   556,   122,  7211,   117,  9821,  8807,   230,  1105,   304,\n",
      "           125,  4569,  7595,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   495,  5923,   157,   117,  4273,  9783,   117,   170,   222,\n",
      "           905,   247,   125,  1945,   256,  2471,   259,   532, 11330,   122,\n",
      "          1685,   481,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101, 17760,   118,   176,   170,   146,  6793,  5926,  6031,   171,\n",
      "         12215,   117,   229, 22280,  6086,  5926, 22282,  9721,   487,   176,\n",
      "           495,   298,   466, 14960,   942,   128,   117,   449,   222,  5926,\n",
      "         22282, 10315,   201,   122,   125, 16493,   117,   222,  7296,   439,\n",
      "           714,  6104,   117,   123, 22202,  1075,   180,  5395,  3292,   117,\n",
      "           123,  5395,  3292,  1075,   180,  5700, 13793,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[ 101,  222, 2643, 8650,  613, 1211,  102]])\n",
      "DEBUG ======================\n",
      " len vetores 11 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.02808988748264108\n",
      " Coesão Score Final: 0.5140449437413206\n",
      " Conectivos encontrados: ['e', 'mas', 'se', 'ou', 'para que']\n",
      " Número de conectivos: 5\n",
      " Número de sentenças: 11\n",
      "======================\n",
      "Resultados para preprocessado_dom_casmurro_machado_cap_4.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'se', 'ou', 'para que'], 'num_conectivos': 5, 'proporcao_conectivos': 0.028, 'similaridade_media': np.float64(1.0), 'num_sentencas': 11}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101, 14049, 20167,  4332,  3629,   125,  7122,  2480,  5356,   117,\n",
      "           582,  8954,   123,  6124,   285,   151,   529,  2143,  1029,   180,\n",
      "         18757,  1382,   581, 14049, 20167,   179,  6788,   145,   271, 17558,\n",
      "           328,   188, 14788,   285,   712, 10414,   171,   969, 16972,   117,\n",
      "           337, 12440,  4212,   260, 21672,   138, 12553,  1748,   703,  1609,\n",
      "           591,   125,   144, 19428,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   333,   912, 22283, 14049, 20167,   117,   122,  1369,   375,\n",
      "         22283, 11152,   246,   123,  5926,  6554, 18711,   375,   117,   221,\n",
      "           179,   146, 10695,  5429,  3750,   397, 12019, 22280,   398,   810,\n",
      "         22279,   123, 12252,   366,  6205,   138,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   582,  2541,   123,   870,  8776, 22278,  6124,  3281,   117,\n",
      "           179,  5308,  6372,   328,   123,  4203,  2992, 15396,   117,  6628,\n",
      "           320, 12837,   303,  2480, 22290,   123,   739, 20262,   136,   582,\n",
      "          2541,   271,  7352,  1809, 10913, 13784,   146, 15998,   430, 22280,\n",
      "          9019,   247,   529, 12475, 22280,   143,   171, 10951,   136,  1510,\n",
      "         22281,  3486, 22281,  9815, 22287,   498,   146,  6247,   215, 16936,\n",
      "           268,   179,  2541, 10417,  2537,  3300, 22279,   117,   528,   173,\n",
      "          1796,   222,  2656, 18263,  3561,   437, 22305,  7352,   229, 22280,\n",
      "         16450,   146,  5052,  3725,   230,   854,  2028,   122,   222,   646,\n",
      "          7629,   157,   179, 12061,   123,  3377,   202, 10420,   303,   366,\n",
      "          9647,   117,   122,  5911,  2227,   925,   148,   128,   117,  2292,\n",
      "          2592,   180,  1589,  2480, 16853,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   123,   599,  1165,   285,   558, 18680,   175,  3889,   180,\n",
      "          7815,   222, 17382, 11744, 11891,   117,   179,  5381,  3632,   420,\n",
      "           146,   528,  1259,   366, 10398,  2010, 11865,  2812, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   146,   390,   303, 18263,   117, 18119,   487,   320,   449,\n",
      "           552,   117,  1904,   259,  5708,  7925,   229, 15419,  4173, 16291,\n",
      "           180,  2480,   123,  1632,   942,   146, 11552,  4136, 14838,   240,\n",
      "          1744,  3897,  1084, 12092,   148, 11314,   498,   146,   274,   364,\n",
      "         22288,   117,   582,  3370,  7567,   260,   924, 14808,   639, 13535,\n",
      "           117, 20216, 22281,   125,   347,   851,  2532, 19206, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,  3876,  2182,   146, 18908,   247, 15700, 22278,   121, 22361,\n",
      "          8410,   222, 10342, 13449, 19088,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   179,  4314, 22278,   368,   229,  2480,   171, 10638,   247,\n",
      "           136,   230,  4131, 22278,   179,   311,  7283,   228,   529,  1863,\n",
      "           591,  8525,  1182,   138,   582,   529,   218,   117,   123,  1945,\n",
      "           251,   180,  2954,   117,   625,   123, 13943,  7282,  2836,   202,\n",
      "          2992, 22288,  7908, 22279,   348,   259,  5097,   117,   122,   123,\n",
      "           235,  1046, 22278,  1315,  2245,  5723,   538,  1877,   956,   143,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[ 101, 1314,  207,  304,  146, 9059,  102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   146,  1315,   326,   366, 10398,  8163,   343,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   146, 10695,  1390,   154,   498,   260,  8950, 20378,   202,\n",
      "         21228,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,  7674,   118,   176,   123, 15235,   292,   298, 20167,   122,\n",
      "           123, 16468, 22281,   304, 19876,   421,   117,   271,   146,  1109,\n",
      "           141,   117,   260,  2360,  1149, 10180,   498,   146,   472,   714,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,  4023,   437,  7211,   123, 13982,   117,   235,  1408, 22280,\n",
      "           122,   313,   697, 10695,   117,   240,   420,   260, 10398, 17464,\n",
      "           117,   122,   437,   302,   537,   229, 22290, 20657, 22278,  1748,\n",
      "         22279,   251,  8932,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,   331,   827, 22287,   221,   964,   260,  4332,  1705,   527,\n",
      "           277,   122,   221,   964,  1941, 22281,   269,   794,   123,  7390,\n",
      "         20798, 20167,   125,  9372,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,  1139,   962,  1557,  1016,   123,  2620,  8253, 22280,   171,\n",
      "          9059,   117,   123,  1058, 22280, 10695,   117,   781,   256,   260,\n",
      "         11563, 11912, 22281,   123, 10428,   261,   117,   179,   437,  6082,\n",
      "           117,   449,   229, 22280,   176,   670,   180,  2480,   582, 14556,\n",
      "         22278,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 14 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.033426183750902\n",
      " Coesão Score Final: 0.516713091875451\n",
      " Conectivos encontrados: ['e', 'mas', 'assim', 'quando', 'enquanto', 'se', 'ou', 'ora', 'como', 'quanto', 'para que', 'quanto']\n",
      " Número de conectivos: 12\n",
      " Número de sentenças: 14\n",
      "======================\n",
      "Resultados para preprocessado_iracema_jose_de_alencar_cap_1.json:\n",
      "{'coesao_score': np.float64(0.52), 'conectivos_encontrados': ['e', 'mas', 'assim', 'quando', 'enquanto', 'se', 'ou', 'ora', 'como', 'quanto', 'para que', 'quanto'], 'num_conectivos': 12, 'proporcao_conectivos': 0.033, 'similaridade_media': np.float64(1.0), 'num_sentencas': 14}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  1202, 22287,   117,   785,  1202, 22287,  7970, 10530,   117,\n",
      "           179,   744,  5580, 22278,   202, 21228,   117,  2714, 11865,  2812,\n",
      "         22278,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101, 11865,  2812, 22278,   117,   123,  2477,   705,   298, 18908,\n",
      "           501,   125,   949,   117,   179,   978,   259, 13841,   325,  7769,\n",
      "           179,   123, 14114,   180,  5032,   324,   117,   122,   325, 10620,\n",
      "          5790, 13665,  1815,   296,   125,  1877, 10443,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   146,   395,   378,   180,  1941,   193,   229, 22280,   495,\n",
      "         11152,   271,   347, 13449, 19088,  2798,   123,   475, 19206, 22290,\n",
      "           252,   746,  1399,   202, 17849,   455,   271,   347,   607,  3093,\n",
      "           183,  3980,   809,   243,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   325,  6372,   328,   179,   123,   549,   304, 16853,   117,\n",
      "           123,  1623,   912,  2477,   705,  4063,   151,   146,   333,   154,\n",
      "         22280,   122,   260, 21021,   171,   254,   862,   117,   582,  2238,\n",
      "          2836,   327, 14739,  1272,  9750,   117,   180,   739,   229,   304,\n",
      "         22280,   316,   581, 17376,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   146,   766,   416,  1697,   122,  1444,   117,  3002,   577,\n",
      "          2746,   117,  1369,   375,   256,   820,   123,  6183,   279,  8497,\n",
      "           151,   179, 12232, 22278, 13038,   124,   170,   260,  2796,  6205,\n",
      "           138,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   222,   644,   117,   320, 13718, 22280,   171,   969,   117,\n",
      "           740, 17520,   375,   256,   173,   222,  6054,   180,  7321,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101, 10137,  2836,   118,  2036,   146,  1831,   123, 15419,   180,\n",
      "           146,  5137, 15586,   117,   325,  1198, 22281,   304,   171,   179,\n",
      "           146,   438,  5505,   180,  2954,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   259, 10752,   180,  1334,   351,  7296,   933,  1186, 17700,\n",
      "         12051, 22287,  2968,   498,   259,   222,   474, 13841,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101, 20559, 22281,   324, 14121,   705,   259,  3852,   128, 13501,\n",
      "           945,   692,   146,  2242,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101, 11865,  2812, 22278,  5127,   171, 17052,   146,   313,  1286,\n",
      "          6774,   121, 22361,  6205, 22278,   744,   123,   577,   130,   524,\n",
      "           117,   271,   123, 11152,  8037, 12764, 22278,   179,  5553, 22288,\n",
      "           173,  1062,   252,   125,  9856,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,  1139, 17520,   375,   117,  4276,   411,   148,   366, 14131,\n",
      "           171,  2727,   260, 16240,  5949,   125,   347,  7055,   117,   122,\n",
      "          7346,   154,   170,   146,  9679,   180,  6629,   117,  1074, 13550,\n",
      "           202,  1187,   268,  6730, 22280,   117,   146,  2242, 21339,  7485,\n",
      "         22279,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   123,   416, 15633, 11997,   117,   327, 20216,   122,  8932,\n",
      "           117,  5911,   304,  1982,  3914,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,   260,  1176, 19423,   712, 10752,   180,   388,  4056,   122,\n",
      "           125,  1084,  3196,  4042, 22282,   705,   423,   655,  1028, 11935,\n",
      "          2650,   146, 13223,   125, 19278,  8788,  2530,   117,   582,  3889,\n",
      "           123, 16853,   532,  3980,  7362,   117,   259, 14874, 19873,   171,\n",
      "          2564,  3239,   117,   260,  6205, 22290,   842,   180,   717,  5105,\n",
      "           170,   179,  1835, 22279,   123,  6472,   117,   122,   260, 17681,\n",
      "         22281,   125,   179,  8788,  2446,   146,  3233,   285, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,  7257, 22282, 21358,  6642,   123, 11152, 14676,   180, 19988,\n",
      "           154,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101, 15568,   123,  2477,   705,   259,  5708,   117,   179,   146,\n",
      "           969,   229, 22280, 17579,  2755,   124,   327,  3122,   526, 21994,\n",
      "           118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,  4271,  3914,   122,  1364,   123,  4108,   266,   118,  1084,\n",
      "           418,   222, 18263, 14848,   117,   176,   122, 18263,   122,   229,\n",
      "         22280,  3179, 11775,  5791,   183,   180, 14151,  2841,   154,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   376,   529, 21347,   146,  4712,   366, 11912, 22281,   179,\n",
      "         18995, 22287,   146,   528,   538,  5708,   146,  5580, 12448,   366,\n",
      "          6205,   138, 16087,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,  9392,   470, 14505,   138,   122, 10128,  9392,   382,  9312,\n",
      "         22287,   118,  2036,   146,  1831,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,   262,  6372,   286,   117,   271,   146, 11552,   117,   146,\n",
      "         22000,   125, 11865,  2812, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   123, 16240,  2034,   173,   483,  3301,   285,   202,  7055,\n",
      "          8383,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,  3746,   470,   125,  5052, 10527,  6605,   546,   229,  8630,\n",
      "           171, 10846,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,   125,   652,  6554,   183,   117,   123,   223, 22280,  4878,\n",
      "           154,  6600,   498,   123,  3466,   180,  9531,   449,  2044, 13449,\n",
      "           172, 22288,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   146,   390,   303, 18263, 13308,  3505, 15249,  1096, 22280,\n",
      "           125,   327,   223, 22279,   117,   582,   123,  2606,   122,  8602,\n",
      "         22280,   125,   370, 13397,   124,   122,  3165,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  4808,   325,   121, 22361,  8410,   179,   180,  1718,   328,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,   146, 10289,   179,   368,   429,   538,  5708,   122,   202,\n",
      "          9169,   117,   229, 22280,   146, 18661,  2779,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   240,   210,   123,  2477,   705, 18253,  1353,   125,   898,\n",
      "           146,  7055,   122,   123,   169,   364, 13714,   117, 17382, 22282,\n",
      "         19870,   221,   146, 18263,   117, 20987,   285,   180,  5923,  3632,\n",
      "           179,  8408, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   123,   223, 22280,   179,  6372,   328,  1718,   364,   117,\n",
      "           418,  3048, 22288,   325,  6372,   328,   122,  4968, 22281, 12569,\n",
      "           256,   146,  5052,   179,  3746,   185,   524,   256,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,   700, 11865,  2812, 22278, 13935,   870,  3355,   252, 14621,\n",
      "           328,  2002,   123, 21639,   320, 10846,   117,  5825,   214, 11631,\n",
      "           123,  8993,  5546, 14004,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,   146, 18263, 11234,  2010,  6642, 22281,   170,  1039,   123,\n",
      "         16240,  2034,   180,  4527,   136,  2010,  1977,   437, 16948,   117,\n",
      "         18263,  4712,   117,   123,  4616,   125, 17080,   925,   148,   128,\n",
      "           136,   171,   323,  7762,  7485, 22279,   123,  3769, 21021,   117,\n",
      "           179,  2364, 12343, 22287,  1342, 18263,   271,  5023,   136,  2010,\n",
      "          1214,   268,   125,  1004,  5533,   117,  2267,   366,  9647,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,  1214,   268,   366,  3145,   179,   437,   249,   925,   148,\n",
      "           128,  1941,  8771,   228,   117,   122,  1790,   376,   259, 17080,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,  2010,  1004,   118,  6661,  1547,   146, 10334,   712,  5097,\n",
      "           298,   316,   581, 17376, 22281,   117, 14415,   366, 12378,   117,\n",
      "           122,   123,  5351,   795,   125, 11997, 20346,   117,  1568, 20933,\n",
      "         10780, 21101,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 31 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.02325581391188584\n",
      " Coesão Score Final: 0.511627906955943\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'logo', 'pois', 'enquanto', 'se', 'nem', 'ou', 'seja', 'como', 'quanto', 'quanto']\n",
      " Número de conectivos: 13\n",
      " Número de sentenças: 31\n",
      "======================\n",
      "Resultados para preprocessado_iracema_jose_de_alencar_cap_2.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'porem', 'logo', 'pois', 'enquanto', 'se', 'nem', 'ou', 'seja', 'como', 'quanto', 'quanto'], 'num_conectivos': 13, 'proporcao_conectivos': 0.023, 'similaridade_media': np.float64(1.0), 'num_sentencas': 31}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,   146, 10334,  5793,   123,  2477,   705,  1032,   143,   180,\n",
      "          7321,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   625,   146,   969,   273,  2227,   581,   256,   498,   123,\n",
      "          7045,   298, 20578,   117,   122,   123,   577,   266,  1950,  5723,\n",
      "           171,  4707,   180,  6629,   259,  1867, 12825, 22290,   683,   117,\n",
      "          1061, 15731,   202,  5488,   123,   739,   316,   581,   122,   325,\n",
      "          5533,   117, 15231, 21794,   202, 15998,   430, 22280,   117,   123,\n",
      "         15419,   298,  7188,   717, 11837,  1044,   117,   123,  5351,   795,\n",
      "           243,   372,   537,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   146,  7889,   151, 22280, 19016,   256,   123,  4303,   117,\n",
      "         21541,   229,   860,   364,   125, 18757,  1382,   581,   117, 13750,\n",
      "          1552,   259, 15797, 22281, 20457, 22281,   125,  5023,   321,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   146,  1744,  3897,   331,  1703,   180,  2042,   375,   505,\n",
      "          2890,   256,   117,   271,  2143,   942,   125,  3233,   285, 22280,\n",
      "           117,   259,   408,  9539, 22281,   122, 16734, 13841,  8618,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   125,  1031,  1281,   178,   179,  1011,   117, 20158,   151,\n",
      "           123,  1069,   701,  5708,   329,  2370,   122,   529,  1315,  1557,\n",
      "         16087,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   146,   372,   537,  1340,  2042,  2904,   259,   682,  5184,\n",
      "           382,   179,  1938, 20798,   692,  4450,   203,   792,   123, 15419,\n",
      "           125,   230,   388,  4056,   969,   931,   151,   179,  8940,  3600,\n",
      "         12674,   214,   118,   176,   423,  5488,  1796,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   625,   259, 19204,  8880,   229, 15592,  2377,  2755,   124,\n",
      "           171, 17849,   455,   117,   318, 13793,   347, 11552,   271,   146,\n",
      "           171, 16815,   130,   117,  4613,   373,   260,  1510,  1671,   117,\n",
      "          6233, 11865,  2812, 22278,   122,  4970,   179,   123, 16246,   222,\n",
      "          2656, 18263,   117,   125, 17253,   646,   304,   122,  5533, 22281,\n",
      "          3145,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   260,  8217,   316,   581, 17376, 22281,   117,   121, 22361,\n",
      "          1202, 22287,  8251,   151,   321,   581,   117, 18402, 22287,   125,\n",
      "           230,   940,   646,   304,   125, 14449,   117, 14874,   271,  2968,\n",
      "           125, 16468, 22281,   304,   117,  2374,   557, 22281,   125,  2873,\n",
      "           154,  1269,   421,   260,  7414,   171,   311,  1620, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   146,  7889,   151, 22280, 17662,   179,  2589,   222, 18263,\n",
      "          4327,   117,  6086,   179, 17573,  2836,   259,  5097,  9283,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101, 20885, 22280,   117, 14657,   203,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   123,  2477,   705, 12110,   221,   146, 10334,   122,  1331,\n",
      "          2010,   368,  3429,   117,  1568,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[ 101, 2010, 3429, 1004,  102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,   122,  5023,   321,   179,  3889,   146,  9730, 22279,   123,\n",
      "          5351,   795,   125, 11997, 20346,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,  1016,  4111,   117,   146,   372,   537,  1367,   146, 13850,\n",
      "         22178,   320, 10334,   122,  8880,  2592,   229,  5351,   795,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   146,  9922,  2152, 22280,  1636,   203,   118,   176,   229,\n",
      "          2551,  1310,   117, 20083, 22278,   202,  1997,   180,  1967,   304,\n",
      "         22280,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101, 11865,  2812, 22278, 10049,  2071,   146,  4848,   180,  5627,\n",
      "           292,   122,  5626,   146,   179,  1021,   125, 18025, 22280,   143,\n",
      "           221, 21619,   123, 11062,   122,   123,  2496,  4250,  2650,   146,\n",
      "          4745,   180,   329,   304,   117,   123, 20080,   118,   121, 22361,\n",
      "          6205, 22278,   117,   259, 11380,  7296,   933, 11060,   117,   259,\n",
      "           395,  2370,   125,   949,   122,   146, 10832,   125,   329,   741,\n",
      "           122,  9480,  1343,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   700,   123,  2477,   705,  3033,   170,   123,   254,   421,\n",
      "         13714,   117,   179, 16386,   371,   229,  3944,  6730, 22278,   125,\n",
      "          6205, 22278, 12837,   304,   221, 11348, 22282,   146,  9169,   122,\n",
      "           260,   223,   128,   243, 10334,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,   625,   146, 18263,  3303,   123, 19538,   304, 22280,   117,\n",
      "           146,  4575,   372,   537, 17896,   203,   146, 13850, 22178,   122,\n",
      "         11234,  2010,  7762,  7485, 22279,   136,  2010,  1976, 22287,   117,\n",
      "          9396,   146, 10846,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,  2010,  1004,  7762,  7485, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   146, 10334,   122,  7258,   229,  5351,   795,   125, 11997,\n",
      "         20346,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   259,   316,   581, 17376, 22281,   376,   592, 14449,   221,\n",
      "         10591,   118,  1340,   117,   173,   209,  2210,   834,  1284,   221,\n",
      "          1312,   118,  1340,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,  1331, 22279,   117,   122,   944,   437, 13486, 18003, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,  2010,   372,   537,   117,  2779,   437, 13406,   303,   146,\n",
      "           762,  7591, 22290,   268,   179,   311,  2166,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  2044,   179,   146,   969, 16737,   117,  4314,   244,  5023,\n",
      "         22278,  5351,   795,   122,   437,   249,  5097, 13793,   323,  1976,\n",
      "         22287,  7955,   449,   229, 22280,  7427,  5308,   118, 15887,   834,\n",
      "          4640,   118,   437,  1977,   122,   146, 18263,   117,   179,  3283,\n",
      "           677,  3695,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,  2010,   262,   123,  5023,   321,   179,   146,   372,   537,\n",
      "          5083,   368,   437,  5626,   117,   368,   437,  4915, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101, 11997, 20346,  3874,  1191,   423,  9730, 22279,   229, 22280,\n",
      "         10573,   171,   323,  2158,   117,   122,   625,  2541,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   176,  3189,   143, 18165,   117,   273,  2227,   498,   964,\n",
      "           259, 13994, 20073, 22281,   176,  3189,   143,  5961,   117,   437,\n",
      "         22288,  9730, 22279, 13083,   154,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,   146, 10334,  1996,  2010,  7206,   298, 14449,  8618,   117,\n",
      "           179, 14748,   228,   123,   316,   581,   529,  7414,   171,  1941,\n",
      "         12470,  7057,   117,  3047,   171,   528,   117,   582,  1967, 22287,\n",
      "           259, 19147,  1185,  9777,   117,  5999,   125,  5023, 22278,   229,\n",
      "           304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,  7343,   655,   122,   528,  2762,   117,   179,   229,  5023,\n",
      "         22278,  3182, 22278,  1331,   271,  1417,   125, 18263,  7343,  5052,\n",
      "           117,   146,   243,   739,  2049,   179,   652,  4970,   260,  3145,\n",
      "           125,  5023, 22278,  9019,   151,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,  1941, 17080, 16863,  2799,  9288, 11306,   240,   528,   260,\n",
      "           956,  1495,   171,   221, 10224,   117,   125,   582,  8198,   122,\n",
      "           146,  3795,   117,  1950, 10490,   825,   298,   532,   117, 13910,\n",
      "          2535,   259, 14985, 22281,   333,   183,   143,   171,  1178,  1125,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,   331,  2779,   125, 14730, 10692, 19623,   117,  2113,  1011,\n",
      "           420,   259, 13779,   193, 12470,   138,   125,  1334,   124, 22288,\n",
      "           117,   229,  5351,   795,   171,  4332,   378,  4928, 22283,   117,\n",
      "           925,   148, 22280,   125,  1941,  5132,   324,   117,   179, 14055,\n",
      "           654,   170,  1039,   123,   388,  4056,   180,  8286,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101,   607,  1510, 22281,   331,   145, 18318,   793,   221,   123,\n",
      "           329,   304,   122,  7955,   298, 17080,   117,  1976, 22287,   712,\n",
      "          5097,   298,   154,   581, 17376, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,  2010,   262,  3179, 11775,  5791,   183,   180,  7321,   179,\n",
      "          2992,  2904,   146, 18263,  4712,   202, 13389,   180,  6629,   117,\n",
      "          9396,   146,  7889,   151, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   123, 17649, 22278, 17554, 22288,   117,  1202, 22287,   117,\n",
      "           229,  4493,   171,  5488,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101, 11314, 22278,   123,  2954,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 35 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.023696682427019458\n",
      " Coesão Score Final: 0.5118483412135097\n",
      " Conectivos encontrados: ['e', 'mas', 'assim', 'logo', 'pois', 'porque', 'quando', 'se', 'ou', 'ora', 'quer', 'como', 'logo que', 'porque', 'tanto']\n",
      " Número de conectivos: 15\n",
      " Número de sentenças: 35\n",
      "======================\n",
      "Resultados para preprocessado_iracema_jose_de_alencar_cap_3.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'assim', 'logo', 'pois', 'porque', 'quando', 'se', 'ou', 'ora', 'quer', 'como', 'logo que', 'porque', 'tanto'], 'num_conectivos': 15, 'proporcao_conectivos': 0.024, 'similaridade_media': np.float64(1.0), 'num_sentencas': 35}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,   146,   372,   537, 11744,   748,   146, 15273,   304,   117,\n",
      "           122,  5127,   180,  5351,   795,   117,   240,   210,   146, 10334,\n",
      "           229, 22280,  1767,   331,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101, 11865,  2812, 22278,  4706, 22278,   170,   260,  2459,  7000,\n",
      "           221,  6202,   146,  9730, 22279,   125, 11997, 20346,   117,   122,\n",
      "           259, 14449, 13545,   221, 12126, 11237,   943,   118,  2036,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,  2010, 18263,  4712,   117,  1996,   123,  2477,   705,   117,\n",
      "           146, 15537, 14368, 22279,  5023, 22278,  2551,   726,   123,  2954,\n",
      "           122,   146,   969, 12424, 22278,  3377,   123,   437,   249,  5708,\n",
      "           117, 14161,   123,  5023, 22278,  8410,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   122,  1016,  4111,   117, 11865,  2812, 22278,   978,   146,\n",
      "         18908,   247,  8574,   452,   117,   122,   222,   328,   123,  1877,\n",
      "           269,  1609,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,  2010,  5023,   311,  5308, 22281,   136, 16795,   528,  2762,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,  2010,   260,   325, 18636,  2459,   180,   739,   316,   581,\n",
      "           336,  1039,  8679,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,  2010,   221,  2859,   123,  2267,   125, 11997, 20346,   229,\n",
      "         22280, 12444,   370, 19440,   146,  9730, 22279,   123,  5351,   795,\n",
      "           171,   372,   537,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,  2010, 10334,   117, 11865,  2812, 22278,   229, 22280,   706,\n",
      "           333,  5023, 22278,   333,   256,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   122,   740,   179,  5825,   146, 11021,   180,  3436, 21101,\n",
      "           122,   146,  9250,   247,   171,  8846,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   327,   148, 22280,  7875,   221,   146,   372,   537,   123,\n",
      "         14422,   125,  5023,   321,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   146, 18263,  7045, 22280,  7570,   203,   123,  5351,   795,\n",
      "           122, 20158,   456,   118,   176,   229,  1510,   256,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   123,   739,   316,   581,  8004,   151,   118,   176,   202,\n",
      "          4707,   171,  5488,   117, 16188,   251,   954,  6611,   128,   180,\n",
      "         14161,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  1315,  1096,   146, 15273,   304,   320,  8420,   157, 15106,\n",
      "           171,  6916, 22280, 16853,   117,  3985,   151,   123,  4654,   304,\n",
      "           173,  4899,   123,  1315,   272,  6846,  3292,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   146,   372,   537,  8424,  4337,   151,   146, 15797,  5849,\n",
      "          8631,   122, 10355,   320,   264,   378,  3960,   175,   259, 21774,\n",
      "           125,  5023,   321,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   146,   636,  3795,   180,   229,   304, 22280,   316,   581,\n",
      "         17376,   117, 11865,   862, 22278,   117,  7433,   124,   171,  2979,\n",
      "           180, 10530,  8251,   151,   321,   581,   117,   221,  4915,   260,\n",
      "          8217,   171,   333,   154, 22280,   598,   146,  2949,  3111,   339,\n",
      "         13779,   193, 12470, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   259, 14449,   171,  5488, 17337,  2872,   123, 13109,   171,\n",
      "          3795,   117,   122,   146,  6730, 22280,  3076,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   146,  9922,  2152, 22280,  7045, 22280,  4970,  5533,   146,\n",
      "          4982, 22280,   180,  4939,   117,   122,  1367,  1202, 22287,   117,\n",
      "           122,  9834, 22288,   146,  2992, 22288,  5580,   834, 15553,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,   123,  4595,  8221,   117,   179,   318, 13793,  6788,   256,\n",
      "           498,   123,  1034, 11312,   180,  7321,   117,   866,  4373,   347,\n",
      "          6793, 11948,   221,   260, 12837,  1149,  7414,   171,  1334,   124,\n",
      "         22288,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,   625,   368,  2153,   618,   203,   146,  5488,   122,  8544,\n",
      "         11092, 14754,   229,  6629,   117,   146,  5184,   183,   125, 11865,\n",
      "          2812, 22278,  4751,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   123,  2477,   705,  3866, 22278,   146, 10334,  3255,   123,\n",
      "           235,  1046, 22278,   233,   630,   179,   398,   810, 22278,   834,\n",
      "           362, 22282,  4643,   130,  4765,   240,   420,   123,  6847,   984,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,  2010,   240,   179,   117,  1996,   740,   117,   146, 10334,\n",
      "         18370,   123,  5351,   795,  9730,   556,   834,  4915,   146,  2981,\n",
      "           180,  1359,   136,  1977,  1191, 17356, 22280, 18263,  4712,   229,\n",
      "          2480,   298,   316,   581, 17376, 22281,   136,   146,  7045, 22280,\n",
      "         10733,  2249,   495, 19219,   123,   179,  1445,   122, 12524,   118,\n",
      "           176,  1486, 21308,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,  2010, 16241,  1537, 22287,  1191,  3002,   320,   437, 22288,\n",
      "          9730, 22279,   117,  2267,   125, 11997, 20346,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   495,   146,  6532,   125,   792,   532,  3667,   179,   146,\n",
      "          4415,  5723,   298, 17151, 22281,   298,   316,   581, 17376, 22281,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,   229, 22280, 16403,   146,  2981,   180,  1359,   449,  1904,\n",
      "           173,   327,  8410,   123, 10201,  2028,   125, 11865,  2812, 22278,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,  2010,   176,   123, 10201,  2028,   125, 11865,  2812, 22278,\n",
      "         10606,   149, 22361,  8410,   171, 10334,   117,   740,   229, 22280,\n",
      "           146,  4314,   151,  1018,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   146,  9059,   229, 22280,  1904,   123, 11912,   285,  8525,\n",
      "          1182, 22278,   117,   625,   123, 11912,  5167, 22279,   123,  6205,\n",
      "         22278,   180,  9856,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   123,  2477,   705,  4217,  5461,  2010, 18263,  4712,   117,\n",
      "          2521,   179, 17649,  3301,   781,   185,   180,   329,   304,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,   146,   925,   148, 22280,   125, 11865,  2812, 22278,   376,\n",
      "           146, 12728,   233,   630,   179,  7525,   403,   472, 22280,  1095,\n",
      "          7270, 22278,   420,   259, 12484,   180,  6629,   122,   146, 11552,\n",
      "           171,   146,  5137,   492,   179,   873,  1407,   229,  1510,   256,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,   368,   437, 13375,   124,   260,  7414,   171,  2187,   366,\n",
      "           702,  1149,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,  2010,  2249,   596,   176,  3852, 22278,  1075,   179,   146,\n",
      "           925,   148, 22280,   125, 11865,  2812, 22278, 10726,   125,  1359,\n",
      "           229,  5351,   795,   125, 11997, 20346,   136,  2010,   146,   969,\n",
      "           117,   179,  2541, 16737,   117,  2962, 22278,   170,   146, 18263,\n",
      "         17649,  3301,   712,  5097,   171,   254,   862,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,  2010,   437, 22288,  9730, 22279,  2521,   117,  2267,   125,\n",
      "         11997, 20346,   449,   176,   146,   969,  2862,   229, 22280,  5626,\n",
      "         22282,   146,   925,   148, 22280,   125, 11865,  2812, 22278,   117,\n",
      "           368,  4915, 22278,   146, 14739,  1339,  4712,   123,   316,   581,\n",
      "           298, 13779,   193, 12470,   138,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[ 101,  528, 2762, 2927,  123, 5351,  795,  171,  372,  537,  102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,   123, 21672, 22278,  2551,   179, 11865,  2812, 22278,  3980,\n",
      "           809,   124,   170,   123,   398,   387,   171,  7489,  1286,   314,\n",
      "          5825,   256,   118,  2036,   222, 11334,  1945,   283,   122, 11152,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   146,  7045, 22280,  2251, 22282,   155,   685,  8362,   214,\n",
      "          4217,   364, 22282,   117,   420,   259,   362, 22282,  4643,  1408,\n",
      "           180,  7321,   117,   146,  2242,   223,  3629, 22280,   180,  2477,\n",
      "           705, 19345,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 34 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.018998272851471033\n",
      " Coesão Score Final: 0.5094991364257355\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'assim', 'quando', 'se', 'ou', 'como', 'quanto', 'antes que', 'quanto']\n",
      " Número de conectivos: 11\n",
      " Número de sentenças: 34\n",
      "======================\n",
      "Resultados para preprocessado_iracema_jose_de_alencar_cap_4.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'porem', 'assim', 'quando', 'se', 'ou', 'como', 'quanto', 'antes que', 'quanto'], 'num_conectivos': 11, 'proporcao_conectivos': 0.019, 'similaridade_media': np.float64(1.0), 'num_sentencas': 34}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  4141, 13793,  2415, 22280,   262,   117,   298,  9967,   712,\n",
      "          4698,   122,  1685,   481,   117, 12531,   125,   222, 12447,   397,\n",
      "           179, 20613,   685,   420,   260,  1256,  7367,   125,   230,  5980,\n",
      "         22278,   122, 21597,   392,   316,   391,   324,   538,  1314,   211,\n",
      "           683,   171,  2907,   171, 11967, 10953,   763,   122,  1971,  7569,\n",
      "          3770,   171,  1695,   179,  5076, 22278,  4922,   623, 12051,   125,\n",
      "           481,   117,   179,   117,   320,  9215,   118,   176,   146,  9019,\n",
      "         13793,   221,   123,  2480,   117,  2036,  2789,   117,   173,  7855,\n",
      "           125, 11263, 22281, 11938, 22281,   117,  2798,   331,   123,  5304,\n",
      "           170,   146,   179,  1011,  1839,   117,   271,   744,   222, 12059,\n",
      "           122, 22191,  5292,   173,  3495,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,  4996, 17551,   122,  8350,   240,   327,  1284,   117,   146,\n",
      "         13254,  7304,   748,   118,   176,   123, 18908,  3239,   304, 22280,\n",
      "           744,   170,   325,   388,   947,   117, 13601,   118,   176,   125,\n",
      "          1815,  2607,  2032, 22280,   125, 20613,   943,   117,   179,  9620,\n",
      "         21206,   398,  4861,   243,   260,   325,  3072, 22281, 11118,   303,\n",
      "           143,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,  4678,  4322,   498,   146,  3568,   304, 22280,   180,  2004,\n",
      "         22278,  5304,   117,   173,  5530,   125,   230,   860,   364,   117,\n",
      "          2636, 17343,   458,   125,   222,   629,   303,   125,  9466,   321,\n",
      "         13140,   125, 19278,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   123,  9652,  5646,   524,   256,   118,   219,   252,   117,\n",
      "          9402,  1256, 18384,  7911,   240,   644,   117,   230,   163,  8860,\n",
      "         17124,   327, 13219,   117,   123, 10420, 18398,   852,   117,  3336,\n",
      "           266, 19125, 11769,   117, 17479,   125,   222,  4575,  2992,   339,\n",
      "         17642,   173,  7472,   125,  1796,   122,  8932,   285,   170,   222,\n",
      "          1456,   143,   179,   978,   230,  3883,   304,   125,   223, 22280,\n",
      "           122,  5057,   958,   555,   229,   651,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101, 10420, 18398,   852, 14619,   210, 10454,  2124,   123,   327,\n",
      "           163,  8860,   285,   495,   123,   325,  1004,   870,  2127,  3897,\n",
      "         20733,   171,  2907,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   125,  1062,   252,  5133,   151,  5450, 22288,   117,   122,\n",
      "           123,  2954, 11371, 10310,   373,   122,   847,  1149,   125, 10692,\n",
      "          1655,  9141,   256,   125,  1955,   123,   347,  8517,  4698,   592,\n",
      "           118,  7911,   240,   454,   117,   122,   117,  2440,  1659,   117,\n",
      "           978,   125,   670,  1821,   179,   146,  1395, 17551,   221,   123,\n",
      "           313,   512,   322,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   222,   644,   117,   240,   210,   117,   146,   347,  2397,\n",
      "           117,   700,   125, 13239,  5899,  2653,  3611,   117, 14537,   348,\n",
      "           230,  5890,  2886,   260,   675,   344,  1149,   117,  6600,  4807,\n",
      "           229,  4768,   117,   320,  1341,   180,  3883,   304,   117, 15872,\n",
      "         10490,   243,   271,   230,  5294,  8849,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,  4141, 13793,  2415, 22280,  6075,   739,  3316,   240,   418,\n",
      "           273,   522,   304,   117,  1191,   118,   176,  2684, 17469,  7600,\n",
      "           298, 14632, 22281,   180, 13219,   117,   122,   170,  3846, 20045,\n",
      "         22280,   123, 19068, 14995,   117,   179,   123,  3264,  2606,   146,\n",
      "          9916,   221, 14809,  9238,   366,   675,   273, 15256, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,  6540,   118,   176,   170,   368,   117,  5222,   118,  2036,\n",
      "           123,   327,  1069,   125, 14283,  7557,  3391, 22280,   143,   122,\n",
      "          5781,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,  1112,   347,  7258,   170,   151,   118,  2036,   123,  6614,\n",
      "           171,  1831,   229, 22280,   495,  5911, 10838, 22280,   221,   230,\n",
      "          6754,  2606,   370,   125,  3240, 22282,   900,   363, 22361,  1369,\n",
      "           117,   944,   259,  2112,   117,  4698,   592,   118,  7911,   173,\n",
      "          3495, 22354,   122, 11021, 22288,   118,  2036,   318, 13793,   146,\n",
      "           179,   978, 10506,   243,   221,   123,   327,  4676,   122,  2467,\n",
      "         11765,   320, 12447,   397,   179,  2036, 15242,   236,   260,  3338,\n",
      "         22281,   117,  2113,  1941,   125,  5288,   576,  1796,  6641,   251,\n",
      "           240,  1956,  3563,   128,   179,  2036,  8880,   229,   163,  8860,\n",
      "           285,   954,  8001,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   180, 22283,   173,  4271,   117,  4141, 13793,  2415, 22280,\n",
      "          1204,   118,   176,   146,  8097,   117,   146, 18737,   122,   146,\n",
      "         12781,   180,  3336,   266,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   202,  1338,   125,  1695,   596,   495,   368,  1977,  5267,\n",
      "           256,  1284,   125,  2745,   179,   740,  1271,   151,   122,   495,\n",
      "         14619,   210,  1977,  5078,   252,   122,  1598, 13808,   298,   532,\n",
      "         12651,   501,   117,   122,  1977,   176, 12771,  2836,   125, 11935,\n",
      "           367,   320,  7258,   259,  4698,   592,   118,  7911,  3509,   441,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  6540,   118,  2036,  2044,   230,  1284,  5820,   117,   122,\n",
      "           123,   163,  8860, 17124,   117,   625, 11736,   125,  3495,   221,\n",
      "          1569,  5664,   117, 10348,   222,  5995, 22280,  2684,   123,  5304,\n",
      "           122, 16653,   118,   146,   366,   223,   128,   171, 12447,   397,\n",
      "           117,   125,  1112,   347,  4141, 13793, 22354,   117,   271,   740,\n",
      "         10355,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   347,  4141, 13793,   125,  1584,   256, 11637,  1780,  3867,\n",
      "          4296, 17088, 22281,  1362,  6846,   140,  4029, 21102,   117,   173,\n",
      "          3561,  1400,   125,  1798,   332,   243, 19449, 22278,   118,   176,\n",
      "           117,  3002,  2685,   122,   173,  4824, 20299, 22281,   125,  1955,\n",
      "          1112,  8748,   122,   723,  1430,   125, 10420, 18398,   852, 22354,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   122,   240,  1815,   547,   262,   146,   316,   391,   514,\n",
      "           397,  7551, 12908,  2028,   202,  5791,   183,   180,  2606,   117,\n",
      "           179,   418,   870,   509,  3874,   325,  2709,   619,   331,   240,\n",
      "           898,   117,   122,  5205,   256,  2461,   117,  2992, 18004,   175,\n",
      "           117,  1364,   122,  1569, 11717,   342,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   240,   169, 14116,   117,   176,   614,   210, 11736,  8364,\n",
      "           170,   740,  1569,  3907,   523,   117,  2798,   325,   176, 10348,\n",
      "           320,  1223,   125,  2863,   118,  1084,   117,  8544,  2044,  2368,\n",
      "           123,  4141, 13793,  2415, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[ 101,  625, 7320, 1154, 2072, 8932,  442,  102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,   368, 19284,   118,  2036, 11460,   210,  5062,   122,   740,\n",
      "         11353,   125,  4332,   942, 13059,   117,  8540,   173,  3285,   140,\n",
      "           118,   176,   125,  1160,   170,   222,  1456,   143,   117,  2113,\n",
      "           117,   271,  1719,   123,   329,  7882,   852,   117, 10420, 18398,\n",
      "           852,   229, 22280,  4750, 22076, 22282,   118,   176,   123,  7769,\n",
      "           122,  2863,   256, 16589, 15500,   692,   403,   146,  2397,  1532,\n",
      "           646,   304,  2886,   123,   327,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,  4141, 13793,  2415, 22280, 10107,   318, 13793,   117,   170,\n",
      "           260,  3338, 22281,   180,  8932,   117,  1089,  1877,   793,   125,\n",
      "          5856,   320,  1341,  9657,   180,  5304,   117,   122, 17760,   230,\n",
      "           504,   508,   125,   924,  6929,   117,  7398,   320,  1423, 20114,\n",
      "           246,   123,  4768,   117,   660,   123,   670,   180,  2375, 11826,\n",
      "           123,   163,  8860,   285,   122,   123,   171,  4707,   221,   222,\n",
      "          4678,  8476,  2758, 22280,   179,   176, 15580, 22288,   170,   259,\n",
      "           329, 17738,   942,   125, 10420, 18398,   852,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,  1021,   117,  1202, 22287,   180,  9461,   117,   230,   271,\n",
      "           285,   125,  1941,  5105,   404,   785,  7492,   170, 12361,  2912,\n",
      "           138,   125,  4437, 11229,  1941,   528, 22279,   401,   117,   222,\n",
      "         11032,   428,   247, 13140,   125, 14125,   122,   344,   825,   125,\n",
      "          1798,   125,   549,   117,   222,   475, 22288,   739,   125, 16064,\n",
      "          4793,   316,  2041,   201,   117,   682,  2764, 19161, 22281,   125,\n",
      "         16341,  5159,   125,   230,   331,  6514, 22278,   122,   222,   928,\n",
      "           328,   415,  5351,   878,   125,   466,   702,   229,  8130,   117,\n",
      "           170,   123,   327,  7740,   403, 13218,   125, 17541,   683,   125,\n",
      "          1224,   343,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   146, 12447,   397,  2364, 18424, 22278, 19020,  8556,   151,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,  2010,  2535,   117,  1996,   368,   123,  3336,   266,   117,\n",
      "           260,  4486,  1447, 22280, 13239,  1407,   221,  2354, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,  2354, 22279,  2541,  4412,   344,   124,  2779, 18450,   170,\n",
      "           146,   179,  3207,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  3876,   644,   368,  5127,   785,   123,  4768,   117,   122,\n",
      "           230,  2767,   700,  4169,   170,   230, 14121,   125,  1798,  1719,\n",
      "          3321,   117,   179, 16625,   173,  4410,  2729,   123, 20216,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,  2010,  2354, 22279,  2535,   229, 22280,   376,   325,  7258,\n",
      "          4492,   173,  2590,   123,  8092,   117,   179,   740, 16143,   420,\n",
      "          1084, 20739, 13406,  7345,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[ 101, 2535,  418, 3039,  102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,  4678,  2836,   175,   146,   179,  2354, 22279,  3283,   140,\n",
      "           122,   331,   347,   122,   325,   125,   532,  2292,   117,   176,\n",
      "           259, 18424,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,  2467,   118,   176,   146, 16195,   125,  7198,   259,  4698,\n",
      "           592,   118,  7911,   123,  1143,   185,   171,  2992,   339,  2010,\n",
      "           144,  2640,   123,  9349,   176,   179,  1445,   122,   180,  7716,\n",
      "           368,   117,   271,  7343,  7258,   117, 19764,   146,  1955,   117,\n",
      "         19764,   146,   179,   495,   347,  2010,   347,   291,   229, 22280,\n",
      "           347,   117,  2467,   118,   176,   122,  1069,   940,   598,  1364,\n",
      "           146, 12215,   117,  6540,   118,   176,  3876,   644,   230, 16317,\n",
      "          1165,   125, 10832,   171,  5457,   117,   122,   259,   682, 19938,\n",
      "           228,   118,   229,   173,  6320,   320,   739, 14611,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,  5147,   117,   123,  1815,  3743,   125,  4676,   495,  1706,\n",
      "           171,  2004, 22280,  4141, 13793,  2415, 22280,   117,   122,  2798,\n",
      "           653,   146,  9264,   117,   179,   368, 18047, 22288,   125,  1143,\n",
      "           269,   702,   118,  2036,   173,  5530,   117,   221,  2822,   123,\n",
      "          9066,   266,   636,  6322,   292,   117, 15010, 15891,   375,  2113,\n",
      "           146, 14657,   183, 17670, 22278,   230, 22107,  5822,  1941, 19946,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,   146,  7258,   125, 10420, 18398,   852,   229, 22280,  1023,\n",
      "         14019,  3482,   171,  2099,   146,   179,  2036,   380,   654,   117,\n",
      "          1141,   117,   262,   179,   123,   327, 17479,  2036,  1021,  4173,\n",
      "           286,   221,   123,   475,  2832,   700,   180,  1386,   171,  3695,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[ 101, 2010,  146, 2992,  339,  179, 1214,  252, 3344,  118, 1084, 5863,\n",
      "          117,  176,  344, 4051,  102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101,  9898, 22288,   146, 12447,   397,   125,   898,   221,   898,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,   368,   179, 11314, 22278,  4922,   122, 20840,   176,   376,\n",
      "           291,   229, 22280,  1174,   337,   138,   229, 22280, 14940,   117,\n",
      "           331,  1767, 20885, 22280,   125,  1364,   180, 22283,   123,  1510,\n",
      "         22281,  2112,   117,   625,  2036,   380,   654,   123,  1386,   171,\n",
      "          4575,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   123, 17479,  3852, 22278, 12931,   173,  2316, 20798,   123,\n",
      "          1569,   298,  2292,   171,  4807,   449,   117,   240,  2983,   117,\n",
      "          3874,  1021,   179,   746,   159,   682,  9196,   272,  1289,   125,\n",
      "          2405,   636,   179,   117,  4276,   211,  3281,   123, 20172, 22278,\n",
      "           117, 13917,   923,   125,  2745,   117,  1528,   125, 20949,   118,\n",
      "           176,   229,  7062,   125,   230,  3336,   266,   123,  1977,   229,\n",
      "         22280,  1413, 22287,   125,  1415,   481,  7583,   670,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,  1112, 11032,  1587,  5723,  1941,   117,   122,   229, 22280,\n",
      "           495,  1695,   117,   146,   179,  2036,  2365,  4816,   201,   726,\n",
      "          1971,   596, 22354, 10420, 18398,   852, 15010,  2535,   320,  1341,\n",
      "           125,  4141, 13793,  2415, 22280,   146,  1798,  5849, 17377,   125,\n",
      "         11314,  2650,   397,   117,   125,  3293,   122,   125, 11003,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,   390,  3750,   524,   256,   123,  5488, 22282,   117,   449,\n",
      "           125,  1354, 20073,   260,  1256,   180, 12495,  1011,  1941,   229,\n",
      "           395,   387,   125,   944,   259,  1564,   117,  4042,   348,   146,\n",
      "         19935,   221,   259,   958,  2118,   143,   122,   700, 19866,   146,\n",
      "         16960,   303,   221,   259,  5684,   125,   230,  1449,  1272,   179,\n",
      "          1021,   221,  1202, 22287,   125,   222,   739,   853, 18983,   162,\n",
      "           712,  8001,   180,  5304,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101,  8525,   322,   123,  1105,   117, 13159,   256,   117,  5133,\n",
      "           151,   320,  3568,   304, 22280,   229,   316,   391,   324,   117,\n",
      "           625,   146,  3695,  6952,   256,  9955,  1084,   240,  1796,  5057,\n",
      "           123,   327,   163,  8860,   285,   726,   146,   644,   202,  9556,\n",
      "           125,   736,  1312,   942,   117,   122,   123,  2954,  9882,   118,\n",
      "           176,   221,   123,  4303,   180,  5304,   117,   122,   117,   975,\n",
      "          1885,   185,   125,   222,   227,   702,   458,   125, 13954,   117,\n",
      "         10310, 11541, 10692,  1655,   122, 10310, 10009, 22278,  9344,  6436,\n",
      "           842,   117,   179,  2415, 22280,  8544,   412,  1062,   252,   117,\n",
      "           173,  8037,   138,   125,  7924,   117,   125,  3541,   942,   122,\n",
      "           834,  5899, 22281,   117,  8977,   123,  7815,   171, 11371,   102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,   122,   146,  3174, 13733,   180,  2606,   744,  7339,   596,\n",
      "           221, 11348, 22282,   122, 18486,   578,   117,  1202, 22287,   180,\n",
      "           327,   117,   123, 11451,   171,   347,  2397,   117,   179,   418,\n",
      "           117,  1201,   252,   123,  3295,   117,   229, 22280,   495, 19020,\n",
      "           122,  2364,  9882,   173,  1364,   146,   454,   125,  1089, 10115,\n",
      "           125, 14790,   138,   125,  1757, 21304,   185,   122,  1028, 17783,\n",
      "          7924, 22281,   125,  3979,  1196,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,  4141, 13793,  2415, 22280,   229, 22280,  8625, 22278,  2364,\n",
      "           123, 17611,   117,  2798,  8544,   123, 13544,   712, 18433,  2745,\n",
      "           179, 10971,   151,   123,   327,  5304,   122,   325,   123,   163,\n",
      "          8860,   285, 16246,   641,  5137, 21102,   221,   123,  8097,  7569,\n",
      "           232,   122,   180, 22283,   318, 13793,   221,   146,  6465,   102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,  1971,  1016,   179,   117,   222,   622,   700,   180,  8402,\n",
      "           232, 22280,   180,  3336,   266,   117,  6738,   173,   607,  8849,\n",
      "         10033,  1450,  4332,  1149,   125,  2480, 18349,   320,  4707,   180,\n",
      "           316,   391,   324,   117, 13860, 14273, 22288,   118,   260,  2044,\n",
      "           122, 20242,   117,   834,  5112,   125,   596,   117,   125,  4902,\n",
      "          1510, 22281,   504,  4242,   125,  4303,   122, 11471,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,   179, 12935,   207,   125, 14657,  7919,   122,   125,  3338,\n",
      "           229, 22280,  4172,   368,  4922,   893,   304, 22280, 15724,   125,\n",
      "          1449,  1339,   117,  3330, 22281,   375,   256,   122, 17754,   256,\n",
      "         13954,   117,  6642,   256,  5028,  5028,   117,   179,   146,  7492,\n",
      "           303,   117,  1796,   125,  2856,   117,  1982,   170,   123,  8932,\n",
      "           117, 16463, 11814,   123,  1449,  1272,   171,  4707,   117,   180,\n",
      "          1589,   547,   179,   695,   436,   923,   146,  3028,   366,  4103,\n",
      "           173,  1706,   179,  1021,   240,  1369,  3047,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,  2983, 16463,   382,  1085,  5159,   170,  1485,   260, 17649,\n",
      "          6204, 22281,   122,  1684, 18728, 22281,   171,  1407,  1518,   117,\n",
      "           416,  1149,   123,  7892,  2148,   351,   125,   179,  3876,   596,\n",
      "           123,   661,  1144,   229, 22280,   176, 15245,   785,   240, 11665,\n",
      "          2847, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,  4141, 13793,  2415, 22280,  5521,   256,   726,   146,   644,\n",
      "          1647,   260,  1860,   173,   179,  9086,  3028,   221,   146,   644,\n",
      "          1457,   117,   122,   123,  2954,  1084,  1011,   368,  5302,   185,\n",
      "           117,   325,   123, 10420, 18398,   852,   117,   123, 16374,   210,\n",
      "         14038,   138,   117, 19108,   117,  4117,   842,   117,   629,   942,\n",
      "           125,  1945,   117,   221,   146,  1423,   180,  4768,   117,   170,\n",
      "          3541,   252,  7349,   179,   176,   229, 22280,  8362, 22278,  1003,\n",
      "           411, 20732,   125,  7257, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101,   700,   117,   222,  5267,   256,   230,  5890,   122, 18318,\n",
      "         22278,   221,  1105,   117,  1139,   146,  1342,  9086,   125, 20482,\n",
      "         14036,   320,  1341,   171,  4745,   117, 13610,   123,  2822,  4227,\n",
      "           117,   173,  1652,   125,  9538,   122,   117,   625,   146,   179,\n",
      "           978, 19480,  1359,   256,   117, 16246,   318, 13793,   146, 10110,\n",
      "           117, 21617,   240,   327,   576,   102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101,  3874,  7707, 15832,   256,   117,  2798,   653,   260, 21733,\n",
      "           298,  1449,  8855,   117,   259,  9701,   125, 16341,   117,   146,\n",
      "          6465,   291,   123, 11383,   298,   528, 12773,  1044,   102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[  101,   122,   146,  2099,   122,   179, 11665,  1510, 22281,   504,\n",
      "          4242,   117,   316, 22280, 21613,  3514,   893,   649,   117,   506,\n",
      "           146,  2009,   125,  2816,   171,   739,   549,   713,   125,   629,\n",
      "         22280,  2415, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[  101,  1790,  1256,  4332,  1149,   125,  2480,   117, 22032,   252,\n",
      "          2139,   117,   700,   325,  1028,   117,  8544,   146, 12447,   397,\n",
      "          9822,  1364,   146,  5856,   179,   176,   860,  1399,   954,  8001,\n",
      "           180,   327,  1151,   272,   421,   122,   117,   123,  3606,   304,\n",
      "         22280,   179,   146,  2562,   256,   117,  9487,   923,   118,   176,\n",
      "           259, 15050,   122,   146,  5492, 22280,   125,  6310,   102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101,  1684,   173,  8037,   138,   125,  7924,   117,   834, 11388,\n",
      "          2798,   644, 11842,   117,   229, 22280,  6757,  2364,   123,  2880,\n",
      "           151, 22280,   125,  3791, 21102,   952, 22282,   118,   176,   171,\n",
      "           313,  9193, 22280,   117,  4513,   125,  7198,  1485,   260,  1176,\n",
      "           179,  4207,   122,  2364,  4513,   125,  3859,   117, 13441,   348,\n",
      "           259,   958,  2118,   143,   117,  6641,   348,   538, 16455,   122,\n",
      "           529,  5772,   117,  6009,   214,   240,  1027,  7911,   125,   949,\n",
      "           144,   201,   146,   179,   259,  5976, 16463, 11814,   180,  1105,\n",
      "           298,   532, 14415,   117,  7837,  1552,  1078,   576,   325,   260,\n",
      "          2004,   138, 14520,   117,  4276,  5822,   214, 11118,   303,   143,\n",
      "           498, 11118,   303,   143,   117,  6564,   122,   325,   123,  8932,\n",
      "           271,   230, 10506,   125,  1151,   145,   117,  4141, 13793,  2415,\n",
      "         22280,  3429,   870,   509,   123,  8977,   230,  3264,   670,   180,\n",
      "         11791,  1449,  1272,   117,   179,   368,   117,   944,   259,  1564,\n",
      "           117,   320,  9322,   180,  1373,   117,  3791,  4409,   222, 16423,\n",
      "         22279,   123,  4303,   180,  5304,   117,  4108, 21307,   125,  5533,\n",
      "           170,   222,   398,  4861,   243, 11552,   125,  8087,   232,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101,   429,  1084,  2139,  2217,   123, 14195,   210,  5028,   122,\n",
      "           736,  2139,   123,  1434,   210,  1084,   537,   442,   122,  4405,\n",
      "           583,  6720,  2055,   128,   117,   122,   318, 13793,   905,  4373,\n",
      "           123,  5076,   173, 15618,   293,   117,   316, 22280,   173, 15618,\n",
      "           293,   179,   117,  1839,   125,   622,   122,  1423,   117, 13860,\n",
      "          7137,   256,  1941,  1364,   146,  1632,   303,  4620,  2828,   420,\n",
      "           260,   675,   504,  4242,   122,   123,  1449,  1272,   117,  3413,\n",
      "           122,   117, 11368, 17784,  4332,  1149,   125,  4707,   498,  4698,\n",
      "           125,  2375,   173,  3204, 13540,  5567,   122,  4929,   907, 22280,\n",
      "           221,  4902,   102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101, 13776,   240,  1921,  2880,   151, 22280,  6414,   118,   176,\n",
      "         14619,   210,   222,   425,   825,   179,  9086,   123,  5065,   180,\n",
      "          5304,   117, 12544,  1014,   820,   240, 11665,  4698,  4332,  1149,\n",
      "           125,  7716,   179,  1364,   146, 13185,   303,  9657,   171,  9078,\n",
      "           247,   117,  5664,   125,  7226,  4698,   122, 14730,  2390,   117,\n",
      "         15891,   524,   256,   221,   146,  5856,   171, 12447,   397,   260,\n",
      "           675,  4167,  9751,   125, 11979,   892,   102]])\n",
      "DEBUG: Tokenized sentence 50: tensor([[  101, 10107,   118,   146,   222,  1815, 15796,   404,   117, 19317,\n",
      "           175,  1456,   143,   117,  8350,   229,  4768,   171, 11014,  9258,\n",
      "           170,   230,  7841,   125, 12466,   240, 13379,   102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[  101,  5728,   230, 15703,  1250,   202,  6759, 13793,   117,  5698,\n",
      "           118,   176,   118,  8544,   368,   221,  1084,   170,   123, 20027,\n",
      "           117,  1502,   179,   123,  2606,   117, 10502,   860,   266,   117,\n",
      "         17704, 20494,  6044,   122,   170, 19016,  1149,   125,  9825,   117,\n",
      "          1941,   229, 22280,  4207, 15515,   123, 18489,   340,   202,  1997,\n",
      "           180,   651,   117,   271, 14619,   210,   327,  9586,   117,   123,\n",
      "          1757,   209,  7902,   508,   117,  1664,   351,   785,  1877,   328,\n",
      "           122, 11736,   125,  2716, 11694, 22278,   221,   432,   172,  4765,\n",
      "           122,  5357,  1831,   102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[  101,  3413,   262,   146,   179,  1996,   146, 15796,   404,   712,\n",
      "          8229,   117,   240,   210,   123,  5907,  2318,   180,  5896,  2028,\n",
      "          1011,   229,  4096,   117,   179,   368,  3009,   151,  5657, 11259,\n",
      "           117,   125, 16083, 10502,   860,   266,   171,  9007,   298,   532,\n",
      "         11314,  2650,  1058,   102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[  101, 10502,   860,   266,   495,   230,  2606,  7248, 11067,   180,\n",
      "          4312,   304, 16995,   118,   176, 12222,  1021,  9967,   481,   122,\n",
      "           726,  1966,   596,  1270, 22278,   320,  4170,  1719,  7716,   125,\n",
      "           273,  1289,   382,   102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101,   744,  1075,   125,  8794,   146,   995,   622,   125, 15813,\n",
      "          3942,   247,   117,   146, 15796,   404, 13779, 22290,  2598,   118,\n",
      "           123,   173,  7229,   522,   175,  2607,   373,   125,  4183,   367,\n",
      "           247,  1767, 18928,  1212,   122,   146,   347,   652, 13419,   262,\n",
      "           125,  3497,   118,  1084,   221,   146,   644,   492,  1982,   170,\n",
      "           146,  4395, 17377,   449,   123,   327,  1105,  2791, 13649,   118,\n",
      "           176,   170,   146,   171,   185,   179,   740,  5626,   124,   117,\n",
      "          7226, 17784,  9342,   173,  9078,   501,   122,  1169,   143,   180,\n",
      "          2450, 22278, 10033,   117,   125,   179,   176, 19203,   146,   273,\n",
      "           522,  1196,  1971,  2249,  2036, 12122,   146,  4130,   171,   477,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101,  1202, 22287,   125,   179,   117,   222, 11103,  1637,  5782,\n",
      "         22281,   303,  1467,  1706,   221,  3240,   404,   326,   117,   122,\n",
      "           117,   995,   123,   327,  5012,   151, 22280,   117,  1569,  3240,\n",
      "           404,   326,  2350,   687,   319,  9086,   785,  3002,   123,   222,\n",
      "         19317,   175,   125,  5288,  2601,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101,   466,   852,   256,   117,  3364,   125,  2745,   117,   123,\n",
      "           327,  3602,   304, 22280,  1979,   122,  8574,   151,   331,   170,\n",
      "           123,  3138,   125,   792,   118,   176,  1993,  6754,   117,   834,\n",
      "          3353,   122,   834, 16151,   221,  5725,   934,   123,  1069,   117,\n",
      "           700,   125,   176,  5110, 19877,  3611,   243,   123, 11368, 17783,\n",
      "           494,  8397,   138,   122,  4613,   373,   123,  5260,  2042,  1076,\n",
      "           125,  1456,   143,  8797,   179,  1941,   229, 22280,   376,  9019,\n",
      "           151,   229,  8768, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[  101,  1169,  9990,   201,   975,  1885,   185,  4461, 18320,  2949,\n",
      "           128,   117,  1519,  2610,   118,   176,   170,   230,  2281,  2531,\n",
      "           304, 22280,   125, 22004,   117,   122,   259,   682,  4058,   123,\n",
      "         18165,   173, 15050, 11053,   102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[  101,   229, 22280,   170,   923,  5062,   117,   122,  3002,  4708,\n",
      "           692,   420,   898,   230,   291,  1858,  3661,   380,   436,   833,\n",
      "           328,   117,   625,  1569, 14313,   243, 19937,   259,  2259,   151,\n",
      "           123,   598,  1289,   183,   102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[101, 146, 679, 692, 118, 176, 102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101,  1078,   615, 16280,   423,  1342,   222, 12186, 10968,  2256,\n",
      "           117,   179,  1695,   123,  1695,   176,   262, 12963,   173,  7729,\n",
      "         10766,   340,  5443,   102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101,   146,  5774,   125,  1757,   209,  7186,  3429,  4305,  2858,\n",
      "           744,   325,   123,  8121,   304, 22280,   123,  6754,   854,  2028,\n",
      "           117,   173,   576,   125,  6202,   125,  4129,   712,   682,   851,\n",
      "          5325,   143,   117,   262,  1075,   222,  1160, 17993, 22282,   179,\n",
      "           176,  5931,   420,  1061,   102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101,   860,   266,  3330,   256,   118,   123,  1528,   171,   179,\n",
      "          2036, 21315,   146, 16589,   234, 17688,   240, 10262, 22280,   118,\n",
      "          1084,  2267,   171,  4170,   117,   122,   860,   123,  4343,  8849,\n",
      "           256,  2113,   978, 18164,   304, 22280,   125,   229, 22280,   333,\n",
      "           347,  1568,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[  101,   230, 11791,  2954,   117,   240,   210,   117,   146, 15796,\n",
      "           404,   117,   179,   495,  2397,   125,  5052, 14657,   183,   122,\n",
      "           438,   304,   256,   318, 13793,   954,   532,  7187,   122,  1685,\n",
      "           481,   117, 10733,   118,   176,   173, 10671,  4496, 20383,   178,\n",
      "          1177,   125,   599,  2042,  2420,   102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[  101,   495,  1373,  1941,   122,   229, 22280,  1021,   173,  1105,\n",
      "          3933,  3293,   179,  2036,  7119,  5488, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[  101,  5069,   748,   118,   176,   180,  2606,   117,   449, 16278,\n",
      "           456,  2044,   418,  3138,   170,   440,  5345,  1522, 22278,  7729,\n",
      "         10766,   340,   102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101, 14372,   123,   146,   679,   118,  1084,   102]])\n",
      "DEBUG: Tokenized sentence 67: tensor([[  101,  5147,   860,   653,  2099,   125, 18237,   304, 22280,   173,\n",
      "           179,   368,   176,  7199,   125,   229, 22280,  6202,   118,   176,\n",
      "          3914,   117,   123,  5296,  1758,   125, 10968,   852,   118,  1084,\n",
      "           117,   271,   179,   744,   325,  2036,   502,  1051,   256,   146,\n",
      "          6532,   180,  7714,   117,  2636,   180,  2772,   851,  1995,   222,\n",
      "         10388, 15318,   102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[  101,   870,   509,   117,  5664, 13310,   117,  4460,   179,  6496,\n",
      "           246,  3874, 16652, 22281,   236,   123,   327,  7729, 10766,   340,\n",
      "           412,   337,   741,   124,   117,   262,   370,   320,  3147,  3914,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101,   123,  2606,  4678,  4322,   123, 11334,   969,   183,   102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[  101, 15796,   404,  3033,   766,  3221,   766,   122, 21161,   118,\n",
      "           176,   180,  9461,   102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101,  1112, 12444,  4706,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101, 17662,   102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101,   229, 22280,  2036,  9086,  1004, 11972,   102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101, 22354,   449,   146,  5052,  5313,   524,   256,   118,  2036,\n",
      "           117, 15158,   214,   118,   123,   102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[  101,   744, 18540,  3529,   222, 16423, 22279,   117,  1031,  1281,\n",
      "           178,   117,   123,  4108,   266,   118,  1084,   202,   347,  6532,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101,   860,   266,   117,   271,   176,   146, 11552,   171,  4170,\n",
      "          2036,   305,   162,  5585, 22279,   146,  1831,   117,  8861, 22288,\n",
      "           118,   176,   498,   146,  1896,   892,   180,  4573,   117,  7729,\n",
      "          2037,   214,   170,   260,   144,  3706,   146, 16936,  3210,   221,\n",
      "           123,  2375,   122, 10063,   348,   230,   872, 22281,   421,   125,\n",
      "          1444, 15182,  9466,  1165,   285,   122,  7352,   102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[  101,   146, 15796,   404,   229, 22280,   706, 17061,   117,  7304,\n",
      "           748,   118,   176,   598,   740,   117,   179,   117,  1362,  3265,\n",
      "           498, 22281, 10557,   183,   117,   325,   125, 10013,   179,   125,\n",
      "          6838,   117, 16668, 22288,   118,   176,   117,  2862,  2044,   122,\n",
      "         18105,   170,   146,  4170,   102]])\n",
      "DEBUG: Tokenized sentence 78: tensor([[  101,   122,  2789,   118,   176,  4276,   211,   702,   954, 11989,\n",
      "         22281,   117,   125,  5708, 19977,   117, 16044,   557,   179, 14372,\n",
      "           123, 18165,   117,   834,   123,  2892,  5594,  3292,   125,  2745,\n",
      "         11972,   102]])\n",
      "DEBUG: Tokenized sentence 79: tensor([[  101,   123, 22296,   740,  7719,   271,  4863,   179,   146, 15211,\n",
      "         22280,   117,  1065,   179,   229, 22280,  1023, 16151,   125, 14661,\n",
      "           118,   176,   125,  1105,   117,  1021,   117,   325,  8545,   291,\n",
      "           325,  1373,   117,   125,  2863,   118,  1084,   125,  1160,   102]])\n",
      "DEBUG: Tokenized sentence 80: tensor([[  101, 18574,   118,  2036,   146,  2829,   310,   117,  2124,   221,\n",
      "          8781, 22282,   122, 13141,   221, 17061,   320,  6532,   102]])\n",
      "DEBUG: Tokenized sentence 81: tensor([[  101,  5149,   201,   146,  2607,   373,   117,   146,  6320,   243,\n",
      "         19317,   175, 10733,   118,   176,   374, 22290,  4095,   243,   125,\n",
      "           792, 12268,   122,  3456,  2040,  1637,   102]])\n",
      "DEBUG: Tokenized sentence 82: tensor([[  101,   229, 22280,  1023,  6015, 22280,   125,  2822,  3661,   117,\n",
      "           122,  9883,   118,   176,  1480,  7485,   181,   268,   122,   362,\n",
      "         22282,  2014,   221,   146,   347,  3147,   125,   273, 10214,   243,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 83: tensor([[  101,   146, 22296,   271,  2036,   171,   151,  2535,   146,   179,\n",
      "          4364,   256,   125, 15707,   229,  2992,  8172,   180,   327, 20957,\n",
      "           292,   102]])\n",
      "DEBUG: Tokenized sentence 84: tensor([[ 101, 2010,  179, 3049, 2382,  102]])\n",
      "DEBUG: Tokenized sentence 85: tensor([[  101, 10355,   368,   762,  2640,   102]])\n",
      "DEBUG: Tokenized sentence 86: tensor([[ 101,  179,  928,  328,  415, 3049, 2382,  102]])\n",
      "DEBUG: Tokenized sentence 87: tensor([[  101,   202,   644,  1457,   117,   259,   682, 12061,   118,   176,\n",
      "           122,  4365,   228,   118,   176,   173, 22243, 22280,   117,   271,\n",
      "           176,  3874,   125, 10376,  2623,   247, 20075, 22278,   420,  1061,\n",
      "         18198,   229,  5693,   574,   102]])\n",
      "DEBUG: Tokenized sentence 88: tensor([[  101,   434, 22282,   118,   176,   118,  8544,  2684,   179,   117,\n",
      "           700,  7970,  1366,   340,   117,   146, 15796,   404, 16280, 11251,\n",
      "           146,   347,   146,   635,   598,   123,  2772,   102]])\n",
      "DEBUG: Tokenized sentence 89: tensor([[  101,   122,   117,   123,  2954,  2811,   653,   644,   117,   625,\n",
      "           176, 12524, 10580,   229,   327,  9461, 12555,   117,  3436,   203,\n",
      "           592,  1176,   712,   532,   235,  1408,  2364,   325,   117,  2364,\n",
      "           325,   117, 15707,  4327,  7406,  7870,   102]])\n",
      "DEBUG: Tokenized sentence 90: tensor([[  101,   449,   117,   180, 22283,   123,   222,   454,   117,   146,\n",
      "          6754,  2397,   117,  1169, 18377,   286,   125,   222,  1160,  2831,\n",
      "           125, 19366,   322,   117,  2927,   320,  3147,   180,  2606,   102]])\n",
      "DEBUG: Tokenized sentence 91: tensor([[  101,   860,   266,  1678,   118,   146,  1014,   576,   271,   180,\n",
      "           681,   117, 16044,   557,   179,   229, 22280, 19994,   256,   229,\n",
      "          2880,   151, 22280,   117,   240,   210,   117,   173,   179,   368,\n",
      "           176,  1178,   545,  2836,  3914,  1154,  2042,  2507,   117,   123,\n",
      "          1301,  1721,   117,   834,   176,   926,  8984,   117,   969,   654,\n",
      "           118,  2036,   173, 13140,   598,   146,  9169,   230,  6742,  1187,\n",
      "          4419,   179,   123,  5155,   331,   269,  2836,   102]])\n",
      "DEBUG: Tokenized sentence 92: tensor([[  101,   146,  6754,   118,   644,   492, 15310,   519,   203,   117,\n",
      "          2643,   138,  3240,   404,  3093,   852,   243,   117,   331,  3307,\n",
      "          3897,   214,   118,   176,   117,  5782, 22281,   303,   117,  1362,\n",
      "          1048,  4549,  8879,   125,  2448, 20342,   326, 19994,   243,   170,\n",
      "          6430,   340,   102]])\n",
      "DEBUG: Tokenized sentence 93: tensor([[  101,   123,  2606, 13053,   123,  8121,   304, 22280,   122,   229,\n",
      "         22280,  2036,  2002,   596,   221,  7912,  1367,   118,  2036,  6372,\n",
      "           286,   260, 11351,   240,  5530,   122,   117,   768, 12717,   243,\n",
      "           118,   176,   118,  2036,   320,  1831,   117,  2992,  2904,   118,\n",
      "           146,   170,   230, 18595,  4419,   125, 10786,  3103,   102]])\n",
      "DEBUG: Tokenized sentence 94: tensor([[  101,   229, 22280,   176,  5961,   228,   102]])\n",
      "DEBUG: Tokenized sentence 95: tensor([[  101, 15796,   404,  2364,   123, 18424, 22278,   117,  2798,  2364,\n",
      "           123,  5401,   117,  1016,   316, 22280, 16353,   202, 15537,   102]])\n",
      "DEBUG: Tokenized sentence 96: tensor([[  101, 14848, 22288,   118,   123,   102]])\n",
      "DEBUG: Tokenized sentence 97: tensor([[  101,   870,  1185,   748,   118,   176,   118,  2036,  2765,   538,\n",
      "          4332,   942,   125,   230, 11003, 20409,  6920,  9918,   146, 12109,\n",
      "          1212, 16473, 22280,   170,   179,   538,   173,   483,   483,  9398,\n",
      "           260, 10089,   138, 13501,  7485,  2208,   229, 13747,  3292,   171,\n",
      "          3746,  2256,  1214,   499, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 98: tensor([[  101,  6920,   118,  2036,   202,   765,   397,   180,  6614,   122,\n",
      "           202,   765,   397,   298, 13841,  3980,  7362,   179,  2364,  2036,\n",
      "         10500, 22278, 15415,   118,  2036,  1342,   607,  3093,   183,   117,\n",
      "          1342,  4081,   538,  2510, 13513, 22281,   122,   538,  4217,  1058,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 99: tensor([[  101,   122,  3746,  2256, 22288,   118,   123,   117,  3746,  2256,\n",
      "         22288,   118,   123,  7406,  2227,   403,   117,   170,  2607,  2032,\n",
      "         22280,   117,   170,  5907, 12458,   304, 22280,   125,  6032,   202,\n",
      "         13747, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 100: tensor([[  101,   122,   740, 14619,   210,   117,   740, 14619,   210,  3746,\n",
      "          2256, 22288,   117, 16405,   251,   240,  7583,  7892,  2148,   351,\n",
      "         12307,  7909,   171,  5381,   265,  1637,   179,   259,   273, 19206,\n",
      "         22278,  3746,  2256, 22288,   123,   273,  4315,  3679,   272,  4566,\n",
      "          5291,   179,   123,  2592,  1334,  7134, 15736,   712,  5708,   222,\n",
      "           171,  1342,  9466, 22282,   685,   118,   176,  1719,   117,   646,\n",
      "         20010,   214,   259, 12141,   117,   768, 22285, 13732,   243,   117,\n",
      "         15702,  4566,   347,  6631,   146,   679,   243,   117,  7093,   214,\n",
      "           118,   146, 14619,   210,  2535,   117,   271,  2397,   117,  1407,\n",
      "           179,  2364,   117,  9697,   451,   348,   118,   146,   538,   532,\n",
      "          4332,   942,  1444, 22281,   117,  3285,  1825,   118,  2036,   412,\n",
      "          9463,   123,  3182, 22278,   222,   328,   122,   173,  1010, 22278,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 101: tensor([[  101,   700,   117,   222, 15700, 22280,   125,  1831,  7821,   117,\n",
      "           170,   222,  7078,   303,   866,  1441,   122,  5404,  2794,   117,\n",
      "          5512, 17675,   185,   122,  5572,  4617, 22278,   117,  8173,   821,\n",
      "         22288,   118,   176,  1362, 15051,   125, 11351,   122,  4332,   942,\n",
      "         13059,   117,   123,  3049,   304,   221,   146,  1341,   117,   259,\n",
      "          5708,  1623,   457,  5895, 22281,   122,  5334,  6031, 22281,   117,\n",
      "          1719,   740,   762,  8723, 10922,   185,   117,   271,   176,   123,\n",
      "         12012,  4793, 16242,   201,   229,  9461,   102]])\n",
      "DEBUG: Tokenized sentence 102: tensor([[  101,   123,  1018,  2990,  2954,   117,   180,   615,   331,   412,\n",
      "          1062,   252,   146, 15796,   404,   176,  9883,   171,  3147,   180,\n",
      "          2606,   117,  5931,   118,   176,   420,  1061,   146, 19877, 22280,\n",
      "           125,   230, 15685,  5185,   117,   316, 22280,  5443,   271,   744,\n",
      "           229, 22280,   123,  2365, 16102,   487,   117,  4460,   179,   202,\n",
      "         14353, 22280,   125,  1078,   222,  4588, 10899, 22281,   236,   598,\n",
      "           146,  1342,   123,  1589,  7729, 10766,   340,  6496,   173,  3874,\n",
      "         16009,  2647,   102]])\n",
      "DEBUG: Tokenized sentence 103: tensor([[  101,   726,  1027,   481, 19581,   785,  1004,  7444, 22281,  2535,\n",
      "           117,   240,   210,   117,  1971,   596,   700,   180,   681,   851,\n",
      "         14266,  1711,  1187,   117,   122,  2535,   179,   146, 19317,   175,\n",
      "          1941,   229, 22280,   495,  1169, 18377,   286,   316, 22280,  3745,\n",
      "           240, 11665, 18695,   179,   146,  7096,   524,   692,  1796,   125,\n",
      "          2856,   320,  4678,  8476,  2758, 22280,   125, 10502,   860,   266,\n",
      "          2535,   117,   122,   145,   179,   123,  1301,  1721,  9821,  4213,\n",
      "           154,   123, 14887,   218, 13133,   229, 12526,   117,  3951, 15445,\n",
      "           712, 11314,  2650,  1058,   171,  4170,   117,   229,  2880,   151,\n",
      "         22280,   173,   179,  2983,   695,   923,   221, 16960,   934,   291,\n",
      "         14890,   102]])\n",
      "DEBUG: Tokenized sentence 104: tensor([[  101,   262,   240,  1257,   179,   146, 15796,   404, 10107,   146,\n",
      "          9078,   247, 12034,   123,  4141, 13793,  2415, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 105: tensor([[  101,   123,  1105,   495,  3264,   347,   877,   319,  2455,   373,\n",
      "          1011,   229, 17138,   171,  6151, 22290,   449,   221,  1257,  1021,\n",
      "         11935,   635,   170,   785,  1695,  6009,   692,   118,   176, 11368,\n",
      "          1027,  4332,  1149,  4566,  5856,   171,  4707,   179,  8544,  2684,\n",
      "           123,  1449,  1272,   117,   122,   325,  7226,  1027,   291,  8184,\n",
      "          1877,   793,   171,  1341,   173,   179,  9086,   123,  5304,   102]])\n",
      "DEBUG: Tokenized sentence 106: tensor([[  101, 15796,   404,   262,  2044,  9050,   118,   176,   170,   146,\n",
      "          2415, 22280,   122, 19284,   118,  2036,  3907,   523,   102]])\n",
      "DEBUG: Tokenized sentence 107: tensor([[ 101,  146,  316,  391,  514,  397, 6719, 9911,  102]])\n",
      "DEBUG: Tokenized sentence 108: tensor([[  101, 15796,   404,   601,  5432,   102]])\n",
      "DEBUG: Tokenized sentence 109: tensor([[  101,  2010,   146,  7258,  2243,   347,   596,   122,   347,  7020,\n",
      "         19652,   381,  1353,   146,  3695,   125, 10420, 18398,   852,   102]])\n",
      "DEBUG: Tokenized sentence 110: tensor([[  101,  2798,   331,   229, 22280,  8545,   230, 12804,  3281,   171,\n",
      "          7343,  5856,   117,   271,   744,  2036,  9176,   117,   176,   390,\n",
      "          8204,   140,  8868,   117,  6086,  7094,   303,   179,  2036,  1968,\n",
      "           320,  4707,   180,  1105,  2010,   146,  6151, 22290,   136,  2010,\n",
      "           122, 20811,   102]])\n",
      "DEBUG: Tokenized sentence 111: tensor([[  101,  2010,  1502,  2354, 22279,  3189,   179,  2779, 10692,   455,\n",
      "           834,  1690,  5105,   117,   834,  9186,   117,   834,  3874,   136,\n",
      "          2010,   221,  9726,   495,   125,  7246,   102]])\n",
      "DEBUG: Tokenized sentence 112: tensor([[  101,  2010, 11032,   117,  1114, 22279,   118,   176,  1659,   117,\n",
      "          2397,   117,   122,  2826, 22278,  1084,  2249,  3189,   423,   179,\n",
      "          2036,  2671,   249,   102]])\n",
      "DEBUG: Tokenized sentence 113: tensor([[ 101, 2010, 1941, 1996,  146,  179,  978,  123, 4640,  102]])\n",
      "DEBUG: Tokenized sentence 114: tensor([[  101,  2010,  9262, 22278,   118,   311,   318, 13793,   320,  1528,\n",
      "           260,  1027,  4332,  1149,   171,  4707,   102]])\n",
      "DEBUG: Tokenized sentence 115: tensor([[  101,  2010,  2798,  1423,  1877,   283,  2010,  1257,   122,  3002,\n",
      "          1076,   125,   327,   670,   117,  4178,   136,  2779,   117,   176,\n",
      "           395,   303,  3846, 20045, 22280,   117,   122,   412,  7122,  3113,\n",
      "           117,   179,  3804,   117,   144,  3249,   117,   125,   222,  1695,\n",
      "           125,  1632,   303,   221,  9456,   702,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 116: tensor([[  101,  2010,   122,  2779,   229, 22280,  8545,   117,  2113,  8911,\n",
      "           171,  7343,  5856,  2010, 11032,   615,   179,   644,   492,   706,\n",
      "          1084,  2354, 22279,  1434,  1369,   136,   230,   240, 15035,   125,\n",
      "           222,  7094,   303,   125,  5856,  1821,   768,  6859,   320, 21173,\n",
      "           122,   712,  8001,   125,  7122,  1105,   625,  2354, 22279,   117,\n",
      "          1369,   138,   117,  4267, 22280, 22279,   125,  1971,  1632,   303,\n",
      "           744,  2010,  2678, 22283,   125,  2036,  6515,   176, 15212,   291,\n",
      "           229, 22280,   146,   179,  1434,  1369,  2010,   122,   179,  2354,\n",
      "         22279,   122,   437,  3556, 22280,  2389,   296,   117,   176,   311,\n",
      "          9262,  1249,   260,  1027,  4332,  1149,   171,  4707,   117,   123,\n",
      "           327,   670, 15109, 20299,   173,  1999,  8145,  2684,   123,  1449,\n",
      "          1272,   117,   122, 13083,   375,   256,  2779,   125,  4412,   170,\n",
      "           230,  3508,   125,  5856,   313,  9193, 22280,   123,  3285,   140,\n",
      "           118,   176,   423,  7343,   102]])\n",
      "DEBUG: Tokenized sentence 117: tensor([[  101,  3189,  4945,   136,   229, 22280,  1052,   791,   146,  6151,\n",
      "         22290,   834,  2354, 22279, 11650,   118,   176,  2010,   318, 13793,\n",
      "          4412, 22278,   170,   146,  6151, 22290,   221,  1684,   834, 16909,\n",
      "           117,  2113,   146,   179,   978,   123,  4640,  1941,  1996,  2010,\n",
      "           449,   117,  2397,   125,  4023,   117,   179,   644,   492,  4174,\n",
      "         22279,   222,  1695,  2354, 22279,  1369,   229, 22280,   706,  4902,\n",
      "          3874,   291,  9767, 22278,   179,  2036,  4314,   244,  8925,  9751,\n",
      "           498,   146,  7343,  6151, 22290,   102]])\n",
      "DEBUG: Tokenized sentence 118: tensor([[  101,  2010,   229, 22280,  8911,  8925,  9751,   498,   146,  6151,\n",
      "         22290,   125, 16241,  1537, 22287,  2010,  2798, 22033,   203,   303,\n",
      "          2036,  4314,   244, 14748,  8130,   117,   316,  7039,   243,   118,\n",
      "           311,   260,  9751,   180,  4573,  2010,   229, 22280,  8911, 14748,\n",
      "          8130,  2811,  1341,   102]])\n",
      "DEBUG: Tokenized sentence 119: tensor([[  101,  2010,   318, 13793,   179,   644,   492,  2541,  2354, 22279,\n",
      "          1434,   125,  1364,   860,  5856,   136,   102]])\n",
      "DEBUG: Tokenized sentence 120: tensor([[  101,  2010,   123, 22296,  1257,  2535,   122,   329,   170,  1039,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 121: tensor([[  101,   146,   179,   344,   331,   186,  2010,  1502,  3960,   151,\n",
      "           179,   176,  3456,  2040, 22279,   125,   229, 22280,   311, 21761,\n",
      "           146,  5856,   102]])\n",
      "DEBUG: Tokenized sentence 122: tensor([[  101,  2010,   176,   311,  3456,  2040,   140,   117,  4500,  3292,\n",
      "           331,  2036,  2826, 22280,   122,   179,   785,  3002,   176,  5197,\n",
      "         22278,  1977,  8204,   140,  3285,   140,   118,   176,   329,   170,\n",
      "           123,  7122,  1069,  2010,  7282,  1004,  2010,  3605,   249, 12682,\n",
      "           203,   118,   176,   318, 13793,   230,  4840, 22278,  5302,  4095,\n",
      "           285,   122,  1401,   285,   420,   146,  1456,   143, 19317,   175,\n",
      "           125, 12466,   240, 13379,   122,   146,  1456,   143, 19317,   175,\n",
      "           125, 20237,   122,  3848,  6913,   102]])\n",
      "DEBUG: Tokenized sentence 123: tensor([[  101,  6086,   229, 22280,   176,  2709,   619,   123,  1434,   146,\n",
      "         16909,   171,  6151, 22290,   117,   834,   370, 20482,  2028,   243,\n",
      "           146,  7094,   303,   125,  5856,   179,   146,  2531,   256,   171,\n",
      "         21173,   122,   146,  1342,   117,   240,   347,  1341,   117,   229,\n",
      "         22280,  4363,   151,   123,  2521,  2028,   125,   305,  1051, 22282,\n",
      "           118,  2036,   744,   117,   423,  1528,   117,   924,   291,  1510,\n",
      "         22281,  4332,  1149,   712,  8001,   180,  1105,   670,   418,   179,\n",
      "           117,  4762,   259,   532, 10315,   128,   117,  5488,   322,  2987,\n",
      "           117,   230,   576,  3064,   146,   739,  1778,   179,   169,  8388,\n",
      "           246,   146, 16555, 18862,  2010,   123,   854,   304, 22280,   125,\n",
      "           230,   418, 15976,   173,  2009,  5223,   117,   230,   418, 15976,\n",
      "         15218,   117,   834,  1416,   117, 11826,   123,  6074,  1719,  7583,\n",
      "         14689,  8497,  3613,   125,   549,  1107,   179, 19310,   436,   692,\n",
      "           240, 11967, 10953,   763,   102]])\n",
      "DEBUG: Tokenized sentence 124: tensor([[ 101,  495,  860,  146,  347, 7503,  102]])\n",
      "DEBUG: Tokenized sentence 125: tensor([[  101,  1021,   785,   179,  4141, 13793,  2415, 22280,  9584,  6856,\n",
      "           221,  1921,  3138,  2448, 15736,   170,   740,  1485,   260, 13674,\n",
      "         10850,   351,   123,   944,   259,  2241,   326,   143,   125,  4918,\n",
      "           125,   893,   304, 22280, 13860,  7137,   256,  4561,   579,  1941,\n",
      "          8227, 22281,  6009,   256,  4117,   252,   173,  1448,   223, 22280,\n",
      "          5057,  6514, 13732,  5949,   125,  1945,   122, 19108,   146,   179,\n",
      "           495,  2745, 12174,  2640,   202,   347, 17733,  1690, 22280, 16026,\n",
      "           117,  3596,  7316,  5267,   256,   173,  5255,   146,  1354,   367,\n",
      "         14848,   125,   230,  5223,  1923,  8253,   285,   117,  1815,   495,\n",
      "           123,  5402,   298,  4674,   179,  1369,   176,   305,   508,   692,\n",
      "          6039, 10887, 14038,   138,   122,  9344,   124,  6137,   117, 14793,\n",
      "         22281,   125,   388,  4056,   117,   449,   534,   125,  4449,   117,\n",
      "         11314, 15546,   117,  8197,   125,  3883,  1149,   117,   849,  9580,\n",
      "           125, 13954,   122,   125,  3050,   117,  4848,   143, 12156,   175,\n",
      "         15069,   117, 18720, 22281,   122, 18720, 22281,   125, 19108,   125,\n",
      "           944,   259, 18190,   128,   117,  1923, 21818,   125, 13747,   310,\n",
      "           117, 20578,   125, 11912,   122,  2480,  9850,   117, 11621, 10780,\n",
      "         22280,   143,   125,  4117,   842, 20991,   117, 21733,  4814,   117,\n",
      "         12174,   721,   125,  1945,   117,   146,   644,   492, 17878,   320,\n",
      "           179,   368,   117,   179,  9679, 15363,   271,  3867,  4486,   176,\n",
      "         16463, 11814,   117,   398, 10750,   256,   117,   969,  1552,   123,\n",
      "          2954,   222,   928,   328,   415,   329, 22280,   125,   450, 22278,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 126: tensor([[  101,   860,   329, 22280,   495,  4252,  2122,   125, 19731, 22281,\n",
      "           398,  7173, 22281,   170,   123,  9349,   171, 15796,   404,   117,\n",
      "           123,  3596,  6151, 22290, 16241,  1537, 22287,   125,  1105,  4207,\n",
      "          7433, 22282,   117,   700,   366,  1027,  2856,   180,  2954,   117,\n",
      "           834, 13239,   146,  5121,   125,   333, 14408,   487,   412,  1718,\n",
      "         22278,   102]])\n",
      "DEBUG: Tokenized sentence 127: tensor([[  101,  2010,   122,  1434,   146, 16909, 10355,   146,  4141, 13793,\n",
      "          2415, 22280,   117,   629,   830,  6436,   243,   259, 20462, 22281,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 128: tensor([[  101,  2010,   229, 22280,   395,   303,  3898,  1436,   256,   146,\n",
      "          1342,   102]])\n",
      "DEBUG: Tokenized sentence 129: tensor([[  101,   176,   368,   122,  5790,   154, 22280,   125,   853,  6522,\n",
      "         22280,  2779, 14619,   210, 15212,   853,  6522, 22280,   173, 10894,\n",
      "          3391, 13793,   117,   229, 22280, 11314, 22278,   202,  6151, 22290,\n",
      "           171, 15796,   404,  3922,   508,   291,  6251,   339,   117,  4173,\n",
      "           474,   171, 17477,   171, 12447,   397,   117,   179,   229, 22280,\n",
      "          1904, 22281,   236, 11007, 20158,   319,   102]])\n",
      "DEBUG: Tokenized sentence 130: tensor([[  101,  4141, 13793,  2415, 22280,  3947,  5723,   598,   146, 18144,\n",
      "           173,  3401, 19641,   117,  7845,   214,  7903, 20798, 22281,  1749,\n",
      "           909,   117, 12402,   173,  2822, 12785,   102]])\n",
      "DEBUG: Tokenized sentence 131: tensor([[  101,  2010,  1502,   122,  1434,   222, 16909,   202,  3922,  2825,\n",
      "          3898,   618,  2836,   146,  4170,   125,   860,   266,   102]])\n",
      "DEBUG: Tokenized sentence 132: tensor([[  101,   180, 22283,   123,  1089,  2112,   117,  4141, 13793,  2415,\n",
      "         22280,   117,   700,   125,  5172,   222,  1270,  8008,   397,  3803,\n",
      "           303,   221,  4914,  1450,  4332,  1149,   171,  6151, 22290,   171,\n",
      "         12034,   117,  9565,   905,  2048,   260,  1860,   180,   418, 15976,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 133: tensor([[  101,  2010,  5308,  2765,   117, 10415,   256,   368,   229,  9461,\n",
      "           170,   123, 10420, 18398,   852,  5308,  2765,   179,   744,  2036,\n",
      "          2678, 22283,   125,  4270,   954,  8001,   180,  1105,   117,   176,\n",
      "           122,   179,   229, 22280,  2036,   420,   412,  2375,   325,  8545,\n",
      "           291,   325,  1373,   271,   118,  2036,   117,   229, 22280,   924,\n",
      "          4332,  1149,   117,   449,  2139,   117,  3003,   117,  1364,   146,\n",
      "          6151, 22290,   122,  2684,   146,  2004, 22280,   425,   825,  5787,\n",
      "           122, 10355,  3413,   170,   230, 18164,   304, 22280,   125,  1977,\n",
      "          2745,   706,   122,  2745,  2521,   180,   327,  4588,  8990,   371,\n",
      "          2028,   117,   171,   347,  3803,   303, 11233,  2152,   587,   154,\n",
      "           415,   122,   180, 22109,   292,   258,  8066,  6044,   171,   347,\n",
      "          3495,   117,  3495,   179,   331,  2036,  8625, 22278,   366,   877,\n",
      "           842,   221,  4706, 19386,   201,   102]])\n",
      "DEBUG: Tokenized sentence 134: tensor([[  101,  1065,   179,   123, 14594,   125,  8771,   176,  1178,   545,\n",
      "           203,  2461,  4171,   117,   944,   259,   532,  7823,   117,   944,\n",
      "           117,  2589,   146,   325,  2281,   117, 22158, 22287,   222,  3316,\n",
      "         11977,  2264,   342,   102]])\n",
      "DEBUG: Tokenized sentence 135: tensor([[  101,   331,   978,   230,  5811,   304, 22280,  5683,   259,  6109,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 136: tensor([[  101,   366,   675,  3428,   470, 12401,   151,   221,   898,   122,\n",
      "           221,   123, 20216,   259, 18967,  2653,  7362,   117,  5022,   179,\n",
      "           117,   240, 14697,   117, 16241,  1537, 22287,  8977,   151,   260,\n",
      "           675,  3922,  4242,  1271,   923,   785,   122,   368,   229, 22280,\n",
      "           170,   151,   222, 18181,   117,   171,   179,   202,  1325, 14971,\n",
      "         21024,  5133,   151,   118,   259,   944,   122,  1519, 21206,   118,\n",
      "           176,   170,   259,  8197,   180,  9652,   298,  5684,   102]])\n",
      "DEBUG: Tokenized sentence 137: tensor([[  101, 11972,  1941,   229, 22280,   495, 16122, 13793,   117,   495,\n",
      "           230,  3848,   687,   151,  8089,  1337,   117,   230,  7406,  7870,\n",
      "           117,   222, 19781, 11097,   125,  6039,   711,   125,  8333,  2745,\n",
      "           123,  9245,   102]])\n",
      "DEBUG: Tokenized sentence 138: tensor([[  101,   122,   347,  1903,  3378,   185,   117,   331,  1196,   117,\n",
      "           125, 13841,   123,  3235,  3711,   252,   117,   123, 17897,  1684,\n",
      "           240,  1434,   117,  8544,   122,  8940,   180,  1449,  1272,   221,\n",
      "           123,  5304,   117,   180,  5304,   260,  3428,   470,   122,   320,\n",
      "           853, 18983,   162,   117,  1684,   173,  8037,   138,   125,  7924,\n",
      "           117,   125,  3541,   942,   117,   834,  5899, 22281,   117,  2389,\n",
      "          7831,   221,   944,   259,  6615,   117,   170,   146,   347,  2337,\n",
      "           140,   300,   388,   125,  8087,   232,   117,  1178,   545,   348,\n",
      "           118,   176,   117,   170,   259,  5708,   117,   125,  2745, 11972,\n",
      "           125,   179,   368,   229, 22280,  4207,  1178,   545,   159,   118,\n",
      "           176,  2044,   170,   260,   877,   842,   102]])\n",
      "DEBUG: Tokenized sentence 139: tensor([[ 101, 5147,  117,  123, 4768, 1084, 1796, 2049, 2836,  118,  176,  125,\n",
      "          222, 2277, 8296,  415,  102]])\n",
      "DEBUG: Tokenized sentence 140: tensor([[  101,   893,   151,   118,   176,  3002,   117,   240,   210,   785,\n",
      "          2366,   923,  1690,   507,   122,   504,  4242,   180,  2954,   221,\n",
      "           146,   644,   695,   923,   259,  5231,  1537,   145,   260,  5077,\n",
      "         15277,   692,   125,  2261,   102]])\n",
      "DEBUG: Tokenized sentence 141: tensor([[  101, 17340, 22278,   118,   176,   230,  7875,   125, 11282, 18836,\n",
      "           122,  1858,   125, 20262, 22281,   117,   122,   259,  5684, 19407,\n",
      "           125,  1062,   252,   122,   260, 15252,   118, 22232,   138,   117,\n",
      "           122,   123,   636,   670,  2866,  8544,  1847,   123,  1105,   125,\n",
      "         14016,   179,  4141, 13793,  2415, 22280,  5646, 17376,   712,  8001,\n",
      "           180,   327, 17935,   404,   102]])\n",
      "DEBUG: Tokenized sentence 142: tensor([[  101, 19914,   118,   176,  2534,   316,   391,  1343,  3963,   117,\n",
      "           240,   210,   117, 17002,   333,   316, 22280,   870,  2127,  3897,\n",
      "         20733,   271,   123,  2461,   102]])\n",
      "DEBUG: Tokenized sentence 143: tensor([[  101,  2364,   146,   347,  3907,   523,  1796,   316, 22280,  1004,\n",
      "           117,  2364,   146,  2135,  2758, 22280,  8868, 22278,  1971,  5133,\n",
      "           151,   325,  2535,   117,   785,   325,   117,   179,   538,   481,\n",
      "          3860,   102]])\n",
      "DEBUG: Tokenized sentence 144: tensor([[  101,  1023,  2684,   125, 18689,   307, 11314,  2650,  1058,   102]])\n",
      "DEBUG: Tokenized sentence 145: tensor([[  101,   260, 12817,   229, 22280,  2036,   221,   692,   529,  1174,\n",
      "          1832,  1908,   146,  3568,   304, 22280,  1011,  1078,   576,   325,\n",
      "         16814,   534, 22280,   117,   325, 20639,   102]])\n",
      "DEBUG: Tokenized sentence 146: tensor([[  101,   122,   146,  3495,   123, 13718,   702,   117,  4698, 22287,\n",
      "           240,  4698, 22287,   117,  1839,   180,  1956, 21305, 22278,   117,\n",
      "           122,   123,  3235, 22282,  6070,   180,  1956, 21305, 22278,   221,\n",
      "           123,  6688,   117,   712, 11330,   122,   712,  5830,   592,   118,\n",
      "          7911,   117,   122,   180,  9066,   124,   221,   146,  6465,   117,\n",
      "           712,  9342,   122,   712,  9342,   102]])\n",
      "DEBUG: Tokenized sentence 147: tensor([[  101,   870,   509,   117,  1941,  2036,   229, 22280,  1587,  5723,\n",
      "         13449,   779,   146,   347,  7616,   538,  9098, 22281,  6996,  2247,\n",
      "           662,  1353,   123,  3859,  1089, 16453,   128,  4188,   180,  8768,\n",
      "         22278,   146, 10832,   117,   240,  1416,   117,   179,   368,  4654,\n",
      "           555,  6009,   256,   712,  5841, 22281,   529,  4103,   125, 13379,\n",
      "           117,  8940,   118,  2036,  2535,   125, 18615,  1187,   260, 13779,\n",
      "          1708,   117,   122,   125,  1078,   230,  5057,  1510, 22281,   170,\n",
      "          6205, 22278,   122, 13850,  3391, 22278,   122, 19537,  2034,   256,\n",
      "         19971, 22281,   125,  1923,  1046,   125,  3001,   945,   117,   125,\n",
      "         17730,   125, 14223,   117, 11314,  1604,   143,   125,  2360, 13331,\n",
      "         22281,   117,  3673, 20480,   117, 20427, 22281,   117,  7406,   304,\n",
      "           122,  1615,  1028, 12817,   102]])\n",
      "DEBUG: Tokenized sentence 148: tensor([[  101,  3336,  9098, 22281,   221, 12174,   373,   117,  8806,   456,\n",
      "           123,   163,  8860,   285,   122, 10564,   146,  4678,  8476,  2758,\n",
      "         22280,   117, 17887,   146,  1632,   303,   221, 17086,   123,  5304,\n",
      "           117,   179, 21244, 22288,   125,  3846,   122,  2473,   325,   924,\n",
      "          6929,   102]])\n",
      "DEBUG: Tokenized sentence 149: tensor([[  101,  1941,   229, 22280,   495,   230,  2281,   316,   391,   324,\n",
      "           117,   495,   222,   475,  4803,   173,   179,   176,  7339,   125,\n",
      "          2745,   117,  4674,   125,  4857,  6812,   117,  5643,  1495,   117,\n",
      "           240,  1927,  3503,   117,   169,  2050,  9760,   128,   125,  4223,\n",
      "           247,   117, 11451,   125,  3979,  1196,   221,   259,  5684,   117,\n",
      "          7698,   221, 11451,   125,  2606,   117,  1690,  7817, 22281,   125,\n",
      "         19278,  2004,   128,   221,   146,  1312,   303,   320,   969,   117,\n",
      "          3980,   809,  1313, 11037,   470,   117,  2377,   555,   125,  1224,\n",
      "           357,   130,   117, 16936,   942,   170, 11454,   125,  3165,   117,\n",
      "           122, 15631,   145,   122,  5911,   942,   125,  4437,  4189,  2623,\n",
      "           247,   102]])\n",
      "DEBUG: Tokenized sentence 150: tensor([[  101,   122,  1719,   123, 14350,  3613,  7970, 22281, 13722, 11157,\n",
      "           138,  8544,  9322,  1084,   117,   291,   318, 13793,  1369,   320,\n",
      "          1341,   117,   229,  1105,   125, 14016,   117,   582,   259, 11309,\n",
      "           501,   366,  7875, 22281,   122,   259,  5684,   180,  1449,  1272,\n",
      "           176,  2259,   923,   700,   171,  1312,   303,   117,   122, 19001,\n",
      "          5167,  1825,   122, 10415,   214,  2684,   260,  1027,  2856,   180,\n",
      "          2954,   117,   420,   146, 15021, 22280,   572,   283,   298, 13850,\n",
      "         22178, 22281,   117,   171, 11371, 10310,   373,   173,  3673, 20480,\n",
      "           122,   298, 19068, 14881,   143,   125, 18691,  2615, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 151: tensor([[  101,   495,  4141, 13793,  2415, 22280,  1977,  7707,  4049,   351,\n",
      "          2745,   117,  2745,   117,  2684,  3495,  8684,  4409,   117,   625,\n",
      "          3179, 11736,   102]])\n",
      "DEBUG: Tokenized sentence 152: tensor([[  101,   240,  1369,   229, 22280,   176,  7339,  1955,   458,   117,\n",
      "          3596, 11263,   229, 22280,  2589,   627,  2032, 21102,  9079,   260,\n",
      "           223,   128,   171,  7492,   303,   102]])\n",
      "DEBUG: Tokenized sentence 153: tensor([[  101,   122,   498,   860,  9312,   117,  1821,  1684, 13122,   712,\n",
      "           374,  7485, 22280,   143,   117, 15210,   256, 19075,   125,  3003,\n",
      "           240,  9143,   320,   454,   117,   222,  1695,   325,   171,   179,\n",
      "         16403,   712,   179, 13649, 22287,   123,  2450, 22278,   170,  2377,\n",
      "         12166,   125,  2987,   291,  5731,   102]])\n",
      "DEBUG: Tokenized sentence 154: tensor([[  101,   229, 22280, 14940,   117,   260,   504,  4242,   171,   549,\n",
      "           713,   117,   123,  3606,   304, 22280,   179,   176,  1316,   464,\n",
      "           304,   692,   117, 16386,   923,   118,   176,  2044,   117,   834,\n",
      "           653,  2822,   596,   123,   179,   260, 17681, 22281, 19660,  6867,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 155: tensor([[  101,  1021,   739,  4042, 15182,   173,  5231,   421,   118, 10497,\n",
      "          6086,   495,   146,  1407,  2009,   171,  2907,   221,   123,  9349,\n",
      "           171,  1223,   102]])\n",
      "DEBUG: Tokenized sentence 156: tensor([[  101,   259, 11677,   180,  1449,  1272,  5674,   923,   944, 11460,\n",
      "          1084,   117,  2113, 19001,   123,   682, 10674,   180, 18237,   304,\n",
      "         22280,   102]])\n",
      "DEBUG: Tokenized sentence 157: tensor([[  101,   146, 15796,   404, 13734,   796,   256,   125, 16001,   102]])\n",
      "DEBUG: Tokenized sentence 158: tensor([[  101,  2010,   222,   549,   713,  2638,  4839,   256,   368,   117,\n",
      "          4720, 22281,   293,   102]])\n",
      "DEBUG: Tokenized sentence 159: tensor([[  101,   222,   549,   713,  3002, 11646,  1547,  6086, 12447,   397,\n",
      "           125,   944,   259,   644,  3471,  1434,   118,   311,   222,   549,\n",
      "           713, 15702,   366,  9751,   102]])\n",
      "DEBUG: Tokenized sentence 160: tensor([[  101,  3254,  2904,   118,   311,   123,  1105,   117,   146,  3002,\n",
      "          2525,   122,   962,  8476,  2836,  1174,  1557,   117,  7845,   214,\n",
      "           179,  1021,   125, 16695,   118,   176,   117,   122,  3947,  1552,\n",
      "           712, 10420,   444,   598,   146,   302,   179,  2036,  5808,   151,\n",
      "           173,  8950,   260,  7289,   117,   122,   598,   146,  3752,  7134,\n",
      "         11037, 22290,   268,   298,  1449,  8855,   122,   505,  6592, 11369,\n",
      "           179, 16403, 22287,   123,   528, 20871,   125,   969,   123,   969,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 161: tensor([[  101,   146,   179,  1369,   138,   229, 22280, 14546,   179,   260,\n",
      "           504,  4242,  4390, 22281,  6867,   123, 13431,   117,   230,  4230,\n",
      "          1858,   117,   122,  5327,  2044,   176, 16386,  1825,   117,   123,\n",
      "         18195,   210,   118,   176,   877,   649,   240,  1369,   123,  1796,\n",
      "           117,  1065,   123,  5304,  2684,  1821,   320, 21173,   117,   122,\n",
      "           700, 15277, 22281,  6867,   221,   146,  1341,   171, 15796,   404,\n",
      "           122,  1938, 20798, 22281,  6867,   498,   146,  6151, 22290,  2166,\n",
      "           117,   179,  9821,  3444,  1196,   240,  7583,   333, 15407,   125,\n",
      "          5028,   122,  1945,   102]])\n",
      "DEBUG: Tokenized sentence 162: tensor([[  101,   146, 15796,   404,  9673,  2044, 14748,   146, 16909,   102]])\n",
      "DEBUG: Tokenized sentence 163: tensor([[  101,  3874,  6086,  3174, 13733,   495,  4051,   125, 17687,   118,\n",
      "          2036,   123,  1105,  2684,   123,  4767,   125, 12338,   122,   259,\n",
      "         15050,   171,   549,   713,  9079,   228, 17878,   125,  5480,   320,\n",
      "         16909,   171, 19317,   175,   117,  6335,   170,   123,  4390,   304,\n",
      "         22280,   180,  1105,  2166,   222,   739,  1896,   892,  1962, 22280,\n",
      "           117,  4446, 22279,   125,  4286,   247,   125, 13974,   117,   582,\n",
      "          4207,  4883,   222,  2971, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 164: tensor([[  101,  4167,   539,   122,  1685,   504,  4242,  9654,   654,   123,\n",
      "         16031,   418, 15976,   102]])\n",
      "DEBUG: Tokenized sentence 165: tensor([[  101, 20466, 22281,   117,  4141, 13793,  2415, 22280,  9673, 14748,\n",
      "           229,  2375,   117,   529,  4698,  4332,  1149,   179,  2531,   692,\n",
      "           123,  5304,   171,   425,   825,   171, 15796,   404,   117,   222,\n",
      "         15618,   293, 16909,   125,  1027,  1877,   793,   125,  2847,   117,\n",
      "         18728,   125,   329,   942,   125, 10809,   122,  8001,   125, 16317,\n",
      "          1165,   117,   122,   170,   222,   739,  4303, 22280,   202,  1997,\n",
      "           117,   582,   176, 10090, 18417,   230, 18253,  2830,   125, 20699,\n",
      "         10780,   138, 17445,   117,   240,  5530,   125,   230, 14038,  4316,\n",
      "         15403,   117,   173,   179,   176, 19449, 22278,   146,  1457,   117,\n",
      "          2685,   123, 17681,  4619, 18416,   122,   834, 14656,  3057,  1112,\n",
      "           418, 15976,   125,   629, 22280,  2415, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 166: tensor([[  101,  5231,  7567,   118,   176,   504,  4242,   122,   964,  1343,\n",
      "           221, 11348, 17124, 22281, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 167: tensor([[  101,   260,   504,  4242,  1085,  5231,  6839,   240,   454,   122,\n",
      "           260,   964,  1343,   240,   644,  2745, 12659,  8684,  4409,   102]])\n",
      "DEBUG: Tokenized sentence 168: tensor([[  101,   146, 14701,   125,  1078,   964,   324,   117,  3285,  1825,\n",
      "           123,  6205, 22278,   117, 22191,  5292,  7911,  6459, 13793,   123,\n",
      "           670,   102]])\n",
      "DEBUG: Tokenized sentence 169: tensor([[  101,   260,  9824, 13314,   171,   549,   713,  2365, 13521,   340,\n",
      "           122,   229, 22280,  9141,   692,  3874,   221, 11348, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 170: tensor([[  101,   416,  1149,   123, 19164,   340,   180,  6205, 22278,   179,\n",
      "          1084,  1021,   117,   271,   173,  3963,  1858,   670,   117,   122,\n",
      "           416,  1149,   320,   785,  1632,   303,   125,   179,   176,  1598,\n",
      "         13808,   202,   549,   713,   221, 18195,   123, 11451,   117,   123,\n",
      "          5277,   340,   260,   964,  1343,   229, 22280,   176,  1191, 12449,\n",
      "           417,  1797,  1429, 11348, 17124, 22281,   125,   944,   259,  2038,\n",
      "           180,   651,   117,   420,  2859,  1450, 20330,   125,  1004,  5533,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 171: tensor([[  101,   122,   117,  3002,  5926,   256,   230,   366,   504,  4242,\n",
      "           117,   291,   222,  3147,   117,   222,  2242,   582,   144,  8102,\n",
      "         22281,   236,   222,  1207,  2034, 22280,   117,  2366,   151,   230,\n",
      "         18637,   125, 11179,   358,   123,  3623,   118, 15887,   102]])\n",
      "DEBUG: Tokenized sentence 172: tensor([[  101,   122, 11972,   176,   262, 16895,  1532,   739, 11348,  1217,\n",
      "           151,   117,   762,  3249,   122,  1923,   209,  7494,   154,   117,\n",
      "           170,   260,   675,  1384, 22281,   125, 17935, 22281,   117,   260,\n",
      "           675,  3428,   477,   585,  6183, 17675,   555,   122,   259,   532,\n",
      "          1941, 18304, 18983,  2552,   125,  1510, 22281,   122,  1256,  1877,\n",
      "           793,   117,   179,  1183,  6451,   271, 18048, 20073, 22281,   240,\n",
      "           420,   123,  2128,   381,   124,   366,  1743,  3832,   964,  1343,\n",
      "           669,  3666, 12717,   555,   122,   146,  7871, 20318,   366, 18592,\n",
      "          6688,  1149,   125,  3233,   285, 22280,  4793,   117, 11823,   498,\n",
      "           259, 16814,   534,   128,  9213,   125, 11348, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 173: tensor([[  101,   122,   259,  3746,   185, 17675,   555,   274,   364,   249,\n",
      "           117, 20648,   125, 11451,  3848,  4419,   117, 11214,  1228,   692,\n",
      "           320,   969,   117,   179,  2798, 14542,   125,  4437,  4712,   102]])\n",
      "DEBUG: Tokenized sentence 174: tensor([[  101,   122,  6621,  2480, 16386,   159,  2382,   122,   572,   155,\n",
      "         10768,   117,  6621, 15682,  8833,   122,  1340,   442, 22278,   117,\n",
      "           662,  1353,   123,  1439,   268,   934,   117,   123, 15518,  1370,\n",
      "          5822, 22282,   117,   123, 11251,   117,   222,  1147,   117,   230,\n",
      "          5664, 12043,   117,   230,  1841,   304, 22280,   117,   179,  9821,\n",
      "         11934,   578, 13609,  2492, 22278,   117,  1369,   653,   117,  4566,\n",
      "         19068,   458,   117,   122, 19386,   159,   118,   176,   271, 19785,\n",
      "           202, 16758,   303,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 175 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.007366151633602919\n",
      " Coesão Score Final: 0.5036830758168015\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'isto e', 'nao obstante', 'nem', 'tampouco', 'ou', 'ora', 'quer', 'seja', 'como', 'tanto quanto', 'quanto', 'em vez de', 'desde que', 'assim que', 'porque', 'posto que', 'gracas a', 'apesar disso', 'no entanto', 'por exemplo', 'alias', 'acima de tudo', 'como tambem', 'tanto', 'quanto', 'se nao', 'a proporcao que', 'de sorte que', 'por isso']\n",
      " Número de conectivos: 41\n",
      " Número de sentenças: 175\n",
      "======================\n",
      "Resultados para preprocessado_o_cortico_aluisio_azevedo_cap_1.json:\n",
      "{'coesao_score': np.float64(0.5), 'conectivos_encontrados': ['e', 'mas', 'porem', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'isto e', 'nao obstante', 'nem', 'tampouco', 'ou', 'ora', 'quer', 'seja', 'como', 'tanto quanto', 'quanto', 'em vez de', 'desde que', 'assim que', 'porque', 'posto que', 'gracas a', 'apesar disso', 'no entanto', 'por exemplo', 'alias', 'acima de tudo', 'como tambem', 'tanto', 'quanto', 'se nao', 'a proporcao que', 'de sorte que', 'por isso'], 'num_conectivos': 41, 'proporcao_conectivos': 0.007, 'similaridade_media': np.float64(1.0), 'num_sentencas': 175}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,   726,   682,   481,   146,   549,   713,  4041, 10955,   125,\n",
      "           644,   221,   644,   117,  7551,   344,  1149,   117,   331,  2746,\n",
      "           118,   176,   125,  9349,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   122,   320,  1341,   146, 15796,   404, 17154,  5723,   118,\n",
      "           176,   117, 11233, 13194, 22280,   170,  7583,   294,  8102,   672,\n",
      "           151, 18997,   125,  1069,   117, 13038,   825,   975,  1885,   185,\n",
      "          7970,  7321,  4955,   304,   415,   179,  2036,  1664,   351,  1982,\n",
      "           180,  1105,   117,   240, 15702,   366,  9751,   117,   122,  8764,\n",
      "         10957,   143,   117, 18967,   122,   325, 15618,  1509,   171,   179,\n",
      "           333, 15407, 22281,   117, 16387,   692,   240,  1719,   123,   670,\n",
      "           117,  3444,  2746, 13734,   796, 22282,   146,  1690, 22280,   173,\n",
      "          3443,  3914,   117,   646, 17425,   243,   146,  3037,   122, 17409,\n",
      "           348,  2745,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,  4460,   179,  1084,   229,  4768,   171, 11014,  9258,   259,\n",
      "           532,  3907,  2698,   229, 22280,  1820, 22281,  6867,  3002,   117,\n",
      "          2803,  5723,   118,  2036,   123,  9533,   123,  3240,   404,  1159,\n",
      "         22278, 12464,   171, 12447,   397,  1112,  6086,  1903,   222,  3061,\n",
      "           371,   415,   117,   222,  5980, 22280,   117,   179,   229, 22280,\n",
      "           879, 11448, 22278,  2364,   222, 12581,   183,   117,   122,   179,\n",
      "          9584,   125,  9461,   122,  9317,   170,   230, 10105, 22354,   123,\n",
      "          2954,   122,   712, 18433,   744,   325,  7491,  1111,   351,   146,\n",
      "           347,  3673,   430,  2766,   117,   625,   368,   117, 12401,  1825,\n",
      "           118,   176,   395,   193,  1655,   171,  1312,   303,   117,  5308,\n",
      "           256,   118,   176,  4412,   860,  2828,  1532,   466, 14960,   942,\n",
      "         22278,   117,  1982,   123,  9317,   180,  4767,   125, 14890,   117,\n",
      "           122,  8362, 22278,   117,   123,   598,  1289,   183,   117,   146,\n",
      "         15618, 15335,   157,  7257, 22282,   179,  8940,   180,   418, 15976,\n",
      "          1532, 19455,  3391, 13793,  2124,   125,  3155,   822, 13550, 22281,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   229, 22280,  4207,  3767,   123, 11471,   834,  3859,   202,\n",
      "          9169,  6086,   475,  1783,   117,  8833,   122, 20957,   117,   179,\n",
      "           146,   173,   483,   483,   285,   256,   170,   146,   347,  5546,\n",
      "         10848,   125,  5294,  8849, 22281,   202,   144,   373,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   122,   700,   117, 10371,   202,  3147,   125, 18165,   117,\n",
      "         21692,   175,   122, 19877,  3611,   243,   260,   516, 14652, 22281,\n",
      "         18757,   441,   180,  2606,   117,   847,   265, 22280,  1941,   298,\n",
      "         19262, 22281,   498, 22281, 10557,   382,   179,  2036,  9144,   117,\n",
      "           123,   368,   117,  1718,   391,   146,  5052,   122,  5986,   123,\n",
      "           338,  7153,   795,   117,   495,   744,   123, 16521,   171, 12034,\n",
      "           146,   179,  2036,  1670,  6152,   256,   146,  5791,   183,   117,\n",
      "           432, 22279,   846,  2146,   118,  2036,   123,  8410,   170,   222,\n",
      "          2996, 22280,  5381,   265,  1637,   125, 21736,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   978,  2401, 22279,   524,   171,  1342,   117,  4566,  1342,\n",
      "          1456,   143,   179,  3283,   371, 12464,   117,   834,  3804, 22282,\n",
      "           577,   140,  3484,  1224,   357,   130,  4566,  1342,   179,   117,\n",
      "           221,   333,   325,  8797,  1510, 22281,  1176,   171,   179,   368,\n",
      "           117,   229, 22280,  1023,   125,  6759,   170,   123,  2267,   171,\n",
      "          9019, 13793,   291,   170,   123,  1587,   578,   285,   125,  3179,\n",
      "         22082,   958,  2118,   180,  1105,   449,   318, 13793,   117,   368,\n",
      "         15796,   404,   117,   179,   176, 10262,  3638,   123,   169,  8388,\n",
      "          9434, 22280,   180, 13503,   387,   705,   122,   180, 14657,  7919,\n",
      "           368,   117,   179,   117,  2044,   700,   171,   347,  2982,   117,\n",
      "         12416,   214,   221, 18615,  1187,   123,   222,   294,   118,  9741,\n",
      "           179,   146, 10840,  1095,  5723,   117,  1996,   124,   179,   146,\n",
      "          1010,   215,   495,   230,  2840,   421,  9217, 17754,   285,   125,\n",
      "          3495,   117,  8764,  2551,   138,   222,  2397,  2135, 22280,  4276,\n",
      "           211,  8041,  6530,   368,   117,   179,   176,   978,   229,  1284,\n",
      "           125,  2401,  4211,   415, 12600,   458,   117,   229, 22280,  9882,\n",
      "           870,   509,   125,   222,  7094,   303,   125,   260,   300, 12103,\n",
      "           170,   146,   347, 12034,  9767, 22278,  1434,   118,   176,  7258,\n",
      "           171,  1010,   215,   122,  3283,   371,   118,   176, 15316,   125,\n",
      "           230,  2509,  3002,   118,  6974,   285,   122,   834,   440,  5345,\n",
      "          1522,   125,  9429, 12223,   186,   118,   176,  1815,  2895,   221,\n",
      "          1491, 10470,   117,   122,   229, 22280,  9882,   125,   230,  1746,\n",
      "           699, 17850,  2951, 22278,   122,  2588,  5271,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,  1141,   202,  1338,   125, 11169,   615,  1796,   123,   327,\n",
      "          5566, 22278,   136,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101, 20613, 18003,   222,  1695,   117,   122,  3295,   117,   449,\n",
      "           271,   136,   123,   179, 14701,   136,  9935, 12766,   214,   118,\n",
      "           176,   123,   222,   644,   492,   117,   179,  2036,  5626,   124,\n",
      "         17784,  9342,   125,  7911,   117,   449,   563,   162,  5187,   909,\n",
      "         10985,   143,   125,   273,  1289,   382,   122,   792, 12268, 22281,\n",
      "          5646, 17376,   123,  1069,   117,  1141,   117,   449,  1023,   125,\n",
      "           829,   900, 19731,   246,   230,  2606,   179,   368,   146,   679,\n",
      "           256,   122,   171,   179,   870,   509,  2036, 17670,  2745,  1257,\n",
      "           136,   615,   495,   870,   509,   123,   327,   739,  1622,   340,\n",
      "           136,   171, 21929,   180,  1105,   221,   146,   879, 22282, 11290,\n",
      "          2758, 22280,   171,  1223,   122,  2700,   118,  1246, 22278,  2401,\n",
      "         22279,   524,   415,  7716,   117,   229, 22280,  1021,   623,  2100,\n",
      "           229, 20703,  1337, 15756,  7919,   125,   179,  1757,   209,  7186,\n",
      "          2589,   327,  2267,   117,   146,   273,   522,  1196,  2798, 14019,\n",
      "          3746,   852,   256,   146, 15537,   125,   333,  1568,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   176,   740,   117,   173,   576,   125, 16737,   125,   860,\n",
      "           266,   117,  1796,   230,   432,   537,   343,  6436,   252, 12401,\n",
      "           328,   240,   368,   117,   122,  2711,   179,   123,  3330, 22281,\n",
      "           236,   122,   318, 13793,   123,  1069,  2036, 13239,   151,   125,\n",
      "          1342,  2277,   449,  6621, 22281,  1109,   319,   143,   117,   123,\n",
      "          6754,   854,  2028,  3874,   325, 15010,   179,   146,  6875,  3361,\n",
      "           171,   599,  1125,  2042, 22280, 17688,   117,   122,   146, 15796,\n",
      "           404,   860,  1399,  2684,   123, 14808,   403,  7248,   121,   112,\n",
      "          5566, 22278,   146,   146,   635,   179, 12595,   256,   598,   123,\n",
      "          2772,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   230,   730,   945,   123,  1815,   180,   327,  1069,  2010,\n",
      "           572, 22283,   230,  5294,  8849, 15912,   456,   368,   117,   173,\n",
      "          4410,  2729,   117, 15975,   348,   118,   176,   180,  9461,   117,\n",
      "           582,   176,  1021, 12401,   286,   238, 14600,   246,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   122,   429,   118,   176,   123,  7282,   159,   202,  3147,\n",
      "           834,  6272,   125, 18165,   117, 21128,   179,   123, 14594,  7970,\n",
      "          2401, 22279,   524,  2036,  9466, 22282,  8253,   256,   259, 14689,\n",
      "          5198,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,  8540,   122, 14657,   183,   495,   146,  4141, 13793,  2415,\n",
      "         22280,  1966,   117,  1141,   117,  7258,   221,  1966,   122,   179,\n",
      "          1021,   125,   333,   123,  1069,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  1417,   180,   223, 22279,   117,   179,  1011,  1790,   316,\n",
      "         22280,  3039,   122,  7259,  3391,   201,   271,   202,   644,   173,\n",
      "           179,  2080,   180,  2480,   834,   222,  4698, 22287,   125,   347,\n",
      "          1966,   117,  1141,   117,   179,   495,   390,   303,   122,  4207,\n",
      "           744,  3746,  4803,   785,   117,  2113,   625,   653,  7762, 22281,\n",
      "           236,   123,  6759,   122,   123,  2606,  2036,  8625, 22281,   236,\n",
      "           230,  1858,   860,   266,   495,   331,  3497,   118,  1084,   221,\n",
      "           146,   644,   492,   170,   222,  8993,   269,  4207,  5637,   118,\n",
      "          1340,   221,  1966,   122,   179,   495,   146,  1010,   215,  2010,\n",
      "           572, 22283,   230,  5294,  8849,  3898,  1145,   256,   368,   834,\n",
      "          4914, 18079,   159,   118,   176,   170,   123, 15685,   171, 12447,\n",
      "           397,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   230, 16318,   613,   699,   202,  1338,   125, 11169,   179,\n",
      "           644,   492,  1047, 22280,  2779,   136,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   230,  1105,   125,  3907,   523,   117,   180,   615,   229,\n",
      "         22280, 21174, 14661,   118,   311,   834,  7766,   367,   146,   179,\n",
      "          1084,   418, 11272,   222,  1855,  3285,   286,  1532,  2551,   125,\n",
      "           669,  3391, 22280,   143,   179,   229, 22280,   176, 17558,   328,\n",
      "         22287,  2364,   117,   122,  1078,   576,   325,   176, 12543,   228,\n",
      "           122,   325,   311,   768,  9398,   320, 21964,   141,  1014,  2480,\n",
      "           117,   582,  4314,   244,   123, 15520,   179, 15212,   125,  7343,\n",
      "           117,   176,   123,  8410,   171,  7343,  6884,   373,   122,   146,\n",
      "           171,   185,   117,   179,   311,  5626,  7583,   834,   118,   792,\n",
      "         12268,   122,   179,   123,   740,   311,   466,   323,   271,   123,\n",
      "          1143,   185,   180,  1105,  2791,   311,   466,   323,   123,   418,\n",
      "          4203,   121, 22361,  5566, 22278,   136,   262,   180, 10262,   392,\n",
      "           304, 22280,  1154,  3679,  5809,  5365,   179,   176,  5192,   202,\n",
      "         16450,   304, 22280, 16026,   171, 15796,   404,   222,  1160,  7503,\n",
      "          2010,   146,  4222,   452,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,  3207,   214,   118,  2036,  2829,   310,  2004, 22280,   221,\n",
      "           259, 21853,   501,  5248,   179, 16386,   210,   123,  1069,   125,\n",
      "           222,  2397,   834, 20027,   117,   123,  1977,  8650,   122,   834,\n",
      "         12223,  3391, 13793,   221,   926,  3746,  4803,   170,   260, 11920,\n",
      "           470,   117,   146, 20456,  8080, 22280, 18825,   748,   118,   176,\n",
      "          7583, 14038, 22278,   117,   271,   222,   762,  8723, 10922,   185,\n",
      "           117, 19054,   180,  1386,   117,   179,   176, 15975,   421,   123,\n",
      "          2521,  2028,   125,   230,  1069,  9021,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   123,  2541,  1076,   125,   860,   266,   117,   179,   123,\n",
      "           905,   247,  2036,  4551,   256,   298, 18908,   501, 15856,   379,\n",
      "          1159, 13449, 19088, 22281,   125,   390,  1165,   117,  2535,  2036,\n",
      "          6009, 12051,   123,  5546,   154,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101, 11556, 11688,   578,   118,   176,   125,   179,   740,   170,\n",
      "          3901,  4841,   186,  5052,  5284,   117,   179,   368,   117,   240,\n",
      "           327,   576,   117,   176,   229, 22280,   146,   978,  4841,   201,\n",
      "           117,  5626,   124,   118,   146,   240,  3402,  2004, 22278,   117,\n",
      "           146,   179, 12444,  5488, 22282,   325,   744,   122,  1065,   318,\n",
      "         13793,   905,  4373,   123,  2448,  2430,   170,   222,  1923,  1464,\n",
      "           117,  2636,  1659,   146,  4947,  3189,   286,   180,   327,  1622,\n",
      "           340,   117,   785,  8174,   788,   202, 14353, 22280,   240,   370,\n",
      "           870,   509,  8085,   230,  5664,   173,   179,  4207, 19226, 22282,\n",
      "          3495,   117,   834,   370,   117,  2364,   325,   117,   125,   398,\n",
      "           569, 22283,   118,  1340,   123,  2606,   117,  2798,   370,   125,\n",
      "          5308,   118,  1340,   123,  2760,  3933,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,  4327,  5811,   304, 22280,  1261,  5990,   118,   146,   173,\n",
      "          8887,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,  2002,  2044,   221, 16044,   307,   118,   176, 15316,   366,\n",
      "          3038,  6034,   784,   117, 19519,   214,   440,  5345,  1522,  3290,\n",
      "           117,  4276, 20057, 13114,   243,   118,   176,  2249,  4207,   122,\n",
      "         12746,  2746,   123,   327,  2401, 22279,   524,   423, 12034,   170,\n",
      "           222,  1065, 21102,   293,   388,   125, 20283,  4917, 22281,  3824,\n",
      "           403,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   320,  3852,   118,  2036,   944,   259,  1564,   412,  5304,\n",
      "           117,  7104,  2349,   256,   118,   146,   170,  2561,   304, 22280,\n",
      "           117, 13449, 11252,   834,  1984, 22282,   122,  3030,  7831,  2044,\n",
      "           123,  1354,   173,  2590,   117,   785,   333,   247,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,  2391,   259,  1867, 10674,   221,   123,  6009,   171,  4222,\n",
      "           452,  6540,   123,  1105,   122,  2002,  8118,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   123,  2606,   117,  4460,   179,  2036, 12110, 22281,  6867,\n",
      "          1941,   259, 13841,  8618,   117,  4912,  1939,  1808, 22288,   170,\n",
      "          1257,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  1757,   209,  7186,   978,   318, 13793,  6136,   221,  9967,\n",
      "           481,   122,   495,   146,  1903, 17609,   180,  2922,   485,  1451,\n",
      "          1877,   328,   117,  5923, 11324,   117,   170,  1283,   194,   986,\n",
      "         18048,   577,  3706,   529,   362,   942,   138,   171, 17749,   117,\n",
      "           366,  1877,   269,  4323,   122,   298, 18908,   501,   117, 21347,\n",
      "         20474,  2157,   162,  6839,   125,  9344,   591,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,  9815,   256,   146,  5902,   222,   286,   366,  2968, 13152,\n",
      "          1343,   117,   230,  2566,   392, 16224,   125,  4929, 21563, 13841,\n",
      "         17291,   268,   118,  6054, 22281,   117,   223,   128,  1821, 19408,\n",
      "           358,   117,   877,   842,  3848,   143,   122, 10647,   117,   271,\n",
      "           260,   180,   223, 22279,   117, 12141,  1695,   325,  6054, 22281,\n",
      "           171,   179,   123,  1034,  4530,   171,  9169,   117,  1143,  1283,\n",
      "           194,  1674,   117,  1896,   892, 15541,   449,   259,  5708,  1491,\n",
      "           117,  7769,   117,  9585,   122,  3002,  9258,  1409,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   240,  1921,  1405,   451, 22278,   117, 13776,   117, 19288,\n",
      "           125, 10802,   117, 16302,   243,   320,  1568,  3914,   117,   146,\n",
      "          1417,   125,   222, 22082,  1168, 18745, 14908, 22280,   179, 10348,\n",
      "         15152, 22281, 16228,   123,  1105,  2791,   125, 15796,   404,   122,\n",
      "           179,   495,  5787,   146,  1407,   958,  2118,   179,   860,  1776,\n",
      "         22278,   202,  2699,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   146, 13254, 13028,   118,   176,  2678, 22285,  3132,   117,\n",
      "           978,  8184,   481,   122,  8940,  8794,   229,  3952,  1089,  3058,\n",
      "           428,   501,   179,  2036,  3207,   692,   221,  4270,   229, 14164,\n",
      "           125,  7930,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101, 15796,   404,  9730,   203,   118,   146,   202,   347,   425,\n",
      "           825,   180,  4768,   171, 11014,  9258,   449,   146,  8609,   179,\n",
      "          1391, 22288,   118,   176,   117,   202,  1338,   125,  1089,  1564,\n",
      "           117,   125,   179,   123, 22283,  9086,  3002, 13851,  6859,   117,\n",
      "           122,   146, 19317,   175,   117,   123,  1977,   229, 22280,  5572,\n",
      "           508, 16550,   671, 22282,   118,  2036,   117,  8404,   203,   170,\n",
      "           368,   221,   123,   327, 18489,   340,  2754,   125, 11967, 10953,\n",
      "           763,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,  2678, 22285,  3132,   495,  7390,  5137, 21102,   117, 13140,\n",
      "           125,  1334, 13808,   579,   117,   170, 11368, 15020,   261,  6471,\n",
      "           125,  9586,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,  9821,   785,  8170, 22280,   298,   532,  2595,   122,   316,\n",
      "         22280,  1695,  3084,   256, 10768,   122,  3598,  5592,   117,   179,\n",
      "           229, 22280, 15891,  1399,   222,  4698, 22287,  1796,   366,  4096,\n",
      "           125,   681,  5657,  3756,   351,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,   125,  4745,   117,   123,   229, 22280,   333,   125,  1062,\n",
      "           252,   221,   260,  6880,   117,   179,  8544,  1684,   170,   146,\n",
      "         15796,   404,   117,   229, 22280,  3456,   285,   256,   766,   125,\n",
      "          1105,  3133, 13793,   173,  4067,   180, 20027,   117,  2166,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101, 10502,   860,   266,   117,   202,  6365,   125,  1695,   596,\n",
      "           117,  6075,   240,   368,  5948,  1821, 17883, 22290,   122, 12771,\n",
      "           203,   118,   176,   125,  5357,  1284,   180,   327,  9317,   285,\n",
      "           117,  9317,   285, 13608, 22278,   423, 19317,   175,   117,  3382,\n",
      "           179,   146,  2678, 22285, 13538,   994,   978,  2601,  1546, 22278,\n",
      "           171,  1568,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,  2364, 21315,  3495,   625, 11736,   125,  1569,  5664,   117,\n",
      "         15158,   256,   118,   123,   125, 10502,   860,   266,   117,   179,\n",
      "           240,   327,   576, 12771,  2836,   146,  4170,   125,  6009,   118,\n",
      "          1084,   117,   660,   146,  4947, 18253,  1196,   229,  1284,   171,\n",
      "         22082,   170,   230,   170,  6501, 22280,   125,  3644,   900,   247,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   327,  9730,   984,  2803,  5723,   623, 15048,   122, 11330,\n",
      "           592,   118,  7911,   240,   454,   117,   171,   179,   368, 17226,\n",
      "           229, 22280,   978,  3482,   117,  2798,  4750,   370,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,  3874,  2036,  3207,   256,   117,   122,   259,  7329,   180,\n",
      "          1105,   146, 16598,   692,   271,   123,   222,  1417,   171,  2004,\n",
      "         22280,  7258,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,   123,  2954,   117,   260,  1176,   117,   625,   146,   596,\n",
      "          1011,  4062,   117, 10502,   860,   266,  8625, 22278,   170,   368,\n",
      "           117,   123,  2267,   122,   222,  3848, 17714, 22279,   117,   146,\n",
      "          5488, 15500, 22287,   117,   123,  2822,   210,   230,  1359,  2684,\n",
      "           123,  7815,   122,   117,   173,  1226,  8431,   221,  1569,  4939,\n",
      "           173,  1105,   366, 19763,   117, 16403,   118,   146,   173,   327,\n",
      "          4067,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101,   123,  3293,   705,   180, 20027,   117,   171, 15796,   404,\n",
      "          5048, 13808,   118,   176,   125,   847,  1382,   124,   117,  1124,\n",
      "          7137,   744,   390,   304,   117,  3848,   458,   324,   122,   374,\n",
      "           266,   117,   179,  3598,  5723,  1364,   146,  4698, 22285,  6199,\n",
      "           179, 18720,   256,   173,  8977,   853,   634,   229,  5304,   125,\n",
      "          4141, 13793,  2415, 22280,   230,  2128, 11324,  2477,   705,   117,\n",
      "          2052,   447,  4275, 22282,   117,   785, 10032,   122, 12043,   117,\n",
      "         21990, 22278,   122,  8742,   271,   222,  3848, 17714, 22279,   117,\n",
      "          7422,   214,   125, 17722,   252,   117,   834,  2036,  3207, 22282,\n",
      "           222,  2476,   117,   123, 11281,  4277,   180, 21597,   194,   292,\n",
      "           117,   122,  4111,   117,  1684,   179,   259, 11314,  2650,  1058,\n",
      "           291,   259,   958,  2118,   143,   180,   316,   391,   324,   117,\n",
      "           331,   221,   311,  2650, 22282,   170,   740,   117,  2036, 21280,\n",
      "          3106,   304,   303,   143,  1112,   146,   151,   117,   179,  2779,\n",
      "           311,   179,  1391,   320,  7472,   125,   438,   780, 22354,   117,\n",
      "           122,  3448,   146,  1815,  5488, 15500, 22287,   117,  1417,   125,\n",
      "           230, 17479,   179,   262,   125, 10502,   860,   266,   122,   123,\n",
      "          1977,   418,  1021,   313,   512, 14996,   102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,   123,  2606,   171, 15796,   404,   978,   240,   860,  3848,\n",
      "         17714, 22279,   230,  4613,   232, 22280,   834,  6559, 10348,   118,\n",
      "          2036,  1719,   123,  4676,   117,  3495,   117,  4400,   117, 16403,\n",
      "           118,   146, 11631,   123, 17611,   117, 16555,   118,   146,  1004,\n",
      "         12413,   122,  5747,   576,  2080,   123,  1434, 13747,  7362,   123,\n",
      "          2267,   117,   125,   316, 22280,  6358,   343,   179,   176, 15245,\n",
      "           170,   368,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,  1502,   176,   123,   853,  6522,  1337, 17704,   646, 22290,\n",
      "         15736,   170,  1757,   209,  7186,   240,  2318,   171,  2128,  6812,\n",
      "          1502,   117,   176,   625,   176,   179,  1445,   692,   259,   682,\n",
      "           117,   222,   598,   146,  1342,   117,   740,  2364, 10348,  3083,\n",
      "         13793,   123,  2267,  1502,   176,   146,   179,  1021,   125,  1407,\n",
      "           229,  1105,   495,   221,   146,  5488, 15500, 22287,  1502,   117,\n",
      "           176,   625,   262,   860, 13379,   125,  5294, 14271,  1557,   122,\n",
      "           146, 15796,   404,   117,  2440,   366, 10262, 21014,   122,   298,\n",
      "          9958,   180,  2772,   117,  9673,   118,   146,   221,   222,  5627,\n",
      "           117, 10502,   860,   266,  5334,  5630,   944,   259,  1564,   122,\n",
      "           726,   123, 12298,   340,  2461,   229, 22280, 10539,  7670,   117,\n",
      "          2798,  9537,   117,  2798,  6075,   259, 12141,   123, 16241,  1537,\n",
      "         22287,   136,   122,   146,  6754, 15796,   404,   117,   176,   229,\n",
      "         22280,  4750,  9533, 16421,  1245,  3292, 22281,   180,  2606,   122,\n",
      "          9226,  3260, 12764,  3318,   975,  1885,   185,   298,  7329,   117,\n",
      "           978,   125,  2822,   320,  3848, 17714, 22279,  1719,   123,   996,\n",
      "           304, 22280,   122,  1434,   118,  2036, 21323,   246,  1485,   260,\n",
      "          6272, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,  1021,   744,   117,   425,   260,  4117,   842,   171, 19317,\n",
      "           175,   117,   222,  1342,  9730, 22279,  1202, 22287,   171,  2678,\n",
      "         22285,  3132,   117,   146,  4575, 11967,  1215,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,   860,   117,   240,   210,   117,   229,  3322,   125, 11718,\n",
      "           343,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,   495,   222,  6754,   118,   644,   492, 18597,   214,   221,\n",
      "           259, 17585,   481,   117,  6267,   321,   713,   117, 10881,  4712,\n",
      "           117,  6995,   122, 16293,   117,   271,  3235,   256,   117, 17897,\n",
      "           122,  4351,   339,   272,   171,   653, 11609,   785, 13376,  3173,\n",
      "           183,   117,   170,  7226, 11224,   128, 13722,  3324,   179,  2036,\n",
      "          8101,   692,   146,  3846,   180,   879,  6720,   266,   122, 21280,\n",
      "           118,  2036,   123,  1354,   230,  9434, 22280,   125, 16526,  1186,\n",
      "           117, 15363,   125,  1365,   170,   146,   347, 17749,   602,   309,\n",
      "           303,   122,   170,   123,   327,  9463,   834, 18908,   501,  1413,\n",
      "         22287,   118,   176,   118,  2036,   744,   944,   259, 12141,   117,\n",
      "           449,   117,   316, 22280, 13478,   117,   179,  9821, 22287,  1743,\n",
      "           308,  2684,   320,  1423,   102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,  6952,   256,  1684,   125,  7967,   117,   170,   222,  5825,\n",
      "           118,  9856, 15702,   171,  4332,   303,   122,   222,  1690,  7817,\n",
      "           125,  4332,   421, 11272,   529, 17722,   842,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101,  1796,   173,   347,   596, 12531,   171,  1847,   523,   117,\n",
      "           700, 17478, 22282,   125,  5976,  7719,   653,   179, 13956, 22278,\n",
      "           325,   125,   230,   576,   229,  5566, 22278, 19317,   214,  7769,\n",
      "           240,   327,  1284,   102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101,  7304,   748,   118,   176,   785,   260, 20519,  3391, 22280,\n",
      "           143,   726,   123,  1658,   171,   221,  4894,   744,  5076, 22278,\n",
      "          2124,   117,  5168,   123,   333,  1004,  8797,   449,   123, 10849,\n",
      "          1950,   214, 22288,   122,   117,   125,  3002,   439,   157,   173,\n",
      "          3002,   439,   157,   117,   262,   118,  2036, 15832,   214,  2745,\n",
      "           240,   420,   260,   675, 16317, 22281,   125, 15252,   125,  6372,\n",
      "           387,   102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[  101,   122,  2535,   117,   144,  2640,   117,  1941,  4575,   117,\n",
      "           170,   286,   125,   273,   215,  5587,   143,   117, 13140,   125,\n",
      "          8394,  6702,   649,   117,  1413,   118,   176,  4171,   834,  3353,\n",
      "           122,  4655,  5723,   123, 15419,   171, 15796,   285,   117,   170,\n",
      "          1977,   240,  1415,   481,  3954,   173, 13254,   117,   425,   260,\n",
      "          6107,   171,   653,  9019, 13793,   117,   122,   125,  1977,   176,\n",
      "         14223,   124,  3695,   117,   123,   905,   247,   240, 19937,   122,\n",
      "           325,  1373,   240,  4096,   102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[  101,  7427,  5630,   118,   146,   117,  2954,   122,   644,   117,\n",
      "           230,  4955,   304,   415,  8650,  7474,   117,   230,  1401,   285,\n",
      "         12448,   852,   125, 11938,   117,   222, 19781, 11097,   525,  3356,\n",
      "           403,   117,   598,  2745,   122,   598,   944,   117,   240,   229,\n",
      "         22280,  2036,   370,   908,   668,  4812,  4276,   211,   702,   146,\n",
      "          1147,   170,   260,   675,   223,   128,  1790,   238,  1973,   244,\n",
      "         22281,   122,  8574,  2191,   102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101,   122,   117,   271,   146,   347,  2233,  1177,   125,  3061,\n",
      "          1994,   229, 22280,  2036, 12122,  8925,   598, 16241,  1537, 22287,\n",
      "           146, 21115,   117,  1950,   581,  1165,   256,  1746,  1499,   371,\n",
      "           214,   260,  5365,   180,  1405,   451, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101,  1016,   117,  1085,   260,  1176,   785, 13278,   260,   498,\n",
      "          1056,   138,   171, 15796,   404,   117,   625,   117,   420,   736,\n",
      "          6578,  1877, 19147, 12822,   117,  8940,   123,  5489, 22281,   375,\n",
      "         22280,   146,  2115,  8806, 21544,   179,   905,   151,   256,   123,\n",
      "          4883,   118,   176,   173,  3443,   180,  2241,  2187,  4712,   102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101,   318, 13793,   146, 11967,  1215,  9086,  4720, 22281,   293,\n",
      "           122,   962,  8476,  2836, 16264,  1749,   909,   117,   221,   123,\n",
      "          5065,   122,   221,   123,  4573,   117,   271,  1977, 10907, 12785,\n",
      "           834,  1434,  6568,   117,   122,  2354, 14406,  2836,  4019,   304,\n",
      "           303,   143,   117, 17887,  7583,  1201, 22292,   565,   221,  5452,\n",
      "           439,   159,   146,  4575,   146,   635,  6039,  2794,  1839,  2461,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 50: tensor([[  101,  2010,  8490,   474, 10420,  5630,  1178,  3066,   713,   102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[  101,   329,  3252, 22278,   125,  1390,   185,   801,   122,   146,\n",
      "           347, 21568,   141, 15888,   679,   256,   118,  2036,   298,  5708,\n",
      "           173, 15351,   138, 15755,   912,   591,   117, 12625,  9804,  2858,\n",
      "           118,   176,   173,  1485,   260,  2566,  3082,   122,   173,  1485,\n",
      "           260, 20390,   589,   102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[  101,   123,  9429,   117,   123,  7828,   117,   146,  9861,   117,\n",
      "           123,   390,  2420,   117,   123,   344,   304,   117,   123, 10428,\n",
      "         22279,   117,   122,  1953,   123, 12464,   117,   122,   145,   146,\n",
      "           179,   368,   229, 22280, 16759,  2836,   123, 16241,  1537, 22287,\n",
      "           117,  3330,  2876,   319,   348,  1364,  6086,   179, 17002,   146,\n",
      "           179,   368,   229, 22280,   463,  6758,   124,   179,  3746,   852,\n",
      "           256,   146,   179,   368,   229, 22280, 16102, 19749,   179,  9679,\n",
      "           146,   179,   368,   229, 22280, 10698, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[  101,   122,   117,   221,  6553,  2854,   146,  4947,   171,   347,\n",
      "           146,   635,   117,  1359,   256,   118,   176,   598,   146,  1010,\n",
      "           215,   117,  1921,  2480,   179,   117,   229,   327,  5012,   151,\n",
      "         22280,   117,   331,   978,   230,  7947, 15500, 22278, 20613,   943,\n",
      "           259,  4966,   117,   122,   179,   117,   202,  1325,   117,   146,\n",
      "          4314, 22278,   117,   123,   368,   117,   229,  2377, 10866, 22278,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101,   532,  1564,  1085,  5149,   474,   171,  1457,  2277, 19994,\n",
      "           256,   260,  3003,   180,  1062,   252,   117, 11348,   256,   118,\n",
      "           176,   653,   202,  3147,   170,   230,   374,  3613,  3848,  4419,\n",
      "           173,  5791,   183,   125, 10832,   700,  8544,  9784,   259,  6349,\n",
      "           221,   123,  4767,   125, 14890,   117,   123,  2521,   171, 16960,\n",
      "           303, 16960,   304,   256,   122,  4767,   117,  5267,   256,   146,\n",
      "          7390,   272,   122,  8544,   641,  5137, 21102,   221,   230, 11628,\n",
      "          3239,   322,   180,  4768,   171, 12728, 22282,   117,   582, 17003,\n",
      "          4412,  3791,  4409,  2684,   260,  2856,   171, 14890,   117,   420,\n",
      "          1672,   123,  4640,  3002,   366,  1101,   179, 19407,  1084,  1796,\n",
      "           117,   975,  1885,   185,  2461,   102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101,   978,   123, 20494, 13793,   125,  8223,  1364,   146,  2187,\n",
      "           125,  1543,   122,   259,   302, 21510, 22281,   125,  1078,   222,\n",
      "           173,  2754,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101,   260,  1176,   117,  6387,   117, 10502,   860,   266, 12771,\n",
      "          2836,   118,   146,   125,  1434,  4296, 18937,   125,  4857,  6812,\n",
      "           117,   146,   179,   146, 11967,  1215, 19005,   256,  1407,   179,\n",
      "         16241,  1537, 22287,   136,   449,   123,   327,   739,  1568,  2037,\n",
      "         22280,   117,   146,   347, 13141,   117,   495,   123,  5546,   285,\n",
      "           117,  2251,  5630,  2745,   179,  1996, 22281,   236,  3953,   123,\n",
      "          2260,   714,   117,  4460,   179, 18424, 22278,  1684,  2401,  4211,\n",
      "           415,  7672,   260,  3191,   125,  1569,  4446, 22279,   117,  1623,\n",
      "           246,   260,   125,  4848,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[  101,   229, 22280,  4207,  9226, 10907, 22282,  3047,   125,   898,\n",
      "           230,   730,  7173, 18304, 22278,   117, 10552,   148,   256,   118,\n",
      "           176,   240,   210,   170,  2745,   179,   765,  1402,   236,   123,\n",
      "          1658,   123,   543,   194,   304,   125,   222,  1538,   173,   739,\n",
      "          9470,  4551,   256,   118,  2036,  1084, 20739,   125,   271,   304,\n",
      "         22280, 18574,   229,  8993,   180,  3182, 22278,   146,   179,   176,\n",
      "          1822,   151,   123,  1069,   125, 13974,  7194,   151,   320,   652,\n",
      "         21574,   125,  5708,   146,  4460,   122,   146,  1831,   123,   179,\n",
      "          7898,  1569, 11793,   122,   117,  2440,   298,   532,  7093,  1374,\n",
      "           117,   495,  9226,  6374,   229,  4768,   123,   549,  2912, 22278,\n",
      "           291,   146, 14619,   141,  4337,   557,   146,  2971, 22280,   117,\n",
      "          9086,  2044,   202,   388,   117,   122,   117,  5747,   576,   117,\n",
      "           625, 10348,   240,   898,   117,  5057,   670,   298,   179,  6082,\n",
      "           692,   123, 19540,   102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[  101,   318, 13793,   117,   229, 22280, 19674,   221,  1105,  1139,\n",
      "           259,  3218, 10775,   176, 12401,  4368,   102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[  101,  1821,  1684,  1359,   256,  2990,  7406,  7870,   260,  2139,\n",
      "           180,  1373,   117,   390,   286,   123,  1434,   171,   117,   834,\n",
      "           926,   370,   118,   176,   529, 11351,   117, 15872, 10490,   243,\n",
      "           125, 10803, 22282,  2856,   122,  2856,   320,  4081,   180,  2293,\n",
      "         22278,   125,  9196,  2382,   322,   102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101,   122,   146,   325, 11236,   122,   179,   368,   117,   320,\n",
      "          2477,   118,  2036,   123,  3655,   304, 22280,   117,  6838,   256,\n",
      "           118,   176, 18928,  1212,   598,   146,  3002, 11646,  5278,   179,\n",
      "           146, 18237,   256,  7583,  9466, 14004,   117,  4956,   146,  2971,\n",
      "         22280,   240,   230,  9211,   292,   125,  5207,   122,  2636,   125,\n",
      "         19284,   373,   146,  3420,   325,  1639,   102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101,  2010,   331,  4048,   117, 19068,   796,   256,   118,   176,\n",
      "           368,   117,   179,   123,  4388,   304, 22280,  4566,  3002,  2525,\n",
      "           495,  2822,   118,   311,  6365,   180,  6614, 11032,   873,  2872,\n",
      "          1510, 22281,  2856,   125, 11978, 22279,   118, 11978, 22279,   240,\n",
      "           230,   331,  4943,   556,   125,   944,   259,   644,  3471,   230,\n",
      "           366,  4351, 22282,   277,   325,   170,   585,   171, 11967,  1215,\n",
      "           495,   146,   347,   146,   635,   423,  5488, 15500, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101,   146,  3848, 17714, 22279,  2318,   256,   118,  2036, 14594,\n",
      "           170,   260,   675,   766,   226,  1053,   784,   125,  9726,  4943,\n",
      "         22280,   117,   122,   117,  7492,   303,   117,  5945,   214,  2249,\n",
      "          2859,   146, 12239, 11814,   117,   744,   325, 18420,  2836,   117,\n",
      "         11120,   229,  2561,   304, 22280,   125, 10502,   860,   266,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[  101,   146, 11718,   343,   125,   785,   179,   146,  2471,  5404,\n",
      "          2794,   117,   176,   229, 22280,  1796,   123,  4096,   125, 15397,\n",
      "           159,   123, 10502,   180,  1105,   102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[  101, 11967,  1215, 18574,   260,  3207, 22281,   125,   860,   266,\n",
      "           271,   260,  1877,   486,   180,  2004, 22278,   223, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[  101,   146, 15796,   404,   653,   117,   179,   146,  1413,   173,\n",
      "          1284,   125,  3695, 11293,   117,  1615,   122,  1615,  1176,   219,\n",
      "           842, 12908,   124,   173,  2880,   247,   143, 15834,   591,   125,\n",
      "          1950,   581,  1783,   117, 17141,  1546,  3514,   146,  2249,   202,\n",
      "         14353, 22280,   123, 10968,   852,   256,   122,   123,  3083, 13793,\n",
      "           240,   179,   229, 22280,   123,  5078,   252,   229,  4768,   712,\n",
      "          8993,  2817,   102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101,   122,   146, 11967,  1215, 10348,   118,  2036,  1719,   123,\n",
      "          3083, 13793,  3486,  1399, 14619,   210,   179,   259,   333,   501,\n",
      "          6399,  5201,  2072,  3364,   125,  2745,   102]])\n",
      "DEBUG: Tokenized sentence 67: tensor([[  101,  2010,   230,  2606,  6621, 22281,  1109,   319,   143,   117,\n",
      "         10355,   368, 18164,   183,   117,  2120,  3874,  1528,   179,   146,\n",
      "          1855,   117,   122,   222,  1855,   173,  1652,  3484,   123,  9349,\n",
      "         10968,   852,  2535,   117,  2354, 22279,   146,   179, 12444,   495,\n",
      "          2364,  3767,   118,   176,   221,   740,   102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[  101,  2010, 11032,  6834,   256,   146,  4170,   102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101,  2779,   311,   898, 22282,   378,  3914,   271,  1977,   176,\n",
      "          7947,   125,   230,  3240, 22282,  8008,   364,   146, 11718,   343,\n",
      "           117,  8540,   240,   792,  2249,   146,  3695,  4042, 22290,  5723,\n",
      "           123,  2606,   117, 13130,   256,   173,  2745, 20824,   117,  3951,\n",
      "           118,  2036,   222,   505,  2552, 22280, 12631,   303,   125,  8296,\n",
      "           304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[  101,   449,   240,  1342,  1341,   117,   625,  8362, 22278,   860,\n",
      "           266,  5961,   171,  4170,   117,   170, 18522,  1065, 22287,   122,\n",
      "          2684,   170,  7355, 22280,   117,   744,   325,  1428,  2001,   383,\n",
      "           151,   125,  1519,   175,   102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101,  2010,  2354, 22279,  3189,  4945,   136, 18601,   740,   117,\n",
      "          2779,  1004, 11094, 22280,  2249,  6086,   338,  7485, 22279,   171,\n",
      "          7258,  7343,  4170,   311,  4343,  8849,   117,   449,  1257,  1971,\n",
      "           176,   311,   180,   271,   123,   681,  7924,   179, 12232,   273,\n",
      "           522,  2382,   246,   221,   538,   117,  2459,   125,  2707,   117,\n",
      "           229, 22280,  7105,  5212,   834, 15211, 22280,   117,   625,  4081,\n",
      "           128, 12222, 22281,   125,   547,   179, 15212,   125,   829,   900,\n",
      "           146,   179,   311,  6600,   173,  7716,   117,  3189,  8766,   185,\n",
      "          2461,  3189,   229, 22280,  8766,   185,  3436, 22280,   118,  2036,\n",
      "           117,   240,   210,   117,   179,   117,   176,   380,  2720,   179,\n",
      "           146, 15796,   404,   176,  1120,  3897,   260,  1176,   221,  9726,\n",
      "           117,   122,  2113,  3486,   214,   179,  9141,   325,   123,  7482,\n",
      "         21761,   171,   179, 14537,   159,  5489, 22281,   375, 22280,   170,\n",
      "           230,  5294,  8849,  7970,  2601,   146, 11967,  1215,   117,   170,\n",
      "           123,   327,  9221,   514,  2647,  2000,  3292,   171,  1147,   117,\n",
      "          2364,  4686,   151,   123,  3484,   298,   682,   146,   179,  1078,\n",
      "           615,  2036, 10355,   598,   146,  1342,  1971,  1016,   179,   117,\n",
      "          5288,  2880,   151, 22280,   117, 12401,  1825,   118,   176,   123,\n",
      "          1105, 20990,  6859,   117,   173,  5314,   179,   229, 22280,   495,\n",
      "           171,   347, 12215,   117, 16143,   117,   320,  3852,   423,  6151,\n",
      "         22290,   117,  2082, 14473,   444,   125, 12514,  3508,  1165,   591,\n",
      "           179,  9821, 22287,  2477,   125,   222,  2242,   870,   439,   201,\n",
      "           125,  3088,   392,   117,   582,   173,  1250,   229, 22280,  8544,\n",
      "         16241,  1537, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101, 16924,   203,   118,   176,   221,  1084,   173, 21115, 22281,\n",
      "           125,  1143,   122,   117,   834,   333, 11094,   286,   117,  6920,\n",
      "           860,   266,   318,   840,   285,   420,   146, 16909,   122,   146,\n",
      "          2678, 22285,  3132,   102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101,  2789,   118,   176,  4412,   730, 17598,   117,   834,  5023,\n",
      "          2245, 22282,  2798,   362,  2245, 22282,   117,   122,   117,   331,\n",
      "           625,   259,   682,   176, 14661,   228,   117,   262,   179,   368,\n",
      "           176,  6075,   102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101,   123, 17704,   969,   654,   222,  3265,  9247,   183,   117,\n",
      "           122,   146, 13254,   117,   125,  6367,   179,  1011,   117,  1191,\n",
      "           118,   176,   549,   125, 21340,   449,   146, 11967,  1215, 11556,\n",
      "         20885,  2446,   118, 15887,   117,  4111,   173,  4410,  8932,   122,\n",
      "         20004,  2010,  1257,   122,   230,   525,   381,  2346,   351,   146,\n",
      "           179,  2354,   143,   418, 22280,  2636,   102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[  101,  3769,  4486,   229, 22280,   122,  2166,  2277,   179,   176,\n",
      "          5646,  2872,  1016,   271,   572, 22283,  2779,   117,  4207,   333,\n",
      "          1858,  2760,   102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101,  1502,  1532,  1105,   173,   179,   607, 14730, 15050,   117,\n",
      "           122,  1084,  8911,  2477,  3285,  5194,   118,   176,  3102,  2242,\n",
      "           171,  6151, 22290,   136,   102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[  101,  2010,   538,   229, 22280,  2072,   128,  2636,  3874,  1996,\n",
      "           860,   266,   117,  3477,   214,   146,  5052,   118, 10343,   102]])\n",
      "DEBUG: Tokenized sentence 78: tensor([[  101,  2010,   123, 22296,  1204,   146,  4575,   117,  1183,  5974,\n",
      "         20158, 22280,  3953,   318, 13793, 13201,   269,   117,  4174,   244,\n",
      "           179, 19864,   102]])\n",
      "DEBUG: Tokenized sentence 79: tensor([[  101,   122,  2389,   296,   179,   117,   176,  1016,  2589,   117,\n",
      "           221,  9726,  1467,   146,   653,   117,  2113, 17386,  1257,   123,\n",
      "          5664,   325,  2711,   171,  1147,   122,  3486,   214,   179,  1014,\n",
      "          1069,   123,  9349,   331,  1904,   146,   179,   662,   102]])\n",
      "DEBUG: Tokenized sentence 80: tensor([[  101,   176,  1976,   117,  3960,   151,   117,   262,   271,   176,\n",
      "          3874,  1003,   236,   117,  2113,  3874, 15212,   123,   765,   364,\n",
      "         22282,   170,   123,  1069,   125,  1078,   222,   102]])\n",
      "DEBUG: Tokenized sentence 81: tensor([[  101,   123, 17704,   418,   390,   304,   117,   418,   229,   344,\n",
      "           304,   298,   481,   347,  4170,   229, 22280,   123, 12458, 22305,\n",
      "           117,   122, 20198,   179,   146,  1747, 22278,   240,  1342,   123,\n",
      "         22296,  3413,   122,   146,  1147,   117,   122,   117,   176,   122,\n",
      "           516,   183,   117,   229, 22280,   227,   793,   538,   179,   146,\n",
      "          3283,  5835,   516,   183,   102]])\n",
      "DEBUG: Tokenized sentence 82: tensor([[  101,  2684,  5288,  2169,   944,  7273,  1839,   222,  7395, 13732,\n",
      "           268,   118,   505,  6592,  3120,   117,   179,   122,  8911,  6074,\n",
      "           117,  1075,   179,   368,   538,  8788, 22279,   229, 22280,  7707,\n",
      "           171,   228,   260,   223,   128,   102]])\n",
      "DEBUG: Tokenized sentence 83: tensor([[  101,   820, 17386,   179,   117,   221,  1858,   576,   117,  3921,\n",
      "           370,   222,  1074, 19161,   325,   125, 12245,   122,   102]])\n",
      "DEBUG: Tokenized sentence 84: tensor([[ 101, 2010,  418, 4062, 1587,  154, 7415,  860,  266,  102]])\n",
      "DEBUG: Tokenized sentence 85: tensor([[  101,  2010,  5112, 22280,  2779,   117,   176,  2826, 22280,  3413,\n",
      "           117,   122,   221,  5308,   118, 15887,  1004, 20885,   128,   123,\n",
      "          7343,  3953,   102]])\n",
      "DEBUG: Tokenized sentence 86: tensor([[  101,   229, 22280, 18691,   117,  2798,   240, 15419,   117,   179,\n",
      "           176, 20469,   228,   125,   179,   102]])\n",
      "DEBUG: Tokenized sentence 87: tensor([[  101,   146,  2678, 22285,  3132,  1316, 22290,  2598,   117,   170,\n",
      "           123,  4410,   744,   271,  2100,  2010,   449,   117,  2547,   593,\n",
      "           117,   347, 11967,  1215,   117,   179,   102]])\n",
      "DEBUG: Tokenized sentence 88: tensor([[  101,   146,  4575,  8082, 13665,   118,   146, 14619,   210,   240,\n",
      "           327,   576,   117,  3891,   118,  2036,   123,   223, 22280,   202,\n",
      "         20462,   122,  4415,  1552,   118,   146, 11631,  2010,   229, 22280,\n",
      "          2660,   746,   247,   117,   179,   229, 22280,   146,  7766,   367,\n",
      "           244,   117,  9778,   122,   117,   271,  1941, 19864, 15262,   125,\n",
      "           860,   266,   117, 11021, 22288,   118,  2036,   173,  5902, 21839,\n",
      "          2010,   229, 22280,   745, 22279,   123,  1434,  3413,  1016,   117,\n",
      "           179,  2354, 22279,   176,  3254,   421,   102]])\n",
      "DEBUG: Tokenized sentence 89: tensor([[  101,  2389,   296,   271,  2036,  8574,   210,   260, 11351, 10502,\n",
      "           860,   266, 14009,   118,   259,   123, 18175,   117,  5926, 11681,\n",
      "           246,   117, 19519,   214,  5811,   304, 22280,   173,  9654,   222,\n",
      "          6847,  4943,   735, 22279,   117,  8764,  2968,   740,  8544, 13301,\n",
      "          1825,   170,  5747,   416,   304,   117, 11032,  1719,   792,  3281,\n",
      "           498,   260,  4924,  5874, 13004,   117, 11032,   302,   214,   118,\n",
      "           176,   229,  9530,   508,   298,  1143,   221, 20482, 15690,   259,\n",
      "          7920,   247,   552, 14881, 22281,   122,   259,  1062,  3391,   138,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 90: tensor([[  101,  2678, 22285,  3132,  5793,   146, 11967,  1215,  2684,   320,\n",
      "          3147,  2166,   117, 10415,   214,   834,  5698,   125,  6797,   102]])\n",
      "DEBUG: Tokenized sentence 91: tensor([[  101,  2010,  2354, 22279,   318, 13793,   229, 22280,  3887,   149,\n",
      "          3411,   117,  2678,   147,   136,  7845,   136, 16795,   118,  2036,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 92: tensor([[  101,   146,  4575,   978,  1941, 10941,   117,   123,  1984, 22282,\n",
      "           117,   179,   259, 18720,   124,   173,  7229,   522,   175,   122,\n",
      "           179,  4412, 22278,  4062,   596,   123,   730,  8393,   154,   102]])\n",
      "DEBUG: Tokenized sentence 93: tensor([[ 101, 2010, 5961,  146,  179,  117,  347,  374,  326,  136,  102]])\n",
      "DEBUG: Tokenized sentence 94: tensor([[  101,  1502,   318, 13793,  1977,  4047,  2354, 22279,   179,  2779,\n",
      "          7206,   136,   102]])\n",
      "DEBUG: Tokenized sentence 95: tensor([[  101,   331,  8925,   244,   146, 21115,   176,  2354, 22279,   311,\n",
      "          1270,  5607,   221,  1257,   117,   449, 12044, 20089,   179,   229,\n",
      "         22280,  2822, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 96: tensor([[  101,  3189,  4945,   136,  2779,  2684, 12060,  2185, 22280,   785,\n",
      "           170,  2354, 22279,   117,  2678, 22285,  3132, 17386,   179,  2354,\n",
      "         22279,   122,   222,  9235,  9778,   117,   230, 12252,   122,  2826,\n",
      "         22280,   118,  2036,   325,  2678, 22283,   125,  7163,   259,   532,\n",
      "          3907,  2698,   170, 10502,   860,   266,   102]])\n",
      "DEBUG: Tokenized sentence 97: tensor([[  101, 12402,  1016,   117,   978,   118,  2036, 12585,   260,   223,\n",
      "           128,   122,   870,   468,  2836,   118,   260,   102]])\n",
      "DEBUG: Tokenized sentence 98: tensor([[  101,  2010,  2389,   296,   117,  3449,   117,  1334,  1653, 17598,\n",
      "           118,   146,  1684,   229, 22280,   176,  5436,   170,   171, 22285,\n",
      "         14837,   138,   117, 18047,   136,   102]])\n",
      "DEBUG: Tokenized sentence 99: tensor([[  101,   629, 22280,   146,   644,   492,   240,   180,   329,  7583,\n",
      "         19278,  1968,   222,  2397,   173,   305,  4418,  2535,  2249,   260,\n",
      "          1028,   117,  5562, 22280,   170,  2859,   229, 22280,  3422, 22279,\n",
      "          3963,   320,  3459, 17551,   117,  2798,  2036,   171, 22278,   123,\n",
      "          3049,   304,   117,  2113,   117,   202,  1338,   125, 11169,   117,\n",
      "           529,  7892,  2148,   784,   125, 10502,   860,   266,   117,   122,\n",
      "          2684,   222,   739,  1312,   303,   179,  2354, 22279,  2036,   659,\n",
      "          7343,  8797, 19421,  4838, 21102,   117,   625,   230,  2606,  1941,\n",
      "          1367,   298,  7187,   122, 18720,   123, 13071,   222, 13254,   373,\n",
      "           180,   327,  2169,   117,   122,   271,   176,  3582, 22281,   236,\n",
      "          2987,   173,   302,  4178,   118,  2036,   123,  1956,  1025, 10692,\n",
      "           455,   318, 13793, 14672,   125,   179,   229, 22280,   122,   331,\n",
      "           123,   740,   179,  2354, 22279,   659,   146,  1670, 17714,   247,\n",
      "           117,   449, 14619,   210,   320,  4170,  2249,   325,  3235,  2858,\n",
      "           118,  2036,  2354, 22279,   123,  2606,   117,  1407,   740,  4412,\n",
      "         22278,   125,  1677,   247,   117,   122,   240,  1104,  1017,  1407,\n",
      "           333, 22278,   221,   146,  6754,  2397,   117,   144,  2640,   179,\n",
      "           376,  1941,  2780,   170,   179,   176, 10363, 19356,   140,  1084,\n",
      "           240,  3378,   117,   170,   259,   532,  3907,  2698,   117,   122,\n",
      "          3804,   125,   222,  1695,   125, 21158,   625,  1359,   171,  1312,\n",
      "           303,   122,  3285, 22279,   118,   176,   173,  1105,  3235,   301,\n",
      "           118,   123,   117,  3235,   301,   118,   123,   179,   123,   240,\n",
      "         22278, 13376, 22278,   179,  2798,  2189,  1350,   146,   179,   122,\n",
      "          8911,   122,   785,  7472,   994,   117,  5945,   136,   229, 22280,\n",
      "         21719,  1858,   854,  2028,   285,   271,   123,   125,  1790,   122,\n",
      "          1236, 22279,   221,  4271,   117,   229, 22280,   331,   170,   740,\n",
      "           117,   449,   170,  1485,   260,   179,  2036,  9322,   210, 15702,\n",
      "           180, 14114,  1447,  3891,  1528,   260,   125,  1105,  6628,   117,\n",
      "           179,  1257,   122, 19199,   240,  2318,   366,  3848,   687,   562,\n",
      "          2798, 22033,   203,   303,   171, 22285, 14837,   138,   229, 22280,\n",
      "           176,  5436,   170,   123,  1757,   209,  7186,   122,  3960,   151,\n",
      "           179,  2036,   988, 22280,  1016,   117,  2113,  7206,   347,  3695,\n",
      "           117,  2113,   146, 17386, 12060,   713,   117,  2113,   146, 17386,\n",
      "         22003,   122,  1334,  6812, 22288,   118,   146,   316, 22280, 12043,\n",
      "           246,  2990,   576,   117,   179,   146,  8609,   117,  4173,   557,\n",
      "           118,  2036,   366,   223,   128,   117, 18518,   118,   176,   170,\n",
      "           222, 22000,   125,  7729, 10766,   340,   122, 10968,  2256,   117,\n",
      "          1139,   146,  4575,  2036, 10355,   173,  4410,   408,  3198,   328,\n",
      "          2010,  2389,   252,  2521,  3539,   329,  2354, 22279,   122, 18224,\n",
      "           243,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 100 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.010039177274721063\n",
      " Coesão Score Final: 0.5050195886373605\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'isto e', 'todavia', 'de forma que', 'por conseguinte', 'nem', 'tampouco', 'ou', 'ora', 'quer', 'senao', 'assim como', 'como', 'quanto', 'em vez de', 'antes que', 'assim que', 'porque', 'visto que', 'posto que', 'por outro lado', 'no entanto', 'com efeito', 'acima de tudo', 'principalmente', 'nao so', 'mas tambem', 'tanto', 'quanto', 'se nao', 'de forma que']\n",
      " Número de conectivos: 41\n",
      " Número de sentenças: 100\n",
      "======================\n",
      "Resultados para preprocessado_o_cortico_aluisio_azevedo_cap_2.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'porem', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'isto e', 'todavia', 'de forma que', 'por conseguinte', 'nem', 'tampouco', 'ou', 'ora', 'quer', 'senao', 'assim como', 'como', 'quanto', 'em vez de', 'antes que', 'assim que', 'porque', 'visto que', 'posto que', 'por outro lado', 'no entanto', 'com efeito', 'acima de tudo', 'principalmente', 'nao so', 'mas tambem', 'tanto', 'quanto', 'se nao', 'de forma que'], 'num_conectivos': 41, 'proporcao_conectivos': 0.01, 'similaridade_media': np.float64(1.0), 'num_sentencas': 100}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  1085,  1685,  2856,   180,  1062,   252,   122,   146,   549,\n",
      "           713, 19994,   256,   117, 14094,   117,   229, 22280,   259,  5708,\n",
      "           117,   449,   123,   327,  9211,   292,   125,  6929,   122,  9751,\n",
      "         17132,   591,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   222, 19994, 22282, 20073,   122,  5546,   183,   125,  1977,\n",
      "          4678,  3111, 22288,   125,   230,  3791,  8440,  1118,  2856,   125,\n",
      "         20201,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   271,   179,   176, 16280, 22287,   744,   229,  6738,  3173,\n",
      "           351,   125,   872, 15617, 22278,   260,  1270,  8008,  1402,  7663,\n",
      "           180,   169,  8388,  6594,   180,  2954, 18431,   403,   117,  6784,\n",
      "          2579,   118,   176,   123,  3377,  7406,   124,   122,  1744,   124,\n",
      "           180,   527,  4626, 22278,   117,   179,  2798,   222,  4217,   397,\n",
      "           125, 10428,   261,  7955,   173,  2480,   313,  9193, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   123, 11451, 11348,   285,   117,   179,  4412, 22278,   125,\n",
      "          5693,   574,   538, 16450, 17351, 22281,   117,   222, 11237,   351,\n",
      "           146,   388,   122,  5078,   252,   118,  2036,   222,  5546,   183,\n",
      "           417,   130,   125,  6459, 13793,  4189,  2623,   247,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   260,  8483,   171,  1690, 22280,   117,   188,  1609, 22285,\n",
      "          8156,  4851,   202,  1247,   180, 18853,   122,   173,  1089,  2038,\n",
      "          5580,   401,   423,   360,   215,   117, 15245, 22287,   230,  1877,\n",
      "          8973,  9247, 10557,   252,   122, 12448,   117,  2850,   125,  6039,\n",
      "           565,   303,   143,   125,   730, 16584, 19660,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,  5147,   117,   366,  6929,  2366,   923,  3049,  1149,  9301,\n",
      "           687,   484,   401,   125, 11334,  8362,   228,   118,   176,  9621,\n",
      "         22281,  1151,   289,  3103,   117,  5248,   271,   146,   528, 10575,\n",
      "         22282,   366,  8950, 19417, 13147,  2836,   118,   176, 15618,   293,\n",
      "           240,  1719,   123,   670,   662,   304,   692,   260,  3043, 18775,\n",
      "           138,   123,   964,  3552,   578,   146,   765,   397,  8833,   171,\n",
      "         19935, 10017,   351,   117, 10262,  1053,  1552,   944,   259,   736,\n",
      "          4708,   692,   118,   176,   125, 11471,   221, 11471,   260,  2796,\n",
      "          3724,   117,   259,  9361,   118,  1564,  3655, 11814,   118,   176,\n",
      "         19229, 18467, 22281,   123,  2954,   123,  3113,   285,   329,  1796,\n",
      "           338, 11198,   256,  1941,   117,   122,  1084,  1839,   366,  4103,\n",
      "         16819,  5334,   444,  3508,  1165,   442,   125,   854,  2028, 22281,\n",
      "           179,   744,   229, 22280,  6952, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   202,   915,  5587,  7257, 22282,   179,   176,   547,   256,\n",
      "           117,  5873,   692,   118,   176,  3979,   128,   117, 10195,   125,\n",
      "         12514,   179, 11518,   304,   692,   117,   834,   176,  4945,   582,\n",
      "           117,   416, 22281, 10656,   125,   528, 19356,   128,   117,  8032,\n",
      "           125,  3922,   128,   117,   329, 17738,  4765,   125,  3922,  4242,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   125,  1089, 15050,  8625,   228,  2459,   179, 16819, 15231,\n",
      "         20973,   329,  1796,   117,   229,  8130,   117,   123,  1956,   247,\n",
      "           266,   171,  5848,   421,   247,   117,   122,   259,  7406,   444,\n",
      "           117,   123,  3098,  2028,   298, 16551,   117,  7104,  2349,   692,\n",
      "           118,   176,  1315,   474,  3514,   117,  1632,   514,  9420,   118,\n",
      "           176,   123,  3377,   940,   171,   644,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   180, 22283,   123,  1695,   117,   173,  1359,   366,  7395,\n",
      "           138,   495,   222,  1757,   309,  9437, 22287,  6478,   230, 11621,\n",
      "         10780, 13793, 17856, 18711,   375,   125, 11024,   122,  1154,  2890,\n",
      "         22281,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,  7226,   117,  4230,   736,   117, 11348,   692,   123,  1354,\n",
      "           117, 20990,  9468,   117, 15702,   171, 13831,   125,  6205, 22278,\n",
      "           179,  3235, 22282,   322,   180,  2847,   125,  7226,  1685,  1877,\n",
      "           793,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   146,  1690, 22280, 15718,  2836,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   260,  2459, 11736, 22287,  1941, 21254,   260,  8625,   138,\n",
      "           420,   260,   144,  3706,   221,   229, 22280,   260,  3848,  2430,\n",
      "          1413,   118,   176,   118,  7707,   123,   374,  8849,   285,  1444,\n",
      "         15182,   298,  4332,   942,   122,   171, 13227,   303,   117,   179,\n",
      "          2859, 12989,   923,   117, 16322,  2986,   146, 10881,  1364,   221,\n",
      "           146,  2979,   171,   504,   303,   259,  2217,   117,  3636,   229,\n",
      "         22280,   176,  5811,   692,   173,   229, 22280,  3848,  2430,   146,\n",
      "           423,   117,   320,   598,   342,  3285,   923,   123,  3049,   304,\n",
      "          1004, 15702,   180,  6205, 22278,   122, 15518,  2127, 14533,   170,\n",
      "           344,   304,   260,  9427,   138,   122,   260, 17897, 22281,   117,\n",
      "          5148,   348,   122,   968,  4212,   598,   260,  1877,   486,   180,\n",
      "           223, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,   260,  6929,   366,  4840, 12065, 22281,   229, 22280, 12604,\n",
      "           375,   692,   117,   495,   222,  8925,   122, 16325,   125,  1078,\n",
      "         16423, 22279,   117,   222,  4270,   122,  5197,   834,  1510,  2635,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   229, 22280,   176,  3174, 18444,  1084,  1839,   122, 16819,\n",
      "           744,  8650,  2537,   260, 14790,   138,   291,   260,  8625,   138,\n",
      "           260,   854,  2028, 22281,   229, 22280,   176, 21280,   320,  1223,\n",
      "           125,  1084,   925,   117, 19537,  2034,   692,   118,   176,  1369,\n",
      "           653,   117,   202,   853, 18983,   162,   298,  8001,   117,   240,\n",
      "           125,   839,   180,   418, 15976,   291,   202, 19726,   234,   366,\n",
      "          3428,   470,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   146,  7257, 22282,  1664,   351,   117, 21057,   348,   118,\n",
      "           176,   146,  1757,   309,  9437, 22287,   125,   944,   259,  1564,\n",
      "         11534,  2836,   118,   176,  1941,   176,   229, 22280,  5873,   692,\n",
      "         12514,  9694,   138,   117,   449,   222,   331,  1315,   286, 16949,\n",
      "           179, 16386,   151,  1364,   146,   549,   713,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   662,   304,   692,   123,  1434, 18937,   229,  5304,  5925,\n",
      "          2901,   692,   118,   176,  5489, 22281,   293,   143,   122,   398,\n",
      "          7173, 22281,  8362,   228,   118,   176,  6742,  1187,  8757,   122,\n",
      "          1174,  1557,  1941,   176,   229, 22280, 18402,   117,  9247,  5723,\n",
      "           118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101, 16280,   118,   176,  6621,  1718,  2349,   304, 22280, 13402,\n",
      "           789, 22278,   117,  6621,   866,   266, 21853,  1337,   125,  4924,\n",
      "          5874, 13004,   179, 15478,   546,   259,  1143,  8773,  2935,   229,\n",
      "         19068, 22278, 13305,   122, 12874,   403,   180,  1069,   117,   146,\n",
      "         15537,  6032,   125,  9205,   117,   123,  8727, 11074, 12458,   304,\n",
      "         22280,   125,  9815, 22282,   498,   123,  2480,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,   180,  4303,   180,  5304,   179, 10348,   221,   146,   549,\n",
      "           713, 16420,   122, 16819,   271,   928,  3670,  2636, 18937,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,   924,  9751,   171, 15796,   404, 19914,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,  4169,  1532,   123,   847,  1382,   124,   117,   179,   176,\n",
      "          1598, 13808,   123,   662,   934,   123, 15703,   180,  1105,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,  2010,   149,   252,   623, 12674,  9247,   654,   740,   221,\n",
      "          3378,   117,   123,   629,   830, 13133,   222, 12617,   125,  9317,\n",
      "           176,  2354, 22279,   376,  2803,   830, 22305,   125, 10985,  1790,\n",
      "           117,  3985, 22278,   229,  4303,   117, 16143,   136,   123,   447,\n",
      "          4275, 22282,  4751,  2044, 14619,   210,   117, 13586, 17598, 12898,\n",
      "         22278,   123,  1354,  6592,   252,   240,   420,   146, 13227,   303,\n",
      "           122,   146, 20462,   180,  1124,  7137,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,   146,  1852,   458,  3033,   229,   418, 15976,   117,   170,\n",
      "           123,   327,   739,  2992,  8849,   123,  3049,   304,   122,   146,\n",
      "           347,  6465,   125, 16341, 10371, 15702,   171,  4332,   303,   117,\n",
      "           122,   262,  9994,   159,   173,  1423,   171,  4286,   247,   117,\n",
      "           123,  2521,   298,   958,  2118,   143,   117,  1074, 22087,   243,\n",
      "           123, 12588,  7485,   124,   498,   146,  2840,   735, 22279,   179,\n",
      "           368,  3748,   203, 20466,   246,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   173,  5255,  1011, 17477,   240,   230, 18637,   125,  9349,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,   260,   854,  2028, 22281,  4183, 14533,   118,   202,   117,\n",
      "           122,   117,   123,  3606,   304, 22280,   179,  1078,  2606,   291,\n",
      "          1078,  2397, 16653,   146,   372, 22280,   117, 10907,   256,   221,\n",
      "          1105,   170,   860, 12631,  1196,   598,   146, 11979,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,   230, 13348, 22278,   117,  2590,   240,   222,  5294,  2964,\n",
      "           157,  3165,   285,  1196,   117,  8544,   117,   964,  3552,  1552,\n",
      "         12448,   246,   146,   347,  5334,  2848,   268,   117,   125,  4303,\n",
      "           173,  4303,   117, 13375,   285,   240,   222,  2397, 21617,   125,\n",
      "          4372,  5822,   155,   125, 14121,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   146,  1757,   309,  9437, 22287, 19288,   320,   347,  1178,\n",
      "          7382,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   123,  7875,   125, 11282, 18836,   117,  1369,   653,   180,\n",
      "         13219,  2028,   117,   662,  1353,   123,  3212,   117,  2787,  7447,\n",
      "           348,   146,  1923,  1259,   170,   146,   347,   388,  6774, 10537,\n",
      "          8468,   125,   223, 11198,   123,  8886,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,   260,  9393,  2684,   123,  5304,  9487,   923,   118,   176,\n",
      "           117, 12963,   118,   176,  1362,   792,  1817, 22282,  4060,   125,\n",
      "           928,  1185,   458,   502,  1051,   243,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,  2535,   117,   202,  1247,   366,  7395,   138,   305,   508,\n",
      "           692,   118,   176,  4840,   138,   125,   944,   259, 18190,   128,\n",
      "           117,   498, 22281,   375,   557,   260,   125, 18691,  2615, 22279,\n",
      "           170,   222,  4332,   303,   125,  4561,   173,  5530, 16280,   118,\n",
      "           176,   146,   338,   269,  4765,   180,  6205, 22278, 15356,   229,\n",
      "         14121,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,  1450, 11348, 17124, 22281, 16386,   923,  1941,   260,   675,\n",
      "           964,  1343,  1028,   860, 12396,   538, 16450, 17351, 22281,   123,\n",
      "         11451,   179,  4412, 22278,   125,  3848,   268,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[ 101,  905,  151,  256,  146, 1223,  102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101, 11103,   923,   366,  6742, 20666, 22281,   259, 22229,   128,\n",
      "          4966,   122,   260,  1261,  4242,  7649,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,   222,  3883,   304, 22280,   125, 14575,  3033,   170,   739,\n",
      "          1923,  1259,   125, 12446,   229,  5028,   117,  4431,   125,   230,\n",
      "           313, 10441,   159,   124,  7672, 13808,   313, 12371,   619,   285,\n",
      "           423,  3883, 20635,   598,   146,  9066,   157,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   122,   117,   726,   785,   596,   117,  1191,   118,   176,\n",
      "           222,  2541,  2158,   125, 11641,   143,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101, 11502,   259, 20860, 22281,   125,  7714, 12837,   304,   122,\n",
      "           736,   125,  5849,   138,   122,  8446,   125,  1151, 22283,   331,\n",
      "           229, 22280, 16819,  3428,   477,   585,   117,  2113,  1021,  1615,\n",
      "          3428,   470,   202,   549,   713,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,  8198,   259,  1315,   474,   128,   449, 18001,   143,   117,\n",
      "           170,   260,   675,  4840,   138,   125,  6799,  8572,  2430,   151,\n",
      "           117,   170,   260,   675, 17730,   125,   822,   272,  1044,   122,\n",
      "          4674,   125, 10809,   122,   170,   146,   347, 14664,   125,   329,\n",
      "           934,  6684,   122,  5334, 19334,  1402,   117,   125, 14121,   118,\n",
      "           125,   118, 13185, 21510, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101,  1078, 12447,   947,   978,   146,   347,  2277,  1199,   125,\n",
      "          2533,   339,   159,   117, 11054,   118,   176,   146,  2397,   366,\n",
      "          9344,  6436,   842,   117,   170,   260,  2992,  8849, 22281,   171,\n",
      "         11371, 10090, 21794, 22281,   117,   123,  7800,   125, 17772,  2028,\n",
      "           117,   125,   222, 16341,   179,   368, 16555,   320, 20462,   102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,  3874,   325,   262,  8911,   171,   179,   146,   347,   652,\n",
      "           866,  2668,   268, 15777,  9238,   122,   866,  1441,   221, 13431,\n",
      "           210,  2044,   117,   271,   240, 16473, 22280,   117,   230,  5223,\n",
      "          5402,   125, 20543,   117,   179,  8198, 17586,  9365, 22282,   118,\n",
      "           176,  2461,   170,   739,  8043,   292,   117,   577,  2746,   118,\n",
      "           176,   118,  2036,   529, 11351,  3456,   421,  4851,   122, 14689,\n",
      "           348, 10262,  1436,  2411,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,   146,  9344,  6436, 16510,   259,  4415,  5723,   170,   146,\n",
      "           766,   117,  1139,  5133,   151,   146,   347, 11371,   123,  4303,\n",
      "           366,   504,  4242,   117,   449,   259,  7395,  3148,   128,   229,\n",
      "         22280,   273, 20941,   122, 14372, 22287,   123, 21822,  1516, 22282,\n",
      "           117,  5646,  7831,   259,  2992,  7485,   128,   179,   146,  2397,\n",
      "          8170,  3514,   316,   321,   256,  3002, 15724,   320,   958,  2118,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,   221,   792,   118,   176,  3039,   240,   222, 16423, 22279,\n",
      "           298,  3229,  3932,   495,  1395, 17551, 20949,   221,  1004,  5533,\n",
      "           222,  5078,  2895,   125,  9344,  6436,   842,   117,   498,   146,\n",
      "           615,   176,  8163, 11541,  2044,   117,   712,  5995,   128,   117,\n",
      "           146,   939,   298,  1449,  2668,   268,   143,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,   123,   681,   179,   176,   429,   123, 11348, 22282,   262,\n",
      "           123,   447,  1403,   124,   117,   240, 20025,   123,  1112, 16803,\n",
      "           324, 22354,   117,  3649, 18781, 22305,   117, 10420, 20193,   117,\n",
      "          5995,  1409, 11753, 20225,   122, 15618,  1409,   117,  7889, 22278,\n",
      "           125,  6032,   171,  2244,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,   978,   924,  8447,   117,   230, 12222,   122, 14136,   171,\n",
      "          4170,   117,  9480,   366, 19659,   117,   123,  1977,   331, 21612,\n",
      "           123,  1112,   366, 19659, 22354,   122,  1858,   171, 22285, 14837,\n",
      "         22278,   744,   117,   123,   872,   514, 22285,   117,   122,   325,\n",
      "           222,  1417,   117,   146,   762,   128,  5321,   117,  9778,  6751,\n",
      "           298,   644,  3471,   117,   179,  9247,  5723,  1971,   291,  1407,\n",
      "           179,   123,   223, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,   123,   366, 19659, 19575,   173,   327,   504,   508,   123,\n",
      "           670,   117,   449,  1719,   123, 20027,  1967,   256,   202,   549,\n",
      "           713,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101, 16241,  1537, 22287,  1369,  9679,   320,  4863,   176,   123,\n",
      "         16803,   324,   495,  4970,   256,   291,   273, 10214,   285,   259,\n",
      "          2292,   229, 22280,   176,  9821, 22287,  7226,   170,   259,   736,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101,   123,   366, 19659,   117,  1141,   117, 18601, 22287,   179,\n",
      "          1796, 12222,   122,   179,  7453,   124,   146,  4170,   221,  3285,\n",
      "           140,   118,   176,   170,   222,  2397,   171,  1847,   523,   122,\n",
      "           179,   860,   117, 20813,   214,   118,   176,   221,   123,  2480,\n",
      "           122,   229, 22280, 19608,   969,   154,   118,  1084,   320,  1950,\n",
      "         10490,   157,   117,  4314, 22278,   146, 17796,   173,   347,  1247,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[ 101, 2471, 4698,  122, 1685,  481,  102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[  101,   872,   514, 22285, 17668,   102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101,   730,   945,   285,   117,  6251,  8149,   324,   122,  2124,\n",
      "           117,   170,   230,   258, 11837,   508,   125, 17275,   180,   327,\n",
      "          2477,  4602,  1076,   117, 15832,   214,   271,  2787,  4838, 22278,\n",
      "           240,   420,   259, 17084,   298, 13254,   143,   179,   123, 11666,\n",
      "           834,   333,   221,  6759,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101,  2787,   703,  2836,  1004,   122,  9679,  1434, 11451,  7352,\n",
      "           125,  2397,   170,  5747, 15156,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101,   320,  1341,   180,   447,  1403,   124,   262,  7201,   118,\n",
      "           176,   123,   327,   964,   324,   123,   527,  3209,   154,  7714,\n",
      "           118,  3848, 22279,   117,  2509,   117,  7352,   117,  2606,   125,\n",
      "          1202,  2037,   158,   130,   117,   222,  1124, 14273,   125,  9836,\n",
      "           481,   117, 11793,   125,   661,  1144,   117,   337,   701,   713,\n",
      "           117,   125,   739,  4351,   339,   272,  7967,   117,   179,  1391,\n",
      "          1684,  3240, 21102,   201,   122,   222, 13702,   125, 14790,   138,\n",
      "         11563,  2787,   703,   401,   122, 11967, 22280,   143,  1743,   459,\n",
      "           229,  5546,   285,   117,   625,  1011,   125,  1312,   303,   102]])\n",
      "DEBUG: Tokenized sentence 50: tensor([[  101, 14619,   210,  2365,  2292,   117,   449,   744,  4385,   117,\n",
      "           222,   298,  1647,   117,   123,   717,   741,   117,  9584,   229,\n",
      "           651,   170,   123,  5836, 11324,   179,   176, 12771,  2836,  3914,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[  101,   418,  5836, 11324,   495,   230,   144,  8351,   125,  7187,\n",
      "           592,   118,  7911,   221,  5530,   117,   123,   447,  8723, 22279,\n",
      "           117,   170,   425,   825,   229,   651,   102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[ 101, 5160, 2346,  351, 4871,  102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[  101,  1202,  2037,   158,   130,   117,   173,  1105,   117,   123,\n",
      "          5314,   125, 21158,   117,   538,   532,  4004,  3152,   122,   229,\n",
      "           327,  7924,  1950, 17789,  3632,   285,   117,   495,   785,  1690,\n",
      "         22280,   170,   259,  9288,   125,   418, 15976,   117, 10415,   256,\n",
      "           117, 18419,   122,  5911,   304,   256,   117,   449, 19876,  4212,\n",
      "           146,  9470,   117,   568,   371,   214,   146,  4351,   339,   272,\n",
      "           122,  4276,  3638,   214,   123,   327,  1224, 10224,   154,   117,\n",
      "           170,   179,   978,   146, 12215,   125, 13526,   193,   702,   260,\n",
      "         14790,   138,   125,   235,  3198,   117, 16241,  1537, 22287,   325,\n",
      "          2036,  1413,   259, 12141,   122,   318, 13793,   123,   944, 18402,\n",
      "          3689, 22280,   122,   240,  5530,   171, 20462,   102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101,   123,  2606,   117,   123,  1977,   368,   331, 10348,  5023,\n",
      "           625,   229, 22280,  1011,  5546,  6859,   117,   495,   125,   230,\n",
      "          3578,   687,   292, 21605,  4456, 22290,   202,   549,   713,   117,\n",
      "          3578,   687,   292,   834,  1707,   373,   117,  2113,  8940,   180,\n",
      "          6738,  3173,   351,   171,   347,  2829,   310,   122,   229, 22280,\n",
      "           171, 11717,   342,   171,   347,  1354,   367,   102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101,  1982,  3914,   429,   118,   176,   123,  3212,   123,   447,\n",
      "           451,  9147,   117,  2606,   125,   222,  1718,  1339,  1565,  5782,\n",
      "           300,   117,  3649,  3113,   122,   331,  2382,   117,   125,  7714,\n",
      "         22281,  3072, 22281,   117,   170,   230,  7989,  1749,   415,   125,\n",
      "          1301,  1721,   420,   260,   675, 13511,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101, 16246,   118,   176,   123,  4735, 22278,   117,   230,  6365,\n",
      "         15439,  7492,   117,  1423,  6220,   154,   117,   123,  1977, 16598,\n",
      "           692,   944,  1676,  9429, 22281,   125,   179,   331,   740,  1598,\n",
      "         13808,   221,  7489,  2964,  2317, 15266,  1440,   138,   122, 16633,\n",
      "         14594, 22281,   240,  1423,   125,   184,  6471,   122, 18190, 15035,\n",
      "         22281,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[  101,   495,  5762,  2996, 22278,   117, 15618,   375,   117, 12448,\n",
      "           117,   170,  5708,   273,   256,   364,   442,   117, 12141, 17584,\n",
      "         22281,   123,  9324,   252,   117,  6335,  8993,   117,   271, 12141,\n",
      "           125,   329, 22280,   117, 13841, 21990,   128,   117,  3235, 22282,\n",
      "          9539, 22281,   122,   744,  2582,   850,  2440,   180,  2169,   102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[  101, 21612,   118,  2036,  1112,  5782,  2037, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[  101,   700, 16246, 22287,   118,   176,   123,   528, 16527,   122,\n",
      "           325,   123,   327,  2267, 12252,   624,   102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101,   123,   681,   117,  1124,  7137,  2557,   117,   785,  1467,\n",
      "           122,  3791,   251,   173, 14155,  1080,   123,   327,  1105,  1011,\n",
      "          1684,   222,   328,   366, 14269, 11348,  1495,   102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101,   173,  2036,   305,  1051,   214,   146, 11775,  6590,  5078,\n",
      "           252,   118,   176,  2044,   123,  1632, 10656,   117,   123,  8525,\n",
      "          6070,  1154,  2042,  2507,   117,   122,   117,   625,   123, 16001,\n",
      "           495,   739,   117,  4063,   151,   123, 10467,   222,  3568,   272,\n",
      "           125,  6205, 22278,   122,  7496,  2127,  2836,   118,   146,   170,\n",
      "         18928, 22278,   423,  1690, 22280,   180,  4767,   102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101,   123,  2267,   978,  8184,   481,   117,   123,  6614,   125,\n",
      "           222,  1623,  1460,  8833,   117, 10786,   942,  3260,  2301,   117,\n",
      "         22003, 22281, 12141,   117,  5708, 19366,  1408,   128,   125, 12361,\n",
      "           304,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[  101,  1719,   740,  1011,   123, 10381,  2397,   117,   449, 12595,\n",
      "           256,   744,   123,   327,  2477,  4602,  1076,   122,   229, 22280,\n",
      "          9262,   151,   117,  2798,   123,   223, 22280,   125,  4023,  7148,\n",
      "           117,   712,   577,  1289,   125,  4141, 13793,  2415, 22280,   117,\n",
      "           179,   123, 17090,   305,  1051, 22282,   123,  1073,   303,   125,\n",
      "          4296, 21386, 22280,   143,   229,  3659,   122,   202,  5274,   366,\n",
      "         18937,   179, 12252,   624,  5057, 14316,   123,  5304,   102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[  101,   700,  1413,   118,   176,   123,  7492,   847, 12764,   178,\n",
      "           117,  3413,   122,   117, 10502,   847, 12764,   178,   117,  2113,\n",
      "          1369,   229,   418, 15976,  7707, 12119, 14533,   944,  5288,   996,\n",
      "           304, 22280,   117, 15627,   251,  1676,   675, 11402, 10278,   125,\n",
      "          2760,   179,  1941,  1023,  3896,   230,  6754,  2606,  9652,   125,\n",
      "           273,  1289,   382,   102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[  101,  1796, 12222,   170,   146,  8517,   125,   230,  1105,   125,\n",
      "          1690,  7817, 22281,   117,   179, 13935,   122,  7231, 12334,   118,\n",
      "           176,   117,  4513,   118,  2036,   230,  2267,   785,   171,   265,\n",
      "           508,   122, 14578,   117,   123,  1977,   847, 12764,   178, 19978,\n",
      "           203,  2745,   221,  6974, 22282,   117,  3951,   118,  2036,  5686,\n",
      "          2684,   125,  1546,   143,   102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101,   978,   230,  1354, 13376,  3173,   154,   125,  7492,  3649,\n",
      "          7427,   154,   117,   179,  1941,   262, 13717,   285,   117,  1151,\n",
      "          2041,  5949,  3848,   143,   125,   412,  2028, 22281,   570,  6021,\n",
      "         14004, 22281,   117,   179,  2036, 15231,   923,   298, 18694,   180,\n",
      "          9463,   271,   629, 19161, 22281, 16026, 22281, 19873,  7769,   202,\n",
      "           179,  1391,   117,  5708, 17291,   683,   117,  1684,  5334,  6031,\n",
      "         22281,  2787,  8551,   442,  1676,  1877,   269,  4323,   102]])\n",
      "DEBUG: Tokenized sentence 67: tensor([[  101, 14537,  2836,   173, 20674, 22281,   498,   260,  4137,   146,\n",
      "          3240, 22281,   293, 10881,  9247, 10557,   268,   877,   487,   125,\n",
      "          2389, 22279, 22280,   125, 13501,   214,   138, 20460,   102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[  101,   625,  8625, 22278,   123,  4768,  5078,   252,   222,  2337,\n",
      "           140,   300, 12413,   125, 17678, 13305,   117,  7093,  4238, 11538,\n",
      "           285,   117,  3561,  8625, 22278,   229, 22280,  5057,  1315,  1557,\n",
      "           117,   122,   222,  3043,  1531,  4619, 14838,   179,  2036, 10348,\n",
      "           123,  1364,   146,  1831,   222, 18190, 22280,  8685, 11540, 22290,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101,   180,   327, 19023, 18550,   331,  2036,  4412, 22278,   230,\n",
      "          8097,   125,  6372, 22279,   125,  2987,   117,   229,   615,   123,\n",
      "         10152,   715,   415, 17704, 13779,  3502, 22279,  2836,  2535,   117,\n",
      "          4217,   364,   214,   123,  1078, 13779,  1378,   102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[  101,   123,  2267,   495,   123, 12252,   171,   549,   713,   102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101, 21612,   118,  2036,   302,   282,   508,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101, 19543,   117,  4460,   179,  9525,  1694,   122,  8089,  1337,\n",
      "           320,   169, 14116,  2009,  7406,   124,   117,   785,  1877,   328,\n",
      "           117,   170,  7226, 12400,   125,  9586,   125,  3264, 20027,   102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101,   123,   223, 22279,   229, 22280,  2036, 12122, 11348, 22282,\n",
      "           117,  2798,  2787,   703,   159,   117,   653,  2113,   146,  6815,\n",
      "         22280,   123,  4535,   364,  9434,   246,   102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101,   978,   146,   347, 20743,   117,   146,  4141, 13793,   180,\n",
      "          4203,   117,   390,   303,   171,  1847,   523,   117, 14031,   171,\n",
      "          9019, 13793,   122,   298,  8229,   117,   170,   785,  3753,   117,\n",
      "           122,   179,   123,  2251,  5630,   122, 18574,  1065,  1283,   194,\n",
      "           343,   449, 10502,   847, 12764,   178,   229, 22280,  4750,   179,\n",
      "           146,  2982,   176, 19636,  1941,   102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[  101,   122,   179,   302,   282,   508,   117,   438,  2746,  1369,\n",
      "           138,   954, 13417,   481,   117,   229, 22280,   978,   744, 12659,\n",
      "           123,  3402,   146,  4793,   265, 22280, 14399,   180,   879,   604,\n",
      "          1076,   117,  2440,   171,  1757,   821,   180,  7492,   122,   298,\n",
      "         19978,   501,   179,   418,  5057,   221, 10241,   123,  3979,   304,\n",
      "           260,   543,  1360,   303,   143,   171,  6815, 22280,   122,   229,\n",
      "         22280,  3207, 22282,   123,  2267,   146,  2892,   273,   415, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101,   202,  1325,   117,   144,  7647,  4566,  2982, 10090,   151,\n",
      "           123, 15685,   125,  4903,   117,  2113,   146,  4203,   117,  1004,\n",
      "         12531,   271,   176, 16995,   173,  1105,   125,   222,  7392,   347,\n",
      "           117,   125,  1977,   325,  1373,  1021,   125,   333, 17796,   117,\n",
      "          1744, 16959,   256,   117,  2044,   179,  5896, 22281,   236,   125,\n",
      "          1177,   117,   398,   569, 22283,   118, 10497,   320,   347, 19262,\n",
      "         16146, 22280,  1979,   102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[  101,   123,  6754,  7492, 15834,   256,   118,   176,   170,   146,\n",
      "          2099,   122, 21315,   123,  4023,   117,  1485,   260, 13674,   117,\n",
      "          1075,   125, 18165,   117,   179,   260, 16889, 22281,   236,   122,\n",
      "          7940,  4886,   123,  2267,   230,   416,   304,   316, 22280,  2281,\n",
      "           179,   368,  5057,   117,   834,  7523,   304, 22280,   125,  9607,\n",
      "           951,   117,   123,  2523,   138, 10173,  6943, 22281,  1021,   423,\n",
      "          1147,   449,   117,   123, 21736,   125,  3846, 20045, 22280,   117,\n",
      "           240,  5664,  3963,  1014,  1069,  2380, 15500,   322,   179,   123,\n",
      "           327,  3113,  4103,   236,  1075,   125,  1112,   333,  2606, 22354,\n",
      "           117,   271, 10355,   740,   102]])\n",
      "DEBUG: Tokenized sentence 78: tensor([[  101,   122,  1112,   179,  5308, 22281,  6867,  1084,  5961,   146,\n",
      "         14914,   117,  3486,  1399,   179,   229, 22280,   495,   693,   403,\n",
      "           117,  2798,   978, 13071,   117,  2822,  2397,   123,   230,   390,\n",
      "           304,   179,   744,   229, 22280,  1796,  3248,   285,  1676,  5080,\n",
      "           229, 22280,  1075,   873,   118,  1084,   969,  7700,  1719,   123,\n",
      "          1069,   122,  4412,   210,  4903,  1273, 10742,   221,  1684,  6086,\n",
      "         21929,   180,   418, 15976, 22354,  1084,   202,   549,   713,  2072,\n",
      "           944,   123,   332,  1014,  4131, 22278,   229, 22280,   495, 11021,\n",
      "           221, 16241,  1537, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 79: tensor([[  101,   122,   229, 22280,   176,  9882,   222,   644,   179,   229,\n",
      "         22280,  6185,  1557,  6867,   924,   122,  1510, 22281,  1176,   123,\n",
      "          7492,   170,  3769, 16264,  2010,   318, 13793,   136,  1941,  3429,\n",
      "           136,  2010,   240,   179,   229, 22280,  2137,   259, 17052, 22281,\n",
      "           125,   528,   136,  2010,   240,   179,   229, 22280,  3196,  1342,\n",
      "          6815, 22280,   136,  2010,  2779,   117,   176,  2589,   123, 17704,\n",
      "           117,  1105,   256,   118,   259,  1016,   653,   123,  7492,  3989,\n",
      "          1399,  4111,   179,   123, 15685,   229, 22280,   176,  3283,   371,\n",
      "           221,   740,   102]])\n",
      "DEBUG: Tokenized sentence 80: tensor([[ 101,  122, 4217,  364,  256,  398, 4861,  285,  102]])\n",
      "DEBUG: Tokenized sentence 81: tensor([[  101,   625,   146,  4203,  1183,   351,   700,   180,   327, 18237,\n",
      "           304, 22280,   221, 10964,   123, 14308,   117,   259,  6310,   180,\n",
      "           418, 15976,  7104,  2349,   692,   118,   202,   173, 22243, 22280,\n",
      "           170,   222,  3953,   293,   388,   125, 10497,  2896,   122, 13779,\n",
      "          1200,   117, 20045,   308,   316,   218,  1121,   240,  6086, 11314,\n",
      "           657,   714,   117,   598,   146,   615,   229, 22280,  1201,   923,\n",
      "          2798,   653,   260,  9429, 22281,   180,  5782,  2037,   102]])\n",
      "DEBUG: Tokenized sentence 82: tensor([[ 101,  302,  282,  508,  495,  785, 3189,  328,  240, 1719, 7583, 9349,\n",
      "          102]])\n",
      "DEBUG: Tokenized sentence 83: tensor([[  101,   495,  1977,  2036,  1633,   619,   260,  6536,  1977,   173,\n",
      "          1250,  5057,   146,   577, 22290,   221,   260, 11348, 17124, 22281,\n",
      "          1977,  4551,   256,   260, 11169,  1977, 19449, 22278,   146,  1955,\n",
      "           221,   259,   179,  8204,  4368,  9226,   102]])\n",
      "DEBUG: Tokenized sentence 84: tensor([[  101,   466,   852,   692,   118,   229,   170,   785,  3953,   122,\n",
      "         21280,   118,  2036,  4400,   117,   146,   179,  2036, 12122,  4863,\n",
      "         13702, 15404,   102]])\n",
      "DEBUG: Tokenized sentence 85: tensor([[  101,  6952,   256,  1684,   125, 11967,  4242,   291, 14230,  5321,\n",
      "         22281,   170,  5899, 22281,   125,   549,   117,   347, 12413,   125,\n",
      "          1224,   343,  2787,   703,   201,   978,   260,   675,  4141,   151,\n",
      "          7248, 22281,   221,  5197,   123,  4768,   117,   122,   117,   712,\n",
      "         18433,   117,  1977,   123,  1377, 22281,   236,   123, 13544,   229,\n",
      "          2567,   125,   629, 22280,  4141, 13793,  3985,   384,   117,   229,\n",
      "         22280,  1467,  4051,   125, 18224, 22282,   179,   740, 19575,   173,\n",
      "           549,   713,   102]])\n",
      "DEBUG: Tokenized sentence 86: tensor([[  101,  3030, 15736,   123,   450, 22278,   366,  2796, 11348, 17124,\n",
      "         22281,   117,   146,   313, 11284, 22280,   117,   222, 10738,  4613,\n",
      "          1817,   243,   117, 13141,   117,   549,   125, 17700,   339, 11899,\n",
      "           286,   122,   170,   222, 11753,   994, 17291,   268,   117, 17579,\n",
      "          2836,   243,   122,  6754,   117,   179,  2036, 11314, 22278,   117,\n",
      "          1532,   331,  1999,   117,  2684,   320, 13227, 11661,   268,  3848,\n",
      "         22279,   122,  2135, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 87: tensor([[  101,   495, 11348, 17892,   122,  9584,  1684,   420,   260,  2459,\n",
      "           117,   170,  1977,  1941,  1011,   316, 22280,  8043,  2303,   179,\n",
      "          2859,   146, 14915, 22287,   271,   123,   230,  2760,   171,   653,\n",
      "          5497,   173,   543,   194,   304,  2461, 18402, 22287,   125,  4486,\n",
      "           179,   229, 22280,  5924,   923,   173,   543,   194,   304,   125,\n",
      "          1342,  2397,  9144,   118,   202,  2684, 14809,  9238,   298,   532,\n",
      "          3165,   143,   122,   366,   675,   851, 14266, 22281,   117,   170,\n",
      "           230,  6251, 17932, 22278,   179,   146,   229, 22280,  6838,   256,\n",
      "           117,  2798,   271,   619,   102]])\n",
      "DEBUG: Tokenized sentence 88: tensor([[  101,   625,   222,  4672, 13672,   256,   291,   924, 19763,   176,\n",
      "          3623,   692,   117,   495,  1684,   313, 11284, 22280,  1977, 14915,\n",
      "           125, 16048,   151,   118, 15887,   117,   294,   890,   214,   260,\n",
      "          2459,   123,  5997,   151,   102]])\n",
      "DEBUG: Tokenized sentence 89: tensor([[  101,  4654,   555, 12771,  2836,   118,   176,   125, 15210, 22282,\n",
      "           146,   577, 22290,   366,  8229,   117,   240,  3330,  1758,   449,\n",
      "           230,   576,   117,  6738,   123,   230,  6527, 22278,   125,  5090,\n",
      "           117,  7320,   118,  2036,  1084,   117, 16241,  1537, 22287,  9679,\n",
      "           240,   179,   117,   230,   623, 12051,   125,  4102,   128,   117,\n",
      "           122,   146,  6754,   118,   644,   492,  3436,   203,   318, 13793,\n",
      "           117,   420,  1084, 20739,   122,  7078,   942,   117,   179,  2364,\n",
      "           325,   176, 20241,  7518,   125,  3859,   259,   577,   145,   102]])\n",
      "DEBUG: Tokenized sentence 90: tensor([[  101,   122,   180, 22283,   173,  4271,   117,   170,  3901,   117,\n",
      "           229, 22280,  3456,   285,   256,   259,   766,  6199, 22281,   171,\n",
      "           549,   713,   117,   123,   229, 22280,   333,   538,  1564,   125,\n",
      "          7095,   117,   173,   179,  8544,   117, 12413,   125,  4654,   934,\n",
      "           387,   117,  7282,   159,   123,  1373,  1676,  5207,   122,   123,\n",
      "          2954,  4654,   934,   538, 21145, 22281,   298, 17863,   102]])\n",
      "DEBUG: Tokenized sentence 91: tensor([[  101,   978,  5907,  1568,  2037, 22280,   240,  1966,  7465, 12411,\n",
      "          1442, 21206,  3495,   726,   146,   622,   221,  3598,   578,  1364,\n",
      "           170,   123,   449,  5105,   285,   102]])\n",
      "DEBUG: Tokenized sentence 92: tensor([[  101,   122, 16241,  1537, 22287,   146,  7339,   117, 11388,   291,\n",
      "           644,   125,  2767,   117, 11348,   214,   291, 12604, 22087,   243,\n",
      "           117,   179,   229, 22280, 10606,   170,   123,   327, 14790, 22278,\n",
      "          7352,  2787,   703,   251,   117,   123,   327,  7924,  1743,   321,\n",
      "           117,   222, 16936,   303,   320, 13227,   303,   117,   122,   117,\n",
      "          8650,   825,   123, 11214, 22278,   117,   222,  5429,   162,   179,\n",
      "          2036, 11314, 22278,   498,   260, 11351,   271,   230,  8625, 22278,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 93: tensor([[  101,   229, 22280, 19016,   256,   117,   229, 22280,  5167,   151,\n",
      "          5791,   382,   122, 16555,  1684,   260,   223,   128, 13605,   401,\n",
      "           122,   222,   649,   102]])\n",
      "DEBUG: Tokenized sentence 94: tensor([[  101,  6621,  1062,   252, 14748, 22278,   118,   176,   744,   222,\n",
      "          1695,   325, 18253, 14960,   243,   179,   171, 12215,   117,  2113,\n",
      "          3852, 22278,  3002,   123,  2954,   102]])\n",
      "DEBUG: Tokenized sentence 95: tensor([[  101,   123,  7492,   847, 12764,   178,   117,   179,  2036,  9086,\n",
      "           320,  1341,  9657,   117,  8362,   214,   118,   146,  4217,   364,\n",
      "         22282,   170,   601,   544,   340,   117, 16795,   118,  2036,   146,\n",
      "           179,   978,   102]])\n",
      "DEBUG: Tokenized sentence 96: tensor([[  101,   123, 22296,  5747,  3848,  2993,   125,  1831,   122,   230,\n",
      "          8993,   285,   171, 16026,   179,   146,   229, 22280,  5308,   256,\n",
      "           123,  7492,   746,  3529,  2086, 11935,  1530,   117,   122,  5157,\n",
      "           259,   682,   117,   202,  1423,   125,  1719,  7583,  1069,   117,\n",
      "           123,  5961, 12448,   246,   498,  3848,   687,   562,   102]])\n",
      "DEBUG: Tokenized sentence 97: tensor([[  101,   122,   117,  1139,   117,   202,  4745,   180, 15433,   364,\n",
      "           117,   123, 16803,   324,   117,   123,   527,  3209,   154,   117,\n",
      "           123,   447,   451,  9147,   117,   123,  5782,  2037,   117,   123,\n",
      "           528, 16527,   122,   327,  2267, 10415,   692,   125,   964,   324,\n",
      "           123,   964,   324,   117, 10420,  2537,   122,  1821,   834,   176,\n",
      "          9226,   210,   117,   123,  4410,   222,  1971,   822, 20733,  1941,\n",
      "           423,  1312,   303,   117,   975,  1885,   185,  4687,   117, 12544,\n",
      "           954,   274,   364,   249,   117,   547,   256,   118,   176,   222,\n",
      "          1160,  5302,   455,   125, 11348, 17124, 22281,   117,   179,   417,\n",
      "          1797,   923,   125,  1796,   117, 17754,   591,   125,  5359,  3706,\n",
      "           117,   122, 16420,  1315,   474,  3514, 12086, 19277,   320,  1341,\n",
      "         11368,   366,  1028,   117,   420,   230,   762,   343,   304, 22280,\n",
      "           834,  1510,  2635,   117,   582,   176,   229, 22280,  7194,   151,\n",
      "           146,   179,   495,  3922,   268,  1165,   122,   146,   179,   495,\n",
      "         13672,   102]])\n",
      "DEBUG: Tokenized sentence 98: tensor([[  101,   230,   123,   230, 19022, 22287,   118,   176,  1485,   260,\n",
      "           964,  1343,   102]])\n",
      "DEBUG: Tokenized sentence 99: tensor([[  101,   122,   125,   944,   259,   504,  1522,   171,   549,   713,\n",
      "          8625,   228,  2217,   221,   260,   675, 18237,   303,   143,   102]])\n",
      "DEBUG: Tokenized sentence 100: tensor([[  101,   240,   230,  4303,   179,  1021,   320,  4707,   180,   418,\n",
      "         15976,  4285,  6451,   259,  5684,   180,  1449,  1272,   117,   171,\n",
      "           323,  8940,  2535,   146,  2582,  4029, 22282,   298, 21672,   247,\n",
      "           143,   122,   366, 12307,   823,   470,   102]])\n",
      "DEBUG: Tokenized sentence 101: tensor([[  101,   146, 15796,   404,   117,   125, 14790,   138,   125,   235,\n",
      "          3198,   117,  1690,  7817,  2979,   122,   498,  1149,  3391, 22278,\n",
      "         13305,   117,  1367,  1084,  1796,   117,   173,  3420,   221,   146,\n",
      "          5209,   210,   117,  8582,   423,  2678, 22285,  3132,   179,  8544,\n",
      "           221,   260,  6880,   102]])\n",
      "DEBUG: Tokenized sentence 102: tensor([[  101,   146,  1202,  2037,   158,   130,   117,   179, 13956, 22278,\n",
      "           125,  1312,   303,  1921, 12495,   117,  3033, 18618,   117,  7570,\n",
      "           203,   146,  4286,   247,   117,   834,  5961,   123, 16241,  1537,\n",
      "         22287,   117,  2798,   653,   123,  2606,   117,   122, 12401, 13665,\n",
      "           118,   176,   123,  1105,   117,   221, 18165,   102]])\n",
      "DEBUG: Tokenized sentence 103: tensor([[  101,   222,   939,   125,   449, 18001,   143,   117,   146,  2607,\n",
      "          3906,   117,   146,   302, 17168, 22280,   117,   146,  1546,  8747,\n",
      "           122,   146,  1961,   952,   117, 15078,  1078,   615,   170,   123,\n",
      "           327,   739,  8097,   125, 10187,  2245, 21947,   138,   117,  5127,\n",
      "           221,   123, 13693,  3391, 13793,   125,   944,   259,  1564,   117,\n",
      "         11518,  2746,   122,  1174,  1537,  9420,   173,  4693,   102]])\n",
      "DEBUG: Tokenized sentence 104: tensor([[  101,   222, 13254,   373,   125, 12581,   183,  3033,   180,  4768,\n",
      "           122,   262, 10573, 22282,   123, 16803,   324,   412,   149,   252,\n",
      "          6188, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 105: tensor([[  101,  2010,   123,  6188, 22278,   475,  1721,   136, 18661,   329,\n",
      "           659, 22032,   252,  3003,  1564,   179,   740,   388,   172, 16681,\n",
      "           123,   447,   451,  9147, 11535,  2044,   179,   123,  1124,  7137,\n",
      "          1011,   170, 11304,   125,  9196,   272,   421,   170,   146,  5101,\n",
      "           283,   102]])\n",
      "DEBUG: Tokenized sentence 106: tensor([[ 101, 2010,  179, 5101,  283,  136, 6185, 2904,  527, 3209,  154,  102]])\n",
      "DEBUG: Tokenized sentence 107: tensor([[  101,  2010,  6086,  5351,  5630, 22281,   303,   179,   176,  3285,\n",
      "           151,   260,  1176,   123, 22283,   170,   740,   102]])\n",
      "DEBUG: Tokenized sentence 108: tensor([[ 101, 1331,  179,  122,  745,  458,  102]])\n",
      "DEBUG: Tokenized sentence 109: tensor([[  101,  2010,   740,  3171,   118,   176,   136, 16795,   146,  3265,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 110: tensor([[  101,  2010,   229, 22280,   117,  1996,   123, 16803,   324,   146,\n",
      "          3147,   418, 10371,   117,   449,   123,  1124,  7137,   376,  4486,\n",
      "          1084,   102]])\n",
      "DEBUG: Tokenized sentence 111: tensor([[  101,  2354, 22279,   146,   179,  4750,   136,  2010,  8940, 10467,\n",
      "           230, 11451,   179,   418,   170,   740,   102]])\n",
      "DEBUG: Tokenized sentence 112: tensor([[  101,  2010,   229, 22280, 18661,   117,  1417,   117, 10573,   229,\n",
      "          5304,   320,  4141, 13793,  2415, 22280,   117,   179,  5787,   437,\n",
      "          6022,  4640,  3933,  5664,   102]])\n",
      "DEBUG: Tokenized sentence 113: tensor([[  101,  2010,  1369,   136,  2010,  1141,   117,  3265,   117,  6621,\n",
      "          4303,   117,   582,   123, 13305,   171, 20860,   418, 13394,   146,\n",
      "           644,   492,  2389,   252,   179, 12249,   138,   123, 21370,   304,\n",
      "           125,   360,   215,  1941,   176,  4970,   179,  7716,   136,  4048,\n",
      "           179,   229, 22280,   873,   582, 12249, 22278,   860,  9849,   125,\n",
      "           854,  2028,   122,   117,  4428,   214,   179,   146,  1417,   117,\n",
      "           146,   762,   128,  5321,   117,   176,  4621,   256,   221,  5357,\n",
      "           146,  1247,   171,  1342,   179,  1941,   176,  8544,  2010,  8625,\n",
      "           180, 22283,   117,  5023, 14619,   210,   117,  1143,   185,  1941,\n",
      "           905,   562,   229, 15831,   304, 22280,   125,   944,   259,  1564,\n",
      "           136,  3539,   221,   329,   117,   179,  1904, 22281,   449,   117,\n",
      "           122,  3295,   117,   179,  5637, 22281,  5023,   179,   229, 22280,\n",
      "          2541, 22281,   494,   159,   123,  3428,   154,   171,   662, 14455,\n",
      "           136,  2010,   368,  1996,  3185,  2097,   179,  2779,  2535,  2589,\n",
      "           123,  1373,   117,   179,   495,  1407,   102]])\n",
      "DEBUG: Tokenized sentence 114: tensor([[  101,  2010,   123, 22296,   122, 22032,   252,   117,   229, 22280,\n",
      "           437,  6969,  1149,   117,  1216,   259,   682,   592,   118,  7911,\n",
      "           117,   179,   122,  1338,   171,   454,   102]])\n",
      "DEBUG: Tokenized sentence 115: tensor([[  101,  2389,   252,  2541,  1084,  1839,   122,  1331,   123,   872,\n",
      "           514, 22285,   179,   437,  8853,   123, 11451,   179,  3429,  3185,\n",
      "          2097,   123,  2954,   102]])\n",
      "DEBUG: Tokenized sentence 116: tensor([[  101,   146,  3265, 18518,   118,   176,   125,  1742,   117,   122,\n",
      "           740,  2036,  9247,   654,   229,  7062,  2010,   122,   179,   229,\n",
      "         22280,   302, 13808,   146,  1314,   439,   201,   202,  4848,   834,\n",
      "          2779,   370,  1084, 19480,   230, 10415,   738,   671, 12682,   186,\n",
      "           118,   176,   202,  4745,   180,   450, 22278,   125, 11348, 17124,\n",
      "         22281,   123,  3953,   180,  6188, 22278,   475,  1721,   102]])\n",
      "DEBUG: Tokenized sentence 117: tensor([[ 101, 2010,  122,  171,  328,  653,  102]])\n",
      "DEBUG: Tokenized sentence 118: tensor([[  101, 11638,   256,   527,  3209,   154,   102]])\n",
      "DEBUG: Tokenized sentence 119: tensor([[  101,  3285,   140,   118,   176,   229,  9196,   272,   421,   834,\n",
      "          2822,  1284,   180, 11451,   179,  2036, 13030,   228,   102]])\n",
      "DEBUG: Tokenized sentence 120: tensor([[ 101, 1016,  607,  125, 4412,  834,  222,  958, 2118,  102]])\n",
      "DEBUG: Tokenized sentence 121: tensor([[  101,  2010,  7583,   229, 22280,  8530,  1235,   343,   325,   102]])\n",
      "DEBUG: Tokenized sentence 122: tensor([[ 101, 1078,  576, 1968, 2684,  325,  502, 1051,  285,  102]])\n",
      "DEBUG: Tokenized sentence 123: tensor([[  101,  4048,   179,   376,  4848,   202, 19036, 22280,   706,  5110,\n",
      "           146,  1312,   303,   179, 20075,   117, 15921, 12659,   272,   117,\n",
      "          2541,  2745,   258,  1341,  2389,   252,   146,   179,  5127,   146,\n",
      "           622,  3714,   170,   123,  4939,   180,  2377,   252,   102]])\n",
      "DEBUG: Tokenized sentence 124: tensor([[  101,  2010,   318, 13793,  2535,   117,   170,   860,  1124, 14273,\n",
      "           117,   146,  5101,   283,   117,   122,   230,  7666,   118,   792,\n",
      "         12268,   334, 22361,   577,   644,   117,  1502,  2354, 22279,   229,\n",
      "         22280,  4970,   136,  6777,   123, 22283,  1532,  5167, 11237,   364,\n",
      "           117,   123,  4654,   934,   122,  8032,   123, 14643,   117,   179,\n",
      "          2798, 18661,   146,   179,  9821,  4023,   437,  3039,  2010,   221,\n",
      "          2745,   607,  2856,   122,   607,  1564,   102]])\n",
      "DEBUG: Tokenized sentence 125: tensor([[  101,  2010,   221,   123,  6188, 22278,   944,   259,  1564,   629,\n",
      "         22280,  1564, 14125,   123,  5790,   154, 22280,   122,  8264,  1977,\n",
      "         14537, 22279,   240,   740,  2010,   744,  1016,   229, 22280,   122,\n",
      "           223, 15799,   102]])\n",
      "DEBUG: Tokenized sentence 126: tensor([[ 101, 4551,  175,  146, 2455,  373,  180, 1447,  679,  705,  102]])\n",
      "DEBUG: Tokenized sentence 127: tensor([[  101,  2010,  4062, 16450,   304, 22280,   376,   740,   117,  2684,\n",
      "          3547,   117,   179,   229, 22280,  5825,   222,  4698, 22287,   258,\n",
      "           644,   125, 22032,   252,   102]])\n",
      "DEBUG: Tokenized sentence 128: tensor([[  101,  4048,   179,   146,  3495,  2036,   659,   170,  1361, 13793,\n",
      "           202,  1831,  2010,   700,   122,   179,   629, 22280,  2859,   102]])\n",
      "DEBUG: Tokenized sentence 129: tensor([[  101,   146,  4141, 13793,  2415, 22280,  1941,  2036,   229, 22280,\n",
      "         10692, 22278,  2010,  1502,  2389,   296,   179,   123,  6188, 22278,\n",
      "          2036,   376, 16386,   286,  1004,   260,   223,   128,   625,   740,\n",
      "           376,  3495,   122,  2113,   146,  3598,   154,   653,   122,   260,\n",
      "         11348, 17124, 22281,   229, 22280,   176,  1945, 14533,   117,  1684,\n",
      "           123, 15518,  2127,   159,   117,   122,   123,  5818,   117,   122,\n",
      "           123,  8861, 22282,  7924, 22281,   122,   738,  6278,   138,   117,\n",
      "         15518,   439,  3897,   401,  1941,   423,  6569,  9258,   102]])\n",
      "DEBUG: Tokenized sentence 130: tensor([[  101,   320,  6793,   179,   117,   173,  3443,   180,   327,   316,\n",
      "           702, 15249,   289,   117,   146,   549,   713,   176,   173, 16979,\n",
      "           556,   256,  1364,   125, 11451,  3848,  4419,   117,   125,   582,\n",
      "           146,   969,  4551,   256, 11214,  1228,   303,   143,   125,  5731,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 131: tensor([[ 101, 2072,  173, 1512,  122,  146,  644,  495,  388, 9238,  102]])\n",
      "DEBUG: Tokenized sentence 132: tensor([[  101,   123, 14286,   298, 16450, 17351, 22281,   978, 19673, 22281,\n",
      "           188, 14788,  6436,   128,   260,  7367,   179, 21280,  2375,   320,\n",
      "         16972,   117, 11314, 10416, 13808, 22281,   125,  1160,   117,  7871,\n",
      "           604, 14533, 16188,   401,   117,   586,   249,  2746,   123,  3122,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 133: tensor([[  101,   173,   230,   366,  9751,   180,  4767,   125, 14890,   171,\n",
      "         15796,   404,   117, 10502,   860,   266,   122,  1757,   209,  7186,\n",
      "           117,  4903, 12232,   591,   125,  6054,   122,  4903,   123,  1743,\n",
      "           823, 22287,   260,   877,   842,   117, 10415,   692,   173,  4410,\n",
      "          1401,   285,   117, 21692,   358,   123,   762,   343,   304, 22280,\n",
      "           179,  8544,  1084,  6805, 22280,   117,   785,  6969,  7345,   229,\n",
      "           327, 20885,   292,   125,  3486, 22281,  8540,   143,   102]])\n",
      "DEBUG: Tokenized sentence 134: tensor([[  101,  5147,   117,  2535,   146,   636,  2115,   495,   229,  5304,\n",
      "           123,  3302,   180,   418, 15976,   102]])\n",
      "DEBUG: Tokenized sentence 135: tensor([[  101, 21280,  4167,  2856,   122,   259, 11309,   501,   366,  7875,\n",
      "         22281, 19288, 22287,   118,   176,   221,   146, 16960,   303,   102]])\n",
      "DEBUG: Tokenized sentence 136: tensor([[  101,   320,  3568,   304, 22280,   146, 18433,   122,   146,  5009,\n",
      "           178,   229, 22280,  2365,   223,   128,   123, 15191,   170,   123,\n",
      "          3293,   705,   180, 13219,  2028,   259,   173, 18271, 22290,   683,\n",
      "           125,  1798, 11229,  4897,   923,   118,   176,   117,   122,   146,\n",
      "          3495, 13718,  8041,   834,   558, 18680,   340,  1839,   180,  1956,\n",
      "         21305, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 137: tensor([[  101,  2010,  1423, 21080,   125, 11902,  2010,   222,   374,  8849,\n",
      "         22280,   125,   417,  8497,   159,  2010,   230, 16317,  1165,   125,\n",
      "          4778, 22175,  2010,   682,   528,  2846, 22281,   125, 10832,  2010,\n",
      "           682,  4698,   255,   125,   572,   283,  2010,  1256,   125,  6459,\n",
      "         13793,   122,   259,  9247,   382,  8963,   923,   118,   176,  1532,\n",
      "          4682,   125, 12514,   125,   944,   259, 14378,   102]])\n",
      "DEBUG: Tokenized sentence 138: tensor([[  101,  8362,   228,   118,   176,  9958,   420,   259, 16007,   207,\n",
      "          2010,   311,  4042, 22279,   117,   347, 18433,  2779,  1114,   244,\n",
      "           123,  9652,   202,  4848,  2010,   146,  1143,   185,   180,   329,\n",
      "           260,  3985,  7796,   117,   179,  2779, 15212,   325,   146,   179,\n",
      "          1434,  2010,   347,  5009,   178,   117,   229, 22280,   311,  3174,\n",
      "           130,  1921,  3001,   945,   320,  1341,   117,   229,   504,   508,\n",
      "           125, 14016,   117,   123, 10420, 18398,   852,   117,   125,  8625,\n",
      "           138,  3456,  1539,   591,   202,  1896,   892,   117,   146, 13850,\n",
      "          3391, 22280, 15618,   293,   122,  8018,   117, 15908,  8149,   214,\n",
      "           125,   233,   141,   117,  8544,   122,  8940,   125,   230,  9196,\n",
      "           747,   123,  1858,   117,  2636, 16032,   117,   179,  4141, 13793,\n",
      "          2415, 22280, 16403,   125,  1742,   712,  5684,  3791,  8052,  1362,\n",
      "          4308, 12411,  1982,   102]])\n",
      "DEBUG: Tokenized sentence 139: tensor([[  101, 18689,   364,   118,   176,   222,  1160, 11314,  2650,   397,\n",
      "           117,   331,   221,   146,   958,   640,   117,   122,   146, 13254,\n",
      "           117,   123,  1078,   662,  2524, 22290,   179,  8544,  5168,   117,\n",
      "          8825,  5723,   117,   173,  5902,  8954,   243,   122, 15777,  9238,\n",
      "           117,   123,   327,   558,  1817,   415,  2925,   366,  9652, 22281,\n",
      "           179,  1021,   102]])\n",
      "DEBUG: Tokenized sentence 140: tensor([[  101,   222,   765,   397,  2124,   125,  3673, 20480, 10310,   373,\n",
      "          9267,   256,   102]])\n",
      "DEBUG: Tokenized sentence 141: tensor([[  101,   146,   221,   193, 17364,   256,   240,  1485,   260,  9317,\n",
      "         22281,   117,   122,  1078,   822,   383, 22278,   125, 19935,   117,\n",
      "           125,  7406,   304, 15021, 22278,   117,  8004,   151,   222, 12349,\n",
      "         13793,   125,   572,   283,  1510, 22087, 12717,   243,   123, 10985,\n",
      "         12909,   243,   102]])\n",
      "DEBUG: Tokenized sentence 142: tensor([[  101,   230,   313, 10441,   159,   124,  7672, 13808,   117,   173,\n",
      "           179, 16241,  1537, 22287,   176,  3486,  1399,  9993,   692,   118,\n",
      "           176, 19229,   173,  1485,   260, 15512, 22280,   143,   117,  5489,\n",
      "          2916,   118,   176,   123, 10420,   444,   117,   170,  5488,   358,\n",
      "          5078,  8757,   498,   260,  9317, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 143: tensor([[  101,   122,  1684,   123,  5197,   117,   122,  1684,   123,  4270,\n",
      "          9349,   117,   122,   259,   179,  8625,   228,   117,   700,  7970,\n",
      "           662,   852,   387, 15618,   375,   117, 16420,  7612,   358,   125,\n",
      "          1519,  6423,   117,   170,   123,  1923,  6943,  1004, 13086,   117,\n",
      "           123,  7096,   578,   102]])\n",
      "DEBUG: Tokenized sentence 144: tensor([[  101,  1362,  6465,   125, 16341,   374, 22281,   303,   117,   179,\n",
      "          8921,   171,  1341,   125,  1796,   117,  1982,   123,  8130,   122,\n",
      "          3047,   180,  5304,   117,   222,  2397,   117,   125, 14790, 22278,\n",
      "           122,  7924,   125,  1757, 21304,   185,   117,  4004,  3152,   125,\n",
      "         16064,  4793,   117, 11690,   117,  1021,  1941,   230,  3264,  5314,\n",
      "           117,   221,  5961,   170,   146, 12447,   397,   102]])\n",
      "DEBUG: Tokenized sentence 145: tensor([[  101,   495,   222,  1456,   143,   125,   532,  7187,   122,  1685,\n",
      "           123,  9836,   481,   117,  2979,   117,  9531,  1350,   117, 17897,\n",
      "         22281,   260, 13001,   117, 13841,  7967, 22281,   122,  3002,   436,\n",
      "           720, 15356,   118,  2036,   498,   123, 15719,   117,   240, 15702,\n",
      "           125,   222,  1690,  7817,   125, 10840,   552,  4189,  2623,   247,\n",
      "         13227,   303,   125, 18016, 22280,   122,  1354,   125,  2316, 17768,\n",
      "           117,   229,   615,   259,  5708, 17226,   117, 21323, 22281,   271,\n",
      "           259,  5708,   125,   222,  1151, 22283,   125,   822,   421,   117,\n",
      "          2468,  3198,   923, 20885, 22278,  7390,  1076,   102]])\n",
      "DEBUG: Tokenized sentence 146: tensor([[  101,  2010,   318, 13793,   744,   229, 22280,   176,   706,  5961,\n",
      "           320,  2397,   136, 16795,   368,   117,  6738,   320,  3568,   304,\n",
      "         22280,  9050,   118,   176,   170,   146, 18433,   102]])\n",
      "DEBUG: Tokenized sentence 147: tensor([[  101,  2010,   146,  9019, 13793,   418,  2535,   785,  9955,   102]])\n",
      "DEBUG: Tokenized sentence 148: tensor([[  101, 14657, 22279,  2010,   449,   629, 22280,  1821,  1027,  2856,\n",
      "           122, 12044,   170,   222, 11166,   125, 19935,   202,  9466, 16606,\n",
      "          2010,   781,   185,  2044,  2010,  1623, 22280,   229,   651,   940,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 149: tensor([[  101,   122,   222,  1241,   124, 22280,   180,  8156,   146, 11314,\n",
      "          2650,   397,  9247,   654,   318, 13793,   221,   123, 13159,   117,\n",
      "           834,  8082,   140,   146,   179,  5057,  2010,   146,  2397,   179,\n",
      "           123, 22283,   418,   117,   347,  4141, 13793,   117,  1331,   179,\n",
      "           176,  2541,  1853,  2010,   368,   179, 14657, 22279,   222,  1695,\n",
      "           117,   179,  1941,  2036,   988, 22280,  9396,   146, 12447,   397,\n",
      "           202,  1423,   125,   230,  1742,   102]])\n",
      "DEBUG: Tokenized sentence 150: tensor([[  101,  2826, 22278,   118,  2036,   179,   229, 22280,  1447,  2010,\n",
      "           449,   122,   179,   744,   229, 22280, 16960,   289, 22283,   122,\n",
      "         12044,  5863,   123,   964,  4029, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 151: tensor([[  101, 10719,   146,  2316, 17768,   170,   123,   327,  4410, 15618,\n",
      "           375,   122,  5276,   102]])\n",
      "DEBUG: Tokenized sentence 152: tensor([[  101,  2010,   146,  1417,   117, 16960,   289,   123, 22283,   653,\n",
      "          5863,   146,   179,   229, 22280,  3207,   122,   125,  1847,   102]])\n",
      "DEBUG: Tokenized sentence 153: tensor([[  101,  1941,  4207,  2765,  4042,   201,  2010,  1502,  1447,  1084,\n",
      "          9565,   146,  1054, 22285,  4803,   124, 22280,   117, 13685,   180,\n",
      "          5304,   221,  4270,   229,  1105,   125, 14016,   117,   582,   259,\n",
      "           179,  1084,   176, 16995, 22287,   146,  7307,   170,   388, 12898,\n",
      "         22280,   117, 13750,   214,   118,   146,   180,  3049,   304,   712,\n",
      "          1143,   117,   271,  9144,  1684,   170,   944,   259,   179,   123,\n",
      "         22283,   176, 20254,   412,   681,   576,   102]])\n",
      "DEBUG: Tokenized sentence 154: tensor([[  101,   122, 20582, 22288,   118,   176,   123,   230,   366,   454,\n",
      "          4242,   117,  6661,  2044,   146, 11314,  2650,   397,  8032,   118,\n",
      "          2036,   123,  2925,   298, 16032,   102]])\n",
      "DEBUG: Tokenized sentence 155: tensor([[  101,  2010, 12424, 22278,  1084,   146,  9805,   243,   170,  3985,\n",
      "          7796,   122,   873,   524,   222,   528,  2846,   125, 10832,   102]])\n",
      "DEBUG: Tokenized sentence 156: tensor([[  101,  2010,  3189,  6183,   291,  2477,   705,   136,  2010,  1214,\n",
      "           252,   146,  6183,   449,  6952,   170,  1257,   117,  1417,   117,\n",
      "           179,  1941,   229, 22280,  3539,   834,   596,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 157 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.007079646016132821\n",
      " Coesão Score Final: 0.5035398230080664\n",
      " Conectivos encontrados: ['e', 'mas', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'embora', 'isto e', 'todavia', 'nem', 'ou', 'ora', 'quer', 'seja', 'como', 'quanto', 'ao passo que', 'logo que', 'porque', 'posto que', 'ao contrario', 'no entanto', 'com efeito', 'alias', 'tanto', 'quanto', 'se nao', 'a proporcao que']\n",
      " Número de conectivos: 32\n",
      " Número de sentenças: 157\n",
      "======================\n",
      "Resultados para preprocessado_o_cortico_aluisio_azevedo_cap_3.json:\n",
      "{'coesao_score': np.float64(0.5), 'conectivos_encontrados': ['e', 'mas', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'embora', 'isto e', 'todavia', 'nem', 'ou', 'ora', 'quer', 'seja', 'como', 'quanto', 'ao passo que', 'logo que', 'porque', 'posto que', 'ao contrario', 'no entanto', 'com efeito', 'alias', 'tanto', 'quanto', 'se nao', 'a proporcao que'], 'num_conectivos': 32, 'proporcao_conectivos': 0.007, 'similaridade_media': np.float64(1.0), 'num_sentencas': 157}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  5899,  5314,   700,   117,   625,  4141, 13793,  2415, 22280,\n",
      "           176,  4970,  1528,  9955,   117,   262,   370,   170,   146, 10738,\n",
      "           179,   146,  2863,   256,   122, 20582, 22288,   118,   176,   975,\n",
      "          1885,   185,  2461,   117, 15356,   125, 22229,   945,   117,   449,\n",
      "           834,   176,   179,  1445, 22282,   117,  2798,   176,  2036,   338,\n",
      "           307,   123, 15578,  4275,  4322,   146,  2892,  6804,   148,   125,\n",
      "           822,   375,   303,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,  2010,  2354, 22279,  3539,   180,   670,   171, 18546,  1149,\n",
      "           136, 16795,   118,  2036,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   368, 11234,   118,   311,   125,   222,  2397,   179,  4178,\n",
      "         14790,   159,  5028,   117, 10497,   934,  4848,   122,  1434,  1084,\n",
      "           537,   243,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[ 101, 2010, 7206, 2779,  102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,  2010,  1011, 12531,   173,  1858,  1449,  1272,   136,  2010,\n",
      "          1011,   122, 12044,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   229,   125,   629, 22280,   434,   763,   117,   449,   273,\n",
      "          1289,   185, 22283,   118,   311,  3914,   122, 18691,  3852, 14339,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,  2010,  2249,  2036,   180, 22280,  1084,   136,  2010, 17585,\n",
      "           592,   118,  7911,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,  2010,   146, 22296,  1257,   122,   222, 10907,   185,  2010,\n",
      "           229, 22280,  1223,   240,  1528,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,  2010,  2779,   117,   146,   636, 11263,   179,   395,   303,\n",
      "           122,   125, 11330,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,  2010, 11330,  4624,   222, 12361, 13981,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,  2010, 11032, 15212,   123, 22283,  1415,  5684,   125,  1084,\n",
      "           537,   243,   240,  1966, 14701,  2010,   623,   857,   179,   543,\n",
      "          2097,  4230,   183,   123,   223, 22280,  5065,   173,   271,   146,\n",
      "          7258,   229, 22280,  1377,   240, 11330,   592,   118,  7911,  1977,\n",
      "         13105,   524,   123, 11934,   304,   117,  1143, 22279,   123,   661,\n",
      "          8254,   122, 10497,   455,  4848,   117,   834,  2036,  3254,   702,\n",
      "           123,  5028,   122,   834,  1434, 14096, 22281,  2010,  1141,   117,\n",
      "           449, 17585,   592,   118,  7911,   122,   222, 11263,  6944,  4812,\n",
      "          2010,  3876,  1652, 17891,   271,  1976, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,  1968,   146,  6775,   240,   229, 22280,  6775,  2010, 17585,\n",
      "           592,   118,  7911,   122,   785,  3495,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  2010,   329,   240,  9726,   117,  3486,   214,   179,  5488,\n",
      "           123,  7482,  7198,   325,   222,  1695,   123,   222, 17509,  4062,\n",
      "           117,   171,   179,  2765,   123,  9533, 14096, 22281,   117,   271,\n",
      "           146,   179,  4808,   327,  1449,  1272,   123,  2767, 19023,   229,\n",
      "         22280, 12402,   229,  1069,   171,  6754,   125,  2544, 22280,   179,\n",
      "          1767, 15702,   180,  5028,  2010,   123, 22296,   146, 18546,  1149,\n",
      "         11234,   118,  2036,   202, 14096,   136,  2010,  5222,   118,   390,\n",
      "           117,  1141,  7258,   117,   122,   146, 14096,   229, 22280,  9963,\n",
      "           151,   176,   146,  2397, 13256, 22281,   236,  1434,   146,  1312,\n",
      "           303,  2010,   449, 17585,   592,   118,  7911,   122,  6944,  4812,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   273,   304,   222,  1695,  2010,   240,  1528,   229, 22280,\n",
      "           311,  7947,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   122, 13083,   375,   793,   125,  3598,   578,  3724,  2010,\n",
      "          2354, 22279,  7422,   123,  1449,  1272,   136,  2010,  2364,   123,\n",
      "          1976,   125,  3047,   117,   449,  8204,   311, 11639,   179,   122,\n",
      "          3264,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   125,  5533,   765,  5461,   118,   311,   123, 20300,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,  2010, 14657, 22279,   222, 16423, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,  4141, 13793,  2415, 22280,  2002,   222,  5995, 22280,   123,\n",
      "          5304,   117,  2789,  1450,  6107,   117,  3486, 22282,   748,   222,\n",
      "          1690,  7817,   229,  3049,   304,   122,  2927,   123,   370,   170,\n",
      "           146,  1342,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,  2010,  1961, 22279,   123,   792,  9247,   654,   118,  2036,\n",
      "           180,  4303,   171,   958,   640,   117,   179,   123,  1695,   122,\n",
      "          1695,   176,   188,   256, 12051,   124,   125,  1364,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   146,   329,  5458, 13981, 20945,  6136,  4698,   255,   423,\n",
      "           347, 16960,   303,   122, 14009,   118,   146,   173, 22243, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101, 17347,   228,   146,   549,   713,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,   123, 18908,  3239,   304, 22280, 14372,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   260, 11348, 17124, 22281,  2365,  1941, 19480, 16960,   934,\n",
      "           122,  2365, 12520,   125,  1160,   221,   146,  1223,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  2535,  2072,  1485,   125,  1690,  7817,   125, 19278,   117,\n",
      "          2440,   366,   374,  2876,   138,   179,   176,  4857,   288,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,   222,  6938,   125,  1248,   713,  1623,   679,   118,  7707,\n",
      "           259,  8589,  1107,   173,  1010, 22278,   122, 11214,  1228,   358,\n",
      "           125,   233,   141,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   222,  1177,  1154,  2042, 22290,  1178,   545,  2836,   118,\n",
      "           176,  4687,  4900,   398,  2848,   243,  7583,  2826,  1009, 22280,\n",
      "          2850,   320,   969,  1718,  2349,   256,   118,  7707,   146,  5052,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   123, 16803,   324, 11518,   304,   256,   170,   230, 13305,\n",
      "           179,  1796, 15158, 22282,   222,   332,   125,  5899, 22281,   122,\n",
      "         16863,   934,   230,  7924,   123,   527,  3209,   154,   117,   785,\n",
      "          3848, 22279,   498,   123,   327, 14038, 22278,   125, 11348, 22282,\n",
      "           117,  9821,  1270, 20509, 22282,   118,   176,   271,   176,   492,\n",
      "           123,   447,   451,  9147,  7453,   256,   125,   576,   173,   625,\n",
      "           123, 11451,   122,   146,  6459, 13793,   221,   144,   934,   260,\n",
      "           170,  1361, 22280,   143,   171,  1896,   892,   122,   366,  2477,\n",
      "         10661,   117,   502,  1051,   591,   423,  1623, 16093,   123,  5782,\n",
      "          2037, 10537, 11933,  2836,   117,   398,  4549,  4212,  1532,   601,\n",
      "           544,   340,   125,  6220,   154,   117,   320,  1341,   180,   528,\n",
      "         16527,   179,   117,   170,   146,   347,  1903,   125,  1124,  7137,\n",
      "          7492,   117,   222, 13850, 22178,   320,  2242,   180,  9463,   117,\n",
      "          8954,   256,   374,   401, 10537, 11769, 22281,   171,   333,   154,\n",
      "         22280,  1112, 22232,  1149,   316, 22232,  5870,   214,   117, 22232,\n",
      "          1149,   316, 22232,  5870,   214,   117,   229,  1425,   640,   171,\n",
      "         18419,  2014, 22232,  1149,   316, 22232,  5870,   214,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101, 22354,   123, 12252,   624,   117, 20073,   117, 15363,  1004,\n",
      "           170,   146,  9708,   171,   969,   117,   123, 14966,  7177,   834,\n",
      "         22229,  3670,   117,  1990,   619,   256,   259,  5334,  2283,   122,\n",
      "           599,  8859, 22281,   179,   176, 17318, 22287,   229,   418, 15976,\n",
      "           117,   122,  1982,  3914,   117,   123,   949,  1601,  8551,   304,\n",
      "         17704, 10502,   847, 12764,   178,  4217,   364,   256,   117, 15518,\n",
      "          2127,   348,   123,   327, 11451,  1839,   180,   964,   324,   117,\n",
      "         13549,   117,   271,   222,  9038,   123,  3212,   202,  3053,   247,\n",
      "           320,  6793,   179,   146,   313, 11284, 22280,   117,  9344,  3391,\n",
      "          3356, 22279,   348,   259,   532,  1896,  1046,  8699,   125,  2397,\n",
      "          1863,  1165,   713,   117,  3985,   151,   229, 14038, 22278,   222,\n",
      "           332,   125, 14790,   138,   117,   202,  7881,  6846,  6882,   122,\n",
      "         14689,  1350,   125,   222, 11899,  2825,   123,  5818,  4351,  6644,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,   146,  1831,  8574,   151,   118,  2036,  1364,   117,   122,\n",
      "           368,   117,   125,   576,   173,   625,   117, 16322,  1399,   146,\n",
      "         16936,   303,   171, 13227,   303,   221, 13540, 19328,   159,   123,\n",
      "          3317,   117,   122,   318, 13793,   222,  2510, 13513,  4217,   364,\n",
      "           243,   695,   151,   118,  2036,   712, 18908,   501,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,   180,   504,   508,  5492, 22280,  1015,  8940,   222, 18612,\n",
      "           735, 22279,  6205,   243,   117,   449,   870,  1862,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,   495,   123,   366, 19659,   179,   905,   151,   256,   146,\n",
      "           347,  1312,   303,   229, 22280,  9679,  2787,   703,   159,   834,\n",
      "          8032,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101,   202,  5492, 22280,   977,   872,   514, 22285,  8032,   715,\n",
      "           256,   173,  5902,   785,   325,  3378,   122,   125,   222,   298,\n",
      "         15050,   171,  4707,   180,   418, 15976,  8625, 22278,   125,  1632,\n",
      "           303,   123,  1632,   303,   230,  4428,   260,   574,   125,  1073,\n",
      "          7052,   514,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,   146, 12447,   397,   117,   320,  3852,   240,   125,   839,\n",
      "           125, 12252,   624,   117,   179,   202,  2182,   305,  1051,   256,\n",
      "         11451,   171,  1690, 22280,   117,  3050, 22288,   118,  2036,   230,\n",
      "          1877,  2555,   229,   670,   171,  1831,   318, 13793,   325,   173,\n",
      "         18374,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,  2010,   229, 22280, 21595, 22278,   117,  2678,   147,   136,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,  9247,   654,   740,   117,  6372,   286,   117, 15568,   214,\n",
      "           118,   176,  3689, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,   122,   117,  3951,   170,  4141, 13793,  2415, 22280,  2010,\n",
      "          2779,  2044,  1976,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101,  1904,  8893,   214,  5863,   170,   123,  9349,   122,   700,\n",
      "           117,  2541,   118,   176,  8977,   229,  5304,   117,   146,   629,\n",
      "          1165,   243,  6641, 22278,   202,  5274,   644,   492,   171, 13588,\n",
      "           339,  2779,   229, 22280,   437, 18691,   117,  4178,   136,   146,\n",
      "         12447,   397,   969,   654,   118,  2036,   940,  1877,  2555,   170,\n",
      "           325,   344,   304,   122, 11579,   117,  2113,   740,   176,  4857,\n",
      "           124,   170,   222,   494,   500, 13140,   125,  6205, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,  2010,  3539,  1174,   329,   117,   176,   188,  4051,   644,\n",
      "           492,   180,  1143,   185,  4141, 13793,  2415, 22280,  1941,   176,\n",
      "          1021, 14426,   170,   146,   329,  5458, 13981,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[ 101, 2010,  146, 7258,  376, 5863, 5747, 9349,  102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101, 10719,   118,  2036,   860,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,  2010,   146, 22296,  1191,   146,  1342,   117,   629,   830,\n",
      "          6436,   243,   259, 20462, 22281,   117,   122,  1996,   700,   170,\n",
      "          4136,  1323,  2010, 15098,   325,  5830, 15050,   179, 16330, 13140,\n",
      "         22281,   449,   122,  2745,  9349,  1467,   229, 22280,   607,  4004,\n",
      "         12886,   255,  3660,   418, 15976,   176,  3379,   230,  3277,   421,\n",
      "           117,  2779,  1120, 22280,   117,   122,  2745,  4364,  2044,  2364,\n",
      "           538,  3033,   329,   123,   661,  1144,   117,  2798,  2364,   123,\n",
      "          4314,  5835,  4270,   122,  2389,   296,   179,   176,  7465,  2097,\n",
      "          1004,   170,   260,   675, 14643, 22281,  2745,  9349,  5747,  3264,\n",
      "          2365, 12319,   320,  1338,   171,  4286,   247,   171,   549,   713,\n",
      "           122,   117,   700,   125,  4636,   210,   230,  4303,   179,   176,\n",
      "          3030, 15736,   170,   222,  5274,  8650,   825,   123,   230, 15445,\n",
      "           117, 16512,   228,   118,   176,   202,   853, 18983,   162,   179,\n",
      "          1021,  1075,   180,  1449,  1272,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,  2010, 19821,   240,  5863,   653,   179,   122,   325,  3047,\n",
      "           117, 12910,  2598,   146, 12447,   397,   102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,   122,   259,   682,   117,   173,   576,   125, 10469,   210,\n",
      "           123,  5675,   117, 17347,   228,   146,   853,   314,  8833,   122,\n",
      "          1510, 22281,  2848,  7909,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[ 101, 1423,  118,  644,  173, 2009,  102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101,   146,   969,  1011,   123, 13718, 22280,  2745,  7871,   604,\n",
      "          2836,   123,  3377, 10984, 11847,  4146,   415,   125,  1512,   117,\n",
      "          1362,   644,   834, 15553,   102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[  101,   123,  1449,  1272,   117,   173,   179,   740,  3985,   151,\n",
      "           125, 14982,   173,  5530,   117,  2992,  8041,  2389,  4419,   125,\n",
      "          2375,   102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[  101,   495,  8911,   528,   779,  2854,   123,  3122,   221,  7544,\n",
      "           260,  1444, 20798, 22281,   180,  5028,  3874,   325,   179,   230,\n",
      "           739,  9922,   252,  7352,   122, 17637, 22278,   117,  8638,   412,\n",
      "           670,   125,  3378,   202,  1690, 22280, 14021,   125, 15520, 22290,\n",
      "           268, 14689,  1350,   117,   179,   320,  5533,  1271,   151,   146,\n",
      "          3901,   125,   222,  5294, 10848, 22279,  1348, 11577,   183,   117,\n",
      "           122,   412,   670,   125,  5530,   229, 16704,  4968, 10394,   171,\n",
      "           388,  4056,   243,   117,   582,   176,   229, 22280,  7194,   923,\n",
      "           736, 14378,   325,   171,   179,   202,   243,   138, 20269,   117,\n",
      "          1004, 20269,   117,   498,   146,  6183,   118, 13389,   102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101,   123,  3606,   304, 22280,   179,   259,   682,   176,  4621,\n",
      "           692,   180, 19601,   403,  1449,  1272,   117,   146,  5856,  8544,\n",
      "           118,   176,  2862,   325,   122,   325, 15520, 22290,  6021,   243,\n",
      "           259, 21423,  9855, 11324,   692,   118,   176,   125,   230, 20076,\n",
      "          4982,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101,   325, 14339,   117,   240,  5863,   122,   240,  1369,   117,\n",
      "          1021,  1615,  3883,  1149,   117,  1450,   173,  2115,   117, 14537,\n",
      "           401,   123,  9066,   157,   122, 13086, 22281,   125,  1945, 15225,\n",
      "          6910,  1028,  1941, 20466, 22281,   221,  3866,   117,   123,  2521,\n",
      "           171,  6032,   117,   122,  1028, 17878,   170,   259,  4332,   942,\n",
      "           221,   146,   388,   117,   271,   176,  4364, 22281,  6867,   125,\n",
      "           333, 15891, 13199,  4900, 16423, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101,  2217, 18908,  3239,   692,   102]])\n",
      "DEBUG: Tokenized sentence 50: tensor([[  101,   123,  4573,   117,   240,  5530,   125,   222, 12232,  1699,\n",
      "           125,  2187,   117,   179,  9821,   370,   908,  5167,   286,   125,\n",
      "           222, 12424, 22280,   240,  6086,   969,  2496,   234,   117,  1021,\n",
      "           230,  5792,   125, 14038,   138,   117,   582,  1510, 22281,  4385,\n",
      "           117,  1821,  1444, 22281,   117, 10415,   692,  3791,  8052,   117,\n",
      "           834,  1434, 15419,   117, 16188,   308,   123,   363,  4387,   423,\n",
      "           969,   171,  1423,   118,   644,   102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[  101,   221, 14339,   117,   229,  1589, 15512, 13793,   117,  4063,\n",
      "           151,   222, 14985,  4117, 16510,   117,  4575,   122,  5980, 22280,\n",
      "           117, 16315,   243,   498,  9303,   125,  5028,   374, 22281,   304,\n",
      "           123, 22283,  1415,  4966, 18739,   125,  2069,   458,   117,   320,\n",
      "          1923,  1259,  4437,   319,   171, 12307, 13793,   179,  1718,   151,\n",
      "           146, 20300,   102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[  101,  2044,   173,  2590,   117,  2366,   151,   230, 17415,   125,\n",
      "          1718,  1339,   117,  1719,  1032, 20798,   285,   125, 16863,   942,\n",
      "           122,  4674, 19797, 22281,   117,   420,   259,  1647,  1938,  8521,\n",
      "         14533, 12446,   125,  3883,   173,  1359,   180,  4351,  5967,   324,\n",
      "           682,  2217,   117,   125,  1831,  1444,   117, 10137,   308,   125,\n",
      "           233,   141,   122,  5231,  4322,   442,   125,  6367,   271,   682,\n",
      "           644,  3471,   117,   528,  6204,   692,  6846,  4211,  7424,   498,\n",
      "           222,  7094,   303,   125,  3050,   173,  1010, 22278,   122,  1369,\n",
      "           653,   117,  3047,  2866,   117,   123,   344,   524,  3240, 15690,\n",
      "          2836,   230,  3746,   747,  3752,  7134,   117,   125,   582,  8625,\n",
      "           228,  4296,  3182,   138,   125,  4848,   117, 10984,  8156, 15268,\n",
      "           122,   866,  1159,   138,   102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[  101,  4141, 13793,  2415, 22280, 13683,   123,  3302,   180, 17415,\n",
      "           122,  9247,   654,   221,   222,   298,  1718,  8855,  2010,   146,\n",
      "          5782,   300,   229, 22280,   176,  6969,   304,   171, 17935, 22290,\n",
      "           180, 18253,  2830,   171,  4303, 22280,   259,   682,  2217, 16322,\n",
      "          7022,   240,   222, 16423, 22279,   146,  1223,   102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101,  2010,  1941,  1084,   572, 22283,   792,   117,  9396,   146,\n",
      "          5782,   300,   102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101,   229, 22280,  5488,   123,  7482, 18486,   154,   118,  1340,\n",
      "           418,  1364,   170,   286,   125,  5643,   705,   659,   118,   176,\n",
      "           118,  2036,   222,  1160,   117,   179,   122,  1407,  2010,  1502,\n",
      "           873,   524,  1084,  1257,   117,   179,   123, 18253,  2830,   418,\n",
      "           123,  9322,   122,   146, 12447,   397,  5793, 14339,   170,   146,\n",
      "          1342,   117,  1139,  7521,  5725,   304,   256,   146,   528, 20871,\n",
      "           498,   123,  4351,  5967,   324,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101,   173,  2590,  1413,   118,   176,   230,  3061,   371,   415,\n",
      "          1048,  2308,   151,   117, 13086,   125,   853,   314, 13427,   122,\n",
      "          2638,   684,   265, 22280,   125,  5294,  8849, 22281,   117,   170,\n",
      "          1247,   221,  5899,   623, 12051,   125,  3155,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[  101,  1011, 12649,   154,   117,   449,   117,   202,  3361,  5546,\n",
      "         10848, 19455,   201,   125,  1084,   117, 16280,   118,   176,   179,\n",
      "          1796, 17971,   744,  7583,  2954,   102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[  101,  1021,   700,   222, 12174,   373,   125,  4561, 22281,   117,\n",
      "         10568,   320,   653,   596,   125, 17415,   125,   505,  6592,  3120,\n",
      "           117,  1226,   123,  4303, 14793, 22281,   125,   388,  4056,   117,\n",
      "          1089,  1941, 10530,   442,   117,  1615, 14038,   138,  4276,  5822,\n",
      "           591,   117,  8197,   125, 20880,   122,   449,   534,   125,  4449,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[  101,   180, 22283,   123,  1449,  1272,  2226,   692,   820,  7226,\n",
      "         11330, 10674,   122,   146,  1690, 22280,   495,  1941,  1364, 14021,\n",
      "           240,   230, 20080,   125,  5028,   390,   328,   179,  5980,  2836,\n",
      "           271,   123,  1945,   102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101,  5863,   117,  1369,   117,   240,  1719,   123,   670,   117,\n",
      "         12063,   118,   176,  5684,   117,  7226,   320,   969,   117,   736,\n",
      "         15702,   125,  4296,  6688,  1149,  4330,   125,  1041, 22278,   291,\n",
      "           125,  7438,   125,  1877, 10443,   102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101,   125,   222,  1341,  8981, 15736, 22287,  5028, 10714,   125,\n",
      "          1342,   123,  6642,   692,   123, 12307,   823,   154,   125,  1342,\n",
      "          4613,   319, 14533,  1084,   537,   442,   123,  8993,   125, 12307,\n",
      "         13793,   325, 14339,  9144,  4405,   583,  6720,  2055,   128,   123,\n",
      "          3235,  1703,   122,   223, 14524,   102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101,   122,  1364,  6086,  2582, 15500, 22287,   125,  8786,   117,\n",
      "           122,   146,   528, 20871,   180,   344,   524,   117,   122,   146,\n",
      "          5553,   298,   179,  1084,   173,  5530, 11934,   304,   692,   123,\n",
      "         13353,   221, 18253,   934,   118,  2036,  4848,   117,   122,   123,\n",
      "          1401,   285, 10273,   251,   320,  5533,   117,   179,  8940,   171,\n",
      "           549,   713,   117,   271,   125,   230,  6321,  9456,  2555,  2745,\n",
      "         10348,   123,  3138,   125,   230,  3546, 18781, 22305,   117,   125,\n",
      "           230,  1998,   125,  7903, 20798,   122,   125,   146,   635,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[  101,  5022,  2217,  3746,   185, 17675,   555,   125,   233,   141,\n",
      "           117,  5167,   308,   125,  6938,   117,   273,   256,   364,   442,\n",
      "           125,   601,   715,   304, 22280,   117,   123, 14195,   210,   117,\n",
      "           123,   730,   232, 17738, 22287,   117,   123, 10697,   684,   123,\n",
      "          5028,   117,  9821, 22287,   222,  5078,  2895,   125,  3174, 13733,\n",
      "         22281,  6838,   442,   229,   327,   525,  3356,  3292,   598,   146,\n",
      "          4764, 22281, 12569,   415, 12477,   179,   259,  4108, 21307,   170,\n",
      "         10968,  2256,   117, 16421, 21994,   415,   123,   944,   259, 16437,\n",
      "           122,   123,   944,   259, 12785,   179,  2036,  5510, 11211, 14533,\n",
      "           202,  4678,   293,   117,  4513,   834,   222,  2510, 13513,   179,\n",
      "          2036,  2751, 22281,  6867,   260,  3837, 13808, 22281,   125, 20300,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[  101,   146,  3262, 18271,   243,   329,  5458, 13981,  1021, 12319,\n",
      "           123,  1925,  2876, 22278,   171, 17275,   293, 15218,   125,  5028,\n",
      "           978,   118,   146,  1354,   123,  1354,   117, 13750, 22288,   118,\n",
      "           146,   125,  2979,   123,  3378,   117,  7096, 10768,   117,  1362,\n",
      "          9898,  1401,   243,   102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[  101,   123,  1449,  1272, 15245,  3876,  2009,   125,  3122,   146,\n",
      "           347,  1341,   325, 19601,   403,   102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101,  1432, 13644,   154,   117,   170,   146,  4721, 20659,   243,\n",
      "         13185,   303, 16786,   320,   969,   117,  8004,   151,   118,   176,\n",
      "          2729,   514,   364,   122, 10151,   293,  3181,   285,   117,  9620,\n",
      "          5974,   146,  2992, 22288,   117,   785,  1486,   684, 22279,   117,\n",
      "         21990, 22278,   117,  5490, 12717,   185,   122, 13086,   125, 10286,\n",
      "           179,   454, 11147,   546,   403,  2036,  3235, 22282,  5283,   412,\n",
      "          5307, 16719, 22278,  1444, 15182,   170,   222,  3901,   125,   437,\n",
      "           562,   125, 11997, 13808,   102]])\n",
      "DEBUG: Tokenized sentence 67: tensor([[  101,   173,  8128,  4488,   117,   785,  2979,   171,  1690, 22280,\n",
      "           117,  2036,  3841,   734,   487,   313,  7557,   735,   143,   125,\n",
      "          3050,   117,  2101,  3273,   243,   117,   498,   222,  8163,  9258,\n",
      "           117,  3061,   371,   909, 14038,   138,   179,   117, 11004,   329,\n",
      "           125,  3378,   117,  9821, 22287,  1877,   721,   117,   449,   173,\n",
      "          5530,   366,  1647,  7226,   361,   130,  3453, 19417, 14649, 22281,\n",
      "           125,   547,  4570, 14208, 18444,   118,   176,   117,  5510, 11211,\n",
      "           348, 16437,   125, 12307,   823,   154,   598,   146, 12477,   102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[  101,   146,   329,  5458, 13981,  1462, 22279,   203,   123,  3049,\n",
      "           304,   170,   388,   125, 10497,  2896,   102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101,   146,   347, 22000, 17293,   256,   256,  1364,  6086,  1312,\n",
      "           303,   102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[  101,  2010,   873,   524,  1084,  1996,   368,   117, 12110,   214,\n",
      "           221,  4863,  2009,   180, 13353,   102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101,  2389,   296,   221, 11972,   327,  9349,   376, 19480,   260,\n",
      "          2992,  1557,   202,  1223,  1014,  1449,  1272,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101, 21494,  5855,   118,  1084, 13776,   240,  6086,  1342,  1341,\n",
      "           117,   221,   229, 22280,   598, 18250,   259,  3429, 22281,   180,\n",
      "          5028,   102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101,   418,   670,  5863,   122,  1719, 20300,   117,   122,   123,\n",
      "          1407,  1502,  2389,   296,   331,   146,   179,  1061,   376,  4551,\n",
      "           243,   125,  1084,  2010, 11368, 10497,  1149,   117,  7226,  1945,\n",
      "         15225,   179,   229, 22280, 13933,   221,  3874,   122,   230,  4678,\n",
      "           125, 16450,   304, 22280,   792,  3254,   702,  1016,   230,  6514,\n",
      "         22278,   316, 22280,  3264,  2535,   146,   179,   607, 22280,   125,\n",
      "          1434,  2990, 15520, 22290,  4419,   179,   123, 22283,   418,  3133,\n",
      "         13793, 12361,   942,   136,   122,  4332,   285,   712,  2992,   249,\n",
      "           117,  3960,   151,   370,  5028,  1014,  2601,   221, 19226,   118,\n",
      "          1084,   173, 12361,   942,   146, 12447,   397, 13083,  5723,   118,\n",
      "           146,   173, 22243, 22280,   117,  7837,  1552,   259, 10786,   942,\n",
      "           117, 10363, 19356,   286,   170,   123,  3138,  4566,  5932,  4815,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101,  2010,   230,   240, 15035,   125,  1312,   303,  3449,   146,\n",
      "          1342,   102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[  101,  1369,   582,   418,  6086,  2397,   122,   179, 21494,   370,\n",
      "          2160,   123, 11934,   304,   117,  2113,   123,  6270, 13793,  5078,\n",
      "           252,  4133,  1719,   418,  3508,   179,   122, 14136,   240,   222,\n",
      "          3429,   102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101,   449,  1977,   376,   123, 22283,   146,  7258,  4051,   125,\n",
      "          1434,  1257,   136, 16241,  1537, 22287,  2113,   122,  8911,   222,\n",
      "         12531,   179,  8625,   581,   146,   179,   659,   179,   117,   176,\n",
      "           123,   661,  8254,   229, 22280,   344,   785,  1004,  3659,   117,\n",
      "          2798,   331,   229, 22280,   176,  7674,   146,  3429,   117,   271,\n",
      "           744,  4897, 22279,   320, 17509,   146,   653,   179, 12519,   320,\n",
      "          1342,   122,  8911,  8223,   785,  1004,   146,  1223,   221,   176,\n",
      "           926, 11076,  3243,  5565, 22278,  3103, 22280,  1014,  1449,  1272,\n",
      "          3264,   122,   740,   117,   449,   229, 22280,   529,   223,   128,\n",
      "           173,   179,   418,   122,   785, 22246,   529,  6270, 22280,   143,\n",
      "           122,   785,   173,   766,  1977,  2036, 10497,   934,  4848,   229,\n",
      "         22280,   706,  7912,  3133, 13793,   221,  5530,   412, 15445,   117,\n",
      "           122,   176,   146, 10738,   229, 22280,   344,  2135, 22280,  1904,\n",
      "           118,   146,   146,  3174,  7206,  2779,  1977,   146,  1331,   122,\n",
      "           700,   125,   230, 18827,   117, 14000,   117, 12086,   229,   327,\n",
      "           223, 22280,   117, 15618,   375,   271,   146,  2004, 22280, 15520,\n",
      "         22290,   268,   117,   222,  4405,   583,  6720,  2055, 22280,   179,\n",
      "          1011,   202,  1690, 22280,  2010,   179,  2826, 22280,  2779,   136,\n",
      "           329,   418, 12361,   942,   125, 20300,  3413,  2684,   122,   230,\n",
      "          5664,   179,  2983,  9066,   444, 21494, 12862,   240,   792, 12268,\n",
      "         19179,   123,  1449,  1272,   423,  1341,  2368,   122,  6456,   118,\n",
      "           123,   229,  1359,   179,   740, 10348,   700,   117,  6335,   222,\n",
      "          5450,   452,   463,  3621, 22280,   117,   122,   179,   176,  1413,\n",
      "          2249,   495,   739,   102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[ 101,  327,  256,  118,  176, 1004, 1075,  125, 3767,  320,  347, 6487,\n",
      "          170,  123, 6629,  102]])\n",
      "DEBUG: Tokenized sentence 78: tensor([[  101,  2010,   179, 16387,   125,  3495,   102]])\n",
      "DEBUG: Tokenized sentence 79: tensor([[  101, 10355,   146,  1054, 22285,  4803,   124, 22280,   117,   221,\n",
      "           214, 10552,  2646,   975,  1885,   185,   171,  1160, 12617,   125,\n",
      "         13353, 12043,   179,   176,   273,   243,  1609,   256,   229,   543,\n",
      "           194,   304,  2461,   102]])\n",
      "DEBUG: Tokenized sentence 80: tensor([[  101,  2010,  1719,   418,   670,   179,   176,  5229,  2535,   117,\n",
      "          4492,  4141, 13793,  2415, 22280,   117,   744,   229, 22280,   122,\n",
      "          7122,   102]])\n",
      "DEBUG: Tokenized sentence 81: tensor([[ 101,  122, 8500,  123, 8054,  221, 4271,  102]])\n",
      "DEBUG: Tokenized sentence 82: tensor([[  101,  2166,  1341, 19386, 14533,   118,   176,   260,  6688, 11147,\n",
      "           842,   259, 12361, 19428, 18739,   123, 15419,  4687,   117, 21692,\n",
      "           358,  5022,   682,   102]])\n",
      "DEBUG: Tokenized sentence 83: tensor([[  101,  1413, 22287,   118,   176,  9196,  2307,   320,  4848,   117,\n",
      "           498,  1256,  8483,   117,   320,   388,  3039,   117,   122, 13254,\n",
      "           721, 20788,   171, 14890,   298,  2639,   102]])\n",
      "DEBUG: Tokenized sentence 84: tensor([[ 101,  125, 2606, 2798, 4227,  102]])\n",
      "DEBUG: Tokenized sentence 85: tensor([[  101,   125,   576,   173,   625,   117,   229,  2377,  2755,   124,\n",
      "           125,   222,  1748,   703,   551,   125,  1041, 22278,   117, 10348,\n",
      "           118,   176,   170,   222,   939,   125,  2217,   117,   662,   214,\n",
      "           125,   144,  5635,   138,   975,  1885,   185,  7226,   298,   736,\n",
      "           117,   230,  9344,  6436,   252,   229,   223, 22280,  4573,   117,\n",
      "           222,   372, 22280,   229,  5065,   117,   320,  1341,   125,   230,\n",
      "         16317,  1165,   125,  6205, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 86: tensor([[ 101, 2010, 1684,  146,  653, 1312,  303, 3002, 8805,  122, 3002, 4542,\n",
      "          102]])\n",
      "DEBUG: Tokenized sentence 87: tensor([[  101,   398,  4549,  2904,   146,   329,  5458, 13981,   102]])\n",
      "DEBUG: Tokenized sentence 88: tensor([[  101,  5147,   117,   123,  1589,  3546,  9821, 15831, 22282,   240,\n",
      "          1719,   123,   670,   102]])\n",
      "DEBUG: Tokenized sentence 89: tensor([[  101,   449,   117,  1084,   202,  1338,   117, 15702,   298,   475,\n",
      "           282,   249,   179,  2405,   692,   146,  6487,   180,  1449,  1272,\n",
      "           117,  1089,  5684,  4678,  4322, 22287,   123, 15419,   117,   125,\n",
      "          5562, 22280,   221,   146,   388,   117,   123, 17897,   734,  1552,\n",
      "           221,   146,  2979,   117,   146, 13227,   303,  8697,  1056,  1859,\n",
      "           125,   549,   243,   301,   562, 15618,  1509,   271, 13540,   159,\n",
      "           784,   125,  4449,   117,   123,  9463,  6628,   117,   123,  9815,\n",
      "           304, 22280,  2124,   122, 20885, 22278,   125,  6032,   629,   635,\n",
      "           117,  1362,  8540,   122,  3974,   428,   319,   398, 13193,   702,\n",
      "           125,  5294,  8849,   822, 20733,   102]])\n",
      "DEBUG: Tokenized sentence 90: tensor([[  101,  2010,   179,   689,  2037,   310,   398,  4549,  2904,   125,\n",
      "          1160,   146,   329,  5458, 13981,   102]])\n",
      "DEBUG: Tokenized sentence 91: tensor([[  101,  2745,  3413,   418,   123, 15158, 22282,   222,  2397,  3689,\n",
      "         22280,   179,  2389,   296,   123,   333,   247,   221,   146,  1312,\n",
      "           303,  2010,  2779,  3874, 15212,   179,   792,   170,   860,  1341,\n",
      "         10719,  2415, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 92: tensor([[  101,  2010,   449,  1084,   180,   327,  1110,   607, 22280,   125,\n",
      "          1434,   146,   653,  2389,   186,  2010, 18420,   228,   117,  2113,\n",
      "         15212,   125, 11552,   423,  3907,   523,  1084,  1796,   102]])\n",
      "DEBUG: Tokenized sentence 93: tensor([[  101,  2010,   170,  1039,  5863,   122,   179,  1061,   229, 22280,\n",
      "         11610, 22287, 21340,   102]])\n",
      "DEBUG: Tokenized sentence 94: tensor([[  101,  1257,  3436, 22280,  2779,  3486,   214,   179,   146, 12531,\n",
      "          1981,   333,  1004, 12659,   117,   370,   221,   123,   327,  9652,\n",
      "           123,  5546,   154,   117,   146,   347, 11166,   125, 10832,   117,\n",
      "           449,   179,  1981,  1434,  1312,   303,   179,   176,   873,   524,\n",
      "           117,   291,   117,   318, 13793,   117,  4768,  4768,   117,   179,\n",
      "           229, 22280,  3207,   240,   123, 22283,  1977,   179,   364,  5076,\n",
      "          3495,  1368, 15134,   118,   311,   123, 11552,   240,  1061,   122,\n",
      "         20840,  2010,   146,   644,   492,   122,   179,  2354, 22279,  3189,\n",
      "         17585,   592,   118,  7911,   102]])\n",
      "DEBUG: Tokenized sentence 95: tensor([[  101,  4217,  5461,  4141, 13793,  2415, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 96: tensor([[  101,  2010,   123, 22296,  2798,  1528,   222,  1752,   102]])\n",
      "DEBUG: Tokenized sentence 97: tensor([[  101,   449,   170,  1039,  5863,   607,   125,   792,   146,   179,\n",
      "          2036,   395,   303,  4270,   221,   313,  2245,   483,   364,  7273,\n",
      "           329,  5747,  9349,   179,   229, 22280,  3804,  2765,   102]])\n",
      "DEBUG: Tokenized sentence 98: tensor([[  101,   221,   179,  1971, 12361, 13981,   117,   240,  1416,   136,\n",
      "         11972,   122,  1312,   303,   221, 21158,   122,  1312,   303,   125,\n",
      "           854,  2028,   173,   576,   125,  1485, 11665,  4878,   486,   117,\n",
      "          9141, 22281,  5787,   123,  7187,   592,   118,  7911,   102]])\n",
      "DEBUG: Tokenized sentence 99: tensor([[  101,  2010,   122, 13776,  2249,  7707,  3687,   102]])\n",
      "DEBUG: Tokenized sentence 101: tensor([[  101,  1407,  1467,  5357,   682,  9361,  5684,   125, 11330,   117,\n",
      "           179,  4366,   146, 21244,   171,   179,  4366,  5022, 10537, 22281,\n",
      "           122,   179,  1146,  6202,   221,  1028,  4486,  4048,   179,  2364,\n",
      "         17070,  2389,   296,   117,   122,  1941,   123,  2914,   576,   179,\n",
      "          6086,   179,  1369,   418,  5308,  9322,   146,  3235,  1703,   170,\n",
      "          3901,  4141, 13793,  2415, 22280,  1767,  1945,   201,   117,   123,\n",
      "         17097,   956,   117,  1139,  1359,   692,   102]])\n",
      "DEBUG: Tokenized sentence 102: tensor([[  101, 16819,  2592,  4047,  1100,   102]])\n",
      "DEBUG: Tokenized sentence 103: tensor([[  101,  2010,   122,  2354, 22279,   117,   176,  2779,   146,  5357,\n",
      "           117,  1996,   700,   146, 12447,   397,   117,  5896,   118,   176,\n",
      "           329,   221,   123,   418, 15976,   136,   102]])\n",
      "DEBUG: Tokenized sentence 104: tensor([[  101,  2010, 12931,   229, 22280,  2678, 22283,   125,  4412,  1084,\n",
      "           229,   651,   940,   117,  1226,   146,  1312,   303,  5863,   102]])\n",
      "DEBUG: Tokenized sentence 105: tensor([[ 101, 2010,  122,  123, 9652,  117, 4049,  303,  118,  123, 2779,  136,\n",
      "          102]])\n",
      "DEBUG: Tokenized sentence 106: tensor([[  101,  2010,  1257,   122,   179,   123,  2606,   122,  1977,   123,\n",
      "           659,   449,   260, 18937, 17699,   118,  2036,   180,  5304,   102]])\n",
      "DEBUG: Tokenized sentence 107: tensor([[  101,  2010,  1502,   418, 10371,   146,  3907,   523,  2607,  8728,\n",
      "           203,  4141, 13793,  2415, 22280,   117, 20089,   125,   179,   229,\n",
      "         22280,  4207,   117,   240,  3338,   117, 12119,   159,   222,  2397,\n",
      "         13340,   102]])\n",
      "DEBUG: Tokenized sentence 108: tensor([[  101,   122, 17662,  1084,   125,   898,   221,   898,  1112,   259,\n",
      "         17080, 17585,   592,   118,  7911,  4706,   118,   311,   118,   320,\n",
      "           123,  1956, 21305, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 109: tensor([[  101,  2745,   311,  1968,   173,  1105, 22354,  2010,   318, 13793,\n",
      "         15060, 21894, 22281,   136,   102]])\n",
      "DEBUG: Tokenized sentence 110: tensor([[  101,  2010, 15060, 21894, 22281,  2010, 21174, 22032,   252,  1434,\n",
      "           123,  5896,  2028,   136,  2010,  1790,   653,   117,   176,  8204,\n",
      "           140, 15212,   222,   271,   243,   179,  2036,   607,   125,  1945,\n",
      "          2430,   102]])\n",
      "DEBUG: Tokenized sentence 111: tensor([[  101,   122,   146,  5492, 22280,  5561,   102]])\n",
      "DEBUG: Tokenized sentence 112: tensor([[  101, 17891,  6515,   118,   219,   268,   102]])\n",
      "DEBUG: Tokenized sentence 113: tensor([[  101,   122,  1369,   640,   364,   214,   146,  6793,   117, 11092,\n",
      "         14754,   228,   229,  5675,   171,   853, 18983,   162,   170, 15512,\n",
      "         13793,   320,  4707,   171,   549,   713,   102]])\n",
      "DEBUG: Tokenized sentence 114: tensor([[  101,  2010,   123, 22296,   122,  3295,   271,  2354, 22279,   176,\n",
      "          3196,   136,  2010, 15605,  1885,  1211,   117,   221,   146,  6202,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 115: tensor([[ 101, 2010, 6202,  123, 4023,  102]])\n",
      "DEBUG: Tokenized sentence 116: tensor([[  101,   327,  2606, 11348,   136,  2010,   122, 11348, 17124,   117,\n",
      "          1141,  7258,   102]])\n",
      "DEBUG: Tokenized sentence 117: tensor([[  101,  2010,  1004,   117, 11185,   128,   792,   118,  2036,   230,\n",
      "           964,   324,   102]])\n",
      "DEBUG: Tokenized sentence 118: tensor([[  101,   122,   146, 12447,   397, 13186,   748,   123,  4303,   171,\n",
      "          4707,   180,   418, 15976,   117,   125,   582, 20954,   117,   271,\n",
      "           125,   230,  9196,   747,  1718,  2579,   179,   176,  1014,   321,\n",
      "           117,   230,   475, 17647,   285,  8833,   117,  4410,  1994,  1510,\n",
      "         22087, 12717,   185,   123,  1718,  2349,   304, 22280,   125,   233,\n",
      "          2841,   122, 11451,  5925,  2204,   285,  8742,   214,   320,   969,\n",
      "           102]])\n",
      "DEBUG ======================\n",
      " len vetores 118 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.0100908173528117\n",
      " Coesão Score Final: 0.5050454086764059\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'isto e', 'nem', 'ou', 'ora', 'quer', 'senao', 'como', 'quanto', 'em vez de', 'ao passo que', 'para que', 'porque', 'com efeito', 'por exemplo', 'tanto', 'quanto', 'se nao', 'a proporcao que']\n",
      " Número de conectivos: 30\n",
      " Número de sentenças: 119\n",
      "======================\n",
      "Resultados para preprocessado_o_cortico_aluisio_azevedo_cap_4.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'porem', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'isto e', 'nem', 'ou', 'ora', 'quer', 'senao', 'como', 'quanto', 'em vez de', 'ao passo que', 'para que', 'porque', 'com efeito', 'por exemplo', 'tanto', 'quanto', 'se nao', 'a proporcao que'], 'num_conectivos': 30, 'proporcao_conectivos': 0.01, 'similaridade_media': np.float64(1.0), 'num_sentencas': 119}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,   495,   222,   644,  3508,  1165, 11551,   122, 10363, 19356,\n",
      "           286,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   123,  6754,   651,   125,   629, 22280,   599,   145,   171,\n",
      "         15273, 13808, 22280,  9821,   318,   141,  1326,   328,  1440, 22280,\n",
      "          6938,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,  1821,   179,   176,   229, 22280,  4207,  5197,   123,  4768,\n",
      "           260,  8483,  5490,   285,   692,   260, 20699, 10780,   138,   122,\n",
      "           259, 19068, 14881,   143,   395,  5548, 14533, 13793,   969,   271,\n",
      "         14563, 20556,   117,   260,  7367,  2365,  7871,   604,  3391, 22280,\n",
      "           143,   125,  5731,   661,   328,   260,  7438,   366,   388,  6254,\n",
      "           514, 22287,   176,   311, 15135, 22287,   260,  3883,  1149,   121,\n",
      "         22361,  6205, 22278, 19407,  1315,   474,  3514,   123,  1364,   146,\n",
      "         16423, 22279,   117, 17409,   348,   259,  9078,   501,   122,   259,\n",
      "           468,  3611, 17892, 22281,   117,   173,  8037,   138,   125,  7924,\n",
      "           122, 11351,  3456,   421,  4851,   117,  5808,   923,   834,   118,\n",
      "          4569,  7595,   260,  4103,   221, 16386,   371, 22281, 10137,  1908,\n",
      "           122,   259,  3806, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   173,  8128,  2038,   229, 22280,   176,  7339, 12043,  8410,\n",
      "           229,  4768,  2745,  1011, 14777,   243,   117,  2251, 22282,   155,\n",
      "          1859,   331,   259,  7967, 22281,  9144,   260, 18937,   221,   146,\n",
      "         14890,   291,  6952,   692,   202, 12650,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   123,  1174,   304,   180, 14161, 10072,   222,   388,   968,\n",
      "          2152,   130,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   125,   222,   504,  2152,   130,  3061,   371,   415,   117,\n",
      "           125,  4303,   122, 11471,   117,  8362,   228,   118,   176,  2510,\n",
      "           580,   259, 15078,   207,  9525,   381, 12443,   125,   230,  2551,\n",
      "           122,   230,  4410,   964, 12569,   304,   122,   870, 11346,  1378,\n",
      "           117,   125,  2606,   117,  8032,   173, 18612,   735, 22279,   123,\n",
      "          1112, 14350,   215, 15045,  3552, 22278,   495, 11791, 22354,   171,\n",
      "          1342,  1341,   180,  1174,   304,   117,   230, 13305,  7492,   117,\n",
      "           792,  3281,   240, 12026, 22280, 20860,   125,  4561,   117,  5980,\n",
      "         22280,   117,   176,  3471, 22280,   117, 13140,   125,  5052,   122,\n",
      "         14021,   240,   230, 18637,   125,  1253,  1149,   117,  2533,   339,\n",
      "          2836,   173,  5902,   785,  9721,   487,   122,   949,  1601,  8551,\n",
      "           303,  1112, 10692,  1655,   117, 11989, 22281,   122, 16450,   304,\n",
      "         22280, 22361, 22361,   495,   230, 12447, 17124,   975,  2320,   125,\n",
      "          1151, 22283,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   260,   854,  2028, 22281,  1444,   138,   117,   170,   260,\n",
      "           337,  4029, 13808, 22281,   516,   470,   423, 12215,   125,  2840,\n",
      "           702,   260,  2526, 22282,  1557, 17883, 22281,   117,   260,  3049,\n",
      "          1149, 19188,  2290,   591,   423,   969,   117,   123,  6614,  1664,\n",
      "          1378,   259,  9427,  7075,  2552,  6674,  3173,   382,   122,  1664,\n",
      "          4607,   117,  4063,   923,  7222,  4838,  8257, 14533,   117,  4276,\n",
      "          7533,  5848,   421,   501,   125,  1798,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   222,   291,  1342,  4712,   117,  6751,   412,  4096,   125,\n",
      "          5197,   117, 13910,   256,   123,  4768,   117,   327,   243,   117,\n",
      "          6367,   117,   870,   439,  3897,   201,   117,   123, 15419,   125,\n",
      "           222,  5223,  1690,  7817,   118,   125,   118,   969,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   259,   329,   143,   117,   860, 11739,  1676, 14790,   401,\n",
      "           117,  2365,   169,  3005,   179,  9821, 22287,  2510, 13513, 22281,\n",
      "          3773,   117,  4785, 11865, 22281,   218,   909,   117,  1623,   679,\n",
      "         22287,   146,   388, 19608,  1623,   545,   259,  1253,  8156,   382,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   320,  5533,   117,   221,   260,  5099,   125,   629, 22280,\n",
      "          9196,  4443, 13793,   117,  8362, 22278,   118,   176,  7904, 17170,\n",
      "           159,  1112, 11902,   125, 12669, 22278,  8037,   138,   390, 20128,\n",
      "          3141, 22354,   260, 21734, 22281,   117,   529,   163,  8860,   591,\n",
      "          7695,   562,   117,  1718,  2349,   256,   222,  2041,   397,   417,\n",
      "           130,   125,  6459, 13793,   180,  2480,   122, 20780,   403,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   146,   163,  8860, 17892,   117,  3791,  4409,   498,   146,\n",
      "          3568,   304, 22280,   117,   144,  5689, 21307,   123,   327,   827,\n",
      "         14960,   304,  1623,  1471,  7494,   154,   117,  1334,  1653, 17598,\n",
      "           146,   347, 21024,   122,  9089,  2646,   766,   273,  2848,   303,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   180,  7815,   125, 11842,   856,  8723, 22280,   194, 20766,\n",
      "         22287,  1719,   123,   651,   259, 10195,  2401,   680,   909,   122,\n",
      "         10537,  8468, 22281,   125,   230, 10187,  8149,   324,   117,  6094,\n",
      "           214,   179,   259, 17484,  2041,  8041, 22287,   171,   528,   221,\n",
      "          1084,  2500,  1096, 22287,   117,   803, 20733, 22281,   122, 13086,\n",
      "         22281,   125,  3316,   117,   260, 11371,  1402,   117,  1821,  1485,\n",
      "         20269,   117,   785, 13717,   591,   117,   146, 20860,   229,  3049,\n",
      "           304,   117, 14966,  8409,   259, 15618,  1409,  1896,  1046,  8574,\n",
      "          1522,   122,   260,   437,   470,  2134, 10491,  1922,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,   123,  7815,   739,   122,   123,  4768,   180,  4595,  6577,\n",
      "         11814, 17226,   170,   146,  4745,   180,   651,   117,  2113,   495,\n",
      "         22278,  3365,  5314, 13776,   123,   125,   636,  2115,  2791,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   173,  1485,   260, 15512, 22280,   143,  9993,   692,   118,\n",
      "           176,  2217,   143,   492,  1165,   442,   122, 21804, 22281,  9993,\n",
      "           692,   118,   176,   259,  7769,   202,  5955,   183,   122,   259,\n",
      "         11314,  2650,  1058,   179,  2072,   173,  1312,   303,   229,  4768,\n",
      "         20383,  8521, 14533,   259, 12581,   382,   118,   629,   942,   117,\n",
      "           125,   235,  3198,   332,   243,   117,  1253,   455,   308,   529,\n",
      "          1632, 18354, 22281,   122,   538,   331,   256,   942,   240,  1491,\n",
      "         18048,   272,   233,   141,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   259, 17478,   207,   125,  5976, 11367,   387,   692,   117,\n",
      "           123,  9378,  3377,   171,   969,   117,   259,  7769,   122,  3848,\n",
      "         17714,   143,   179,  1369,  1009,   692,   221,   333,  8515,  2428,\n",
      "           692,   118,  7707,   259, 12141,   117,   259,  1143,   122,   260,\n",
      "          2477, 10661,  9144,   118,  7707, 14179,   498,   526, 14360,   470,\n",
      "           117,  3985,   923,   118,  7707,   170,   123,  4351, 14139,   171,\n",
      "          1690,  7817,   538, 20462, 22281,   122,   529,   144,  3706,   117,\n",
      "          2000, 11005,   118,  7707,   146,  8773,   285,  3270,  5187,   406,\n",
      "           117,   271,   176, 19864,   123,  8977,  9701,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   229,  1105,   180,  1174,   304,   117, 15702,   366, 13501,\n",
      "           214,  1908,   117,   529,  4303,   591,   298,  9098, 22281,   117,\n",
      "           420, 18720, 22281,   125, 11314,  1604,   143,   125,  2992,  1309,\n",
      "           138,   122,  3985,  7796, 10863,   117,  5489,  2916, 22287,   118,\n",
      "           176,   637,  1387,   247,   117,   146, 14701,   171,  3233,   285,\n",
      "         22280,   117,   123,  4581,   171,   417,  8497,   159,   117,   123,\n",
      "         17270,  1165,   298, 16453,   128,  4644,   781,  4387,  1409,   662,\n",
      "         14455,   143,   207,   211,  8980,  3907,  2698,   117,  9144,   669,\n",
      "          3391, 22280,   143,   117,  4363,   923,   117,  4624,   692,   117,\n",
      "         14915, 22287,   125,  4565,   892,   159,  7226,   712,   736,   117,\n",
      "           170,  5747,  1062,   252,   125,  9349,   125,  3907,  2698,   117,\n",
      "         12402,  1532,   237,  7518,   331,  2866,  4708,   214,  1690,   266,\n",
      "          1149, 15448,   117,   449,   210,  9378, 12908,  2028,   125,  8286,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   259,  2241,   326,  1044,  8954,   692,   173,  4410,  2729,\n",
      "           146, 14701,   366, 12817,   117,   170,   230,  2042,   310, 19519,\n",
      "           243,   125, 16714, 10355, 22287,  1112,  3002,   118,   646,   145,\n",
      "          1112,   173,   576,   125,   592,   118,  7911,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,   123,  4303,   298,  2241,   326,   828, 15101,  3309, 18444,\n",
      "           118,   176,   259,   179, 11666,  8977,   122,   259,  2281, 12898,\n",
      "           128,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,  4063,   151,   222,  8833,   122, 15618, 15335,   157,  1757,\n",
      "           309,  9437, 22287,   272, 14258,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   146,  2241,   326,   458,   978, 13613,   128,   125,  5708,\n",
      "         17120,   125,   528,  2846,   173,  5078,   268,   117, 10552,  2646,\n",
      "           117,   146,   388, 12424,   319,   117, 15245,   170,   146,  4332,\n",
      "           303, 19767,   222,  1945,  1156,   125, 13850,  3391, 22278,   117,\n",
      "           291,   117,   170,  1780,  1169,  5635,   201,   117,   188,   551,\n",
      "           304,   256,   170,   586, 17335, 22282,   259,  9196,  1044,   125,\n",
      "         20080,   122,   125, 10985,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   122,   117,   625, 19288,   123,  2880,   151, 22280,   125,\n",
      "         21761,   123,  7698,   117, 18968, 22278,  2134, 19356, 22280,  1615,\n",
      "          1176,   117,  9247,  1552,   117,   122,   870,   509,  3985,   151,\n",
      "           146,   528,  2846,   170,   739,  1923,  1259,   117,  9721,  1552,\n",
      "           123,  4410,   173,   222,  5902,  6916,   201,   122, 15777,  9238,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,  1413, 22287,   118,   176, 17579,  2854,   412,  1174,   304,\n",
      "           259, 19601,   639,   122,  9222,   497,  2935,   472, 10164,   479,\n",
      "           298, 20177, 22281,  1413, 22287,   118,  8742,   483,  1149,  3240,\n",
      "         22282,   266,   555,   122,   273, 17845, 15770, 22281,   117,  3746,\n",
      "           185,  9420,   233,   141,   240, 15702,   171,  1690,  7817,   125,\n",
      "           423,  3979,  2552,   125,  2561,   304, 22280,   117,  9463, 22281,\n",
      "           834,  4351,   339,   272, 13880,  7137,   591,   423,  6938,   117,\n",
      "           337,  4029, 13808, 22281, 14657,   470,   122,   327,   591,   229,\n",
      "         14790, 22278,   125,   235,  3198,   125,   607, 13150,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,  2337,  1657, 22278,   418,  3546,   117,  4460,   179,   222,\n",
      "          1971, 16044,   328,   117,   495,  1250,   122, 17276,   567,  2684,\n",
      "           259, 10926,   637, 10758,   117,   179, 16420,  1266,  1369, 16386,\n",
      "           140,   146,   644,   117,   122,   259, 11314,  2650,  1058,   117,\n",
      "           179,  1112,  9144, 21340, 22354,   122,  2684,   259,  2004,   128,\n",
      "          1447,  1530,  2184,   827,  3895,   117,  1183, 21206, 22287, 13880,\n",
      "         18982,   340,   122,  8367,  3679, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,   123, 17935,   404,   171,   425,   825,   125,  5009,   178,\n",
      "          9805,   285,   117,   230, 17935,   404,  7453,   122,   834,   344,\n",
      "           157,   202, 12639,   117,  4513, 20840, 22281,  1984,  1708,   122,\n",
      "           259, 11314, 15546,   179, 12595,   692,   260,  4117,   842,   117,\n",
      "           978,   222,  7316,   325,   291,  1528, 13779,  1102,   303,   170,\n",
      "           123,   327,  5829,  3122,   498,   146,  2187,   475,  2733,   421,\n",
      "           122,   260,   675, 16776,  2191,  2157,   401,   125,  6183,   118,\n",
      "           332,   145,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,  1719,   740,  2751, 22278,   221,   146,  6151, 22290,   117,\n",
      "         15541,   122,  1639,   117,   582,   117,   123,  1439,   903,   125,\n",
      "           969,   117,   176, 14689, 22282, 18444,   924, 12448, 22281, 13779,\n",
      "          2148,  8172, 22281,   122,  7282,  2836, 18618,   246,   389,   372,\n",
      "           256, 22280,   180,  2480,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   260,  7367,   117,  6688,   591,   125, 20459,  3103,  4966,\n",
      "           122,   117,   221,   146,  2979,   117, 19853,   125,  1798, 20367,\n",
      "           117, 15245, 22287,   117,   538,   532,  7462, 18968,   442,   125,\n",
      "          6578,   125,   329,   304,   117,  1089,  4488,   834, 17681,   117,\n",
      "          8764, 18048,  7352, 16829,  7485, 19806,   923,   123,  3138,  4141,\n",
      "          6069,  1908,   125, 14790,   138,  1401,  2208,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   320,  1341,   117, 11170,   214,   123,  9317,   125, 14890,\n",
      "           117,   305,  6557,  2836,   118,   176,   222,   415,   268,  4857,\n",
      "           342,   125,  1941,  5105,   404,   661,   286,   117,   785,  1004,\n",
      "          5687,   117,   170,   260, 20699, 10780,   138,  1004,  1743,  1708,\n",
      "           117, 21346,   243,   260,  5731,   236,   260,   240,  1927,  3503,\n",
      "           125, 10303,  6808,   123,   222,  2242,  4678,  4322,   117,  6969,\n",
      "          2647,   229,   327,  8097,   125, 13718,   268, 19876,  4029,   852,\n",
      "           243,   117,   230,   223, 11198,   125,  3766,   124,   125,  2702,\n",
      "         17790,   181,   117,   366,  2796,   179,  5063,   320, 15273, 13808,\n",
      "         22280,   538, 16462,   366,  4496,   138,  1141,   735,  1573, 14533,\n",
      "           118,   176,  1256,  2595,   125,  4031,  6034,   117,  7501,   173,\n",
      "          8817,  5462,   260,   418,   303,   143,   171,   622,   975,  1885,\n",
      "          3640, 22280,  5825,   118,  7406,   304,   222,  5973,   439,   247,\n",
      "           125,  5820, 14368,  2836,   949,  1601,  8551,  2227,   403,   123,\n",
      "           327, 15231,   565,   171,  3846,   125,   222,  7164,   183,   122,\n",
      "         12110,   256,   221,   260,   924,  2856,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[ 101,  924, 2856,  180, 1373,  102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,   229, 22280, 14940,   117,   744,  2437,   351,   498,   123,\n",
      "          9317,   123,  7406,   304,   179,  6202, 22278,   320, 16960,   303,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,   230, 16317,  1165, 16509, 22278,   117,   170,  7226,  8197,\n",
      "           125, 10832,   125, 21990,  2204, 11214,  1228,   256,   123, 20390,\n",
      "           292,  7871,   604,  7909,   179,  8940,   171,  6151, 22290,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,  2002,   148,  1956,   247,   266,   117, 10090, 21794,   420,\n",
      "           260,  9751,  2811,  1341,   117,  1224,   215,   952,   256,   222,\n",
      "          9679,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101,  5057,   466, 14960,   304,  2765,  1369,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,   123,  5401,   304, 22280,   171,   475,  2733,   421,  1314,\n",
      "           207,   304,   256,   146,   388,   180, 17935,   404,   122, 10348,\n",
      "           320,  3580,   222, 19561,  1623,   300,   122,   305, 19806,  4812,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,  1021,   123,   163, 10359,   304, 22280,   298,  1564,   238,\n",
      "          1973,   244, 22281,   117,   230,  6272, 10497,   375,   125, 16325,\n",
      "           259,  5708,   122,   687, 18775,   260, 11351,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,  1084,   975,  1885,   185,   117,   529,  7414, 21536, 22281,\n",
      "           171,  2187,   117,   123, 22243,  1337,  4655,   154,   304, 22280,\n",
      "           171, 18718,   180,  5825,  1009,   256,   123, 14832,  7799, 19988,\n",
      "           470,   498,   146,   853,   314,   117, 15702,   366,  8037,  3897,\n",
      "          1402,   260,   388,  6254,  9821, 22287,  8925,  2607,  1962, 22279,\n",
      "           259,  4332,   942,   117, 12294,   123,  9349,   221,   123,  1945,\n",
      "           148,   437,  6720, 15182,   366,   675, 15419, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,  2010,   318, 13793,   117,  9480, 14948,   117,   179,   311,\n",
      "         12416, 22281,   136,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101,  1996,  5009,   178,   117,  1241,  2746,   118,   176,   325,\n",
      "           229,  9104,   173,  5790, 22279, 16995,  3791,  4409,   117,   123,\n",
      "          3049,   289,   364,   180,  9317,   117,   173,  2375,   180,  2267,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,  1004,  4178, 22281,   179,   437,   229, 22280,   598,   342,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[ 101, 6532,  677, 2982,  117, 6532,  102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,   449,   117,   173,   652,  1247,   117, 14558, 22287,  4945,\n",
      "           176,   368,   122,   171,   437, 22288, 10303,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101, 19821,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,   117,  3887,   795, 14948,   229, 22280,  9396,   122,  3449,\n",
      "           785,   173,   483,  3301,   285,   117,   271,  1011,   117,   123,\n",
      "           577,  7177,   425,   123,  8993,  5635,   118,   125,   118, 14948,\n",
      "           298,   532, 17084,   260, 12821,  3613, 22281,   125,   372, 22280,\n",
      "           179,  8544, 13632,   498,   123,   374,  3613,   102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,  5009,   178,  1449,   157,   180,  7296,   256,   117,   325,\n",
      "          1420,   240,  5009,   178,  9805,   285,   117,   495,   222,  1456,\n",
      "           143,   125,  7226, 11330,   481,   117,  2124,   117,  6367,   122,\n",
      "         17509,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101, 10355, 22287,   118,   202,  7304,  5872,   221,   146,  1847,\n",
      "           523,   122,  3695,   171,  1010,   215,   102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101, 14971,   180,   327,  8092,   529,  2856,   125, 21158,   117,\n",
      "          5670,   256,  3953,   375,   246,   259,  6349,   333,   501,   180,\n",
      "         11126,   351,   122, 16653,  1089,   125, 21990,  2204,   102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[  101,   173,  3265,  3285,  3730,   118,  2036,   229,  3049,   304,\n",
      "          2858,   501, 12877,   171,   610, 22280,   143,   122,   229, 22280,\n",
      "          2036, 12862,   228,   125,  1364,   146,   655,   125,   736, 14720,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[  101,   466,   852,   256,   170, 15406,   352,   714,   146,   528,\n",
      "          1374, 14772, 18006,   117,   125,  1977,  9679,  1615, 15631,   243,\n",
      "           470,   122,   978,   230,  8526,   202, 10862,  1456,   143,   117,\n",
      "           123,   615,  2036,  8160,   256,  1528,   123,   368,   171,   455,\n",
      "           123,  2267,   117,   179,   495, 13745,   423,  4313,   102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101,  5009,   178,  1449,   157,  1796,  7444,   170,   230, 17704,\n",
      "           125, 20482,  2443, 22278,   117,  2052, 22232,   795,   117,   785,\n",
      "         13823,  1337,   122,   117,   271,   123,  1407,   670,   366, 15273,\n",
      "         22285, 21817, 22281,   117,  4493,   285,   173,  2038,   125,  1768,\n",
      "           151, 22280,   625,  2931,   117,  2789,   173,   191,  1655,  2139,\n",
      "          5976,   123,  7308, 17704,   171,   505,   283,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101,  1004, 12448,   262,  1921,  1405,   451, 22278,   117,  1971,\n",
      "           221,   146,  4970,   378,   271,   221,   123,  2267,   117,   438,\n",
      "         12190,   251,   144,   343,  6436,   252,   117, 13776, 18187,   325,\n",
      "         11736,   171,  2101,   159, 22280, 17883, 22290,   102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101,  3876,   596, 19575, 22287,   202,  3420,   739,   117,  1532,\n",
      "           504,   508,   367,   952,   117,   221,   582,   123,  3848,   687,\n",
      "           151,   125, 22232,   795,   259,  4915, 22278,   173,  3344,   125,\n",
      "         17246,   325,  7489,   992,   128,  5009,   178,   117,   240,   210,\n",
      "           117,   179,   371,  1941,   318, 13793, 19317,   175,   122,   978,\n",
      "           146,   347,  5209,   210,   229,  7815,   739,   117,  3171,   118,\n",
      "           176,  2044,   170,   123,  3113,   221,   259, 17378,   243,   180,\n",
      "          4768,   180,  4595,   117,   173,  8764,  7067,  4041,   574,   256,\n",
      "           117,  1021,  1027,   481,   117,   202,  1847,   523,   125, 12466,\n",
      "           240,  7137,  1196,   102]])\n",
      "DEBUG: Tokenized sentence 50: tensor([[  101,   221,   229, 22280,  4412,   331,   170,   123,  2267,  1112,\n",
      "           179,   176,  5057,   230,  2606, 22354, 13221,   123,   331,   522,\n",
      "           117,   121,   102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[  101, 22232, 22278, 17897,   124,   117,   123, 12764,   348, 10656,\n",
      "           146, 20975,   247,   173,   179,  9584,   122,   925, 11460,   170,\n",
      "           368,   122,   325,   123, 17962,   102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[  101,  1112,   123,  9586, 11736,   125,   614,   210,   179,  6205,\n",
      "           562,   236,   117,   179,   123,  4337,  4886,   222,  2397,  2364,\n",
      "          4207,  6202,   221,  3867,  4486,   122,   117,   176,  2589,   123,\n",
      "          3285,   140,   173,  1105,   809, 14501,  6654,   124,  2010,  7343,\n",
      "          4062,  9631,   249,  2010,   179,   229, 22280, 13105,   228,   240,\n",
      "           123, 22283,   136,   102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[  101,   202, 15273, 13808, 22280, 18402,   118,   176,   125,  2745,\n",
      "           121,   102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101, 22232, 22278, 17897,   124,   179,   176,  2121,  4886,   123,\n",
      "          4314,   146,  8788, 22280,   122,  2589,   125,  5896,   221,   123,\n",
      "          4768,   180,  4595,   229, 22280,  2471,   179,   176, 13147,  2040,\n",
      "           140,   102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101,  1021,   125,  2765,   271,   173,   327,  2004, 22278,  1105,\n",
      "          2010,  4062,  3147,   117,  3264,  9317,   117,   122,  9378,  4676,\n",
      "         22354,   123,  7492,  7954,   122,  1084,   262,   117,  9721,  1552,\n",
      "           259,   532, 11330,   122, 14730,   481,   117, 10478,  4765,   118,\n",
      "           176,   173,  1105,   171,  1677,   157,   117,   170,   222,  2971,\n",
      "         22280,   125,  3848, 17714,   143,   117,   675,   854, 22281,   117,\n",
      "           122,   170,   259,   329, 17738,   249,   744,   171,   596,   171,\n",
      "           975,  5242, 22280,  4170,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101,   173,  1381,   301,   117,   240,   210,   117,   146,  4062,\n",
      "          1456,   143,  1011,  3456,  2040,   286,   171,  6793,   179,  1270,\n",
      "         22278,   121,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[  101, 22232, 22278, 17897,   124,   117,  2440,  1907,  4838,   183,\n",
      "         13779,   430,  1337,  2440,   125,   229, 22280,  5197,   171,  3147,\n",
      "           834,  2477,  1004,  2377,   185,   251,   117,   834,  2036,  3207,\n",
      "         22282,  3484,   298, 13850,  2552,   272, 17678, 13305,   117,   170,\n",
      "           179,   740,  5088,  2876,   392,   256, 10907,  1378,   246,   146,\n",
      "          9169,   432,   381,  1655,   122, 13376,  3173,   183,  2440,   171,\n",
      "           347, 18042,  1718,  2248,   412,  2567,   122,  2440,   366, 13544,\n",
      "         22281,   179,  5848,   256,   240,   644,   117,   121,   102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[  101, 22232, 22278, 17897,   124,   117,  2440,   125,  2745,  1257,\n",
      "           117,  5197, 22278,   118,  2036,  1112,   223, 10502,   125,  1105,\n",
      "         22354,   102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[  101,   495,   230, 18928, 22278,   230, 11744,  1516, 10348,   538,\n",
      "          5976,   240, 19877, 22280,   122,   240, 10303,   331, 18402,   123,\n",
      "          9247,   578,   122,   117,   625,   176,  5078,   252,   123,   646,\n",
      "         22290,  2430,  2010,  4023,   538,   417,  6915,  2010,   117, 20990,\n",
      "           285,   256,  1719,   123, 13219,  2028, 10671,  4496, 20383,   178,\n",
      "         15662, 17897,   124,   978,   146,  6149,  1903,   366, 20991, 15273,\n",
      "         22285, 21817, 22281,  8934,   229,  7698,   102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101, 14915,   785,   298,  1938,   128,   117,  1821,  5179,  4966,\n",
      "           785, 17275,   375,   785, 13086,   125,   440,  5345,  1522,   125,\n",
      "          5052,   102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101,   625, 18402,   538,  7967, 22281,   117, 10355,  1112,   259,\n",
      "          5980,   128, 22354,   122,   117,   625,   176,  1822,   151,   123,\n",
      "           222,  1124, 14273, 10355,  1112,   146,  5351,   124, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101,  1684,  1796,  1016,   122,   117,   271,  7427,   154,   117,\n",
      "           229, 22280,  1021,  1858,   173, 20482,  2443, 22278,   117, 18424,\n",
      "          1382,   148,  5246,   125, 19462, 17897,   124,   122, 18237,   256,\n",
      "           123,   327, 17479,   406,   123,   184,  4803,   123, 22283,  1485,\n",
      "           260, 13674,   117,   173,  5553,   117,   125,  4332,   942, 13059,\n",
      "           117,   260,  1176,   162,   705,   308,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[  101, 10201,   256,   118,   176,   170,  1491,  4217,  1058,   171,\n",
      "          4170,  1112,   171,   347,  4141, 13793,  9935,  3093,   183, 22354,\n",
      "           222,  1456,   143,  2135, 22280,   117,   125,  5708, 16467,  4256,\n",
      "         12764,  3152,  7406,   444,   102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[  101,   860,  4141, 13793,  9935,  3093,   183,   262,  1910,  2251,\n",
      "           697,   122,  2080,   123,  1434,  3933,  3602,   304, 22280,  1538,\n",
      "           229, 11098, 14996,  1161,   180, 11126,   351,   102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[ 101, 2931,  170,  146, 4460,  125, 9447,  102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101, 22232, 22278, 17897,   124,   978,   739,  8296,   304, 22280,\n",
      "           954,  4966,   117,  9713,   256,   118,  7707,   222, 18624,   834,\n",
      "          4453,  2893,   117,  5674,   151,   118,   259,   173,  2745,   712,\n",
      "          4594,   102]])\n",
      "DEBUG: Tokenized sentence 67: tensor([[  101,   625,   123,  2267,   262,  1449,   328,   240,  5009,   178,\n",
      "          1449,  6031,   117,   318, 13793, 16590,  1816,   185,   202,  1847,\n",
      "           523,   180,  1855,   117,   740,  1996,   124,  1112,  1004,   320,\n",
      "          1528, 15212,   123, 11304,   125,   179,   122,  4712, 22354,   449,\n",
      "           146,  9805,   285,   229, 22280, 12123, 22288,   123,  2772,   117,\n",
      "          2798,   262,  3330,   243,   240,   740,   123,  9429,   117,   291,\n",
      "          5787, 14908,  2181,   246,   123,  2013, 16644,   117,   820,  2286,\n",
      "          1434,   125, 22232,   795,   230, 20216, 11293,  6366,  1916,  3246,\n",
      "           249,  3797,   221,   123,  2267,   102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[  101,   122,   179,   123,   273,   522,  2382,   117,  1065,   259,\n",
      "          8184,   481,   117,   744,   202, 10984, 22281,  5068,  3139, 22282,\n",
      "           130,  4776,  1845,   171,   652,  3165,   117,  1021,  2074,  1941,\n",
      "           146,  2397,   123,  1977,   327,  8410,  2471,   125, 17805,   240,\n",
      "          1719, 22278,  1069,   102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101,  1966,  2397,  1622,  1790,   229,  4131, 22278,   171, 15273,\n",
      "         13808, 22280,   117,   495,   146,   762,  2640, 22282,  4141,   236,\n",
      "           822,  1125,   243,   125,  9824,   143,   122,  7296,   256, 17504,\n",
      "         12904,   423,  1112,  5546,   211, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[  101,  1191,  1364,   146,   668,  4812,   221,  6759,   170,   368,\n",
      "           117,   449,   506,  3568,  3244,   259, 10981,  3803,   942,   117,\n",
      "          2798,   331,   173,  9429,   366,  4537,   319,   143, 12067,   585,\n",
      "           179,   117,   316, 22280,  8545,   117,  2679, 14803, 22287,   123,\n",
      "          6003,  1622,   340,   285,  3365,  4448,  3309,  7134, 15799,   117,\n",
      "           271, 14619,   210,   412,   851,  5988,  4812, 13638,   232, 22280,\n",
      "           179,  1815,  3138,  5555,   229,  2004, 10953,  8588,   584,   180,\n",
      "         10173,  6943,   102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101,  5147,   117,   146,  5831,  3914,   176,  1021,   466,  2828,\n",
      "           123,  7716,   171,   273, 15256,   243, 15273, 22285, 21817,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101,  1977, 13105, 22278,   455,  7583,  6754,   390,   304,   117,\n",
      "         14685,   122,  3293,   538,   333,   183,   143,   171,  1410,   117,\n",
      "         10500,   151,   117,   271,  1569,  2267,   366,  1491,  8193,  4610,\n",
      "           117,   123,  5923,   232,  8045,   179,   259,  2217,  7380, 16996,\n",
      "         22287,   498,   146,  5791,   183,  6435,   136, 14283, 22288,   118,\n",
      "           146,   117,   834,  4945,   240,   179,   102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101, 10500, 22278,   118,  2036,   123,   344,   304, 17037,   124,\n",
      "           171, 11552,   117,   259,  6554,   382,  6671, 17551, 22281,   171,\n",
      "           347,  1354, 14570,  2056,   690,   117,   146, 11593,   714,  9019,\n",
      "           247,   713,   180,   327,  6553,   292,   316, 22280,  2886,   320,\n",
      "          1423,   173,   179,  2968,   685,   272,  5635,   186,   118,  2036,\n",
      "           260, 16264, 20409, 22281,   122, 11744, 11891, 22281,   125,   733,\n",
      "          4861,   304, 22280,   117,   170,   179,   368,   572, 22290,  1817,\n",
      "           256,   259, 16556,  2247,   285,   327,  9019,   151,  1048,   155,\n",
      "          2647,   122,   259,  5999,   180, 19032,  1772,   122,  2745,  1257,\n",
      "           117,   834,   179,   740, 13256, 22281,   236,  1916,  9877, 22282,\n",
      "           117,  3456, 20507, 22288,   118,   123,   221,   146, 15152,   122,\n",
      "          2166, 13513,   390,   303,   170,  1364,   146,   388,   947,   171,\n",
      "           347,   652,  6532,   125,  2606,   102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101,   625,   117,   229,  4768,   298, 11935,  1530,   117,   179,\n",
      "          3876,   596,   495,   744,   222, 18142,   541,   272,   117,   146,\n",
      "           273, 13426, 22280, 11593, 22283,   117,   820,   170,  1695,   325,\n",
      "           125,  4698,   122,  1685,   481,   125,  2169,   233, 20799,   456,\n",
      "           320,   717,   339,   171,   347,  2004, 22280,  9861,   122,   180,\n",
      "           327,  9811,   124, 12067,   232,   117, 11224,   183,   117,  1796,\n",
      "          2245,   243,   117, 13140,   125,  3061,  1994,   117,   146,   679,\n",
      "           243,   240,  7226,   271,   222, 12961,   122,  2251,   825,   240,\n",
      "          8776,   444,   271,   222,  4023,   117,   123,  6754, 17704,  2789,\n",
      "           118,   176,  8771,   125,   230,   739, 12448,   852,   122,   262,\n",
      "         16009,  2146,   117,  9165,   214, 12541,   117,   122,  4949,  2996,\n",
      "         22278,   122,  1078,   576,   325, 12448,   117,  2684,  8111, 22243,\n",
      "          7424,  3885,   481,   700, 19189, 22288,  3330,   243,   102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[  101,  9480, 14948,   229, 22280,  2080,   123,  8223,   146,  5546,\n",
      "           211,   123,   223, 22279,   240,   210,   117,   785,   173, 11021,\n",
      "           117, 14705, 22278,   118,  2036,  2584,  3388,  1217,   122, 16598,\n",
      "         22282,   123, 14876,   151,   171, 10325, 22280,  6671, 17551,   117,\n",
      "          3596,   655,   125,  1658,  7278,  5723,   744,   117,   420,   259,\n",
      "          4966,   117,   123, 16001,  2557,   171,   390,  2762,   125,   977,\n",
      "           125,  1600,   125,  3919, 22302,   102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101,  1112,  7122,  2267,   117,  1996,   118,  2036,   123,   851,\n",
      "          5325,   524,   529,  5693, 13001,   180,  1386,   117,   229, 22280,\n",
      "           380,  4239, 22281,  2364,   179,   437,   504,   210,   117,   834,\n",
      "           179, 13501, 22281,  2643,   138,   146,  2397,   123,   964,  1111,\n",
      "          3878,   243,   221,  4170,   102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[  101,   229, 22280,   437,   504,   143,   202,   388, 10201,   118,\n",
      "           437,   179,   146,  2982,  1981,   333,  1684,   123,  5395,  3292,\n",
      "           272,   924, 11417,   387,   303,   143, 10984, 12569,  7485,  1086,\n",
      "           145,   102]])\n",
      "DEBUG: Tokenized sentence 78: tensor([[  101,   123,  9349,  1981,  6759,  2113,  3330,   117,   122,   229,\n",
      "         22280,   370,   125,  8650,  2113,  4226,   102]])\n",
      "DEBUG: Tokenized sentence 79: tensor([[  101,   176, 11429, 22095,   146,   179,   437,  2826, 22280,   117,\n",
      "           333,   138,  8540, 22354,  8028, 11765,   118,  2036,   179, 19790,\n",
      "         22281,   236,   117,  1652,  3179,   644,  7762, 22281,  6867, 11543,\n",
      "           436, 20010,   118,  1084,   123,  8800,  4170,   598,   347, 10303,\n",
      "           117,  7096,  8849, 22282,  2745,   117,  2745,   117,   221,  4365,\n",
      "          4327,   273,   522,   304,   117,  1953,   176,   318, 13793,  9480,\n",
      "         14948,  1941,  5971, 22281,   236,   125,  1342,   122,   240,   860,\n",
      "           117,  1141,   117,  2589,  1977,  2589,   117,   662,   555,   236,\n",
      "           259, 19091,   143, 19978,   501,   117, 18994,  9847, 22279,   123,\n",
      "          2004, 22278,  1069,   117,  2113,   495,   149,  2291,   179, 11782,\n",
      "           123,  5907,  3578,   687,   292,   272,   230,   390,   304,   102]])\n",
      "DEBUG: Tokenized sentence 80: tensor([[  101,   122,   325,   229, 22280,   506,   259, 13790,   179, 22232,\n",
      "           795,  2002,   123,  2267,   102]])\n",
      "DEBUG: Tokenized sentence 81: tensor([[  101,  9480, 14948,   495,   854,  2028,   117,   229, 22280,   259,\n",
      "         12123,   452,   339,   117,  2798,   316, 22280,  8545, 11556, 12123,\n",
      "           118, 15887,   449,   117,   316, 22280,  8642,  2072,  1061,   123,\n",
      "          1386,   180,   223, 22279,   117,   179,   123,   878,   151,  1014,\n",
      "           229, 22280,  2036,   417,  1797,   151,   123, 14876,   151,   834,\n",
      "           260,  3724,   180,  1623,   457,  4194,   102]])\n",
      "DEBUG: Tokenized sentence 82: tensor([[  101,  5009,   178,  1449,   157,   117,  2440,   125,  4062,   117,\n",
      "           495,   222,  4438,  2217,   325,   179,   313, 11883,   128,   260,\n",
      "           233, 21072,  6471, 19189, 14463,   221,  1858,  2606, 16314,  5787,\n",
      "           222,  9235, 15211, 22280,   117,   229, 22280,   221,  7583,   117,\n",
      "          3561, 15639, 18473, 19649, 22278,   117,  5533,   125,   146,   271,\n",
      "           391,   117,  1021,  5747,   576,   125,  3229,  2666,   118,  1340,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 83: tensor([[  101,   625,   176, 12524,  4970,   378,   117,   229, 22280, 10733,\n",
      "           117,   123, 21736,   180,   327,  2711,  7390,  1076,   117,   325,\n",
      "           171,   179,  4863,   273,  1289,   183,   412, 12298,   340,   125,\n",
      "           230, 20216,   170,   455,  1941,   176,   978, 19877,  3611,   243,\n",
      "          6033,   117,   229, 22280, 17662,   173,  2962,   123,  6759,   117,\n",
      "         20089,   125,   179,   146, 14477, 22280,   180,  2267,  2036,  2041,\n",
      "           702,   151,   125,   425,   124,   221, 13501,  4029,  4803,   260,\n",
      "           822, 15335,   277,   171,  1223,   117,   122,   179,   146, 13440,\n",
      "           247, 11007,   180,   331,   522,  1587,  3784,  1266,  6598,   123,\n",
      "           693,  3292,   180,   327,  1105,   122,   123,  3264,  6667,   366,\n",
      "           675, 14520,  2350,   687,   585,   102]])\n",
      "DEBUG: Tokenized sentence 84: tensor([[  101,  9480, 14948,  6469,  1502,   117,   271,   176,   873,   117,\n",
      "           420,   259,   273,   415,   128, 10671, 17689,   171,  1568,   122,\n",
      "           146, 11775,  1677,   247,   180,  1938, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 85: tensor([[  101,   744,  1016, 10698, 22278,   125,   549,   123, 14286,   478,\n",
      "           171,   331,  5312,   298,  7911,  9784, 22278,  3933,  5664,  9679,\n",
      "          1315,  1125,   579,   975,   672,   143,   122, 17318,  1261,  4242,\n",
      "         20987,  4306,   320, 14643, 22280,   122,   320,  7670,   102]])\n",
      "DEBUG: Tokenized sentence 86: tensor([[  101,   229, 22280,   495, 21964,   328,   978,   123,  8697,   232,\n",
      "         22280, 13420,   285,  9429,   117,   222,  2277, 22003,   117,   122,\n",
      "           240,  1176, 19068,   796,   124,   229, 22280,   333,   325,  2314,\n",
      "           328,   102]])\n",
      "DEBUG: Tokenized sentence 87: tensor([[  101, 18574,  1415,  2851,   272,  6205, 22290,   252, 18995,   256,\n",
      "           271,  6387,   117,   122,  1598, 13808,   125,   230,  6742, 20666,\n",
      "          7248,   125,   598, 22290,   183,   179,  5057, 10303,  9226,   102]])\n",
      "DEBUG: Tokenized sentence 88: tensor([[  101,  1971,  1016,   179,   117,   173,  3113,   117,  6202, 22278,\n",
      "          2421, 22281,  1176,   125, 18718,   180,   792,  8723,   304,   529,\n",
      "          2047,  2291,   143,   180,  1717,   143,   148,   102]])\n",
      "DEBUG: Tokenized sentence 89: tensor([[  101,   122,   259,   317, 22279,  1289,   180,   176,  1956,   581,\n",
      "           692,   118,  2036,   146,  4437,   180,  4410,   122, 21280,   118,\n",
      "          2036,  1491, 20197,  5082,   125, 13501,   214,   138,  1907,  3870,\n",
      "          1939,   314,   117,   785, 13586,  1375,   442,   529,   675,  6807,\n",
      "           117,   374, 22281,  1149,   122,  1630, 11582,   138,   117,  4330,\n",
      "           123,  3746,   148,   118, 11997,  3301,   304,   122, 17681, 22281,\n",
      "           125, 17789,   232,   102]])\n",
      "DEBUG: Tokenized sentence 90: tensor([[  101, 16149,  2880,   247,   143,   740, 16280,   118,   176,  7612,\n",
      "           175,   117,   170,   260, 21347,   505,  1817,   591,   117,   123,\n",
      "          3049,   304, 13218,   125, 19730,  8889,  3760,  2860,   117,   739,\n",
      "         10849,   202, 12413,  6995,   117,   123, 13071,   125,  4654,   934,\n",
      "           387,   102]])\n",
      "DEBUG: Tokenized sentence 91: tensor([[  101,   122,   117,   785,   606,   252,   117,   169, 12190, 22278,\n",
      "           298,   532,  3922, 22280,   143,  7642,   124,   154,   122,  2987,\n",
      "           122,   366,   675,  8574,  2191, 10180,   125,  1798, 13793,   122,\n",
      "         13083,  9546, 22290,   252,   117, 18597,   256,  8727, 11074,   122,\n",
      "          8540,   202,  1423,   171,  5635,   285, 22280,   366,   925, 15493,\n",
      "         22281,  7083,   117, 20373,   123, 14417,   125,   222, 16936,   303,\n",
      "           117,   171,   615,   146,  1568, 12292,   256,   320,  1973,   124,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 92: tensor([[  101,  3413,  1085, 13246, 22281,  4330,   412,   223, 22279,   291,\n",
      "           412,  1938, 22280,   173,  1564,   125,   739,  9525, 11540,   272,\n",
      "           229, 20027,   102]])\n",
      "DEBUG: Tokenized sentence 93: tensor([[  101,   122, 11251, 22278,  1684, 19543,   125,  3241,   102]])\n",
      "DEBUG: Tokenized sentence 94: tensor([[  101,   978,   259,  5708,  7967, 22281,   122,   259, 13841, 17291,\n",
      "           683,   125, 22232,   795,   117,  1405,  9370,   186,   320,  1568,\n",
      "           260,  1984,   537,  6471,   125,  1831,   122,   259, 12141,  5248,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 95: tensor([[  101,   170,   123,  4621,   304, 22280,   180,   879,   604,  1076,\n",
      "         11502,   118,  2036,  8193,  6522,   128,  2415, 19649,   128,   122,\n",
      "         12112, 22281,  6792,   899, 14971,   298, 17611, 22281,   320, 13943,\n",
      "         22282,   117,   366,   333,   912,   470, 15580, 22288,   320,  1341,\n",
      "           243,   347,  3147,   222, 10862,   125,  3024,   117,   230,  7770,\n",
      "          7248,   125, 14720,   122,  2415,  6017,  8849, 22281,   978,   222,\n",
      "          4735, 22280,  2374,   307,  4602,   151,   125,  7949,   830,   284,\n",
      "           498,   123,   418,   175,   122,   117, 20559,   240,   125,   839,\n",
      "           125,   222, 17664,   117,   146, 12071,   171,  5546,   211,   117,\n",
      "           179,  4841,   186,   272, 22232,   795,   102]])\n",
      "DEBUG: Tokenized sentence 96: tensor([[  101,  9784, 22278,   170, 18624,   123,   416,  8149,   747,   125,\n",
      "         19068,  2215,   789,   102]])\n",
      "DEBUG: Tokenized sentence 97: tensor([[  101,  5334,   748,   785,   170,  1921,  8092,   122,   117,  1065,\n",
      "           123, 22283,   117,  1485,   138, 13674,   117,  1075,   125,  2251,\n",
      "         22282,   155,   943,   117,  2863,   256, 16589, 15500,   692,   403,\n",
      "          1031,   931,   146, 13449, 19088,   125, 14808,  3292,   179,   123,\n",
      "          2047,  8860, 13793, 14615,   351,   320,   347, 11003,   102]])\n",
      "DEBUG: Tokenized sentence 98: tensor([[  101,  8230,   256,  1004,   170,   259,  8699,   117,  2251,  5630,\n",
      "           259,  3852,  2552,   122,   229, 22280,  4207,   792,  6074,   526,\n",
      "           183,   125,   898,   230, 20607,  4316,   102]])\n",
      "DEBUG: Tokenized sentence 99: tensor([[  101,   495,   222,  9463,  6436,   268, 22005,   393,  6044,   229,\n",
      "         22280,  4750,   260,  4004,  2307,   173,  3666,  4851, 15702,   285,\n",
      "          2551,   122,   331,  2778,  2836,   259, 13841,   726,   146,  3147,\n",
      "          6478,   180, 13943,   102]])\n",
      "DEBUG: Tokenized sentence 100: tensor([[  101,  1112,   229, 22280,   179,  2997, 22281,   236, 16149,  4486,\n",
      "         22354,   117, 13268,   256,   118,   176,   740,   117,  1112,   449,\n",
      "          5057,  2113,   259,   736,  9144,   102]])\n",
      "DEBUG: Tokenized sentence 101: tensor([[  101,  1112,   498,   123,   271,   285,   117,  1021,   785,   596,\n",
      "           117,   978,   230,  1009, 10490,  8817,  4009,  1282,   122,  9164,\n",
      "           328,   125,  7308, 17704,   298, 11935,  1530,   122,   184,   852,\n",
      "           256,   118,  2036,  1485,   260, 13674,   117,  1075, 21959, 22282,\n",
      "          7902,   102]])\n",
      "DEBUG: Tokenized sentence 102: tensor([[  101,  3874, 18574,  1407,   122,   325, 15397, 20383,   178,   171,\n",
      "           179,   222, 17611,   320,  1034,  2762,   117,   122,   117,   625,\n",
      "         13256,   179, 21333,   157,   537,  5723,   230,  1999,   125,  7390,\n",
      "          1111,  2684,  1084,   117,  1023,   230, 12458,   304, 22280, 16353,\n",
      "           122,  8089,  1337,   102]])\n",
      "DEBUG: Tokenized sentence 103: tensor([[  101,  5159,   259,  8184,   481,   117,   740,   662,  1353,  1695,\n",
      "           123,  1695,   123,  7544,   173,   898, 17253, 22281,  5896,  2028,\n",
      "         22281,   526,  5484, 13665,   117, 10733,   179,   230,  3022,   304,\n",
      "         22280,  1848,   176,  1577,   256,   202,   347,  5791,   183,   122,\n",
      "           202,   347,  6512, 12126,  8167, 22290, 11814,   118,   229,  6482,\n",
      "           143,   851,  4194,   442,  1169, 18377,   923,   118,   229, 12448,\n",
      "          6471,   834,  5607, 13268,   415,   102]])\n",
      "DEBUG: Tokenized sentence 104: tensor([[  101,   222,   644,   117,   870,   509,   117,  1365, 22288,   325,\n",
      "          5811,   285, 20582, 22288,   118,   176,   229,  2551,   117,   123,\n",
      "         17097,   956,   102]])\n",
      "DEBUG: Tokenized sentence 105: tensor([[  101,   122,   117,   170, 10013,   117, 13429,   203,   179,   532,\n",
      "          1823,  8521, 14545,   176,  2365, 19103,  6170, 15415,   179,   173,\n",
      "          1364,   347,  1831,   123,  1999,  9562, 10262,  1053, 19749,   123,\n",
      "          8145,   122,   179,   138,   675,  3241,  1085,  1941,  4304,   125,\n",
      "          2606,   102]])\n",
      "DEBUG: Tokenized sentence 106: tensor([[  101,  3429,   118,  2036,   318, 13793,   222,   498, 22281, 10557,\n",
      "           183,   125,  1519,  6423,   449,  2044,   700,  6600,   123,   318,\n",
      "          2413,  3710, 16280,   118,   834,  4838,   183,   331,   229, 22280,\n",
      "          2036,  1587,  5723,   146,  3165,   171,  1568,   122,   180,  7492,\n",
      "         17897,   124,  4750,   230,  4613,   232, 22280,   325, 15615,   117,\n",
      "           325,  6301, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 107: tensor([[  101,  5069,   748,   118,   176,   298,   532, 20401, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 108: tensor([[  101,  1984, 22288,   118,   176,  1112,  4486,   125,   854,  2028,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 109: tensor([[  101, 22354,   712,  6136,   481, 21029, 22278,   222,  8609,   171,\n",
      "          4073, 13665,   102]])\n",
      "DEBUG: Tokenized sentence 110: tensor([[  101,  3841, 10415,   243,  1510, 22281,   291,  1256,  1176,   529,\n",
      "           840,   171,  1568,   122, 10262, 15295,   118,   176,  2643,   138,\n",
      "         16283, 22281,   222,   423,  1342,   146,  8609,  5793,   221,   123,\n",
      "          2183,  2692,   285,  3952,   117,   122,   740,  2364,   325, 17662,\n",
      "          8807,   102]])\n",
      "DEBUG: Tokenized sentence 111: tensor([[  101,   700,   262,   222,  1538,   125, 10006,  1112,   271,  2036,\n",
      "          9086,  1004,   870,  2945,   102]])\n",
      "DEBUG: Tokenized sentence 112: tensor([[  101,   179,   390,   303, 13187,  1196, 22003,   122,   271,  9679,\n",
      "         12232, 22282,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 113: tensor([[  101,  9480, 14948,  2080,   123,   905,  2048,   123, 18995,  6557,\n",
      "           332,   125,  4004,  2307,   221,   219,   268,  9112,  1075,   240,\n",
      "           210,   125, 15025,   146,   652,   766,   117,  1941,   146, 20674,\n",
      "          5957,  1021,  1111,  7904,   823,  1859,   170,   123,   549, 21305,\n",
      "         22278,  1112,   475,  1721, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 114: tensor([[  101,  5793,   118,   176,   222, 12531,   171,  1847,   523,   102]])\n",
      "DEBUG: Tokenized sentence 115: tensor([[  101,  1112,   785,  4062, 13254,   785,   830, 21996, 22280,   180,\n",
      "         11451,   122,   366,   877,   842,   102]])\n",
      "DEBUG: Tokenized sentence 116: tensor([[  101, 22354,  9821,   118,  2036,   179,   744,  1011,   123,   873,\n",
      "           118,  1340,   117,  1364, 11637,   319,   117,  6627,   214,  4484,\n",
      "         20383,   277,   221,  2036, 10381,  1112,   123, 16783,  6320,   125,\n",
      "          4654,   934,   170,   740,   230,  1896,  2901, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 117: tensor([[  101,  2010,   123, 22296,  3492,  3492,   102]])\n",
      "DEBUG: Tokenized sentence 118: tensor([[  101,   122,   229, 22280,  4750,  9767,   744,   173,  6572,   374,\n",
      "         17377, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 119: tensor([[ 101, 1112, 4486,  125,  854, 2028, 4486,  125,  854, 2028,  102]])\n",
      "DEBUG: Tokenized sentence 120: tensor([[  101, 22354,  2535,   117,   331,   146,   179,  2036,  5572,   508,\n",
      "           495,   222,  4170,  1112,   146,   347, 22354,   117,   146,  6149,\n",
      "           117,   146,  2653, 22278,   146,  2397,   180,   327,  1105,   117,\n",
      "           146,  7299, 22280,   171,   347,  1831,   117,   123,  1977,   740,\n",
      "          7119,  8650, 17712,   271, 11003,   122, 13486,   943,   173, 11021,\n",
      "           271,  3656,  5630,   102]])\n",
      "DEBUG: Tokenized sentence 121: tensor([[  101, 11736,   125,  2822,   118,   176,   122, 10641,   118,   176,\n",
      "           123,   614,   210, 16280, 12507,  4096,   125,   240,   173,  1334,\n",
      "         22280,  2584,   735,  3292,   117,   179,   740,   173,   898,  3009,\n",
      "           151,   117,   221,  5357,  1284,   125,   230,  1105,   122,  6974,\n",
      "         22282,  1415,  2292,   102]])\n",
      "DEBUG: Tokenized sentence 122: tensor([[  101,   170,  2983,   125,  3027,  6378,   117,   417,  1797,   151,\n",
      "           118,  2036,  1684,   222,  3456, 14881,  6199,   125, 14594,  9086,\n",
      "         16571,  1378,   117,  7503,  4820,   389,  2397,  2124,   117, 16450,\n",
      "          3103, 22280,   117,   170,   222, 22003,  9861,   117,   122,  4051,\n",
      "           125,  6074,   118,   176,   240,   740,   102]])\n",
      "DEBUG: Tokenized sentence 123: tensor([[  101,   122,   117,   538,   532, 13994, 20627,   720,   117, 18804,\n",
      "          2037,   256,   118,   176,   222,  5184,   183,   915,  5587,   117,\n",
      "           449, 16473,   500,   117,   179,  3922,  8041,  8163,  9258, 22281,\n",
      "           117,   221,  3767,   582,   747,  1011,   122,  9607,   943,   118,\n",
      "          2036,   123,  9427,   392,   125,   222, 13449, 19088,   117,   230,\n",
      "         11152,  2521,  2028,   125,  2982,   102]])\n",
      "DEBUG: Tokenized sentence 124: tensor([[  101,   122,  2448, 15736,  3185, 22280, 17213,   222,  2764,  6685,\n",
      "           730,  3173,  1125,   243,   122,  1982,  3914,   117,   320,  9007,\n",
      "           125,   532, 18908,   501,   117,   222,  9922,  2152, 22280, 16283,\n",
      "          1691, 19560,  1212,   117,   222,  2154,   125,   344,   304,   117,\n",
      "           416,   304,   122,   370, 13397,   124,   117,   179,   123,   532,\n",
      "          1143,   388,   679,   125,  4764, 19773,   340,   122,  7427,  5630,\n",
      "           118,   123,   271, 11552,   173,  4848,   102]])\n",
      "DEBUG: Tokenized sentence 125: tensor([[  101,   700,  2010,  1413,   118,   176, 10502,   125,  1105, 18648,\n",
      "           785,   538,  2292,  2448, 15736,   118,   176,  8540,   117,   785,\n",
      "         14044,   324,   369,   375, 22280,   171, 16241,   268,   122,   202,\n",
      "          3400,   247,   505,  2552, 22280,   171,  4170,   102]])\n",
      "DEBUG: Tokenized sentence 126: tensor([[  101,   122,  2448, 15736, 11368,   854,  6017, 13808, 22281,  7406,\n",
      "           277,   117,   370,  1343,   117,  3568,   694,   351,   214,   374,\n",
      "         17377, 22281, 13187,  4851,   122,   271,  9964,   277,   117, 12294,\n",
      "           118,  2036,  1112, 10256, 22278, 22354,  2010,   146, 22296,   271,\n",
      "         12444,   333,  4062,   102]])\n",
      "DEBUG: Tokenized sentence 127: tensor([[  101,   122,  9767,   179,  1021,   240,   123, 22283,  2459,   179,\n",
      "          1085,   598,   146,  2982,   102]])\n",
      "DEBUG: Tokenized sentence 128: tensor([[  101,   229, 22280,   740,   229, 22280,  4207, 18689,   307,   146,\n",
      "          4436, 10224,   183,   117,  1953,   221,   123,  2606,   102]])\n",
      "DEBUG: Tokenized sentence 129: tensor([[  101,  1112,   221,   146,  2397,  2010,   744, 15394,   256,   102]])\n",
      "DEBUG: Tokenized sentence 130: tensor([[  101,  5212,   151, 12448,   117,   331,   449,   173,  1364,   146,\n",
      "          1652,  2010,   495,   222,  2397,   102]])\n",
      "DEBUG: Tokenized sentence 131: tensor([[  101,  2471,  1028, 22238,   303,   143,   449,   230,   264,  1381,\n",
      "          2606,   117,   179,  1407,  3753,  2686, 16122,   484,   159,   179,\n",
      "           146,  2982,   136,   102]])\n",
      "DEBUG: Tokenized sentence 132: tensor([[  101,   179,   325, 20172, 22280, 15537,   171,   455,   123,  2013,\n",
      "         16644,   179,  4067,   325, 20073,   171,   179,   123,   298,  2292,\n",
      "           117,  3636,   644, 11284,   683,   316, 22280, 18190, 20635, 22281,\n",
      "           136,   102]])\n",
      "DEBUG: Tokenized sentence 133: tensor([[  101, 22354,  1202, 22287,   125,   179,   117,  1684,  5971,   124,\n",
      "           785,   125,   854,  2028, 22281,  5747,   576, 10381, 22278,   123,\n",
      "          1977,   260,   978,   179,   219,   842,  3497, 22281,   236,   870,\n",
      "         11837,   140,   118,  2036,  4067,   117,   122,   117,  1139,   260,\n",
      "         18720,   256,   173,  1105,   117,   229, 22280,  2380, 15500, 22278,\n",
      "           179,   325, 16241,  1537, 22287,   176, 20990,   591,   236,  1988,\n",
      "          2859,  4750,   333,   123,  2004, 22278,   123,  2822,   118,  7707,\n",
      "           123,  9652,   117,   123, 11348,   118, 10497,   117,   123, 12232,\n",
      "           118, 10497,   117,   122, 21439,   796,   118, 10497,   122,  1011,\n",
      "          7888,  1383,   246,   123,  1815,  2430,   610, 15266, 13808, 22281,\n",
      "           122,  1925,  2876,   138,   117,   123,  1434,  8589,  1149,   122,\n",
      "         14230,  5321, 22281,   125,  1084,   117,   122,  2745,   170,  5747,\n",
      "           321, 19773,   340,   117,   170,   785,  3165,   117, 13776,   271,\n",
      "           117,   173,  1283,   194,   387,   117,   740,  5057,   170,   260,\n",
      "           675, 21370,  1149,   102]])\n",
      "DEBUG: Tokenized sentence 134: tensor([[  101,   625, 14710,   148,   125,   675, 19763,   176,  1105,   256,\n",
      "           117,  9480, 14948, 19764,  3914,  1684,   222,  9804,   378,   171,\n",
      "          6847,  4943,   735, 22279,   291,   222, 11967, 13793,   366, 14151,\n",
      "          2841,   125,  2716,   190,   537,   364,   180,  9247,  7134,   285,\n",
      "           860,   291,  6086,   117,   466,  8041,   118,   259,  5936,   246,\n",
      "           202, 17387,   170,   222,   298,   313,  7557,   735,   143,  2404,\n",
      "          2283,   180, 14308,   117,   122,  4867,   256,   118,   176,   123,\n",
      "         11586,   118, 15887,   117, 17097,  3684,   117,  2684,   179,   298,\n",
      "         18908,   501,  2036, 18318, 22278,   222,  4217,   397,  1639,   117,\n",
      "           785,  1639,   117,   271,   146,   171,  8022,   175,   179,   173,\n",
      "          1423,   171,  3420,  1941,   176,  5440,   822, 13550,   122,   744,\n",
      "           229, 22280,  7598,   154,   146,  2716,   102]])\n",
      "DEBUG: Tokenized sentence 135: tensor([[  101,   449,   146, 20743,   240,   582,  6952,   256,   179,   229,\n",
      "         22280,  8940,   136,  1966, 15152,  9922,  2152, 22280,   117,   316,\n",
      "         22280,   388,  9238,   122,   316, 22280, 16283,   117,   240,   179,\n",
      "           176,   229, 22280, 10072,  2044,   136,   298,  2217,   179,  9480,\n",
      "         14948, 18574,   229, 11126,   351,  3484,   693,  3671,   264,   679,\n",
      "           333,   102]])\n",
      "DEBUG: Tokenized sentence 136: tensor([[ 101,  122,  117,  202, 1325,  117,  740, 3330,  256,  102]])\n",
      "DEBUG: Tokenized sentence 137: tensor([[  101,   123,  1977,   136,   229, 22280,  9679,  1331, 22279,   118,\n",
      "          1340,   117,   449,  3330,   256,   102]])\n",
      "DEBUG: Tokenized sentence 138: tensor([[  101,  1141,  2589,   123,  1977,  2589,   117,   740,  3330,   256,\n",
      "          2113, 16280, 11744,   900,   118,  2036,  3069,   146,  1831,   117,\n",
      "         19098,   240, 19098,   117, 18648,  3876,  2010,   614,   210,  2010,\n",
      "         14353, 22280,   122, 10846,   221,   740,  1966,  2010,   614,   210,\n",
      "          2010,   179,   229, 22280,  8940,   122,   229, 22280,  2036,  8625,\n",
      "         22278,   171,  6314,  1966,  2010,   614,   210,  2010,  3561, 12298,\n",
      "           340,   123,  5057,   851,  5325, 22279,  2036, 16386,   151,   123,\n",
      "          1622,   340,   125,  1084, 20739,   102]])\n",
      "DEBUG: Tokenized sentence 139: tensor([[  101,  4058,   118,   176,  2112,  2010,  3874, 13239,   228,  1510,\n",
      "         22281,   481,   102]])\n",
      "DEBUG: Tokenized sentence 140: tensor([[  101,  9480, 14948,   905,  4373,   123,   173, 22175,   943,  1003,\n",
      "          5366,   102]])\n",
      "DEBUG: Tokenized sentence 141: tensor([[  101,  2535,  4678,  4322,  1528,  1011,  1877,   328,   123,  9317,\n",
      "          3002, 17318,   538, 16032,   102]])\n",
      "DEBUG: Tokenized sentence 142: tensor([[  101,  2010,   146,  3113,   117,  5023, 10354,  3933,  5664,  1996,\n",
      "           118,  2036,   222,   644,   146,  1568,   117,  1941, 20990,  6859,\n",
      "           170,  6086,   388,   243,   265,   247,   180,  2267,   102]])\n",
      "DEBUG: Tokenized sentence 143: tensor([[  101,   229, 22280,   311,  4048, 22281,   123,  1589,   179,   122,\n",
      "          1257,   117,   360,   232,   136,   229, 22280,   495,  3874,   102]])\n",
      "DEBUG: Tokenized sentence 144: tensor([[  101,   122,  9480, 14948,   498, 22281, 10557,  5723,   118,   176,\n",
      "           117,   271,   176, 18424, 22278,   662,  1672,   230,  3207,   102]])\n",
      "DEBUG: Tokenized sentence 145: tensor([[  101,  1112,   822,   375,   303,  8089,   128,   229, 22280,   495,\n",
      "           303,  1145,   179, 16817,   236,   123,  7482,   102]])\n",
      "DEBUG: Tokenized sentence 146: tensor([[ 101, 1112,  449, 5334, 5630,  102]])\n",
      "DEBUG: Tokenized sentence 147: tensor([[  101,  2010,  2389,   252,   123, 22283,  7273,  2535,   146,  5334,\n",
      "           157,  3874,   122,  8911,  6542,   146,  6815, 22280,  2010,  6542,\n",
      "           146,  6815, 22280,   136,   102]])\n",
      "DEBUG: Tokenized sentence 148: tensor([[  101, 11032,  5848, 22283,   117,   229, 22280,  5488,   123,  7482,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 149: tensor([[  101,   122,   374, 22281,  1870,   102]])\n",
      "DEBUG: Tokenized sentence 150: tensor([[  101,  1112,   179,   123,  5308, 22281,  6867,   173,  4527,   179,\n",
      "           229, 22280,   123, 19864,  1178, 10422,   214,   170, 14179,   102]])\n",
      "DEBUG: Tokenized sentence 151: tensor([[  101, 22354,   122,   374, 22281,  1870,   325,   117,  9697,   451,\n",
      "           251,   102]])\n",
      "DEBUG: Tokenized sentence 152: tensor([[  101,  2010,  5693,   136,  3769,  7093,  2382,  1904, 22281,  3876,\n",
      "          1112,  1224,  6557,   117,  1224,  6557,  1224,  6557,  1224,  6557,\n",
      "         22354,   122,   122,   331,  1112,   229, 22280,  5488, 15975,   324,\n",
      "           229, 22280,  3804,  6542,   146,  6815, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 153: tensor([[  101, 22354,   229, 22280, 17704,   170,  3848,   687,   562,   229,\n",
      "         22280,   176,  5911,   304, 22280,  6815, 22280,   746,  3529, 17052,\n",
      "         22281,   125,   528,   229,  8993,   121, 22361, 11912,   102]])\n",
      "DEBUG: Tokenized sentence 154: tensor([[  101,   262,   222,   596, 15020,  5092,   221,   740,   259,  1510,\n",
      "         22281,  2112,   179,   123, 22283,  1367,   102]])\n",
      "DEBUG: Tokenized sentence 155: tensor([[  101,   259, 17246,   180,  4203,   117,   259, 17052, 22281,   693,\n",
      "           268,   455,   117,   259, 10620, 17611, 22281,   123,   766,   117,\n",
      "           398,   569,  1429,   118,  2036,   146, 15975, 15588,   122, 20613,\n",
      "          2573,   118,  2036,   146,  5052,   102]])\n",
      "DEBUG: Tokenized sentence 156: tensor([[  101,  1767,   325, 22028,  2080,   123,  2787,  3070, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 157: tensor([[  101,   229,  8993,   121, 22361, 11912, 12682,   186,   230,   940,\n",
      "          8286,  2010,   121,   102]])\n",
      "DEBUG: Tokenized sentence 158: tensor([[  101,  2779,  4221, 16257,   252,   102]])\n",
      "DEBUG: Tokenized sentence 159: tensor([[  101,  4970,   256,   125,   222,  1538,   171,  5841, 20933, 22285,\n",
      "         12190,  3784,   117,  2971, 22280,   179,  2931,  1364,   229,  1658,\n",
      "           171,   221,  4894,   102]])\n",
      "DEBUG: Tokenized sentence 160: tensor([[  101,   785,  2415, 19649, 22278, 18402,   171,  4170,   130,   455,\n",
      "          1609,   214,   118,   176,   117,   122,  6792,  9001,   256,   118,\n",
      "          2036,   123,  6003,  4131, 22278,  1112,  1027,  1564,   700,   125,\n",
      "          7444, 22281,   117,  3866, 22278,   368,   221,   146,  2244,   272,\n",
      "          2971,   122,   117,   202,  2131,   243,   180,   327, 16151,   117,\n",
      "          1796, 13910,   243,   240,   230, 17772,   125,  8495,   117, 17924,\n",
      "          2044, 17409,   694,  6658,   170,   146, 18908,   247,  5925,   833,\n",
      "          3897,  4409,   146,   655,   180,  2772,  1048,   155,  2647,   102]])\n",
      "DEBUG: Tokenized sentence 161: tensor([[  101, 22354,   122,   170,   222,  4217,   397,   117,  2160,   125,\n",
      "         16617,  3002,  8174,  1789,   117,   123,  4970,   256, 17800, 22278,\n",
      "         22269, 11681,   179,  1112, 15537,   143,  3816,   154,  1069,   117,\n",
      "          8223, 22278,   820,  1027,  1564,   122,  1027, 13674,   102]])\n",
      "DEBUG: Tokenized sentence 162: tensor([[  101, 22354,  9480, 14948,  4968,   272,   351,   118,   176,   180,\n",
      "          8932,   122, 13083,  5723,   118,  2036,   125,  3264,   118,  1154,\n",
      "           260, 10343,  7099,   277,   102]])\n",
      "DEBUG: Tokenized sentence 163: tensor([[  101,   229,   327,  1486,   194,  3611, 17382,   283,  2100, 15172,\n",
      "         19927,  1076,  6530,   176, 14851,   256,   170,   123,  4131, 22278,\n",
      "         13310,  4566,  2982,   316, 22280,   851,  5325, 22279,   316, 22280,\n",
      "         12060,   713,   102]])\n",
      "DEBUG: Tokenized sentence 164: tensor([[ 101,  240,  325,  125,  230,  576, 2080,  123, 5334,  900,  412, 1386,\n",
      "          171, 6754,  390,  303, 1538,  125, 9719,  102]])\n",
      "DEBUG: Tokenized sentence 166: tensor([[  101,  2779,  4221, 16257,   252,  2314,   456,   123,   327,   940,\n",
      "          8932,   173,  1615,  4486,   179,   418,  3002,  2448, 15736, 16948,\n",
      "           118,  2036,   943,   382,  9250,   501,   180,  1069,  1711,  1187,\n",
      "           706,  4640,   118,   176,   179,  2036,  2002,  4073, 22280,   143,\n",
      "           125,  3165, 11234,   785,   538,  1112,  2217, 22354,   117,  1996,\n",
      "           118,  2036,   271,   123,  2606, 14657,   154, 12444,  9544,   170,\n",
      "          1061,  1647,  1085,   260,  1062,   842,   122,   259, 20843,   298,\n",
      "          4170, 22281,   291,   442, 13746, 22281,  1647,  1085,   259,  3385,\n",
      "          5674,  1086,   145,   146,   179, 17144,   370,  1112,  5708,  4954,\n",
      "           117, 10786,   942, 15618,  1409,   117, 17749,   408,  9539, 22354,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 167: tensor([[  101,   123,  1858, 18419,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 168: tensor([[  101,  1112,   229, 22280,  5267,   256,   123,   333,   247, 11665,\n",
      "          1151,   581,  1495,   180,  2779,  4221, 16257,   252, 22354,   449,\n",
      "         18034,  8544,   117,   834,  2822,   240,  1257,   117,  5757,   557,\n",
      "           146,   347,  7503,  1676,  2314,   303,   143,   180,  4970,   256,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 169: tensor([[  101,  1154,   118,  1340, 16342,  7195,   117,   325,  4481,   117,\n",
      "           325,   792,  1785,   314,   215,   117,   325, 20608,  6758, 22290,\n",
      "           125,   333,  8085,   122,   117,  1065,   318, 13793,   117,   146,\n",
      "          6418,   117,   820, 18804,  2037,   243,   320,  4707,   298,   532,\n",
      "         13994,   117,  3429,   221,   123,  2375,   117, 11534,   203,   118,\n",
      "           176,   271,   230,  4074,  3189,   383,  2152,  1249,   259,   169,\n",
      "         14116, 22281, 15699, 22281,   171,  7090,   122,   117,   700,   125,\n",
      "           873,   118,  1340,  1004, 17478,   117,  1004,   173,  3870,   201,\n",
      "           122, 13610,   117, 14283,  9030,   744,   325,   117,   785,   325,\n",
      "           117,  1971,  2249,   146,  8650,   151,   176,   368,  1796,   170,\n",
      "          3901,   230,  4657,   102]])\n",
      "DEBUG: Tokenized sentence 170: tensor([[  101,   123,  1018,   180, 22283,   117,   495,  1966,  7503,   117,\n",
      "         17478,   122,   173,  3870,   201,   117,   123,  1037,   366,   675,\n",
      "         15495,   303,   143,   123,  3953, 18758,   375,   310,   495,   123,\n",
      "         19932,   715,   117,   240,   582,   740,  4613,   322,  1364,  6086,\n",
      "           179,   123, 13564,  8849, 22281,   236,   102]])\n",
      "DEBUG: Tokenized sentence 171: tensor([[  101,   176,   146, 11179,   175,   229, 22280,  6344, 22280, 17749,\n",
      "           117,   146, 11552,   117,   146, 22000,   117,   146,  2154, 17878,\n",
      "           125,   179,  8051,   256,   146,  1852,   124, 22280,   117,  4207,\n",
      "           117,  1065,  2044,   117,  5986,   123,  8304,  2028,   125,  9322,\n",
      "           529,   416,  1149,   180,  2267,   125,  5009,   178,  1449,   157,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 172: tensor([[  101,  2779,  4221, 16257,   252,  3171,   118,   176,   221,   123,\n",
      "           651,  9480, 14948,  1941,  1084,  1011,   102]])\n",
      "DEBUG: Tokenized sentence 173: tensor([[  101, 10964,   228,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 174: tensor([[  101,   122,  3769, 12338,   117,   179,   176,  4975,   785, 14353,\n",
      "           138,   122, 16852, 22281,   117, 18496,   362,  4408,   246,   125,\n",
      "          5411, 22280,   117,   320, 10953,  2668,   201,  4436, 10224,   183,\n",
      "           125,   230,   122,   123, 16251,  4970,  4199,   180,  1858,   102]])\n",
      "DEBUG: Tokenized sentence 175: tensor([[  101,  1021,   117, 12531,   202,  5209,   210,   171,  1568,   125,\n",
      "          9480, 14948,   117,   222, 13254,  1456,   143,   117,   125,   655,\n",
      "           599,   145,  1564, 18206,  8748,   117,  7569,   319,   117, 11831,\n",
      "           183,   117, 17509,   117,   170,   230, 19543,  5991,   117,   122,\n",
      "           785, 14031,   229,  1174,   304,   102]])\n",
      "DEBUG: Tokenized sentence 176: tensor([[  101,  7719, 22287,   123,   347,  2748,  2401, 22279,   524,   909,\n",
      "          4814,   125,   964,   300,  2791,   117,   122, 16241,  1537, 22287,\n",
      "          1467,  4051,   125,  4640,  3002,   125,   316, 22280,  1916,  1927,\n",
      "           403,   390,   303,   102]])\n",
      "DEBUG: Tokenized sentence 177: tensor([[  101,   320,   598,   342,   117,  1821,  1684,   179, 18402, 22287,\n",
      "          2461,   117, 10355, 22287,  1112,   144,  2640, 22354,   122,   860,\n",
      "          2010,   144,  2640,  2010,   495,  1017,  1429,   403,   834,  3083,\n",
      "         13793,   125,   333,   117,  2113,   320,  1564,   117,   416,  1149,\n",
      "           123,  4023,   117,  3874,  3207,   256,   978,  1105,   117,  9652,\n",
      "           117, 11451, 21307,   285,   122,  2787,   703,   251,   117,   122,\n",
      "           117,   744,   240,  5530,   117,   259,  9312, 22281,   171,  7007,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 178: tensor([[  101,   449,   123,  5664,   495,   179,   146,   644,   492,   171,\n",
      "          2397,   117,  2440,   366,   675,  4041, 13001,  7892,  2148,   784,\n",
      "           117,   525,  3638,  5288, 10497,  2896,   117,  6851,  2836,   170,\n",
      "           146,   347,  2337,   140,   300,   388,  7642,  9025,   117,   125,\n",
      "         10262,  1436,   117,   125,   398,  4861,   304, 22280,   122, 12133,\n",
      "          1076,   102]])\n",
      "DEBUG: Tokenized sentence 179: tensor([[  101,  5057,  7482,   117,   563,  1973,   151,   171,   173,  1977,\n",
      "           146,  1003,   236,   117,   316, 22280,   695, 20691,   293,   117,\n",
      "           316, 22280,   723,  1430,   117,   316, 22280,  6754, 13254,  2010,\n",
      "           316, 22280,  5294,  8849,   125,  5890,   102]])\n",
      "DEBUG: Tokenized sentence 180: tensor([[  101, 16241,  1537, 22287,   117,   173,  1652,  3179,   117, 14748,\n",
      "           151,   123,   223, 22280,   498,   368,   117,   834, 21066,   123,\n",
      "          7729, 10766,   340,   180,   144,  9990,   151,   102]])\n",
      "DEBUG: Tokenized sentence 181: tensor([[  101, 13173,   692,   118,   202,  5147,  1112,   179,   229, 22280,\n",
      "          5327,  7521,  4566,   388, 18913,   183,   117,  2113,  1369,  1011,\n",
      "           222, 21813,  2127,   251, 22280,   125, 13182, 22305, 22354,  2344,\n",
      "           128, 19317,   358,  9112,   228,   118,  2036,  7799, 12816,   221,\n",
      "          5267,   118,  1340,   320,   347,  1312,   303,   449,   146,  1564,\n",
      "           117,  1684, 21323,   122,   125,  3049,   304,  4098,   117,   398,\n",
      "          4681,   118,  7707,   123,   766, 11948,   102]])\n",
      "DEBUG: Tokenized sentence 182: tensor([[  101,   122,   117,  1815,  8051,   340, 13638,   260, 16852, 22281,\n",
      "          8658,   117,   179,  1364,   146,  1847,   523,   117,  3951,   271,\n",
      "          4863,   146,   347,  2982,   170,   123,  2267,   171,  9019, 13793,\n",
      "           117, 10789,   123,  4990,  6461, 13397,   178,  1449,   157,   122,\n",
      "         10375, 13450,   712,  1444,  9228,   555,  1112,   222,  3753,   785,\n",
      "         22003,   122,   785,  8797, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 183: tensor([[  101,  2010,   262,  3813,   487,   117,   262, 10355, 22287,   170,\n",
      "         11552, 10692,   183,   102]])\n",
      "DEBUG: Tokenized sentence 184: tensor([[  101,  5009,   178,  1449,   157,  1413,   117,   170,  3901,   117,\n",
      "          6621, 15799,   117, 17509, 22278,   122,   723,  1319,   271,   222,\n",
      "          1151, 22283,   125,  5890,   122, 12535, 18899,   303,   271,   222,\n",
      "          3644,   900,   247,   117,   146,  2397,   325,   202,  1652,   125,\n",
      "          1434,   123, 15685,   180,  2267,   102]])\n",
      "DEBUG: Tokenized sentence 185: tensor([[  101,  4750,   118,   146,   221,  3756,   157,   122,   221, 17796,\n",
      "         10355,   123,   944,   259,  8229,   179,   146,  1112,   347,  1564,\n",
      "         22354,   820, 20813,   256,   240,   622,   117,   221,   260,   675,\n",
      "          1111, 14217,   117,   123,  5044,   670,   171, 11263,   102]])\n",
      "DEBUG: Tokenized sentence 186: tensor([[  101,  2010,   376,  1941,   146,   347, 12651,   247,   117,   376,\n",
      "         11614,   368,   102]])\n",
      "DEBUG: Tokenized sentence 187: tensor([[  101,   123,  2606,   179,   146,  8204,  1249,   117, 16403,   222,\n",
      "          4062,  4170, 22278,  2323,  5401,   123,  8771,  3933,  5664,   102]])\n",
      "DEBUG: Tokenized sentence 188: tensor([[  101,   122,   390,   303,   125,   785,  3753, 22279,   117,  1695,\n",
      "           123,  1695,   262,   118,   176, 19877,  3611,   214,   123, 19399,\n",
      "           118,  1340,  1941,   180, 20027,   122,   123,  5948,   118,  1340,\n",
      "           122,  7194, 22283,   118,  1340,   271,  1815,   293,  3207,   256,\n",
      "           179,   123,  3113,   176,  2121,  4886,   102]])\n",
      "DEBUG: Tokenized sentence 189: tensor([[  101,   449,   615,   740,  2798,  4750,   873,   118,  1340,   978,\n",
      "           118,  2036,  4351, 22282,   124,   229, 22280,  4207,   293,  1198,\n",
      "         22282,  6086, 10881,   123,  3235,  3711,   252,   117,  6086,   329,\n",
      "          3027,   252,   455,   834,  4351,   339,   272,   117,  5022, 12141,\n",
      "          5980,   128,   117,  7583,  3338,   428,   269,   122,  5022,  4785,\n",
      "           125,  2397,   834,  6272,  2004, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 190: tensor([[  101,  2010,   222,  4081,  5137,   303, 13631,   256,  9480, 14948,\n",
      "           117,  6251,  8149,   214,   146, 17749,   102]])\n",
      "DEBUG: Tokenized sentence 191: tensor([[  101,   230,  2880,   151, 22280,   117,   146,  1568, 10539,   118,\n",
      "          2036,   202,  2982,   102]])\n",
      "DEBUG: Tokenized sentence 192: tensor([[ 101, 2010,  170,  146, 1564,  136,  102]])\n",
      "DEBUG: Tokenized sentence 193: tensor([[  101, 16795,  1632,  8440,   102]])\n",
      "DEBUG: Tokenized sentence 194: tensor([[ 101, 2010, 1141,  102]])\n",
      "DEBUG: Tokenized sentence 195: tensor([[  101,  2010, 11032,   117,  5848,   794,   969,   654,   230,  3979,\n",
      "           251,   102]])\n",
      "DEBUG: Tokenized sentence 196: tensor([[  101,  5009,   178,   229, 22280,   176,  6015,   203,   123,  4640,\n",
      "           325,  3661,   123,  2954,   117,   240,   210,   117,  5222,  2745,\n",
      "           173,  2754,   320,  5066,   142,   130,   117,   222,  3695,  4575,\n",
      "           117, 14353, 22280,   180,  1105,  2010,   146,   317, 22279,   339,\n",
      "           434,   763,   102]])\n",
      "DEBUG: Tokenized sentence 197: tensor([[  101,  2010,  2134,  2896,   331,  7826, 22279, 15891, 10394,  5440,\n",
      "          6017,   203,   860,   102]])\n",
      "DEBUG: Tokenized sentence 198: tensor([[  101,   122,  8911,  2822,   596,   320,   596,   117,   347,  4968,\n",
      "         21510,  1169,  1145,   607,   125,   333,   102]])\n",
      "DEBUG: Tokenized sentence 199: tensor([[  101,  1114, 22279, 13239,   146, 10695,   300,  1325,   117,   146,\n",
      "          1564,   229, 22280,   176, 14465, 22278, 11690,  1945,   201,   117,\n",
      "          4500,  6131,   117,   834, 15568, 22282,   259,  5708,   117, 13140,\n",
      "          6867,   827,   125, 12133,  1076,   122,   398,  4861,   304, 22280,\n",
      "           102]])\n",
      "DEBUG ======================\n",
      " len vetores 199 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.007664303504950315\n",
      " Coesão Score Final: 0.5038321517524752\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'contudo', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'apesar de', 'isto e', 'todavia', 'nao obstante', 'nem', 'ou', 'ora', 'quer', 'como', 'tanto quanto', 'quanto', 'em vez de', 'assim que', 'porque', 'posto que', 'em razao de', 'gracas a', 'ao contrario', 'no entanto', 'com efeito', 'principalmente', 'como tambem', 'tanto', 'quanto', 'se nao', 'em virtude', 'por isso']\n",
      " Número de conectivos: 40\n",
      " Número de sentenças: 200\n",
      "======================\n",
      "Resultados para preprocessado_o_mulato_aluisio_azevedo_cap_1.json:\n",
      "{'coesao_score': np.float64(0.5), 'conectivos_encontrados': ['e', 'mas', 'porem', 'contudo', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'apesar de', 'isto e', 'todavia', 'nao obstante', 'nem', 'ou', 'ora', 'quer', 'como', 'tanto quanto', 'quanto', 'em vez de', 'assim que', 'porque', 'posto que', 'em razao de', 'gracas a', 'ao contrario', 'no entanto', 'com efeito', 'principalmente', 'como tambem', 'tanto', 'quanto', 'se nao', 'em virtude', 'por isso'], 'num_conectivos': 40, 'proporcao_conectivos': 0.008, 'similaridade_media': np.float64(1.0), 'num_sentencas': 200}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  1016,   495,   117,   625,  5009,   178,  1449,   157,   117,\n",
      "           229, 17935,   404,   125,   327,  1105,   117, 21315,   123,  2267,\n",
      "           230,  4299, 12845, 22278,  3953,   171,  2982,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,  1941,  1084,   176, 16420,  1510, 22281,  2112,   700,   180,\n",
      "          2147, 22278,   229,  8993,   121, 22361, 11912,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,  9480, 14948,  3449,  5896,   202,   347,  1247,   117,   123,\n",
      "         11586, 22282,   123,   374,  3613,   180,  9317,   117,   271,   176,\n",
      "          2863, 22281,   236,   123, 22283,   230,   207,   211,  8497, 13793,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[ 101,  146, 9679, 8954,  256,  229, 1956,  247,  266,  102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,  2010,   318, 13793,   117,  7122,  2267,   117,   229, 22280,\n",
      "           366, 14019,   230,  2521,  2028,   136,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[ 101, 2010,  706,  333,  102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   122,   740, 15568, 22288,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[ 101, 2010, 4062,  102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,  1016,   122,   179,   437, 18691,   792,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   146, 19317,   175,  1367,   146,  4332,   303,   173,  1359,\n",
      "           180, 11214,   392,   180, 10173,  6943,   117, 16620,   123, 10415,\n",
      "         22282,   744,   117,   449, 18305, 19006,   240, 11368, 19023, 22281,\n",
      "           202, 14148,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,  2010,   180,  4073,   194,   304,   136,  1996,   146,   317,\n",
      "         22279,   339,   117,  1941,   229,  4303,   180, 17935,   404,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,  2010,  1447, 12022,   117,  4968, 21510, 22280,   317, 22279,\n",
      "           339,  3033,   117,   125,   256,   702,   117,   170,   146,   347,\n",
      "         13449, 19088, 11831,   183,   122,  3330,   415,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,   495,   222,  4575, 22003,  2471,   625,  1528, 14958,   481,\n",
      "           117,   240,   210,  1011,   744,  2124,   122,  1004,  7888,  2263,\n",
      "           243,   146, 11552,  3361,   117,   146,  1831,  3689, 22280,   117,\n",
      "           449,   877,  2245,   243,   125,  4332,  8859,   124, 19462, 22282,\n",
      "          1885, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101, 14790,  2836,   118,   176,   170,   188,  7506,   117, 14772,\n",
      "          4453,   265, 22280,  3497,   256, 10467,   180,  8768, 22278,   117,\n",
      "           221,   347,  1700,   117,  5899, 22281,   122, 20327,  2552,  4797,\n",
      "           117,   122,   117,   625, 18419,   117, 15245, 19629,  1743,   459,\n",
      "           117,   944,  4273,  5870,   442,   123,  2987,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   978,   259,  4785, 10745,   223,   128, 11563,   122, 13841,\n",
      "         14874,   455,  5057, 10303,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   434,   763,   495,   146, 14809,  9238,   122,   146, 12781,\n",
      "           171,  4062,   122, 12518,  5009,   178,   860,   229, 22280, 10348,\n",
      "           222,  6793,   834,  7888,  8521,   159,   146,  4968, 21510,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,  4883, 22278,   118,   176,   173,   144,  6562,   117,   171,\n",
      "           323,  7719, 14192, 10661,   222,  9463,  6436,   268,  8797,   117,\n",
      "           122,   229,  5507,   266,  2037,   256,   146,   347, 17611,   123,\n",
      "         21990,  2204,   117,   125,   576,   173,   625,   117,  1112,   221,\n",
      "          7496,  2127,   159,   481,   180,  4203,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101, 22354,  6834,   256,   368,   117,   388,   307,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,  2044,   179,  3033,   117,  2002,   123, 10786,  4765,   123,\n",
      "          9480, 14948,   146,   347,   739,   122, 12646,  9854,   125, 13501,\n",
      "         10225,   117,  1706,   171,  3906,   117,  2850,   125, 19385,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   122, 18260,   118,  2036,   229,  6726,   170,   123,   223,\n",
      "         22280, 16907,   122,  4019, 10766,   285,   125, 17874,  2912, 22279,\n",
      "          4072,  2010,   318, 13793,   117,  7122,  8707,  4419,   117,   271,\n",
      "          2541,  1921,  4351,  4803,   322,   136,  8544,  1004,   117, 13406,\n",
      "          2647,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101, 13449,   172, 22288,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[ 101, 2010, 5750, 6436,  268,  418, 4062,  136, 2010,  271, 1684,  102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   179, 11489,   562,   125,   121,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,   475,  1584,   136,  1011,   125, 17611,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,  2010,  1502,   229, 22280,   873,   123,  1105,   331, 22281,\n",
      "           236,  3281,   136,  6185,  2904,  5009,   178,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   262,   123, 13544,   122, 12931, 16960,  1353,   240,   123,\n",
      "           319, 22287,  3933,  8932,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,  4023,   123,  5249, 22279,   240,  1084,   449,   179, 12935,\n",
      "           130,   146,  5626,   123,  3769,  2856,   329,   240,  1105,   117,\n",
      "           347,  5066,   142,   130,   136,  2010,   222,  3907,   523,   179,\n",
      "          2036, 18691, 17528,  2754,   117,   222,  9463,   243,  2754,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,  9480, 14948,  1191,  2044,  1462,   304, 22280,   125, 16083,\n",
      "           118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[ 101, 2010, 5308,  118,  437, 4412,  117, 1996,  118, 2036,  146, 1568,\n",
      "          102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,   538, 19821,  5863,   221,   146,  4223,   247,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,   122,   259,   682,  4968, 21510, 22281,   117, 10415,   214,\n",
      "           173,  4410,  4098,   117, 16924,   490,   118,   176,   221,   230,\n",
      "          1390,  1970,   179,  1021,   324,  2375,   180,  1105,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[ 101,  123, 1390, 1970,  495, 1283,  194,  387,  117,  170,  924, 9751,\n",
      "          221,  123, 4768,  180, 4595,  102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,  1690, 22280,   860,   364,   243,  7367,   344,  2208,   272,\n",
      "          1798,   122,   146, 12639,   125, 17343,  4242,   125,  5848,   124,\n",
      "          1939, 22278,  2157,   401,   125,  4712,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,  1021,   230,   505,  7700,   125,  3321,   117,   785,   162,\n",
      "           154,   117,   170,   146,   347,   390,  2014, 11417,  1862,   117,\n",
      "           222,   144,  1198,   125,  3050,   117,   230, 18720,   125,  2978,\n",
      "           125,  1484,   406,   304, 22280,  5886,   938,   215,   117,   230,\n",
      "           827,  2524,   117,   146, 10798, 22139,   320,  1341,   122,   325,\n",
      "           222, 10798, 22280,  5980, 22280,   125,   302,   117,   173,  8764,\n",
      "         18995, 22281, 12604,   375,   256,   222, 13718,  1927,  1690,   183,\n",
      "         18758,   492, 12399,   230,  9104,   125,  1877, 13732,   252,   117,\n",
      "           222,  8097, 22280,   125,  5562,   244, 22281,   238,  1973,   244,\n",
      "         22281,   117,   222, 21115,   125,  3598,   122,   924,  3240, 22282,\n",
      "          8008,  1402,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,   123, 22296,   744,  1021,   229,  8130,   117,   498,   123,\n",
      "         11098,   322,   117,   222,  9599, 11647,   247,   171,   622,   122,\n",
      "          1342,   180,  2767,   117,  2592,  1988,   260,   313,  2245,   483,\n",
      "          1402,   766, 13199,   125,  7663,   122,  8825,  3471,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,   495,  3413,   179,  5009,   178,  1449,   157, 13028,   372,\n",
      "         13644,  3514,  1112,   146,   347,  4223,   247, 22354,   122,   582,\n",
      "          5057,  1169, 22282,  2864,   323,   340,  2791,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101,   123, 22283,   117,   625,   368,   125,  1831,   122,  8410,\n",
      "           176,  9358,   256,   712,  6399,   180,   327,  1069,   117,  4018,\n",
      "           138, 20519,  3391, 22280,   143,   117,   320,   347,  1223, 17878,\n",
      "           117,  8273,  1084,  1796,  2684,  8111,   117,   179,   146,  4062,\n",
      "          2397,   229, 22280, 10348,   240,  2291,   102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,  3330,   256,  2643,   138,   146,  1223,   122,  1467,   230,\n",
      "         19462, 15799,   176,   229, 22280,  1796,  5288,  1062,   151,  7248,\n",
      "           125, 14903, 14738,  4195,   170,  2745,   117,   146,   179,   260,\n",
      "          1176,  2036,   273,  3718,  4408,   256,   260,  2980,  4388,   303,\n",
      "           143,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,   625,   259,   682,  8880,   117,   368,   262,  2044,  3030,\n",
      "          7831,   123,  4303,   117, 11831,  1121,   117,  1139,   146,  1342,\n",
      "           176, 15267, 22282,   603, 15736,   229,  9104,   117,   170,   222,\n",
      "          4217,   397,   125,   822,   375,   303,   117,  1904,  5974,  2684,\n",
      "           320,  1423,   180,   822,   747,   123,   327,  3985,   509,  2059,\n",
      "          1337,   122,   125,  4062,  1815,   268,   102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,  5009,   178,  1021, 12585,   222, 14883, 19450,   125,  1798,\n",
      "         11229,   125,  5530,   180,   505,  7700,   122, 20626,  1399,   118,\n",
      "           146,  2588, 18004,   175,   146,   317, 22279,   339, 11690,   240,\n",
      "           368,   117,   170,   230, 11489,   151, 20083, 22278,   298, 18908,\n",
      "           501,   117,   271, 15267,  4409,   123,  9463,  1423,  6628,   146,\n",
      "         14793, 11417,  1862,   221,   123,  2375,   117,   260,   223,   128,\n",
      "          9089, 12666,   538, 18456, 22281,   117,  1642,   383, 22278, 19255,\n",
      "           122,   222, 11552,   125,   425,   672,  6069,   138,  3456,   421,\n",
      "          4851,  1032,   143,   171, 10357,   298, 11224,   128,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,  2010,  4178,  1977,   418,   123,  3767,   240,   123, 22283,\n",
      "           136,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101, 16795,   870,   509,   117,   625,  4970,  5009,   178,  1941,\n",
      "          9294,   202,   283,  2014,   180, 11098,   322,   102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,  2010,  1977,   136,  2010,   146,   646,   314,  5895, 22279,\n",
      "           146,   317, 22279,   339, 13449,  1352,   230, 13779,  1378,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101,  2010,   179,   646,   314,  5895,   136,  2010,   146, 11522,\n",
      "         11551,   146,  1417,   171,  4141,   236,   117,  2397,   437, 22288,\n",
      "          9931,  7583,   854,  2028,   117,   179,   437, 22288, 10018,  1023,\n",
      "          3433,   485,  1557,   102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101,  2010,  1141,   117,  1141,   117,  1941, 18661,   117,   449,\n",
      "           318, 13793,   136,   102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[ 101, 2010,  418,  123, 3767,  240, 1564,  102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[  101, 11032,  2521,   102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101,   146,  7148, 15399,  5562,   244, 22281,   180,   313,  2245,\n",
      "           483,   364,   122, 13734,   249,  1353,   420,  1061,   230,  3743,\n",
      "           117,   179,  1367,   320, 19317,   175,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101,  2010,   122,   171,   766,  1391,   183,   117,   146,   766,\n",
      "          1391,   183,   125, 21990,  2204,   102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101,  2010,   125, 21990,  2204,   117,   271,   136,  2010,  1141,\n",
      "           117,  2397,   171,   766,  1391,   183,   125, 21990,  2204,   117,\n",
      "           179,   418,   607,  1510, 22281,   481,   202,  2187,   102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[ 101, 1257, 1141,  117, 2113,  978, 3138,  125,  179,  146, 3265, 4360,\n",
      "         2765, 2535,  229, 3952,  102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[  101,   123, 22296,  2080, 22280,  8886,   171,  1567,   102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[ 101, 2010, 1502,  122,  102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101, 16572, 13397,   178,  3748,   203,   259, 11224,   128,   202,\n",
      "         17749,   122, 16625,   221,   898,   123,  1457,  3743, 13407,   171,\n",
      "          2187,   125,  1543,  1112,  1465,   283,   102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101,  3695,   122,   139, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101,   317, 22279,   339,   434,   763,   125,  9606,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[ 101, 3370, 7567,  128,  179,  418, 1447, 3530,  189,  102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[ 101, 1465,  148,  102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[  101,   202,  3746,  2256,   180,   325,   526,  7629,   154, 10428,\n",
      "         22279,   102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101,  7273,   240,  1338, 17528,   123,   189,   102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101,  7871,  3870,   613,   699,   179,   117,   202,   372,  6685,\n",
      "           125,   997,   171,  5820,   117,  5229,  1266,  1921,  1855,   146,\n",
      "           121, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101,   646,   314,  5895,  4141,   236,   180,  7296,   256,   117,\n",
      "           125,  1977,   538, 12771,   203,   189,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[ 101, 1465,  148,  102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[  101,   122,   146,   139, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[  101,  5009,   178,  2055,   157,   180,  7296,   256,   625,   744,\n",
      "           538, 16995,   793, 14331,   173, 21990,  2204,   102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101,  7273, 14619,   210,   123, 17137,   117,   176,   483, 22287,\n",
      "           179,  1941,   173,   596,  7740,   403,   146, 15098,   793,  2160,\n",
      "           117,   179,  2587,  9398,   128,   318, 13793,   259,  2980,  3803,\n",
      "           942,   221,  7888, 22279, 14960, 22282,   171,  7275, 16302,   243,\n",
      "          1968, 22281,   236, 12531,   173,  7308,  1105,  2791,   122,   179,\n",
      "           117,  3382,   229, 22280,   637,   553, 22279, 14960, 22282,   793,\n",
      "           117,  5267,   793,  2044,   123,  2709,  8497, 13793,   125, 11935,\n",
      "           185,   118,  1340,   221,   144,  6562,   170,   146,  1338,   125,\n",
      "          4883,   118,   176,   368,   173, 18137,  4984,   117,   146,   179,\n",
      "          6643,   229, 22280,   176,  4172,   117,  2113,   117,  2160,   146,\n",
      "          3418,  3058,   428,   247,   117,  9916,   146,  7275, 19356,  3309,\n",
      "          6170,   123,  1742,   125,  2368,   117,   229,   615,   176,  7093,\n",
      "          4264,   170,  7523,   303,   143,   122, 19543, 22281,  7663,   102]])\n",
      "DEBUG: Tokenized sentence 67: tensor([[  101,  4395,   130,   118,   538,   744, 17137,   170, 15537,   123,\n",
      "           189,   102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[ 101, 1465,  148,  102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101,   179,   146,   121, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[  101,   646,   314,  5895,   262,  1684,  2533,  2994,  1440,   128,\n",
      "           532, 21642,   122,  1109,  5548,   559,  1522,   122,   179,   376,\n",
      "          2160,  3264,  4074,   117,  1971,   173, 18615,  1187,   117,   271,\n",
      "           700,   229,  1202,  2328, 22279,   229,  7231, 22278,   117,   122,\n",
      "           271,   169,  8388,   246,  3660,  3952,   117,   582,   117,   995,\n",
      "          1331,   368,   117,  1744, 16959,   256, 13707,   230,  1799, 18206,\n",
      "          1848,   102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101,   449,   117,  1075,   125,  6339,   118,   176,  5863,   117,\n",
      "          8781,   146,   121, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101,   646,   314,  5895, 10446,   159,  4922, 11126,   351, 15252,\n",
      "           404,   125,  3145,   122,  1028,  5077,   125,   179,   123, 22283,\n",
      "          4267, 22280, 22279,   117,   122,   170,  1966,  1338,  5229,   102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101,   240,   418,  1589,  1413, 10210,   793,   320,   139, 22282,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101,  5009,   178,  1449,   157,   180,  7296,   256,   117,   123,\n",
      "          1977,  1993,   543,  2599,   128,  5371,   138,   366, 14520,   179,\n",
      "          3283,  5835,   170,   146,  9931,   102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[  101, 22354, 16246, 22287,   118,   176,   259, 15322, 22281,   171,\n",
      "          2178,   102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101,  5009,   178,   117,  5651,   285,   123,  8092,   117,  5608,\n",
      "           146,  4127, 11646,   117,   222,  3848, 17714, 22279,   180,  1105,\n",
      "           117,   122,  7415,   118,  2036,   179,  2589, 13793,  5209,   210,\n",
      "          4945,   176,  1021,  1941, 12319,   123,  8059,   340,   171,  1567,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[  101,   146,  3848, 17714, 22279,  2927,  1695,   700,   117,  4111,\n",
      "           179,  1112,   744,   229, 22280,  7258,   117,   449,   179,   347,\n",
      "          1564,   123,  1796, 10467,   320,  1820,   247, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 78: tensor([[ 101, 2010, 2397,  368,  122, 1257,  102]])\n",
      "DEBUG: Tokenized sentence 79: tensor([[  101,  2638, 14376,  9805,   285,   102]])\n",
      "DEBUG: Tokenized sentence 80: tensor([[  101,   146, 13254,   418,  1004, 16924,   201,   117,  3189, 17558,\n",
      "          5799,   146,   179,  2097,   240,   329,   122,  6339,   118,   176,\n",
      "           202,  2187,   102]])\n",
      "DEBUG: Tokenized sentence 81: tensor([[  101,   229, 22280,  1684,   122,  1342,  3753,   102]])\n",
      "DEBUG: Tokenized sentence 82: tensor([[  101,  2010, 11032, 11032, 11032,   331,  1703, 22288,   146,   317,\n",
      "         22279,   339,   173,  1510, 22281,  3492,   102]])\n",
      "DEBUG: Tokenized sentence 83: tensor([[  101,  2798,  4763,   793,   149,  2291,   146,  2187,   125,  1543,\n",
      "           122,  1860,   215,   368, 11610,   230, 16318,   613,   699,   260,\n",
      "           514,   364,   176,  1968, 22281,   236,  5863,   102]])\n",
      "DEBUG: Tokenized sentence 84: tensor([[  101,  2010,   176, 11610,   102]])\n",
      "DEBUG: Tokenized sentence 85: tensor([[  101,  2010,  2684,  2036,  2826, 22280,   325,   102]])\n",
      "DEBUG: Tokenized sentence 86: tensor([[  101,  2798, 11736,   329,  2477,   117,  2113,   102]])\n",
      "DEBUG: Tokenized sentence 87: tensor([[  101,  3449,   434,   763,   117,  3508,  1445,   214,   123,  4410,\n",
      "           117, 16241,  1537,   148,  8156,  2036,  9392,   124,   123, 13444,\n",
      "           944,  4178, 22287,   125,  1977,   368,  5127,  2010,   179,   229,\n",
      "         22280,  7762, 22281,   236,   117,   229, 22280,  2826, 22280,   117,\n",
      "          2113, 17878,   102]])\n",
      "DEBUG: Tokenized sentence 88: tensor([[  101,  1112,  1977,  3189,  2541,   122,  1977,   229, 22280,  3189,\n",
      "          3497, 22354,   117,   271, 13503,   281,   146,  1342,   449,   122,\n",
      "          3767,   117,  4042,   159,   146,   179,   376,   123,  1434,   122,\n",
      "         14748,   125,  1160,   146,  3050,  2010,   123, 22283,   117,   123,\n",
      "         22283,  2010,   122,  3547,   117,   179,   644,   492,  9086,   368,\n",
      "          2636,  5863,   136, 16386,  1825,   260,  5207,   125, 11351,   122,\n",
      "          3598,  1552,   146,  1695,   455,   376,   102]])\n",
      "DEBUG: Tokenized sentence 89: tensor([[  101,  1141,   179,   368,   376,  3933,   144, 15266, 13808,   221,\n",
      "           577,   140,   102]])\n",
      "DEBUG: Tokenized sentence 90: tensor([[  101,   376, 11665,  9824,   591,   125,  1105,   173,   629, 22280,\n",
      "          9196,  4443, 13793,  2097,   146,   347,  5078,  2895,   125,  1169,\n",
      "           143,   376,   146,   274, 22178,   329,   229,  1105,   117,   582,\n",
      "           240,  1004,  4640,   122, 17796,  4517,   931,   247,   117,   122,\n",
      "           376, 22227, 11837,  3870,   138,   171, 14948,   342,   117,  3413,\n",
      "           122,  2010,   123,  7698,   117,  2113,   230,   122, 15978,   124,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 91: tensor([[  101,  2010,  1921,   122,   179, 16241,  1537, 22287,   123,  3189,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 92: tensor([[  101, 10719,   146,   317, 22279,   339,   117,   122,  3050, 22288,\n",
      "           146, 11552,  1362,  2009,   117,  4513,   526,  5484,   140,   179,\n",
      "          3933, 12448, 10200,  2949, 22281, 12773,   351,   146, 11170,   256,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 93: tensor([[  101,  2010,  9951,   529, 18544,  6134, 22280,  1147,   102]])\n",
      "DEBUG: Tokenized sentence 94: tensor([[ 101, 8480,  456, 5009,  178,  102]])\n",
      "DEBUG: Tokenized sentence 95: tensor([[ 101,  146, 1652,  122,  179, 2364,  325, 1104, 5799,  118, 2036, 5831,\n",
      "          102]])\n",
      "DEBUG: Tokenized sentence 96: tensor([[  101,  1502,  2389,   296,   117,   347,  4968, 21510,   117, 11665,\n",
      "          3145,   629, 22280,  1004,  7799,   221,   123, 12588,   102]])\n",
      "DEBUG: Tokenized sentence 97: tensor([[  101,   146,   317, 22279,   339,  2437,   351, 18862,   412, 10201,\n",
      "          2028,   180, 15978,   124,   102]])\n",
      "DEBUG: Tokenized sentence 98: tensor([[ 101, 2010, 2535,  102]])\n",
      "DEBUG: Tokenized sentence 99: tensor([[  101, 14000,   146,  1342,   117,   146,  1407,  1467,   179,   368,\n",
      "           176,  6344,  2160,  7148,   102]])\n",
      "DEBUG: Tokenized sentence 100: tensor([[  101,   146,   317, 22279,   339,  7278,   654,   102]])\n",
      "DEBUG: Tokenized sentence 101: tensor([[ 101, 2010, 7148,  136, 2010,  495,  123, 6272,  171, 4141,  236,  102]])\n",
      "DEBUG: Tokenized sentence 102: tensor([[  101,  2010, 11032,   117,  1114, 22279,   118,   176,  1659, 19652,\n",
      "           381,  1353,   434,   763,   117,  1904,  5974,   118,   176,   170,\n",
      "          6554,   183,   102]])\n",
      "DEBUG: Tokenized sentence 103: tensor([[  101,   538,  1941,  7273,   240,   123, 22283,   785, 15545,   130,\n",
      "           125,   549,  2010,   449,   117,  4968, 21510,   117,  1214,   252,\n",
      "           329,   117,   229, 22280,   122,  1257,   102]])\n",
      "DEBUG: Tokenized sentence 104: tensor([[  101,  2010, 11032,   146,   179,   117,  2397,   125,  4023,   122,\n",
      "           331,  2010,   333,  7148,   122,   331,  2010,   333,  7148,   122,\n",
      "           202,  1338,   125, 11169,   418, 22280,   176,  2579,   117,   260,\n",
      "           924,   240,  1510, 22281,   117,  7380,   325,  7769,   179,   260,\n",
      "         16594, 11899,  1599,  1908,   318, 13793,  3413,   376, 13071,   136,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 105: tensor([[  101,   146,  8152,   300,  2010,   122,   146,   317, 22279,   339,\n",
      "           563, 15736,   260,  3724,  2010,   146,  1161, 12444,  2684,  5357,\n",
      "           230,  3659,  1467,   123, 16758, 14738,   373, 12444,  4535,   307,\n",
      "           712,  5351,   277,  8128,  9250,   143,  2010,   449,   117,  4968,\n",
      "         21510,   102]])\n",
      "DEBUG: Tokenized sentence 106: tensor([[  101,  2010,   179,   818,   228,   347,  1247, 22279,   146,   317,\n",
      "         22279,   339,  3022,   256,   118,   176,   320,  6938,  7970,   733,\n",
      "          4861,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 107: tensor([[  101,  2010,   122,   318, 13793,   117,  4048,  1941,   125, 13425,\n",
      "         10780, 22278,   117,  4332,  2404,   117,   122, 16737,   222,  3848,\n",
      "         17714, 22279,   529,  1109,   319,   143,  2166,   102]])\n",
      "DEBUG: Tokenized sentence 108: tensor([[  101,   122, 15245,   123,  3743,   117,   188,  4643, 22282,  2537,\n",
      "           118,   123,  2010,   706,  7283,   118,   176,  2044,   170,   222,\n",
      "          2397, 11598, 21494, 11448,  9066,   444,  9066,   444,   179,   331,\n",
      "           543, 19758,   653,   221,   538,  6202,  3002, 13426,  2010,   449,\n",
      "           117,  4968, 21510,   117,  2354, 22279,  1014,   576,   229, 22280,\n",
      "           376,  3083, 13793,   102]])\n",
      "DEBUG: Tokenized sentence 109: tensor([[  101,  2010, 11032,   146,   179,   117,  2397,   125,  4023,   102]])\n",
      "DEBUG: Tokenized sentence 110: tensor([[  101,   229, 22280,  2826, 22278,   260,   514,  1402,  1502,  2354,\n",
      "         22279,  4750,   792,   327,  2267, 15982,   251,   117, 12222,   117,\n",
      "           240,   222,  8018,   136,  2354, 22279,  4750,   347,  5009,   178,\n",
      "           179,   123, 10502,   360,   232, 10786,  2460,   236,   123,   223,\n",
      "         22280,   125,   222,  1417,  3433,   485,  1557,   136,   176,  2354,\n",
      "         22279,  7762, 22281,   236,   123,   370, 10588, 22281,  4750,   179,\n",
      "          1061,   305,  4278,  6867,  1877,  6340,   401,   125,   222,  2917,\n",
      "           325, 12461,   179,   418,  3985,   387,   136, 11032,   117,   347,\n",
      "          4968, 21510,   117,  2354, 22279,   260,  1176,  2684,   311,  4048,\n",
      "           374,   326, 14351,  4133, 22288,   123,  3049,   304,   117,  5906,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 111: tensor([[  101,  2010, 11032,   117, 11032,   117, 11032,  1428,  7173,   256,\n",
      "           146, 12217,   117,   271,   260,   169,  8388, 22281,  3746,   470,\n",
      "           125,   222,  6205, 20626,   397,   102]])\n",
      "DEBUG: Tokenized sentence 112: tensor([[  101,   122,  7282,  2836, 13382,  3514,   173,  1719,   123, 12686,\n",
      "         22280,   180,  1390,  1970,   117,  7304,  2537,   125,   230,   221,\n",
      "           123,  1858,   223, 22280,   146,   347, 16936,   303,  2135, 22280,\n",
      "           125, 17678,   180,  1987,   151,   102]])\n",
      "DEBUG: Tokenized sentence 113: tensor([[  101,  2010, 11032, 11032,   117,  1114, 22279,   118,   176,  1659,\n",
      "           117,   347,  4968, 21510,   139,   226, 22290,   428,   389, 11913,\n",
      "          1486,  3643,  6429,   102]])\n",
      "DEBUG: Tokenized sentence 114: tensor([[ 101,  149, 3411, 5818,  228,  123, 4303,  102]])\n",
      "DEBUG: Tokenized sentence 115: tensor([[ 101,  495,  146, 1564,  170,  123, 8059,  340,  171, 1567,  102]])\n",
      "DEBUG: Tokenized sentence 116: tensor([[ 101, 2010,  125,  329,  102]])\n",
      "DEBUG: Tokenized sentence 117: tensor([[  101,   123,  3743,   125,  5009,   178,  1695,  8684, 21206,   180,\n",
      "          1858,   102]])\n",
      "DEBUG: Tokenized sentence 118: tensor([[  101,  2010,   449,   117,   870,   509,   179,  7093,  2354, 22279,\n",
      "           117,  4968, 21510,   136,   102]])\n",
      "DEBUG: Tokenized sentence 119: tensor([[  101,  1996,   368,   117,  3891,   123,  3743,   320,   317, 22279,\n",
      "           339,   117,   700,   125, 14998,   102]])\n",
      "DEBUG: Tokenized sentence 120: tensor([[  101,  2010,   179,   644,   492, 21174, 16512,   136,   102]])\n",
      "DEBUG: Tokenized sentence 121: tensor([[ 101,  123, 5664,  418, 2850,  240,  898,  102]])\n",
      "DEBUG: Tokenized sentence 122: tensor([[  101,  1114, 22279, 13239,   146, 10695,  2354, 22279,   229, 22280,\n",
      "          1996,   230,  4199,   179,  4750,  4270,   173,  3907,   523,   170,\n",
      "           123,  7698,   171,  7450, 22278,   136,   229, 22280,   607,  1407,\n",
      "          2880,   151, 22280,  2010,   338,   185,   118,   123,   170,  2134,\n",
      "          7755,   342,  8517,   102]])\n",
      "DEBUG: Tokenized sentence 123: tensor([[  101,   653,   260,  4103,   125,   629, 22280,  9196,  4443, 13793,\n",
      "          5572,  2104,   118,  2036,   102]])\n",
      "DEBUG: Tokenized sentence 124: tensor([[  101,  2389,   296,   176,   368,   260,  2811,   173,  1284,   117,\n",
      "          2779,   477,  4199,  1968, 22281,   236,   170,  3933,   102]])\n",
      "DEBUG: Tokenized sentence 125: tensor([[  101,  2010,   449,   146,   179,  2779,  2826, 22280,   117,  4968,\n",
      "         21510,   117,   122,   176,  7427,  1216,   118,  1340,   229,  3322,\n",
      "           125,  7343,  9931,   102]])\n",
      "DEBUG: Tokenized sentence 126: tensor([[  101,  2010,  9931,  1587,   578,   243,   117,   418,  6054,   179,\n",
      "           644,   492,   376,  2354, 22279,   170,   260,  3049,  4851,   125,\n",
      "           347, 10018,  4141,   236,   136,   102]])\n",
      "DEBUG: Tokenized sentence 127: tensor([[  101,  1054, 22281,   375,  2010,   449,   117,  4968, 21510,   117,\n",
      "          2354, 22279,  7093,   179,   229, 22280,   311,  1968,  3002,   136,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 128: tensor([[  101,  2010,  3002,   240,   179,   117,  2397,   125,  4023,   136,\n",
      "          1257,  3874,   376,   179,   792,   170,  2354, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 129: tensor([[ 101, 2010, 1084, 1257,  122, 3295,  102]])\n",
      "DEBUG: Tokenized sentence 130: tensor([[  101,   123, 22296,  1858,  5664,  7427,  9730, 22278,   118,  1340,\n",
      "          5863,   173,  1105,   136,  2010,   122,   102]])\n",
      "DEBUG: Tokenized sentence 131: tensor([[  101,   240,   222,  1341,   117, 12444,   333,  1016,   102]])\n",
      "DEBUG: Tokenized sentence 132: tensor([[  101,   944,  4178, 22287,   260, 18237,   303,   143,   179,  2354,\n",
      "         22279,  1981,   320,   975,  5242, 22280,  3103, 22279,   122,  5628,\n",
      "          1151,   455,  4765,   240,   123, 22283,   117,   202,  1652,   179,\n",
      "           229, 22280,  2036,  9730,  1875,   146,  1417,   102]])\n",
      "DEBUG: Tokenized sentence 133: tensor([[  101,   449,   117,   240,  1342,  1341,   117,  7343,  8588,   339,\n",
      "           117,   229, 22280, 18661,   146,   179,  2036,  2826, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 134: tensor([[  101,   122,   700,   125,   230, 18827,   173,   179,   146,  1342,\n",
      "           229, 22280, 11234,  2010,  2397,   117,   347,  4968, 21510,   117,\n",
      "          3413,   125,  3285,   140, 13254,   143,   173,  1105,   102]])\n",
      "DEBUG: Tokenized sentence 135: tensor([[ 101,  122,  146,  644,  492, 2010,  125, 7716,  179,  102]])\n",
      "DEBUG: Tokenized sentence 136: tensor([[  101,  2010, 17331,   514, 22287,   602,  6181, 22287,  3002,   145,\n",
      "         10144, 13806, 13397,   178,   229, 22280, 12123, 22288,   117,   240,\n",
      "           210, 14000,  2010,   449,  2779,  9730, 22280,  9442,   259, 17080,\n",
      "           958,  2118,   143,   171,  2699,   102]])\n",
      "DEBUG: Tokenized sentence 137: tensor([[  101,  2010,  1257,   122,   785,  3575,  2010,   122, 17080, 11314,\n",
      "          2650,  1058,   136,   229, 22280,  9824, 22287,  5863,   170,  1039,\n",
      "           136,   102]])\n",
      "DEBUG: Tokenized sentence 138: tensor([[  101,  2010,  1141,  1996,   146,   317, 22279,   339,   117,  4764,\n",
      "         19309,   348,   118,   176,   117,   449,   259,  8699,   298, 11314,\n",
      "          2650,  1058,   629, 22280,   944,  7226,   793,  1149,   118, 19908,\n",
      "           117,   122,   538,   229, 22280,  4178,   793,   123,   179,   538,\n",
      "          5127,   146,  1815, 14914,   125,   144,  6562,   102]])\n",
      "DEBUG: Tokenized sentence 139: tensor([[  101,  2397,   117,  4968, 21510,   117, 17331,   178,   157,  3539,\n",
      "           125,   332,   145,   117,  1981,  2765, 14689,  8725,   102]])\n",
      "DEBUG: Tokenized sentence 140: tensor([[  101,  2010,  5787,   229, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 141: tensor([[  101,  2010,  1141,   117,   449,   122,   325,  2711,   179, 10726,\n",
      "         22279,   146,   317, 22279,   339,  8697,  1056,   351,   123,  5848,\n",
      "           285,   170,  4863,   388,  2000,  7336,   102]])\n",
      "DEBUG: Tokenized sentence 142: tensor([[ 101, 2010,  173, 1364, 1652,  102]])\n",
      "DEBUG: Tokenized sentence 143: tensor([[  101, 18994,  1353,  5009,   178,   117,   122,   240,  1695,   596,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 144: tensor([[ 101, 5787, 5664,  125,  222,  454,  102]])\n",
      "DEBUG: Tokenized sentence 145: tensor([[  101,   122,   117,   331,   269,   348,   123,  4410,   117, 11831,\n",
      "          1121,   117,   170,  7672,  1202, 22287,  1659,   102]])\n",
      "DEBUG: Tokenized sentence 146: tensor([[  101,   229, 22280,   311,  5572,   508, 16550,   671, 22282, 11032,\n",
      "           321, 22305,   102]])\n",
      "DEBUG: Tokenized sentence 147: tensor([[  101,  1141, 15212,   125,  4270,   173,  3907,   523,   170,   368,\n",
      "           117,   122,   102]])\n",
      "DEBUG: Tokenized sentence 148: tensor([[ 101, 3413,  329,  221,  538,  102]])\n",
      "DEBUG: Tokenized sentence 149: tensor([[  101,  1467,   230,  2135,  2993,   117,   179,   311,  9086, 22278,\n",
      "          2643,   102]])\n",
      "DEBUG: Tokenized sentence 150: tensor([[  101,  2113, 17878,   102]])\n",
      "DEBUG: Tokenized sentence 151: tensor([[  101,  2354, 22279,  4178,   179,   102]])\n",
      "DEBUG: Tokenized sentence 152: tensor([[  101,  2010,   123, 22296,  8082, 13665,   146,   317, 22279,   339,\n",
      "           117, 12086,   230,   940,  9110,   102]])\n",
      "DEBUG: Tokenized sentence 153: tensor([[ 101, 1257,  122, 1342, 8032,  102]])\n",
      "DEBUG: Tokenized sentence 154: tensor([[  101,   240,   123, 22283,   122,   179,   378,   289, 12444,   370,\n",
      "           905,  2472,  2010,  1141,   117,  1204,  5009,   178,   117,   170,\n",
      "           325,  6015, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 155: tensor([[  101,  2354, 22279,  1004,  4178,   179,   229, 22280, 15212, 18237,\n",
      "           304, 22280,   125,  2765, 14283,   140,   118,   311,   170,   146,\n",
      "           149,  9811,   268, 11522, 11551,   102]])\n",
      "DEBUG: Tokenized sentence 156: tensor([[ 101,  122,  117,  176, 1004,  179,  102]])\n",
      "DEBUG: Tokenized sentence 157: tensor([[  101,  2010,   126,  5689, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 158: tensor([[  101,  1191,   146,  7148,   117, 20485,   214,   123, 10415,   117,\n",
      "           122,  1996,  2010,  9730, 22279,   146,  2397, 22279,  5127,   180,\n",
      "          1390,  1970,   117, 18763,   214,  2044,   146,   347, 12164,   914,\n",
      "         13573, 22280,   122, 18223,   388,   125, 19462, 22282,   124, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 159: tensor([[  101,   320,  3767,   210,   123, 17935,   404,  9480, 14948,   117,\n",
      "          1941,   173, 20170,   125, 17611,   117,   259, 11690,   221,  5197,\n",
      "          1719,   125, 18271,  2382,   300,   221,   269,   373,   180, 11471,\n",
      "           122,  1270,  3626,   214,   498,   146,   475,  2733,   421,   222,\n",
      "         11552,  3848, 22279,   122, 13140,   125, 15756,  7919, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 160: tensor([[  101,  2010,   318, 13793,   117,  1684,   437, 14209,  7485, 22279,\n",
      "           117,  7122,   853,  6522,  1337,   136,   102]])\n",
      "DEBUG: Tokenized sentence 161: tensor([[ 101, 1996,  146, 1568,  102]])\n",
      "DEBUG: Tokenized sentence 162: tensor([[  101,   122,  4108, 21307,   123,  2267,   117,   170,   222,  3979,\n",
      "           994,   125, 17275,   102]])\n",
      "DEBUG: Tokenized sentence 163: tensor([[  101,   740,  1011,  5034,  3264,   170,   146,   347, 12413, 18206,\n",
      "          6568,   125, 13526,   154, 22280,   117, 20073,   117,  1364,   765,\n",
      "           364,   214,   712,  1941, 22281,   485, 22281,   180,  1956, 21305,\n",
      "         22278,   170,   146,   347,  1690,  7817,   125,  1877, 13732,   252,\n",
      "         20933,   477,   151,   117,  5088,  2876,   392,   214,   146,  9169,\n",
      "         15440,   162,   117, 12837,   303,   122,  1004,  2160,   170,   146,\n",
      "           347, 10881, 17291,   268,   117,  5546,   183,   122,   176,  8662,\n",
      "           117,   179,  7904,   823,   351,   173, 20674, 22281,   202,  2979,\n",
      "           180,  3049,   304,   122, 18049,   351,   202, 13227,   303,   432,\n",
      "         14472,  5822,   243, 10968,  2050,  6044,   246,   102]])\n",
      "DEBUG: Tokenized sentence 164: tensor([[  101,  2010,   978, 22281,  6775,   179,   229, 22280,  8544, 22281,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 165: tensor([[  101,  2010,  1447,   176, 12232, 22282,   117,  5848, 22283,   102]])\n",
      "DEBUG: Tokenized sentence 166: tensor([[  101,   122, 20582, 22288,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 167: tensor([[  101,  2010,  1084, 17891,  1084, 17891, 14351, 13754,   202, 20462,\n",
      "           171,   317, 22279,   339,  2010,  3285, 22280,   118,  2036,  2401,\n",
      "         22279,   524,   117,  2678,   147,   117,  4968, 21510,   136,   102]])\n",
      "DEBUG: Tokenized sentence 168: tensor([[  101,  2389,   296,   271,   146,   644,  2014,   180,  3113,   418,\n",
      "          6726,   364,   117,   229, 22280,   122,   136,  2010,   872, 14527,\n",
      "           555,  3061, 19927, 22281,  2010,   179,   136,   102]])\n",
      "DEBUG: Tokenized sentence 169: tensor([[  101,   558,   537,  2462,   203,   146, 19317,   175,   117,  2389,\n",
      "          7831,   221,   146,  5973,   439,   247,   180, 17935,   404,   102]])\n",
      "DEBUG: Tokenized sentence 170: tensor([[  101,  1256,   122,  5899,   122,  2779,   179,   744,   978,   125,\n",
      "           925, 12932,  8364,   171, 19537,  2014,   125,   222,   417,  8497,\n",
      "           159,   102]])\n",
      "DEBUG: Tokenized sentence 171: tensor([[  101,   122,   262, 12022,   803, 13550,   202,  3147,   117,   123,\n",
      "          9247,   578,   221,   146,  4127, 11646,  1112,   179,  2036,  1904,\n",
      "         22281,   236,  6205, 22278,  1623,   324,   221,  5319,  2430,   146,\n",
      "          9169, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 172: tensor([[  101,   146,   317, 22279,   339, 20582, 22288,   118,   176,   975,\n",
      "          1885,   185,   125,  9480, 14948,   102]])\n",
      "DEBUG: Tokenized sentence 173: tensor([[  101,  2010,   318, 13793,   582,   122,  1790,   146, 17611,  7122,\n",
      "          8598,  8707,  4419,   136,  2010,   123,  1105,   171,   958,  1025,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 174: tensor([[  101,   229, 22280,   176, 10201,   136,  1863,   243,   304,   659,\n",
      "           481,  1790,   102]])\n",
      "DEBUG: Tokenized sentence 175: tensor([[  101,  2010,   504, 19147, 22279,  7273,   318, 13793, 16573,   125,\n",
      "          4783, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 176: tensor([[  101,  2010,  5848, 22283,  1968,   221,   146, 14890,   102]])\n",
      "DEBUG: Tokenized sentence 177: tensor([[  101,   962, 22281,  6867,   383, 22279,   229, 22280,  2541,   117,\n",
      "          5750,  6436,   268,   136,  2010,  5787,  1183,   304,   123,  2954,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 178: tensor([[  101,   170, 11304,   607,  4654,   304,   102]])\n",
      "DEBUG: Tokenized sentence 179: tensor([[ 101, 2010, 4429,  118, 4429,  102]])\n",
      "DEBUG: Tokenized sentence 180: tensor([[  101,   449,  3960,   247,   179,   146,   958,  1025,  1284,   170,\n",
      "           230, 10013,   180,   450, 14505,  8723,   304,   102]])\n",
      "DEBUG: Tokenized sentence 181: tensor([[  101,  1996,  8069,  1337,   117,   420,  3679,   123,  8530,  1235,\n",
      "           931,   259,  3370,   683,   171,   347, 12413,   170,   123,  4351,\n",
      "         14139,   180, 10978, 11324,   102]])\n",
      "DEBUG: Tokenized sentence 182: tensor([[  101,   149,  3411,   117,  9226,   228,   118,   176,  5818,  6805,\n",
      "         22280,   260,  6929,   171,  5209,   210,   117,   179,   176,  3030,\n",
      "         15736, 22287,   170,   739,  1315,   286,  2455,  2034, 18182,   117,\n",
      "           122,  2044,   173,  2590,   146,  4081, 12518,   125, 10674, 18968,\n",
      "           442,   229, 19104,   102]])\n",
      "DEBUG: Tokenized sentence 183: tensor([[  101,  1085,   259, 11314,  2650,  1058,  5790,  1939,   923,   221,\n",
      "         14890,   102]])\n",
      "DEBUG: Tokenized sentence 184: tensor([[  101,  3033,   652,   229, 17935,   404,   146,  7489,   183,   549,\n",
      "         17892,   102]])\n",
      "DEBUG: Tokenized sentence 185: tensor([[  101,  1456,   143,   298,   532,  7187,   122, 14730,   481, 12825,\n",
      "         17213,   117,  2996, 22280,   117,   125,  4351,   339,   272,   122,\n",
      "         17897,   122,   329,  3027,   252,   455,   102]])\n",
      "DEBUG: Tokenized sentence 186: tensor([[  101,  1956,   581,   256,   118,   176,   125,   739,  8230,   125,\n",
      "          3568,   304, 22280, 21612,   118,  2036,  1112,   230, 22290,   268,\n",
      "         22354,   102]])\n",
      "DEBUG: Tokenized sentence 187: tensor([[  101,   221,  4042,   159, 19385, 22281,   171,  2699,   229, 22280,\n",
      "          1021,  1342,   549, 17892,  1112,  3285,   151,   202,  4102,   293,\n",
      "           146,   853, 10583,   397,   325,   375,  3301,   243, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 188: tensor([[  101,   298, 11677,   180,  1105,   495,   146,   325,  2600,  2364,\n",
      "           117,   240,   210,  7543,   900, 22278,   370,  3316,   229,  2707,\n",
      "          5371,   838,  2836,  1684,   125,  1796,   122,   978,   240,  1257,\n",
      "           222,   146,   635,  1401,   243,   320,  9019, 13793,   146,   635,\n",
      "           117,   179,   146,  4286,  5477, 12746,   304,   256,   240,   389,\n",
      "          4060, 13449, 19088,   125,  3264,  6272,   102]])\n",
      "DEBUG: Tokenized sentence 189: tensor([[  101,   449,   146,   347,   636,  2455,   373,   146,   179,  2643,\n",
      "           138,  3153, 13808,   598,   368,   712,   211,   683,   366,  2010,\n",
      "          6372,  3832,  2010,   171,  1847,   523,   146,   179,  6834,   256,\n",
      "           229,  1174,   304,   123,   327,   229, 22280,  3302,   229,  2707,\n",
      "           180,  1105,   210,   179, 10454,  1021,  1971,   596,   117,   495,\n",
      "           834,   623,  2100,   123,   327,  4867,   221,   146, 10832,   102]])\n",
      "DEBUG: Tokenized sentence 190: tensor([[  101,   712, 18433,  3285,   151,   118,  3133, 22278,  7392, 22282,\n",
      "           421,   122,  9086,   125,  1364, 10671,  4496, 20383,   178,   102]])\n",
      "DEBUG: Tokenized sentence 191: tensor([[  101,  7489,   183,  7570,   203, 22243,  1212,   123, 17935,   404,\n",
      "          3952,  9420,   170, 19519,   285, 12133,  1076,   146,   317, 22279,\n",
      "           339,   122,  9480, 14948,   117,   122,  5793,  2044,   221,   146,\n",
      "         15796,   175,   117,   582, 19575, 22287,   944,   259, 11314,  2650,\n",
      "          1058,   180,  1105,   102]])\n",
      "DEBUG: Tokenized sentence 192: tensor([[  101,   146,   995,   123,  3852,   262,   866,  8849,   378,   125,\n",
      "          4411,  8598, 12060,   713,   122, 22003,   390,   289,   154, 22280,\n",
      "           125, 12530,   481,   117,   170,   260,   675, 19423,  1665,   138,\n",
      "          4516, 10863,   117,   179,   146,  4885,   171, 15273, 13808, 22280,\n",
      "           744,   229, 22280,   978, 10667,  8334,   102]])\n",
      "DEBUG: Tokenized sentence 193: tensor([[  101,  1011,  1684,   125,  4062,  6590, 21990,   181,   537,  2836,\n",
      "           118,   176,   125,   222, 15975, 15588, 11233,  2152,   587,   154,\n",
      "           415,   122,   125,  2364,  5110, 13188, 18758,   148,   202,  1010,\n",
      "           215,   102]])\n",
      "DEBUG: Tokenized sentence 194: tensor([[  101,   173,  1105, 17226,  5076, 22278,  7989,   125,  3084,   256,\n",
      "         10768,   122,   179,  3497,   256,  1434,  8446,   125,   504, 11705,\n",
      "         22278, 22278,  7800,   117,   221,  7282,   159,   712, 18433,   122,\n",
      "           221,   925,   712, 21145, 22281,  8339,   125, 17229,   304, 22280,\n",
      "           117,   122, 12909,   256, 11628,  5567, 22281,   272,   682,  4698,\n",
      "           255,   102]])\n",
      "DEBUG: Tokenized sentence 195: tensor([[  101,   146,   739,  2455,   373,  2166,   495,   230,  8526,   202,\n",
      "         10862,  1456,   143,   117,   146,   179, 16403,   123,  3264, 11259,\n",
      "           171,  1847,   523,   123,  4640,  1112,   179,   368,   495,   222,\n",
      "           739,  3425,  1186,   117,   222,   337,   162,   154,   117,   179,\n",
      "          1011,  1684, 12625,   146,   455,  9784, 22354,   146,  7489,   183,\n",
      "           549, 17892,  4332,   285,   256,   118,  2036,   260,  1176,   117,\n",
      "         18928,  1212,  2010,   170,   259,   644,  3471,   146,  9019, 13793,\n",
      "          1941,  2036,   376,  3433,   123,  9050,   179,   229, 22280,  5971,\n",
      "           125, 11314,  2650,  1058,  3667,   125, 10441,  1970,   136,   102]])\n",
      "DEBUG: Tokenized sentence 196: tensor([[  101,   176,  2354, 22279,  3189,   333,  5991,   243,   117,  1447,\n",
      "          1174,   144,  6562,   117,   347,  9066,   157,  3209,   154,   378,\n",
      "          8362, 22278,  9442,  5809,   122,  6134,   138,  3330,  5146,   117,\n",
      "           449,   117,   179,  1434,   136, 11736,  5076, 22278,  1069,   102]])\n",
      "DEBUG: Tokenized sentence 197: tensor([[  101,   146,  1342,   495, 11314,  2650,   397,   325,  2600,   229,\n",
      "          1105,   102]])\n",
      "DEBUG: Tokenized sentence 198: tensor([[  101, 18079,  2836,   118,   176,   117,   834,  1428,  7173, 22282,\n",
      "           117,   122,   173,  8206,  2880,   247,   828,   185,  8174,   788,\n",
      "           117,   416,  1149,   320,   347,  4062,  6590,   102]])\n",
      "DEBUG: Tokenized sentence 199: tensor([[  101,   320,  3852,   412, 17935,   404,   262,  1528,  5782, 22281,\n",
      "           303,   202,   347, 15322,   123,  2267,   171,  9019, 13793,  2080,\n",
      "           653, 22278,  9079,   117, 13449,   172, 22282,   117,   122,  4640,\n",
      "           117, 11417,  7533,   123,  3049,   304,  1112,  7122, 17704,   102]])\n",
      "DEBUG: Tokenized sentence 200: tensor([[  101, 22354,   146,   317, 22279,   339,  1023,   230,  3979, 11538,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 201: tensor([[  101,  2010,   179, 14689,   436,   102]])\n",
      "DEBUG: Tokenized sentence 202: tensor([[  101,  4031,  2904,   170,   259,   532, 11967, 22280,   143,   102]])\n",
      "DEBUG: Tokenized sentence 203: tensor([[  101,   173,  2590,   117,  7570,   203,   123, 17935,   404,   117,\n",
      "           785,   803, 13550,   117,   170,   260,   223,   128,  3235, 13097,\n",
      "           529, 14563,  8037,   138,   272,   222,  1941,   455,   154, 22280,\n",
      "           117,  3561,  1716, 22278,  2036,   695,   151,  2684,   123,  1444,\n",
      "           304,   117,   230,   854,  2028,   125,  7226,  1027,   481,   125,\n",
      "          2169,   102]])\n",
      "DEBUG: Tokenized sentence 204: tensor([[  101,   978,   146, 10881,   123,  8747,  3711,   252,   259, 21423,\n",
      "           739,   246, 15914,   657, 14659, 14790,   138,   125,  1757, 21304,\n",
      "           185, 15277,   591,   229,   475,   508,  5708, 15267,  8052, 22000,\n",
      "         22281, 18224,   442,   117,   122,   222,  4863,  2115,  6372,   286,\n",
      "           125, 12862,   123,  3049,   304,   538, 20462, 22281,   117,   179,\n",
      "          2662,   338,   151,   146, 19877, 22280,   125,  4915, 13227,   303,\n",
      "           143,   102]])\n",
      "DEBUG: Tokenized sentence 205: tensor([[ 101,  860,  495,  173, 2745,  325, 1160,  179,  259,  736, 2010,  173,\n",
      "         2169,  117,  229, 1105,  117,  122,  202, 1010,  215,  102]])\n",
      "DEBUG: Tokenized sentence 206: tensor([[  101,  3767, 22278,  1021,  5664,   272,  2139,  2112,   180,   327,\n",
      "          6321,   202,  5457, 10355,  6542,   118,   176,  5009,   178,  6199,\n",
      "           122,   978,  1684,   259,  5708, 19613,   272,  5334,   900,   123,\n",
      "          2954,   170, 10428,   513,   180,   223, 22279,   122,   180,  2480,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 207: tensor([[  101,   240,   333,   146,   325,  1160,   229,  1105,  8525,   322,\n",
      "           146,  5209,   210,  1743,   321,   256,   260, 17772,  2028, 22281,\n",
      "           122,  9066,  2264,   259, 16455,   125,  4840, 13793,   102]])\n",
      "DEBUG: Tokenized sentence 208: tensor([[  101,   944,  2036,  3985,   923,   834,  5296,  1758,   117,   229,\n",
      "         22280,   978,   123,  1977,   176,   179,  1445, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 209: tensor([[  101,  7465,  2916, 22287,   118,   176,   123,  2803,   154,  2461,\n",
      "         18419, 22287,   118, 13427, 22287,  7729, 10766,   340,   366,   675,\n",
      "         17722,   842, 13086, 22281,   125, 21340, 15357,   102]])\n",
      "DEBUG: Tokenized sentence 210: tensor([[  101,  5510, 22279,  2836,   118,  2036,   123, 15719,   230,   739,\n",
      "          3131,   352,  1573,   262,   222,   338,  7052, 22290,   252, 22280,\n",
      "           179,  2628,   229,   681,  2954,   173,   455,  2036,  7320,   230,\n",
      "          2551,   221, 18165,   102]])\n",
      "DEBUG: Tokenized sentence 211: tensor([[  101,   146,  6754,  2166, 22282,   825,  6199,   117,   179,   229,\n",
      "         22280,  9679,  5110,   118,   176,   170,  4327, 13192,  9635,   304,\n",
      "           117,  6600,   229,   260,   514,   364,   125,  3285,   140,   652,\n",
      "           259,  1143,   117,   122, 11510, 22281,  1084,   262,   240,  5530,\n",
      "           125,   230,  8097,   125, 13718,   268,   125,   222,   442,  9288,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 212: tensor([[  101,  1065,  1966,   644,  1767,  1420,   173,  1105,   412, 20025,\n",
      "           125,  1112,  1390,   154,   118,  1690, 22280, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 213: tensor([[  101,  5078,   546,   118,  2036, 21630,  2996,   128,   122, 21612,\n",
      "           118,  2036,  1112,   146,  5664,  2010,   146,   528,  3356, 22280,\n",
      "          2010,   146,  7949,   304, 22354,  2745, 15724,   221,   146,  6542,\n",
      "           210,   117,  1528, 22280,   347,  6149,   655,   102]])\n",
      "DEBUG: Tokenized sentence 214: tensor([[  101,  8544, 13910,   214,   123, 17935,   404,   117,   271,   222,\n",
      "          7395,   268, 17154,   487,   117,  1821,   123, 13239,   102]])\n",
      "DEBUG: Tokenized sentence 215: tensor([[  101,   146,   317, 22279,   339,  9247,   654,   240,   368,  2010,\n",
      "           146,  3265,   136,  6952,  9461, 13397,   178,  6199,  2927,   117,\n",
      "           915,  5587,   117,   144,  2746,   123,  1444,   304,   117,   785,\n",
      "           598, 14996,   834, 14748,   259,  5708,   102]])\n",
      "DEBUG: Tokenized sentence 216: tensor([[  101,  9480, 14948,  1023,   222, 11552,   125, 13779,  1200,   102]])\n",
      "DEBUG: Tokenized sentence 217: tensor([[  101,  2010,   318, 13793,   179,   122,  1257,   136,  1996,   146,\n",
      "           317, 22279,   339,   102]])\n",
      "DEBUG: Tokenized sentence 218: tensor([[  101,  4048, 22281,   118,   311,   222,  7395,   268,   171,  8788,\n",
      "         22280,  3887,  2368,   170,   123,  9349,   117, 13254,  1904,   539,\n",
      "          1921, 13850, 17739,   151, 22279,   117,   170,   123,   327,   223,\n",
      "         22280,  7352,   122, 16907,   117, 16322,  2071,   118,  2036,   423,\n",
      "           179,  1391,   123,  3049,   304,   117,   179,  5009,   178,  6199,\n",
      "           601,  4681,   210,   370,  4098,   102]])\n",
      "DEBUG: Tokenized sentence 219: tensor([[ 101, 2010,  860,  744,  418,  785,  279, 1350,  102]])\n",
      "DEBUG: Tokenized sentence 220: tensor([[  101, 14000,   102]])\n",
      "DEBUG: Tokenized sentence 221: tensor([[  101,   122, 16795,   118,  2036,   700,   230,   240,   304, 22280,\n",
      "           125,  4486,  1112,   176,   978,  6272,   125, 20613,   943,   117,\n",
      "           176,   229, 22280,  2448, 15736,  1941,   170,   230,   662,   404,\n",
      "           176,   978,  3382,   146,  3852, 22280,   866,  1620,   581,   117,\n",
      "           176,  3530, 22278,   123,   388,  4056,   366, 19271,  1149,   102]])\n",
      "DEBUG: Tokenized sentence 222: tensor([[  101, 22354,   146,  3265,   449,   193,  8041, 12566,  8909, 22282,\n",
      "           393, 15776,   117,   170,   222, 13449, 19088, 10953,  3093,   183,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 223: tensor([[  101,  2010,   271,   437, 19493,   136,   368,   229, 22280,  9396,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 224: tensor([[  101,  2010,   318, 13793,   229, 22280, 12416, 22281,   136,   102]])\n",
      "DEBUG: Tokenized sentence 225: tensor([[  101,   170, 11304,   188,  5009,   821,  1456,   143,   994,  1462,\n",
      "         22279,   203,   123,  3049,   304,  2275,  1612,   117,   122,  7837,\n",
      "           654,   123,  9463,   117,   221,  8984,   146,  3979, 22280,   179,\n",
      "          1703,  7870,   256,   230,  1201, 22292,   565,   102]])\n",
      "DEBUG: Tokenized sentence 226: tensor([[  101,  2010,   318, 13793,   122,   170,   123,  3049,   304,   179,\n",
      "           176, 12416,   136,  5023,   229, 22280,  4178, 22281,  5961,   117,\n",
      "         22232,   715,   136,   122,   117, 12809,   118,   176,   221,  9480,\n",
      "         14948,  2010,  3413,   122,   222, 10195, 22280,   117,  7122,  8707,\n",
      "          4419,  2389,   296,   173,   179,  1177,   368,  3889,   260, 17722,\n",
      "           842,   176, 10354,   123,  8410,   271,  2050,   146,  1831,   117,\n",
      "           706, 22281,   180,   118,  1084,   320,   644,   492,  5023,  1941,\n",
      "           437, 15982,  6148,  5863,   117,   528,  3356, 22280,   136,  5009,\n",
      "           178,  6199,   117,   229, 22280,  5735,  1941,  4317, 22282,   259,\n",
      "         10786,   942,   117,  6540,   123,  9463,   122,   117,   170,   123,\n",
      "           344,   304,   125,   230,  1945, 17124,   117,   331,  1703, 22288,\n",
      "           146,  3979, 22280,   179,   123,  1971,  5155,  1314,   952,   256,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 227: tensor([[  101,  2010,  2389,   252,   179,  3769,   123,  2803,  6720, 22282,\n",
      "           118,   311,   117,   146,  4286,  5477,  9247,   654,   146,   317,\n",
      "         22279,   339,   102]])\n",
      "DEBUG: Tokenized sentence 228: tensor([[  101,  4062,   117,  4062,  2541,   118,   437,  2541,   118,   370,\n",
      "          7826, 15249, 22288,   118,   146,   122,  1743,  9390,   123,  3985,\n",
      "           387,   170,   146, 16936,   303,   102]])\n",
      "DEBUG: Tokenized sentence 229: tensor([[  101,  9480, 14948,   318, 13793, 17522,   259, 17084,   412,  3049,\n",
      "           304,   171,  9778,   122, 14537,   203,   118,   146,   221,   898,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 230: tensor([[  101,  3456,   421,  1353,   118,  2036,   260,   464,  1557,   180,\n",
      "          1941,   455,   154,   122, 12633,   654,   118,  2036,   260,   877,\n",
      "           842,   102]])\n",
      "DEBUG: Tokenized sentence 231: tensor([[ 101, 2072, 1664, 7345,  122, 5980,  138,  102]])\n",
      "DEBUG: Tokenized sentence 232: tensor([[  101,  2010,   123, 22296,  9162, 18417,   740,   117,  2354, 22279,\n",
      "         14619,   210,   229, 22280,   122,   316, 22280,  3265,   117,   179,\n",
      "           176, 13201,   269,  3413,   102]])\n",
      "DEBUG: Tokenized sentence 233: tensor([[  101,   122,   117,  4551,   214,   171,   347, 21229, 20383,   178,\n",
      "           230,  3689, 15737, 13808,   117,   662,  1353,   117,   170,   739,\n",
      "         10013,   171, 11314,  2650,   397,   122,   352, 22279,   171,   317,\n",
      "         22279,   339,   117,   123,  1743,  1329,   260,   877,   842,   180,\n",
      "           854,  2028,   117,  4111,   320,  1342,   117,  6779,   994,  2010,\n",
      "           229, 22280, 18661,   271,   607,   223,   143,   179,   176,  2531,\n",
      "         22287,   125,  2292,  1014,  2169,   102]])\n",
      "DEBUG: Tokenized sentence 234: tensor([[  101, 14619,   210,   117,   144,  5760,  3921, 13806, 22282,   702,\n",
      "           785,   102]])\n",
      "DEBUG: Tokenized sentence 235: tensor([[  101,   123,   327,  4410,   978,  1941,  5443,  6358, 12250,   125,\n",
      "          3165, 17688,   102]])\n",
      "DEBUG: Tokenized sentence 236: tensor([[  101,   146,   317, 22279,   339, 17760,   118,   176,   122,   262,\n",
      "         18119,   578,   118,   176,   320,   221,   269,   373,   180, 17935,\n",
      "           404,   117,  1139,  9480, 14948,   117,   179,  5371,   838,  2836,\n",
      "           123, 16633,   260,   877,   842,   171,  9778,   117,  8544,   173,\n",
      "         11021, 10573,   214,   123,   860,   176,   229, 22280,   978, 10428,\n",
      "           513,   180,   327,  2578,   122,   176,   229, 22280,  5334,  5630,\n",
      "           320, 13519,   118,   176,   180,   223, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 237: tensor([[ 101, 5009,  178, 6199, 1011, 1063, 2646,  102]])\n",
      "DEBUG: Tokenized sentence 238: tensor([[  101,   495,   123,   681,   576,   179,   202,  1010,   215,  2036,\n",
      "         18402, 22287,   170,  7583,   370, 13397,   124,   102]])\n",
      "DEBUG: Tokenized sentence 239: tensor([[  101, 17760,   123,  3049,   304,   122,  4619,   203,  9480, 14948,\n",
      "           368,   117,   179,   978,  1684,   146, 11552,  3378,   122,  9597,\n",
      "           117, 11556,   117,   834,   256, 19720, 22282,   117,   259,  5708,\n",
      "           180, 10173,  6943,   122, 10692,   654,   118,   259,   117, 13140,\n",
      "           125, 12908,  2028,   117, 21128,   240,   740,   222,   695,   373,\n",
      "          3953,   117,   230, 14738, 19773,   125,  2251, 10780, 13793, 14313,\n",
      "           285,   102]])\n",
      "DEBUG: Tokenized sentence 240: tensor([[  101,   870,  1185,  5630,   118,   176, 10376,  2623,   247,   320,\n",
      "          6754,  8149,   183, 10968,   852,   243,   125,   944,   117,   179,\n",
      "         22278,  3365, 17704,  2509,   117,   316, 22280,  1743,   321,   117,\n",
      "           316, 22280,  1004, 12232,   285,   117,   316, 22280,  3980,   809,\n",
      "           285,   122,   170,   260,   223,   128,   316, 22280, 13376,   138,\n",
      "           117, 10606,  8397,   123, 16633,   118,  2036,   122,  3791,   159,\n",
      "           118,  2036,   260,   877,   842,   102]])\n",
      "DEBUG: Tokenized sentence 241: tensor([[  101,   123,   905,   247,   262,  3413,   221,   368,   222, 19978,\n",
      "           247,  3428, 15558, 22290,   117,   222, 10262,  2063,   247, 10671,\n",
      "          4496, 20383,   178,   102]])\n",
      "DEBUG: Tokenized sentence 242: tensor([[  101, 17090,   117,   125,   898,  1266,   898,   117,   792,  5651,\n",
      "           285,  7583,  4187, 20990,   285,  4750,  7912,  7970,  3602,   304,\n",
      "         22280,   760, 17241,   398, 13193, 22279,  8041,   117,   834,  3877,\n",
      "           159,   311,  2650, 22282,   170,   123,  3049,   304,   117,  2389,\n",
      "          7831,   221,   259,  6615,   117,   125,   188, 19035,   252,   117,\n",
      "           271,   123,  2863,   125,   230,  8625,   285,   117,   125,  3179,\n",
      "           411,   702,   582,   176,  3235,  1029,   236,   291,   125,  1569,\n",
      "          4252,  2122,   179,   146, 15700,  1875, 17811,   102]])\n",
      "DEBUG: Tokenized sentence 243: tensor([[  101, 16280,   118,   176,  3002,   170, 11972,   117,   179,   623,\n",
      "          2100,   229, 22280,   176,  2026,   256,   123,  9815, 22282, 15970,\n",
      "           117,   746,  1212,   125,  1434, 20729,   159,   146,   347,   607,\n",
      "          3093,   183,   412, 17704,  1941,  2036,   171,   923,   260, 13824,\n",
      "           171,  1831,   117,  1815,   495,   123,   327, 13175,   292,   598,\n",
      "          7629, 13000, 22280,   311, 15135, 14019,   170,   222, 21959,   102]])\n",
      "DEBUG: Tokenized sentence 244: tensor([[  101,   700,   171,   652, 13026,   125, 19978,   247,   117,   146,\n",
      "           233,   141,   662,  1353,  2044,  1169, 22282,  6070,   118,  2036,\n",
      "           173, 22268, 22281,   180,  3049,   304,   412,  1716, 22278,   171,\n",
      "          1941,   455,   154, 22280,   117,   122,   146,  3265,  1023, 17145,\n",
      "          1945, 10953,  1408,   449, 18187,  9480, 14948,  2036, 11234,   180,\n",
      "          9019,   151,   122,   180,   223, 22279,   117,   170,  7583, 11092,\n",
      "           436,   175,   311,  1185,  1156,   179,   331,   260,  2004,   138,\n",
      "           223,  2061,   483, 22287,  1434,   117,   260,  1084, 20739, 13734,\n",
      "           796,   288,   118,  2036,   298,  5708,   122,  7433,   288,   118,\n",
      "          2036,   173, 22243, 22280,   412,  1354,   102]])\n",
      "DEBUG: Tokenized sentence 245: tensor([[  101,  1502,   176,   495,   123,   681,   576,   179,   202,  1010,\n",
      "           215,  2036, 18402, 22287,  4756,  4486,   102]])\n",
      "DEBUG: Tokenized sentence 246: tensor([[  101,   146,   317, 22279,   339,  7635,   151,   123,  2745,  3413,\n",
      "           117,  1945,   201,   117,  1315, 12190,   243,   498,   123,   327,\n",
      "           316,   581, 14139,   125,  2987,   260,   877,   842,  9066,  4029,\n",
      "           591, 22278, 15811,   125, 11628,  5567,   122,   123, 13449,   172,\n",
      "         22282,   271,   222,  4062,  4575,   102]])\n",
      "DEBUG: Tokenized sentence 247: tensor([[  101,   122,   117,  1139,  9480, 14948,   117,   125,  3049,   304,\n",
      "          4098,   117,  1719,   273,   415,   128,   117, 14915,   171,   273,\n",
      "           522,  8073,   994,   117, 15922,   118,  2036,   260,  1084, 20739,\n",
      "           122,  6483,   260,  2004,   138,   117,  4178,  4023,   271,  9882,\n",
      "         22280,  1564,   423,  4707,   180, 17935,   404,   117,   834,   333,\n",
      "          3288,   117,   146,  8054,   125, 15997,   117,  4956,   202, 16450,\n",
      "           304, 22280,   230,   739, 16001,   117,   331,   423,  2099,   125,\n",
      "           792,   123,  2267,   171,  9019, 13793,  1334, 11324,   214,   146,\n",
      "          1342,   102]])\n",
      "DEBUG: Tokenized sentence 248: tensor([[  101,   646, 21307,   118,   146,  7583, 12797,   102]])\n",
      "DEBUG: Tokenized sentence 249: tensor([[  101,  1112,   368,  2364, 18424, 22278,  1977,  2036, 20485, 22281,\n",
      "           236,   260,   877,   842,   102]])\n",
      "DEBUG: Tokenized sentence 250: tensor([[  101, 22354, 14283,  7557,  2836,   118,   146,   792,   260,   124,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 252: tensor([[  101,  9480, 14948,   260, 17915,   170,  4327,  7949,   304,   102]])\n",
      "DEBUG: Tokenized sentence 253: tensor([[  101,  1112,  5078,   252,   123,  5986,   125,  1364,   123,  1143,\n",
      "           185,   171,  3265,  2010, 11032,  1266,   179,  2036,  1021,   125,\n",
      "          2822,   102]])\n",
      "DEBUG: Tokenized sentence 254: tensor([[  101,   173, 20416,   934,   146,   233,   523,  4750,   118,   146,\n",
      "           170, 11304,   221,   347,  1224, 15336, 22281,  8571,  7719,  1941,\n",
      "          1988,   368,   221,  4915,   118,  2036,   260,  6536,   171,  5452,\n",
      "          2309,   122,  8430,   118,  2036,   259,   543,   265,  2552,   125,\n",
      "          2968,   122,   259, 19726,   442,   298,  1440, 14637, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 255: tensor([[  101,   123, 22296,   449,   368,   117,   146,  1564,   117,  1369,\n",
      "          1011,   221,  7707, 16633,   260,  7695,   138, 22354,   146,  1564,\n",
      "           117,   179,  5443,   256,   146,  4012,   180,  1105,   125,  5009,\n",
      "           178,  9805,   285,   117,   495,   222,  1903, 10371,   271,   222,\n",
      "          1281, 22280,   117,   222, 18181,  5334,   303,   179,  3002, 15527,\n",
      "           229, 15520,   123,   302, 22284, 13294, 22280,  2699,   102]])\n",
      "DEBUG: Tokenized sentence 256: tensor([[  101, 17226,   117,   529,  4516,  3425, 13373,   171,  9169,   117,\n",
      "           202, 10968,  2256,   171,  2004, 22280,  1831,   117,   229,   316,\n",
      "           218, 14442,   292, 10268,  7970, 14155, 17743,  3338,   117,   602,\n",
      "           280,   508,   256,   118,   176,   118,  2036,   809,  3138, 11333,\n",
      "           117,   222,  6568,   117,   221,   146,   615, 18597,   256,   146,\n",
      "           417,   157,  4776, 22278,   117,   834, 11552,   298,  6615,   117,\n",
      "         18862,   117,  2798,  5790, 22279, 14208, 10982,   498,   222, 15445,\n",
      "          3689, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 257: tensor([[  101,   229, 22280,  1065, 13808,   256,  1569,  1423,   221,  3767,\n",
      "           325,  7642,  8167,   712,  5996, 20626, 11541,   117,   834, 11367,\n",
      "          2623,   117,  1569,  3420,   117,  1065,   179,  2036,  4048, 22281,\n",
      "           236,   325,  6995,  2745, 15724,   117,  2745,   495,   492, 22287,\n",
      "           117,  1284,   234,   179,   146,  1904, 22281,   236,   325,  3369,\n",
      "           320,  2009, 22077,   102]])\n",
      "DEBUG: Tokenized sentence 258: tensor([[  101, 19068, 22278,   291,  1010, 22278,  2010,  1021,   125,  3852,\n",
      "           657,  5530,  1021,   125,  3767,   320,  6568,  2010, 20613,   943,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 259: tensor([[  101,  2249,   123,  4074,   117,  7729, 10766,   175,  5923,   157,\n",
      "           122, 13376,  3173,   183,   117,   222,  1971,  3378,   117,   222,\n",
      "          1971,  9562,   243,   117,  7666,  2308,   581,   117, 15719,  6003,\n",
      "           122,  5708,  8001,   102]])\n",
      "DEBUG: Tokenized sentence 260: tensor([[  101,   146,  1700,  4060,   298,  4004,  3152,   125,  8196,   304,\n",
      "          3283,   371,   118,  2036,   259,  1143,  9222,   497,  2935, 22279,\n",
      "          1690,   382,   625,   368,  6952,   256,   117, 18253,   304,   256,\n",
      "           118,   259,  1950,  1058,  3514,   221,   259,  6615,   117,   271,\n",
      "           146,  2115,   298,  1877,  3111,  2055,   143, 18416,   214,   102]])\n",
      "DEBUG: Tokenized sentence 261: tensor([[  101, 10363, 19356,   151,   118,   146,   146, 11628,  5567,   117,\n",
      "           146, 17611,   117,   146,  3822,   122,   260,  2259,   247,   143,\n",
      "           173,   179,  2589,  1395, 17551, 15891,  1217, 14710,   148,  5664,\n",
      "           625,  1011,  3047,   180,  9349, 16280,   118,   176,  2044,   222,\n",
      "           765,   397,  3673,   430, 22280,   125,  6970,  5980,   138,   102]])\n",
      "DEBUG: Tokenized sentence 262: tensor([[  101,  9480, 14948,   229, 22280,  4207,  9046,   140,   271,   230,\n",
      "          2606,   125,  5288,  2601,  7119, 15515,  4327,   657,   303,   102]])\n",
      "DEBUG: Tokenized sentence 263: tensor([[  101,  1112, 17878,   117, 15912,   151,   740,   117,   625,   117,\n",
      "         10415,   214,   170, 19763,   117,  4750,  2822,   118,  7707,   230,\n",
      "          3138, 19219,   171,   179,   371,   146,  1564,  2010,  1684,   607,\n",
      "           222,  2397,   179,   229, 22280,   376, 16151,   125,  8977,   230,\n",
      "          3235,   256,   125, 12141, 22354, 14114,  3111,  1557,  3989, 12396,\n",
      "          1112,   254,  2041, 22354,   449,   173,  1250,  2365,   118,   202,\n",
      "           229,  1284,   125,   390,   303,  7489,  1165,  1182,  1286,   122,\n",
      "           125, 13756, 17254,   102]])\n",
      "DEBUG: Tokenized sentence 264: tensor([[  101,   123,  2954,   331,  5308,   256,   123,  4303,   171,  9019,\n",
      "         13793,   538,  6459,   308,   117,   221,   925,   320, 11371, 10310,\n",
      "           373,   173,  1105,   125,   230,  1124,  7137,  5967,   285,   117,\n",
      "           179, 19575,   170,   924,  8447,  1084,   221,   259, 14809,   255,\n",
      "           180,  4768,   366,  3336,   966,   102]])\n",
      "DEBUG: Tokenized sentence 265: tensor([[  101,  8544,  1684, 10580,   102]])\n",
      "DEBUG: Tokenized sentence 266: tensor([[  101,  1112,  3874,   125,   552,  1149, 22354,  2010,   229, 22280,\n",
      "         15212,  3667,   102]])\n",
      "DEBUG: Tokenized sentence 267: tensor([[  101, 10355,   368,  9442,   117, 15212,   820,  1089,  4088,   102]])\n",
      "DEBUG: Tokenized sentence 268: tensor([[  101, 15142, 17611, 22281, 16403,   260,  1176,   230, 16317,  1165,\n",
      "           125, 10832,   171,  5457,   291,   230,  4840, 22278,   125,   528,\n",
      "          1277,   251,   117,  4256,   546,  2836,   123,  1257,  1112,  1434,\n",
      "           260,   675,  3084,   256,   861,   784, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 269: tensor([[  101,   123,  1124,  7137,   962,  5723,   118,  2036,   739,  8296,\n",
      "           304, 22280,   122,  5078,   252,  8807,  4643,   343, 12908,  2028,\n",
      "         10348,   118,  2036,   123,  5825, 22282,  1112,   259,   532,  2987,\n",
      "         22281, 22354,   122,   260,   675,  3338, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 270: tensor([[  101,  1202, 22287,  1014,   117, 16241,  1537, 22287,  2036, 11600,\n",
      "           151,  1858,   689,   304, 22280,  2754,   230, 11791,  1062,   252,\n",
      "           117,   240,   210,   117,   146,  1112, 17254,   390,   303, 22354,\n",
      "          8264, 22278, 20990,  6859, 22279, 10381, 22278,   320,  9019, 13793,\n",
      "           179,  2036,  5308, 22281,   236,  4412,  6086,   644,   202,  3147,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 271: tensor([[  101,  5009,   178,   117,  1364,  6358,   373,   423,   347,  4062,\n",
      "         21813,  7471,   117,  9673,   118,  2036,  1084,   146,  6815, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 272: tensor([[  101,  2010,   318, 13793,   117,   179,   978,   146, 13254,   136,\n",
      "          2010, 11972,   122,   325,   240, 15035,   179,  1858,  5664,   117,\n",
      "          9396,   146, 13477,   154,   697,   117,  6251,  8149,   214,   146,\n",
      "         17749,   449, 19356,   788, 22288,   117, 16302,   214, 17052, 22281,\n",
      "          1623,   701,   102]])\n",
      "DEBUG: Tokenized sentence 273: tensor([[  101,  1112, 17052, 22281,   125, 17052, 22281,  1953,   122,   179,\n",
      "           368, 11736, 22354,   122,   117,   625,  4970,   146, 12541,   412,\n",
      "          1448,   576,   117,   229, 22280,   176,   706,   370,   117,   179,\n",
      "          2036,   229, 22280,  1996, 22281,   236,  2010,  2389,   296,  1084,\n",
      "           117,  7343,  3695,   117,   179,   146,  3791,   247, 14619,   210,\n",
      "           659,   670,   171,  3896, 22279,  2467,  2310,   214,   179,   123,\n",
      "         15703,   229, 22280,   495,  1528,  1395,   680,   320,  1831,   171,\n",
      "           179,   123, 19024,   304, 22280,   117,  1953,   173,   222,  4885,\n",
      "         13340,   173,   179,   222,  2397,   418,  1684,   123,   669,  2079,\n",
      "         22282,   102]])\n",
      "DEBUG: Tokenized sentence 274: tensor([[  101,  5009,   178,   262,   123,  2954,   320,  3147,   171, 11314,\n",
      "          2650,   397,   102]])\n",
      "DEBUG: Tokenized sentence 275: tensor([[  101, 11234,   118,  2036,   170,  4332,  8859,   124, 10528,  7134,\n",
      "         19068, 14995,   118,   146,  4968, 21268,   277,  8932,   909,   117,\n",
      "           122,  1950,   654,   222, 13189,   117,   173,   547,   125,   333,\n",
      "           148, 22280,   117,   598,   146,  4885,   122,   259, 10541, 15277,\n",
      "         17513,   102]])\n",
      "DEBUG: Tokenized sentence 276: tensor([[  101,  2010,   230,  1749, 13808,   170,   179,   122,  8911, 12245,\n",
      "         22246, 22246, 10355,   368,   102]])\n",
      "DEBUG: Tokenized sentence 277: tensor([[  101,  5863,   123,  9349,   376,  4042,   285,   240,   222, 13831,\n",
      "           125, 10881, 11459, 22288,   700,   117,   170, 18624,   117,   125,\n",
      "         18615,  1187,  5069,   748,   260,  7799,   662,   852,   986, 10863,\n",
      "          1112,  7355,   162, 17124,   591,   121, 22361,   122,   397,  2271,\n",
      "           117,   123, 17722, 14418,   125, 20640,   170,  2996,   524, 22280,\n",
      "          4712,   117,   123, 19994,   117,   146,  1945,   243, 13717,   243,\n",
      "           117,   146,  4828,   475,  2848,   252,  1350,   313,   702,   301,\n",
      "         22354,  2010,   123, 22283,   146,  9805,   243,  4217,  5461,   146,\n",
      "          1564,   117, 10428,  1212,   412,  2480,   102]])\n",
      "DEBUG: Tokenized sentence 278: tensor([[  101,   179,  8797, 13779,  4432,  2010,   122,   259, 10811, 10692,\n",
      "          1289,   125,  1611, 21510,   117,   122,   260, 16594, 17291,   842,\n",
      "           502,   401,   117,   122,   146, 10832,  6183,   136,  1564, 13083,\n",
      "          5723,   170,  6205, 22278,   229,  9463,   102]])\n",
      "DEBUG: Tokenized sentence 279: tensor([[  101,  2010,   123, 22283,   123,  2480,   102]])\n",
      "DEBUG: Tokenized sentence 280: tensor([[  101,   146,  9019, 13793, 11234,   118,  2036, 14619,   210,   366,\n",
      "           271, 20258,  1111,   117,   298, 17246,   117,   366, 15022,   122,\n",
      "           240,  1338,   298,  7465, 12411, 22281,  2607,   145,  2204,   117,\n",
      "          8638,   240,  7283,  8446,   125,  3848,   687,   151,  2919,  2106,\n",
      "           560,   320,   171,  1564,  4636,   654,   118,   176, 11989,   243,\n",
      "           712, 13665,   596,   125, 13254,   117,   122,   117,  1941,   125,\n",
      "           766,   117, 13610,   221,  5197,   117, 13754,   118,  2036,   202,\n",
      "         20462,   117,   505,  2552,  3514,  2010,  2354, 22279,   117,  2397,\n",
      "           117,   146,   179, 12444,   495,  6759,   102]])\n",
      "DEBUG: Tokenized sentence 281: tensor([[ 101,  122, 3436,  203,  118, 2036,  179,  146, 2982, 2036, 1011,  653,\n",
      "         1945, 7831,  102]])\n",
      "DEBUG: Tokenized sentence 282: tensor([[  101,  1112,   146,  1564,   117,   170,  6086,  1677,   247,   122,\n",
      "          1611,  2323, 11637, 22280,   117, 10348,   240,   344,   304,   222,\n",
      "          4062,  4170,   102]])\n",
      "DEBUG: Tokenized sentence 283: tensor([[  101,   179,   176,  4103,   236,   117,   122,  1021,   125,   792,\n",
      "           176,   229, 22280,  2471,  1858,  7814,   890,   340,   102]])\n",
      "DEBUG: Tokenized sentence 284: tensor([[  101, 22354,  2010,  2389,   296,  8028,   117,  2826, 22280,   118,\n",
      "          2036,  2535,   271,   146, 14914,  1112, 17052, 22281, 17052, 22281,\n",
      "           117,  7343,  3695, 22354,   449,   179,  4694,   272,  2567,   117,\n",
      "         12123,   136,   122,   117, 11989,   243,   170,   123,  2004, 22278,\n",
      "         13779,  2662,   322,   122,  1364, 13140,   125, 13449, 19088, 22281,\n",
      "           125,  3264,  4388,   304, 22280,   117,  5127,   171,  3147,   229,\n",
      "          8993,   442,  1143,   117, 17649,  2846,   375,   246,   117,   221,\n",
      "           179,   259,   736, 11314,  2650,  1058,   117,   123,  1977,   368,\n",
      "           229, 22280, 10348,   123,  6320,   125,   230,  3248,   285, 15619,\n",
      "           117,   229, 22280,  2036,  8362, 22281,  6867,   260, 12249,   401,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 285: tensor([[  101,   625,  9480, 14948,  2467,   125, 16633,   260,   877,   842,\n",
      "           125,  5009,   178,  6199,  2002,   118,  2036,   125,  6865,   179,\n",
      "          2874, 22281, 10119, 22290, 20657, 22278,  5664, 17400,   179,  5646,\n",
      "          4765,   151,   170,   146,  1568,  3285, 22279,   118,  1340,   173,\n",
      "           230, 14190, 13152,   324,   125,  2796,  4824,   117,  2317,   383,\n",
      "          3309,   214, 22288,   118,  2036,   179,   944,   259,  1564,   125,\n",
      "          1062,   252,  5267, 22281,   236,   146,   347, 17052, 15702,   180,\n",
      "         10662,   171,   302,   303,   102]])\n",
      "DEBUG: Tokenized sentence 286: tensor([[  101,  2010, 21719,  1257,   117,   179,   333,   244,   240,  2354,\n",
      "         22279,   117, 15297,   654,   123,   390,   304,   117,  4415,  1552,\n",
      "           118,   146,   170,   230, 10032,  1877,  2555,   229, 17845,   304,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 287: tensor([[  101,   146,  9778,  9883,   118,   176,   117,   785,   271,   857,\n",
      "           117,   221,   146,  8054,   125,  5530,   117,   449,   146,  1564,\n",
      "           117,   125,   766,   117,   202,  6972, 22279,   180,  3656,   251,\n",
      "           117, 11690,   240,   368,   117, 18928,  1212,   102]])\n",
      "DEBUG: Tokenized sentence 288: tensor([[  101,  2010,   179,  1011,  2636,   117,   347,   338,  7485, 22279,\n",
      "           136,  2010,  3874,   117,  9396,   123,   854,  2028,   117,   123,\n",
      "          8574,   140,   102]])\n",
      "DEBUG: Tokenized sentence 289: tensor([[  101,  1796,   123, 17704,   179,   146,  6542, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 290: tensor([[  101,  1564,   117,   170,   222,   362, 22282,   157,   117, 11535,\n",
      "           179,   146,   528,  3356, 22280,   229, 22280,  4207,   240,   118,\n",
      "           176,   125, 19553,   229, 17935,   404,   117,   173,   576,   693,\n",
      "          4838,  2387,   366, 18237,   303,   143,   102]])\n",
      "DEBUG: Tokenized sentence 291: tensor([[  101,  2010,   122,   176,   311,  8051, 22282,   117, 14000,   117,\n",
      "          1078,   576,   325, 11510, 12674,   243,   117,   179,  2354, 22279,\n",
      "           311,  2976,   123,   925,   170, 19068, 10866,   138,  1266,   146,\n",
      "          1341,   125,   121,   102]])\n",
      "DEBUG: Tokenized sentence 292: tensor([[  101,   360,   232,   117,   170,  1039,   176,   376,   125,  5110,\n",
      "           117,   347, 22232,   715,  2541,  2745,   712, 22073,   171,  9019,\n",
      "         13793, 14351,  6199, 19103, 22288,   118,   176, 17811,   117, 20089,\n",
      "           125,   179,  1021, 17139,   230,  8574,  3870, 22278,  3207,   202,\n",
      "         14353, 22280,   117,   240,   210,   117,  8544,   785,  8174,   788,\n",
      "           170,   123,  3138,   125,   179,  1941,   229, 22280,  1011,   316,\n",
      "         22280,  1950, 10490,   825,   117,   122, 21128, 10133,   943,   118,\n",
      "          2036,   117,   229, 12126, 22281,  7870,  5923,  3632,   171,   347,\n",
      "          2166, 22282,   157,   117,   222,  6532, 20073,   125,  6028,   123,\n",
      "          5212,   102]])\n",
      "DEBUG: Tokenized sentence 293: tensor([[  101,   123,  2259,   151, 22280,   173,  1105,   171,   958,  1025,\n",
      "          3851, 16248,   102]])\n",
      "DEBUG: Tokenized sentence 294: tensor([[  101,  3363, 14643, 22280,   117,  4249,   151,   117,  5747,  4654,\n",
      "           304,   102]])\n",
      "DEBUG: Tokenized sentence 295: tensor([[ 101, 5063, 3605,  931, 5334,  825,  180,  475, 2832,  102]])\n",
      "DEBUG: Tokenized sentence 296: tensor([[  101,   449,   117,   412,  1359,   180,  5899,   118,  2954,   117,\n",
      "          9480, 14948,   117,   700,   125,   230,  1201,   375,   117,  1796,\n",
      "          1169, 18377,   328,   125,   222,  3041,  5759,  1370,   128,   102]])\n",
      "DEBUG: Tokenized sentence 297: tensor([[  101,   495,   146,  2402,   179,  2036, 10348,  1016,   117,   834,\n",
      "           325,  2798,  1528,   102]])\n",
      "DEBUG: Tokenized sentence 298: tensor([[  101,  8540,   246,   146,  6815, 22280,   117,  1565,   123,  1719,\n",
      "           123,  7525, 22278,   870,  1816,  1353,   179, 11972,   229, 22280,\n",
      "          1201,   151,  3874,   102]])\n",
      "DEBUG: Tokenized sentence 299: tensor([[  101,  1112, 22238,   303,  2464,  4062,  1425,   635, 22354,   746,\n",
      "          3529,   368,   117,   122,   117,   320, 13156,   307,   118,   176,\n",
      "           125,  5009,   178,   117, 11021, 22288,   118,  2036, 13449, 11252,\n",
      "          2010,   176,  8204,   140,  2822, 10428, 22279,   123,   327,  2267,\n",
      "           117,   338,   185,   125,  1105,   118,  1084,   102]])\n",
      "DEBUG: Tokenized sentence 300: tensor([[  101,  2010,   449,   146,   179,   376,   740,   117, 14914,   136,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 301: tensor([[  101,  2010, 11032,   146,   179,   376,   376,  4698,   481,   418,\n",
      "           229,  2169,   125,  1434,   146, 16241,   268,   449,   117,  1139,\n",
      "           229, 22280,  4147,  2880,  1845,   117,   740,   179,  1447,  3951,\n",
      "           259,   532, 17611, 22281,   123,   766,   102]])\n",
      "DEBUG: Tokenized sentence 302: tensor([[  101, 17052, 22281, 21314,   117,  6569,  9258, 22281,   117,  4062,\n",
      "          1425,   635,   122, 22238,   303, 14738, 22282,  5484, 22279,   136,\n",
      "          5009,   178,   117,   229,   327,  9392,   672,   151,   117, 12223,\n",
      "           203,   179,   123,  2267, 19024,   256, 11224,  1121,  3179,  3165,\n",
      "          3002, 16165, 22281,  1108,  1125,   243,   102]])\n",
      "DEBUG: Tokenized sentence 303: tensor([[  101,   629,   830, 16305,   259, 20462, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 304: tensor([[  101,  1112,   229, 22280,   495,   318, 13793,  5664,   125, 12245,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 305: tensor([[  101, 22354,   122,   117,   173, 15322,   260,  6107,  2350, 19181,\n",
      "           303,   117, 16108,   170,   123,  9525,   148, 10620, 17611, 22281,\n",
      "           412, 12837,   304,   180, 12495,   102]])\n",
      "DEBUG: Tokenized sentence 306: tensor([[  101,   180, 22283,   123,  1564,   117,   146,   317, 22279,   339,\n",
      "           434,   763,   117,   598,   944,   259,   532, 19877,   128,   117,\n",
      "          2863,   256,   146,  4968, 21510,   260,  1118,  2856, 22186, 13808,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 307: tensor([[  101,  7570,   203,   146,  5209,   210,   117,   803, 13550,   271,\n",
      "          1977,  3889,   739, 19173,   117,   122,   117,  3002,  2080,   320,\n",
      "         19317,   175,   117,   262,  2036,  4111,   173,  5902, 17065,  2010,\n",
      "          4178,   136,   659,  4227,   125,  8264,   117,   122,   122,   146,\n",
      "         19026,   102]])\n",
      "DEBUG: Tokenized sentence 308: tensor([[  101,  5009,   178, 12399, 22288,  2044,   125,   223, 22280,   146,\n",
      "          1312,   303,   179,  5057,   117, 10996,   123, 17935,   404,   117,\n",
      "          2002,   260,   675, 12950,  3292, 22281,  9079,   383,  2152,   140,\n",
      "           222,  9730, 22279,   117,   122,   173,  2590,  2473,   123,  4768,\n",
      "           170,   146,  3695,   102]])\n",
      "DEBUG: Tokenized sentence 309: tensor([[  101,  1061,   123,  5197,   210,   125,  1105,   122,   123,  9337,\n",
      "           125,   629, 22280, 10567, 22281,   123,  7861,   117,  6094,   214,\n",
      "           170,   222,  7520,   117,   123,  3302,  7642, 22278,  6685,  1910,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 310: tensor([[  101,   259,   682, 12737,   222,  5490,   140,   122,   506,   123,\n",
      "          7701,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 309 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.006661865321090769\n",
      " Coesão Score Final: 0.5033309326605454\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'alem disso', 'isto e', 'todavia', 'nem', 'ou', 'ora', 'quer', 'seja', 'como', 'quanto', 'igualmente', 'em vez de', 'desde que', 'para que', 'logo que', 'porque', 'gracas a', 'por outro lado', 'realmente', 'principalmente', 'nao so', 'tanto', 'quanto', 'se nao', 'de sorte que', 'por isso']\n",
      " Número de conectivos: 37\n",
      " Número de sentenças: 311\n",
      "======================\n",
      "Resultados para preprocessado_o_mulato_aluisio_azevedo_cap_2.json:\n",
      "{'coesao_score': np.float64(0.5), 'conectivos_encontrados': ['e', 'mas', 'porem', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'alem disso', 'isto e', 'todavia', 'nem', 'ou', 'ora', 'quer', 'seja', 'como', 'quanto', 'igualmente', 'em vez de', 'desde que', 'para que', 'logo que', 'porque', 'gracas a', 'por outro lado', 'realmente', 'principalmente', 'nao so', 'tanto', 'quanto', 'se nao', 'de sorte que', 'por isso'], 'num_conectivos': 37, 'proporcao_conectivos': 0.007, 'similaridade_media': np.float64(1.0), 'num_sentencas': 311}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,   180, 22283,   123,  1695,   117,   420,   260, 11004,  6185,\n",
      "          7358,   138,   298, 12898,   128,   117,  7570,   203,   123,  1174,\n",
      "           304,   171,  1847,   523,   222,   124,   321, 22305,  1004, 16408,\n",
      "           117,   179,  8544,  8582,   423,   317, 22279,   339,   434,   763,\n",
      "           122,   240,  5009,   178,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   123, 19173,   262,  2044, 20783,   285,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   259,  4966, 16819,   117,   170,   260,   675,  1491,  1923,\n",
      "          6943, 22281,   117,   260,  6929,   298,  9098, 22281,   125, 20237,\n",
      "         22279,  3848,  6913,   259,  6688, 19428,   730,   151,   692,   240,\n",
      "          5530,   298, 11224,   128,   125, 19889,   421,   259,  7967, 22281,\n",
      "           822,  9168, 22281,   221,   692,   221,  1112, 15796, 22282,   146,\n",
      "          1354,   118,   940, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   146, 16573, 22278,   118, 13717,   285,   117,   173,  8037,\n",
      "           138,   125,  7924,   117,   271,  1821,   944,   259,   736,   117,\n",
      "           417,  1797,   456,  2044,   123,  4768,  2010,  1977,   333, 22278,\n",
      "          1966,  1956,  1286,   117,   146,  5664,   136, 16795,   368,  1315,\n",
      "           474,  3514,   123,   222,   233,   523,   179,  9882,   229,  2880,\n",
      "           151, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,  2010,  3179, 18369,   291, 16302,   243,   171,  5009,   178,\n",
      "          9805,   285,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[ 101, 3429,  171, 1567,  102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,  2010,   146,  6086,  4178, 22281,  1977,   122,   146, 21574,\n",
      "           397,   179,  2541,   170,   146,  9805,   285,   136,  2010,   229,\n",
      "         22280, 18661,   117,  2397,   117,   449,   122,   222, 10173,   421,\n",
      "           703,   190,  2568,  3794,   146,  9931,   123,  2344,   128,  2201,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,  3363, 13449, 19088, 22281,   125, 15020,   261,  6471,   122,\n",
      "          1491,  7837,   382,   272,   223, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,  2010,   122,   146,  1417,   125,   222, 10018,   171,  9805,\n",
      "           285,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101, 10355, 22287,   700,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101, 19854,   128,   118,  2036,   785,   123,  1069,  3196,   118,\n",
      "           333, 18376, 11788,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[ 101, 1011,  538, 2595,  102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  2010,  3539,  6339,   118,   176,  5863,   136,   733,   468,\n",
      "           203,   146,  4141,   236, 10187,  1604,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,  2010,   229, 22280,   117,  3960,   247,   179,  3539, 17340,\n",
      "           230,  4067,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   736,   870,  1816,   304,   692,   179,   646,   314,  5895,\n",
      "           495, 17796, 20177,   180,  1105,   125,  5009,   178,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,  5489,  2916, 22287,   118,  2036,   388,   203,   321,   117,\n",
      "           146,  2277,   125,  8054,   117,   123,   549,   122,   259, 13841,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   146,   599, 15266, 21102,  3182, 22278,   125,  5731, 18601,\n",
      "           179,   368,  1112,   978,   504,   154, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,  5147,   259,  1510, 22281,   695,   923,   123,  4768,   180,\n",
      "          4595,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101, 12319, 22281,   123,  1105,   117,   582,  1941,  1021, 13610,\n",
      "           222,  3147,   221,   146,   139, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   646,   314,  5895,  4141,   236,   180,  7296,   256,   117,\n",
      "           637,  1857,   339,   122,  5009,   178,  5510, 15134,   288,   118,\n",
      "           176,   173, 15020,   261,  6471,   170,   146, 13254,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,  2010,  4127, 11646,   873, 18099,   291, 13521,   317,   252,\n",
      "           455,   117, 14914,   136,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,  2389,   252,  3848, 17714, 22279,   117,  3058,   866,  3273,\n",
      "         22278, 14914,   117,  1214,   252,  1075,   221,   860,  1341,   179,\n",
      "           418,   325, 12837,   303,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,   229, 22280, 21719,  4569,  7595, 22281,  1447, 12022,  1447,\n",
      "         12022,   221,  1938,  3273,   285,   146,  7258,   418,   173,   327,\n",
      "          1105,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[ 101,  646,  314, 5895,  179, 1445,  256,  118,  176,  171, 6938,  102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,  2010,   418,  3428, 15558, 22290, 10355,   368,   117,   123,\n",
      "          1743,  1329,   146,  9169,   170,   146, 16936,   303,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,  2364,  9160, 22283,  1971,  2010,   146,  1407,   318, 13793,\n",
      "           122, 21615,   118,   176,   222,  1695,   122,  4412,   123,  6272,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,   706,  5698,   125, 11451,   117, 12346,  4765,   118,   176,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,  3508,   421,   705,   229, 22280,  1258,   285,   123, 22283,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,  2389,   296,   117, 14914,   117,   420,   117,   420,   122,\n",
      "           873,   524,   176,  1968,  1004,  5863,   128,  1510, 22281, 11092,\n",
      "         14754,   228,   202,  3147, 10243,   320,  9730, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,  2010,   146,  7258,   117,  1996,  5009,   178,   117,   376,\n",
      "          5863,  9751,   221,   123,  4768,   122,   221,   146,  6151, 22290,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101,   302, 13808,   118,   176,   123, 10303,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101, 21333, 19356,  1145, 22282,  1569,  5664,   117,   122,   331,\n",
      "          6542,   423,  4127, 11646,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,  3874,   125,  4569,  7595, 22281,   124,   314,  5895, 13406,\n",
      "           685,   785,  2377,  3146,   243,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,  2010,  3422,   244,  2822,   118,  2036,  9461,   117, 14000,\n",
      "           146, 19317,   175,   117,  2113,   146,  7258, 12931,   229, 22280,\n",
      "           418, 10953,   788,   123,  2551,   117,   202,  1325,   176,  8204,\n",
      "           140,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,  2010,   229, 22280,   117,   229, 22280,   117,   785,  9773,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[ 101,  418, 2745,  785, 4062,  102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,   146,   179,  6532,   122, 17520,  8889,   222,  1695, 13776,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,   744, 15212,   123,  3049,   304,   123,  8054,   123, 10849,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,  2010,  1502,   318, 13793, 12604,   236,   117, 12604,   236,\n",
      "           117,   221,   700, 16960,   934,   170,   325, 15975, 15588,   119,\n",
      "           119,   119,  2684,  2044,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,   122,  5009,   178,   122,   325,   146,  4968, 21510, 16083,\n",
      "           228,   118,   176,   117, 13140, 22281,   125, 10089,   151,   122,\n",
      "         13449, 19088, 22281,   125,   870, 12764, 16760,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,   646,   314,  5895,   978,  4698,   122,  2139,   481,   122,\n",
      "          1467,   222,  1903, 17609,   125,  1910,   176,   229, 22280,   506,\n",
      "           259,  1491,   211,   683, 16467,   117,   179, 14537,   186,   171,\n",
      "          1568,   102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101, 13841,   785,  7967, 22281,   117, 16814,   534,   128,   122,\n",
      "          1664,   459,   437, 22305,  1623,   912,   122,  1052,   565,  1378,\n",
      "           117,   449,  7557, 22278, 12141,  6054, 22281,   179, 15908, 12051,\n",
      "         22287,   425,   123,  2128,   381,   124,   171,  4351,   339,   272,\n",
      "          6958,   124,  2729,   122, 20882, 13227,   303, 12399,   117, 17749,\n",
      "          8219,   373,   122,  3317,  1632,   942, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101,   123,   670,   325,  1630, 11582, 22278,   180,   327, 15578,\n",
      "          4275,  4322,   495,   259,  5708,  2010,  1491,   117,  6847,  4943,\n",
      "         20225,   117, 13140, 22281,   125, 15419, 22281, 16467,  1143, 13000,\n",
      "         22281,  2317,   232,   591,   122, 20269,   117,  1877,   269,  4323,\n",
      "           125,   222,   577,  1604,  8886,  1212,  2779, 13513,   260,   425,\n",
      "           672,  6069,   138,   117,   785, 15600, 22281,   202,  9169,   117,\n",
      "           271,   123,   229, 22285,  4083,   117,  9144,   498, 22281,   375,\n",
      "           307,   123, 12837,  7870,   180,  7826,  2776,   155,   117,   179,\n",
      "           117,   202,  1247,   180, 17897,  5874, 14004,   117, 10201,   256,\n",
      "           259, 14378, 13782, 22281,   122, 19408,   358,   125,   230,  1474,\n",
      "           823,   966, 12126,   130,  1798,   125, 11902,   102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101,   978,   259, 22000, 22281,  1004, 17831, 22281,   117,   425,\n",
      "          1408,   117, 12989,   474,   125, 20494, 13793,   117, 18402,   173,\n",
      "          4410,  4098,   117, 16851, 10011,   210,  4857, 22282,   320,  3901,\n",
      "         12232, 22278,   118,   176,   170,   333,  9025,   122,  4062, 10303,\n",
      "          3330,   256,   260,  4026,   117,   260, 13747,  3292, 22281,   117,\n",
      "           123,  4701,   122,   117,   222,  9390,   303,  1528,   117,   123,\n",
      "         12067,   232,   102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[  101,   173,  1719,   123,   327,  1069,   117,  1684,  5533,   180,\n",
      "          9019,   151,   117,   420,  5317,  2086,   117, 13086,   125, 16085,\n",
      "           143,  1755,   117,  7254,   125,  5811,   303,   143,   125,  2595,\n",
      "           117,  9871,  4914, 22278,  3767,   123,   230,   125,   379,   304,\n",
      "         22280,  7543,   232,   122, 12458,  5847, 17246,   269,   373,   180,\n",
      "           327,  5160,  2346,   351,   102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[  101,   229, 22280,  9679,   320,  4863,  1647,  1085,   260,  7892,\n",
      "          2148,   784,   173,   179,  7762,   124,   320,  1147,   324, 22280,\n",
      "          9679,   123,  1977, 12444, 13406,   943,   123,  1069,   122,   259,\n",
      "          6109,   125,   179,  1598, 13808,   102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101, 10201,   256,   118,   176,   117,   202,  1325,   117,   125,\n",
      "          5110,   375,   286,   173,  3265,   171,  1010,   215,   122,  4207,\n",
      "          7845, 22282,   179,  2364,  2036,  3207,   124,   146,  1395, 17551,\n",
      "           122,  2684,   146,  3258,   411, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101,   173, 12460,  2204,   978,  2601,  1546, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101,   449,  1977,  8940,   123,   333,  1921,  2760, 12771,   251,\n",
      "           125,  6082,   118,  1340,   125,   316, 22280,  5533,   136,   102]])\n",
      "DEBUG: Tokenized sentence 50: tensor([[  101,   347,  5023,   428,   117,   170,   943,  7919,   117,   291,\n",
      "          5664,   179,   146,  1201,   252,   117,   291,  5787,   347,  2004,\n",
      "         22280,  7392,   117,  1502,   117,  2249,   320,  1568,   117,  9679,\n",
      "           646,   314,  5895,   179,  1941,  3185, 13793,   978,   625,   262,\n",
      "           221, 21990,  2204,   102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[  101,   229, 22280,  2113,  4147, 22281,   236,   123,  7422,   118,\n",
      "          1340,   117,  2798,  2113,   176, 18127, 22281,   236,   125,   370,\n",
      "           203,   857,   125,   614,   210,   146, 11152,   655,   125,  1417,\n",
      "           117,   449,  9679,   118,   146,   240, 10411, 22280,   171,   347,\n",
      "          8620,   122,   423,   179,   272,  1320,   151,   125,  1450, 10398,\n",
      "         10200,  2949, 22281, 12773,   784,   180,  1462,  2949,   289,   102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[  101,  1112,   327,   223, 22279,   117,   240,   210,   117,  1977,\n",
      "          1467,   136,   102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[  101, 22354,  5787,  3933, 17704, 12526,   285,   122,   746,  1337,\n",
      "           125, 10063,   159,   123, 13782, 22282, 12268,   102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101,  1112,  1467,  3264,   136,  1467, 13823,  1337,   136,   102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101, 22354,   646,   314,  5895,  4363,   151,   118,   176,   173,\n",
      "           317,   537,  1650,   122,   117,  3002, 14755,   146,   347, 10968,\n",
      "          5154,   423,  3714,   117, 16280,  3933,   303,  1145, 20514,   118,\n",
      "          1340, 10984, 12569,  7485,  5366,   221,   123,  9019,   151,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101,  1112,  1977,  9679,   176,   123, 22283,   229, 22280,  7544,\n",
      "           151,   123,  8993,   171, 21960,   148,   136,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[  101,   368,   117,   179,  1684,  5212, 22278,   438,  1165, 22280,\n",
      "           125,  4613,   319,   143, 20172,   138,   122,  3072,  2404,   277,\n",
      "           117,   271,   318, 13793,  1467,  8540,   102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[  101,   123, 22296,   117,   176,  4147, 22281, 10119,  4945,  1977,\n",
      "           495,   327,   223, 22279,   117, 16759,   159,   118,  2036,   118,\n",
      "          8544,  2745,   117,  2745, 22354,   146, 22191, 13793,   125,   370,\n",
      "         13397,   124,   117,   179,   123,   740,  7898,   117,  1011, 18826,\n",
      "          2175,   202, 16450,   304, 22280,   171,  1417,   102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[  101,   495,  8911,  9358,   118,  1340, 22278,   614,   210,   495,\n",
      "          8911,   273,   987,  2387,   260,  7892,  2148,   784,   179,  7625,\n",
      "           228,   146,   347,  5774,  1112,   449,   117,   202,  1338,   125,\n",
      "         11169,   117,  3929,  2916,   646,   314,  5895,   117,   173,   222,\n",
      "         14792,   885,  2711,   125, 16085,   143,   117,   179,   644, 17789,\n",
      "           508,   368,   170,  2745,  1257,   117,   176,  2684,   123, 22283,\n",
      "           117,   229,  9392,   672,   151,  4438,  8446,   117,  5212, 22278,\n",
      "         14031,   122,  8540,   102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101,   229, 22280,   262,   693,  3671,  1266,  4327,  5664,   179,\n",
      "          7762,   124,   123, 11126,   351,   240,  1104,  1017,   117,   495,\n",
      "         17558,  5799,   259,   532,  3907,  2698,   117,  8868,   259, 10981,\n",
      "          6109,   122,  2010,   240,  5863,   122,   146,  3420,   146,  2187,\n",
      "           125,  1543,  1084,  1011,   123,   327,  2521,  1112,  8925,   151,\n",
      "           117,   320,  3767,  1084,   117,   146,   347,  4223,   247,   117,\n",
      "          3212,   151,   117,   122,   117,   320,  1341,   180,  2606,   170,\n",
      "          1977,  4103,   236,   122,   298,  3252,   683,   179,  7762, 22281,\n",
      "           236,   123,   370,   117,  2798, 14019,  1021,   125, 13519,   118,\n",
      "           176,   171,  3714,  1112,  1141,   117,   179,   325,  2686,  8781,\n",
      "         22282,  1407,   136,   102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101, 12198, 22278,   259,  2595,   117, 10962, 22278,   785,   117,\n",
      "           978, 10428, 22279,   117,  1776, 22278,  1089,  6109,   125, 12464,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101,  2010,   495, 18597, 22282,  1174,  2375,   122,  4314,   173,\n",
      "          4527,   146,  1815,  2010,  3714,  2010,  2134, 12657,   243,   117,\n",
      "          3714, 11032,  3605,   249, 22354,   122,   117,  5168,   123,   418,\n",
      "          5700, 13793,   117, 16280,   118,   176,  8540,   117,  3566,   117,\n",
      "         11120,   598,   260,  3061, 17627,   180,  1069,   117, 13140,   125,\n",
      "         12908,  2028,   202,  3753,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[  101,  1112,   122,   240,   179,   229, 22280,  1021,   125,  1434,\n",
      "          1742,   136, 16241,  1537, 22287,  4207,   370,  2980,  1017,  3048,\n",
      "           143,   171,   179,   368,   136,   102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[  101,   229, 22280,   495,   222,  1447,   635,   117,  2798,  2397,\n",
      "           125, 14697, 16589,   850, 12183,   256,   320,  2982,   117,   123,\n",
      "          1009,  1758,  4750,   117,   202,  9248,   293,   125,   327,  1105,\n",
      "           117, 13030,   118,   176,   320,  1223,   333,   247,   117, 11076,\n",
      "          3243,   171,   179,  5964, 22278,   117,   171,   179, 10698, 22278,\n",
      "           229,  1202,  2328,   117,   229,  1546, 22278,   117,   229,  7231,\n",
      "         22278,   122,   538,  3545, 19631,   102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[  101,  3207,   256,   118,  2036,   820,  2477,   320,  2265, 13808,\n",
      "         22280,   122, 17558,  5799,   259,   532,  3907,  2698,   102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101,  2010,  1502,  1004,   329,  1011,  2010,   495,  4042,   159,\n",
      "           122,   240,   118,   176,   125,  1160,   123,  3420, 22354,   262,\n",
      "           170,  3769,  5365,   179,   368,  2080,   123,   651,   125,   629,\n",
      "         22280,   599,   145,   102]])\n",
      "DEBUG: Tokenized sentence 67: tensor([[  101,   122,  2535,   117,   229, 18768,   124,  4676,   171, 21402,\n",
      "           183,   117,   700,   125,   222, 17052,   437, 16489,   117,   146,\n",
      "          1831,   744,  1423, 19797,   180,  4036,   117,   146, 11628,  5567,\n",
      "           420,   259, 17084,   117, 16280,   102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[  101,   176, 15363,  8540,   117,  8174,   788,   170,   123,   327,\n",
      "          7716,   122,   170,   123,   327,  5594,  3292,  2010,   123, 22296,\n",
      "          1151,   289,  5254,  3030,  7831,   259,  5708,   102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101,   122, 17558,  5799,   259,  3907,  2698,   122,   240,   118,\n",
      "           311,   320, 12837,   303,   102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[  101,   122,   117,   170,   222,  1160,  1151,   289,  1286,   117,\n",
      "          2789,  9322,   320,  1690, 22280,   146, 11628,  5567,   117,   122,\n",
      "          2251, 22282,   155,   685, 20885,  3514,   102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101,   202,  1325,   117,   123,  4131, 22278,   125,   646,   314,\n",
      "          5895,   117,   123,  4131, 22278,   179,   368,  9392,  5630,   117,\n",
      "           495,  6459,   328,   240,  2249, 22281, 11600,  3730,   259,   532,\n",
      "         10034,   202, 15273, 13808, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101,  2714,  1532,  7698,   125,  5976,   229,  4411,   171, 14948,\n",
      "           342,   117,  1415,   481,   700,   179,   347,  1568,   117,  4141,\n",
      "           236,  2055,   157,   180,  7296,   256,   123, 22283,   176, 22074,\n",
      "           124,   117,  4063,   286,   171,   221,   320,  9247,   183,   125,\n",
      "          1112,  6629,  7395,  1350, 22354,   529, 17464,   125,  3919, 22302,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101,  4141,   236,   180,  7296,   256,  1021, 20613,  1859,   202,\n",
      "           598, 21910,   298,  7769,   180,  5566, 22278,   122,  1796,  1684,\n",
      "           325,   291, 16342,  4537,   286,   122,  3002, 19270, 22280,   423,\n",
      "          2049,   171,   221,  2684,   179,   117,   222, 15152,   644,   117,\n",
      "           176, 17760,   598,   368,   123,  2004, 22278,  3656,  5630,   406,\n",
      "           117,   179,   146,  2471,  4818,  1817,   243,   117,   176,   230,\n",
      "           366,   675, 17479, 22281,   325,   390,  1149,   117,   240,   655,\n",
      "          3400,  1557,   117,   229, 22280, 22280, 15079,  4886,   123,   596,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101,  7543,   748,  3852, 19456,   411,   155,   320, 15273, 13808,\n",
      "         22280,   117,   229, 22280,   834,  7482,   125,  9336,   532,  5110,\n",
      "          2464,  5121,   125,  9322,   173,  2432,   146,  1530,   117,   179,\n",
      "           418, 11126,   351,   117,   271, 13219,   122,  6942,  3784,   171,\n",
      "          1847,   523,   180,  1858,   117, 12595,   256, 16589,  3281,   423,\n",
      "          5546,   211,   117,   598,   259,  4594,  2251,  1100,   122,   598,\n",
      "           259,  4966,   102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[  101, 17226,   117,  2286,  6867,   827,  7861,  3179,  2987,  4437,\n",
      "           179,  4900,  4062,   596,  4063,   151, 17569,   240,  1364,   146,\n",
      "          1010,   215,   122,   179,   325,   578,   272,   123,  1658,   171,\n",
      "           221,  4894,   978,   125,  9351,   173, 11069, 10780, 22280,   143,\n",
      "           122, 19016,   304,   102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101,   123,  8931,  4112,  1061,   117,  7258,   122, 17479,   117,\n",
      "           123,   766,   117,   240, 14697, 10602,   117, 13910,   214,   259,\n",
      "           333,   183,   143,   102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[  101,   744,   324, 22280,  8921,   123,  4067,   125,  8886,   143,\n",
      "           122,   259, 13654, 22232,  9536, 10090,   923,   318, 13793,   125,\n",
      "          5926, 11681, 22281,  1923,  1149,   117, 15252,   266,   122,  2873,\n",
      "           122,   117,   260,  1176,   117, 14537,   401,   123, 15445,   117,\n",
      "           538,   254, 12371,  2817,   102]])\n",
      "DEBUG: Tokenized sentence 78: tensor([[  101,   506,  2822,   170,   259, 12684,   202, 14948,   342,   102]])\n",
      "DEBUG: Tokenized sentence 79: tensor([[  101,   637,   618,   124, 16979,   384, 15580, 22288,   118,   176,\n",
      "           146,  1407,   179,   706,   170,   123, 17479,   179,  2036,  2226,\n",
      "           256,   117,   122,   117,   325,  1373,   117,   202,  1247,  2346,\n",
      "         18899, 14838,   629, 22280,  1010,   117,  3429,   123,  8977,   230,\n",
      "          2636,   266,   117,   582, 11771, 22288, 19935,   117,  3233,   285,\n",
      "         22280,   117,   316, 19240,   122, 11902,   102]])\n",
      "DEBUG: Tokenized sentence 80: tensor([[  101,   700,   125,  2344,   128, 15648, 22281,   117,  3400,  1557,\n",
      "          2002,   123,  3377,   222,  1417,   125,  4141,   236,   180,  7296,\n",
      "           256,   102]])\n",
      "DEBUG: Tokenized sentence 81: tensor([[  101,  5608,   118,   176,   146,  3459, 17551,   285,  4568,   122,\n",
      "           117,   202,  5291,   171, 17859,   180,   854,  2028,   117,   418,\n",
      "           117,   271,   123,   223, 22279,   117,  7307, 18618,   246,   123,\n",
      "          3743,   125,   162,   512,   322,   102]])\n",
      "DEBUG: Tokenized sentence 82: tensor([[ 101, 1921,  854, 2028,  495,  646,  314, 5895,  102]])\n",
      "DEBUG: Tokenized sentence 83: tensor([[  101,   229,  1855,   117,  5147,   117, 21439,   148,   692,   118,\n",
      "           176,   259,  6015,   128,   102]])\n",
      "DEBUG: Tokenized sentence 84: tensor([[  101,  4141,   236,  4041, 10955,  3369,   202, 14948,   342,  8810,\n",
      "          3611, 11003,   122,   146,  1417,   125,  8170,  2089,   203,   118,\n",
      "           176,   170,   123, 13219,  2028,   117,  3336,  8286, 22281,   117,\n",
      "           122,   117,   202,  1338,   125,  1695, 10730,   117, 16653,   173,\n",
      "          2982,   123,   139,   124,   102]])\n",
      "DEBUG: Tokenized sentence 86: tensor([[  101,   163,  2550,   151, 14808,  3292,   125,   958,  1025, 18805,\n",
      "          5919,   117,  4970,   256,   117,  2509,   117,  8598,   117,   125,\n",
      "          5747,  1768,   151, 22280,   122,   440,  5345,  1522,   125,  5052,\n",
      "           117,   122,   221,  1977,   222, 15316,   229, 22280,   495,   222,\n",
      "          2397,   117,   122,   146,  2099,   125,   229,  6889, 22282,  4712,\n",
      "           117,  8883, 22278,   331,   240,   898,   222,  5846,   102]])\n",
      "DEBUG: Tokenized sentence 87: tensor([[  101,   262,   230,  1718, 22278,   260,   675,   223,   128,   117,\n",
      "           291,   240,  2601,  3914,   117,  2344,   128,  5976,   233, 20799,\n",
      "          1429,   320,  5973,   268,   117,   320, 14793,   117,   123, 11062,\n",
      "           117,   123,  2496,   117,   122,   320,  3050,   173,  1010, 22278,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 88: tensor([[  101,   449,  2364,  2789,   125,   333,  7427,   154,   117, 13086,\n",
      "           125, 22005,   713,   143,   978,   230,  8193,   747,   229,  7698,\n",
      "           117,   582,   123, 17479,   406,   117,  1485,   260, 13674,   117,\n",
      "           170,   260,   223,   128,   563,  8757,   954,  4102,   128,   117,\n",
      "           291,   260,  8523,  1053,  8757,   423,  1224,   319,   185,   117,\n",
      "           318,  3632,   256, 10262, 21014,   123,  2477,   705, 18805,   613,\n",
      "           699,   117,   223, 22279,   298,   851,  5325,   143,   102]])\n",
      "DEBUG: Tokenized sentence 89: tensor([[ 101,  320, 1341,  180, 5246,  146, 5830, 2550,  247,  366,  675, 1746,\n",
      "         4464,  102]])\n",
      "DEBUG: Tokenized sentence 90: tensor([[  101,  6759, 22278,   170,  4141,   236,   180,  7296,   256,   240,\n",
      "           682,  6593,  4877,  2113, 11736,   125,   222,  2397,   117,   122,\n",
      "          1369,   324, 22280,  1021,   785,   582,  8671,   117,   122,  2113,\n",
      "          2036, 10355, 22287,   179,   259,  4966,   629, 22280,  8618,   125,\n",
      "           681,  6205, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 91: tensor([[  101,  2364, 18424, 22278,  2292,   102]])\n",
      "DEBUG: Tokenized sentence 92: tensor([[  101,   222,   644, 13429,   203,   179,   146,  4170,   117,   123,\n",
      "          4222,   452,   125,  1852,  6812,   117,  7194,   151,   170,  5288,\n",
      "          5598,   392,   146,  3336,   326,   180,  3400,  1557,   122,  4492,\n",
      "          2044,   179,   229, 22280, 18689,   151,   117,  2798,   325,   222,\n",
      "         16423, 22279,   117,  6086,  3848, 17714,   912,  7698,   102]])\n",
      "DEBUG: Tokenized sentence 93: tensor([[  101,  2010,   347,  2128,  1339,  9247,  5723,   740,   320,  4170,\n",
      "           117,   572,   266,   125, 16001,   102]])\n",
      "DEBUG: Tokenized sentence 94: tensor([[  101,  2354, 22279,  4047,   179,  2036,  4314,   244,  3166,   117,\n",
      "           173,   485,   252,  4067,   117,   259,  2292,   179,  2354, 22279,\n",
      "           376,   366, 20269,   136,   102]])\n",
      "DEBUG: Tokenized sentence 95: tensor([[  101,   495,   331, 14619,   210,   146,   179,  3207,   256,   229,\n",
      "         22280,   338,   185,   125,  1111,   321, 18659,   118,   311,   117,\n",
      "          2249,  1075,   117,   146,  3848, 17714, 22279,   117,   179,   333,\n",
      "           244,  2779,  1977,   146, 19537,  2034,   117,   449,   607,   125,\n",
      "           333,   221,  1369,   117,   221,  9101,   180,  5246,  3103, 22279,\n",
      "           117,   179,  9679, 15363,   125,  2249,   740,   495,  4051,   117,\n",
      "         17522,  2044,   123,  4411,   221,  2822,   260, 12950,  3292, 22281,\n",
      "           514,  1412, 14348,   123, 12292,  2028,   171,  1417,   102]])\n",
      "DEBUG: Tokenized sentence 96: tensor([[  101,   449,   117,   320,  4706,   123,  7698,   117,  9247,   382,\n",
      "         17251,  2935, 12111,   228,   118,   202,   320, 21568,   268,   298,\n",
      "           827,   382,   117,  3033,  1432,  8554,   201,   122,  4970,   146,\n",
      "          1457,   677,  1802,   240,  2480,   117,   170,   259,  1143,   202,\n",
      "         14793,   117,  3049,   304,  5874, 14004,   122,   223,   128,  8650,\n",
      "          2208,   221,   338, 22281,   117,  2437,  2994,   485,  1557,   117,\n",
      "          4304,  1444, 22278,   122,   170,   260,  2844,  1677,  4610, 12909,\n",
      "           591,   123,  3050,   173,  1010, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 97: tensor([[  101,   320,  1341,   117,   146,   450, 13732,   268,   272,  1510,\n",
      "         22281,   481,   117,  9247,  5723,   271,   222,  4720, 22281,   293,\n",
      "           117,  7404, 12631,   304,   118,  1084,   117,   122,   117,   125,\n",
      "          1078,   576,   179,   368,   176,  4621,   256, 22186, 22279,   117,\n",
      "           682,  7769,   117,   123,  2601,   125,   163,  2550,   151,   117,\n",
      "         16946,   692,   146,  5973,   268,   366,  8523,   180, 17479,   221,\n",
      "          2822,   272,   524,   118,  1340,   598, 22278,   854,  2028,   102]])\n",
      "DEBUG: Tokenized sentence 98: tensor([[  101,   123,   311,  2324, 22278,   117,   125,   766,   117,  3428,\n",
      "         15558, 22290,   117,  5167,   251,   125,  1571,   124,   117, 18419,\n",
      "           118,   176,   117,  1174,  1537,   524,   256, 21597,   194,   589,\n",
      "           117,   169,  1319,   214,   538, 15267, 22281,   793,  7229,   522,\n",
      "           358,   180,  1571,   124,   102]])\n",
      "DEBUG: Tokenized sentence 99: tensor([[  101,  3400,  1557,   117,  1821,  8221,   117,  2510,  4322,   117,\n",
      "          9466, 22282,  2146,   118,   176,   202,  1690, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 100: tensor([[  101,   146, 18043,   587,  1286,   273, 20899,  3724,   122,   298,\n",
      "           532, 22000, 22281, 15527,   256,  1941,  8093,   125,  7406,  7870,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 101: tensor([[  101,   146,  1568,   125,   646,   314,  5895,   117,   202,   652,\n",
      "          1990,   283,   125,   733,  4861,   304, 22280,   117,   316, 22280,\n",
      "         18928,  1212,  1169, 18377, 13665,   498,   123,  2772,   117,   179,\n",
      "           123,  1191,  9322,   102]])\n",
      "DEBUG: Tokenized sentence 102: tensor([[  101,   173,  2590,   117,  7415,   179, 12401,  4368,  3400,  1557,\n",
      "           123,  1105,   298,  8618,   122,   179,  2036,  1703,  8066,  6351,\n",
      "         22281,  6867,   944,   259,  8170,   102]])\n",
      "DEBUG: Tokenized sentence 103: tensor([[  101,   163,  2550,   151,   117,   123,  6865,   171,  3459, 17551,\n",
      "           171,  1247,   117,   222,  7148,   744,   390,   303,   117,  1565,\n",
      "           434,   763,   117,   146,   653,  8420,   352,  2854, 22278,   646,\n",
      "           314,  5895,   117, 11579,  1921,  2954,   221,   123,  7698,   125,\n",
      "           327,   223, 22279,   117,   121,   102]])\n",
      "DEBUG: Tokenized sentence 104: tensor([[  101,  5657, 17072, 22278, 18805,  5919,   117,   123,  5899,  2653,\n",
      "          3611, 17811,   102]])\n",
      "DEBUG: Tokenized sentence 105: tensor([[  101,   146,  3459, 17551,   495,   785,   180,  1105,   366, 18805,\n",
      "          5919, 10355,   118,   176,  2684,  1183,  4409,   170,  2859,   102]])\n",
      "DEBUG: Tokenized sentence 106: tensor([[  101,   146,  1652,   122,   179,   262,   229, 13398,   292,   125,\n",
      "         15982,   141,   117, 18369,   122,  3695,   117,   179,   368, 14009,\n",
      "           163,  2550,   151,   102]])\n",
      "DEBUG: Tokenized sentence 107: tensor([[  101,  4141,   236,   180,  7296,   256,   117,   240,  1966,   596,\n",
      "           117, 19288,   123,   651,   125,   629, 22280,   599,   145,   170,\n",
      "           146,  1417,   102]])\n",
      "DEBUG: Tokenized sentence 108: tensor([[  101, 11556,   347,   925,   148,   703,   441,   390,   303,   117,\n",
      "           146,  5009,   178,  1449,   157,   117,   122, 13206,   118,  2036,\n",
      "           146,  3265,   117,   179, 15109,   425,   260, 11004,   171,  7392,\n",
      "          2684,   370,  2169,   221,   148,  9581,   711,   118,   176,  1362,\n",
      "          1571,  1699,   125, 21990,  2204,   102]])\n",
      "DEBUG: Tokenized sentence 109: tensor([[ 101, 2160, 1257,  117, 1204,  125, 1160,  221,  123,  327,  577,  304,\n",
      "          102]])\n",
      "DEBUG: Tokenized sentence 110: tensor([[  101,  1112,  2535,  7719,  5212,   325, 12604, 13550,   495,  2711,\n",
      "           455,   123,  2606,   176,  5308, 22281,   236,  4412,   173,  1105,\n",
      "           180,   223, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 111: tensor([[  101, 22354,   320,  3767,  1084,   117, 14672,   179,   229, 22280,\n",
      "           146, 11690, 22287,  1921,  2954, 22279,   271,  1003,   236,  3377,\n",
      "           202,  3147,   180,  2772,   117, 15975,   203,   118,   176,   173,\n",
      "         18175,   122,   117,   221,   229, 22280,   176,  3530,   170,   740,\n",
      "           117,  8098,   203, 22280,  8271,   122,  3033, 22243,  7424,   229,\n",
      "          7698,   102]])\n",
      "DEBUG: Tokenized sentence 112: tensor([[  101,   259,   329,   143, 21397,   118,   202,   423,  5546, 22280,\n",
      "           122,   820,  4991, 10656,   228,   102]])\n",
      "DEBUG: Tokenized sentence 113: tensor([[  101,   449,   117,   229,  2880,   151, 22280,   173,   179,   368,\n",
      "          9882,   975,  1885,  3640, 22280,  3147,   125,   163,  2550,   151,\n",
      "           117, 16143,   123, 22283,  2082, 14473,   444,   125, 12514,   179,\n",
      "         10415,   692,   102]])\n",
      "DEBUG: Tokenized sentence 114: tensor([[  101, 21161,   118,   176,  6751,   412, 16055,   501,   292,   122,\n",
      "         18119,   654,   146, 12728,   123,  4303,   102]])\n",
      "DEBUG: Tokenized sentence 115: tensor([[  101, 12248,  2044,   123,  4410,   180,  2606,   102]])\n",
      "DEBUG: Tokenized sentence 116: tensor([[  101,  1112,   449,   117,   170,  1977,   117,   644,   492,   117,\n",
      "           740, 10415,   322,  7583,  5314,   136,   102]])\n",
      "DEBUG: Tokenized sentence 117: tensor([[  101, 22354,  1519,   301,   123,  4764, 19773,   340,   122, 14657,\n",
      "           203,   125, 12728, 16887,   102]])\n",
      "DEBUG: Tokenized sentence 118: tensor([[  101,  1112,   229, 22280,  1021,   623,  2100,  2010,   123,  1858,\n",
      "          4410,   495,   125,   222,  2397,   102]])\n",
      "DEBUG: Tokenized sentence 119: tensor([[  101, 22354,   834, 12449,   325,  3874,   117,  3285, 13665, 20462,\n",
      "         22281,   123,  4303,   122,   117,  8163,  3529,   118,   176,  1839,\n",
      "           171,  3147,   117,  7304,  2537,   118, 13427, 22287, 18928, 22278,\n",
      "           498,   123,  2772,   117,   179,  5986, 22278,  2044,   259, 15424,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 120: tensor([[  101,   146,  7148,   434,   763,   117,  1502,   495,  2461,   123,\n",
      "          1858,  4410,   117,   229, 22280, 18424, 22278,   596,   125,  7912,\n",
      "           122,  9322, 22278,   117,  8574,   452,   117,   712,  1143,   125,\n",
      "          3103, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 121: tensor([[  101,   625,   860, 12399, 22288,   366,   223,   128,   123,   338,\n",
      "         18070,   117,   221,   176,  4230,  8889,   171,  1342,   117, 13429,\n",
      "           203,   179,   123,   978,  5404,  2794,   102]])\n",
      "DEBUG: Tokenized sentence 122: tensor([[  101,  1767,   337,  3066,  1604,   122,   374, 22290,  4095,   243,\n",
      "           125,  1990,   974,   102]])\n",
      "DEBUG: Tokenized sentence 123: tensor([[  101,  3363,   318, 13793,   222, 22243, 22280, 12207,  5092,   102]])\n",
      "DEBUG: Tokenized sentence 124: tensor([[  101,  8362, 22278,   118,   176,   146,   398, 13193, 22279,   702,\n",
      "           298,   682,  2217,   102]])\n",
      "DEBUG: Tokenized sentence 125: tensor([[  101,   123,  8121,   304,  1657,   907,  8521,  2836,   118,   176,\n",
      "           449,   146,  3459, 17551,   117,  3477,   214,   146,  5052,   118,\n",
      "         10343,   117, 15568, 22288,   118,   176,   117, 18486,   654,   260,\n",
      "          6970,   122,   117, 12110,   214,  1266,   146,  1831,   180, 11003,\n",
      "           117,  1996,   170, 11948,   852,  2010, 10077,   118,   123,  2354,\n",
      "         22279,   122,   222, 18173,  2010, 21809,   122,  5023,   136,  5023,\n",
      "           333,   138,   240, 15256,  1528, 18173,   171,   179,  2779,   136,\n",
      "          2010,  9505,   260,  4971,   117,   693,  3671,  2113,  2354, 22279,\n",
      "          2364,   926, 22278, 12384,   123,  7122, 16789, 12526,   122,   117,\n",
      "           176,  2137, 22281,   236,  1165,  1182,   118,  1340,   117,   123,\n",
      "           792, 12268,   171,  2099, 19726,  7518,  1719,   498,   123,   327,\n",
      "          2004, 22278,  3049,   304,   117,   320,  6793,   179,  2779,   117,\n",
      "          1202, 22287,   171,  5846, 20933, 22285,   741,   322,  5149,   201,\n",
      "           229,  7122,  5261,   671,  2760,   117,  7206, 17485,   171, 14891,\n",
      "           247,  1014,  7122,   851,  5325,   122, 14808,   403,   865,  6644,\n",
      "         20733,   117, 14891,   247,   179,  6530,  5661,   823, 22283,   170,\n",
      "           146,  1831,   125,  2607,   373,   179,  5863,   418, 22279, 15245,\n",
      "           123,  2405,   366,   223,   128,   125,  4141,   236,   229,  6742,\n",
      "         20666,   171,  1078,   391,   102]])\n",
      "DEBUG: Tokenized sentence 126: tensor([[  101,   146, 12961,  1767, 13038,   825,   122,  4133, 22288,   123,\n",
      "          3049,   304,   102]])\n",
      "DEBUG: Tokenized sentence 127: tensor([[  101,  2010, 19821,  1084,   102]])\n",
      "DEBUG: Tokenized sentence 128: tensor([[  101,  1996,   146,  7148,   870,   509,   117, 13449, 11252,   122,\n",
      "         18260,   202, 20462,   171,  1456,   143,   102]])\n",
      "DEBUG: Tokenized sentence 129: tensor([[  101,  2745,  3102, 11788,   176,   706,  5646,  4765,   117,   170,\n",
      "           123, 14380,  2565,   125,  4023,   102]])\n",
      "DEBUG: Tokenized sentence 130: tensor([[  101,   331,   221,   123,  1386,   229, 22280,   607, 11935,   635,\n",
      "           176,  8204,   140,   117,  3605, 19391,   154,   333, 22278,  7223,\n",
      "          1378,   170,  1485,   260,  6322,   589,  6645,   122,  7083,   102]])\n",
      "DEBUG: Tokenized sentence 131: tensor([[  101,   122,   117,  3951,   123,  4410,   222, 22237,  2754,   125,\n",
      "          5852,  2010,   820,   117,   423,  7343, 22243, 22280,   498,   146,\n",
      "          5846,   117,   294, 12200,   173,  4708,   146,   347,   221,   123,\n",
      "          7122, 12526,   102]])\n",
      "DEBUG: Tokenized sentence 132: tensor([[  101,  5205,   136,  4141,   236,  5127,   171,  3147,   117,  2992,\n",
      "           339,   125,  1571,   124,   117,   125,   792, 12268,   122,   125,\n",
      "          2873, 20477, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 133: tensor([[ 101, 2010,  179, 1069,  123,  327, 2638, 4839,  256,  102]])\n",
      "DEBUG: Tokenized sentence 134: tensor([[  101,   179,  1069,   117, 11842,  4023, 22280,  7148,  7104, 22288,\n",
      "           123, 13246,   146,  1078,   391,  3486, 22282,   748,   118,   176,\n",
      "           229,  5246,   125,   629, 22280,  1010,   117,   320,  1341,   366,\n",
      "           675,   419,  5511,   122,   944,   259,   171,  1247,   117,  2684,\n",
      "           653,   259,   125,  1105,   117, 16266,   288,   123,  1386,   125,\n",
      "           163,  2550,   151,   320,  5791,   183, 20295,   300,   455,   176,\n",
      "          2036,  1021,  3285,   286,   202,  1831,   102]])\n",
      "DEBUG: Tokenized sentence 135: tensor([[  101,   146,  3459, 17551,  9202,   256,  3636,  3264,   382,   122,\n",
      "         14372,   123, 11904,   159, 20885,  3514,   146,   347, 10444, 21102,\n",
      "           117,  1684,  1672,   240,  2397,   125,  5747, 18805,   292,   122,\n",
      "           125,  1491,  9429, 22281,   437,  1026,   441,   102]])\n",
      "DEBUG: Tokenized sentence 136: tensor([[  101,   259,  7427,   382,  8500,   123,  8430,   118,  2036,   117,\n",
      "           125,  1615,  2653, 20899,   125, 18175,   117,   259,  2980,   475,\n",
      "          5635,   128,   117,  3922,  4242,   122, 16573, 22281,   298,   532,\n",
      "         17477, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 137: tensor([[  101,   173,  5255,   117,   260,  4486,  1359,   692,  1485,   712,\n",
      "         17457,  4141,   236, 13206,   123,  7698,   123,  3400,  1557,   122,\n",
      "           325,  1510, 22281,   827,   382, 12301,   117,   179,   313,   512,\n",
      "           342, 22288,  2044,   117,   122,   117,  8582,   423,  4745,   180,\n",
      "         17479,   406,   117,  5793,   221,   123,   651,  1950, 22280,   599,\n",
      "           145,   117,   202, 19284,   373,   125, 17558,  5799,   532,  6109,\n",
      "           122, 21615,   118,   176,   123,  9019,   151,   170,   146,  1417,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 138: tensor([[  101,   123,   223, 22279,   125,   646,   314,  5895,  2286, 17878,\n",
      "         12604,  8889,   102]])\n",
      "DEBUG: Tokenized sentence 139: tensor([[  101,   629, 22280,  1010,  3336,   123,   327,  8721,   122,   262,\n",
      "           712,  3885,   861,  7831,  7989,   125,  3330,  2876,   319,   251,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 140: tensor([[  101,  5147,   117,   146,  3265,   117,   625,  2080,   123,  1105,\n",
      "           171,  7392,   229,  1855,   117,  1011,   117,   271,  6530,   176,\n",
      "           706, 21122,   117,   170,   123,  6614,   498,   259, 12684,   102]])\n",
      "DEBUG: Tokenized sentence 141: tensor([[  101,   123,  3207,   125,  8170, 18100,   124,   118,  2036,   229,\n",
      "           505,   508,  4149,   251,   230,  9434, 22280, 12448,   125,  3848,\n",
      "           687,   151,  1821,   179,   229, 22280, 17002,  8925,   259,  5708,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 142: tensor([[  101,  1364,   368,   495, 11775,   338,   183, 22279, 19042,  2993,\n",
      "           978,   146,  9466, 16606,   785,  5980, 22280,   117,   123,  3182,\n",
      "         22278,  6459, 10583,   539,   117,   146,  1831,   123, 16907, 22282,\n",
      "           118,   176,   125,   184,   809,  7961,   122,   374, 22281,   236,\n",
      "           865, 22292,  4617, 22278,   117,   146,  5052,  9078,  2878,  7485,\n",
      "         22280,   123, 15631,  4322,   440,   157, 20126,  1337,   102]])\n",
      "DEBUG: Tokenized sentence 143: tensor([[  101,  2440,   171, 16589,   234, 17688,   117,   179,   123,  2745,\n",
      "           398,   544,  2374,  7401,   117,   123,  6754, 17479,   229, 22280,\n",
      "          4207, 11552,  2364,   423,  1417,  1084,  1011,   163,  2550,   151,\n",
      "           221, 16946,   118,  1084,  2461,   117,   221,  5635,   578,   118,\n",
      "          2036,   260,   505,  1144, 22281,   123,  1224,   319,   185,  1971,\n",
      "          1016,   117,   179,   117,   625,  4141,   236,  2036,  3127,   179,\n",
      "           646,   314,  5895,  8544,   221, 12733, 22278,   171,  7392,   229,\n",
      "           651,   117,   123,   851,  5325,   472,   194,   303,   203,   170,\n",
      "          1084, 20739, 15834,   591,  7583,  2531,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 144: tensor([[  101, 17226,   117,   146,   273,   522,  8073,   994,   262,  3530,\n",
      "           173, 22232,   795,   117,  8981,  4419,   125,   347,  1568,   117,\n",
      "           123,   325,   505,  2552, 22278, 19731,   366, 21839,   138,   102]])\n",
      "DEBUG: Tokenized sentence 145: tensor([[  101,   123,  3264, 17704,   117,   271,  9679,   179,   146,  4170,\n",
      "           146,  1695,   179,   978, 12444,   123, 16453,   128,   292,   243,\n",
      "           925,   148, 22280,   117,  4031,  2904,   118,   176,  2044, 20616,\n",
      "           123,  6202,   125,   223, 22279,   320,  1417,  2166,   102]])\n",
      "DEBUG: Tokenized sentence 146: tensor([[  101,  9480, 14948,   117,   877,   319, 10388,   171,   347,  2982,\n",
      "           117,   744,   229, 22280,   495, 14685,  3876,   596,   117,   125,\n",
      "          7716,   179,   260, 22202, 22281,   180,   327,  2013, 16644, 17805,\n",
      "           228,   320,   862,  6720,   326,   102]])\n",
      "DEBUG: Tokenized sentence 147: tensor([[  101,  1839,   173,  1695,   117,   202,   762,  7591, 22290,   268,\n",
      "           505,  2552, 22280,  7970, 22281, 10180,   125,   223, 22279,   117,\n",
      "           646,   314,  5895,   117,   125,  2996, 22280,   179,   495,   117,\n",
      "          1204,   118,   176,   230,   854,  2028,  2124,   117,   629,   122,\n",
      "         19543,   102]])\n",
      "DEBUG: Tokenized sentence 148: tensor([[  101,   262,   318, 13793,   179,  9480, 14948,  3429,   320,  1147,\n",
      "           123,   905,   247,   785, 19042,   508,   122,  1821,   834,  2822,\n",
      "          1365,   273, 22283,   102]])\n",
      "DEBUG: Tokenized sentence 149: tensor([[ 101, 5009,  178, 6952,  256,  870, 3093,  183,  117,  170, 7672,  125,\n",
      "         2243,  118, 1084,  102]])\n",
      "DEBUG: Tokenized sentence 150: tensor([[  101,   179,  1998,   117,   259,  1510, 22281,  1867,  2112,   125,\n",
      "           327,  1069,  9821,  3972,  6070,   123,  1364, 16423, 22279,   117,\n",
      "           144,   343,  6436,   252, 16241,  1537, 22287,  4678,  4322,   229,\n",
      "          1105,   146, 19317,   175,  5334,  5630,   271,   222,  7955,   117,\n",
      "          1139,   123,  2606,  5057, 13246, 22281,   712, 14125,   180,   327,\n",
      "          7427,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 151: tensor([[  101,   495,   240,  3413,   179,   123,  9586,   117,   325,  1373,\n",
      "           117,   176, 18127,   256, 15397, 20383,   178,   246,   125,   370,\n",
      "          2160,   146, 18718,   180,   792,  8723, 12220, 22281,  2047,  2291,\n",
      "           143,   180,  1717,   143,   148,   102]])\n",
      "DEBUG: Tokenized sentence 152: tensor([[  101,   122,   320,  1341,   125, 22232,   795,   117,   179,  2954,\n",
      "           122,   644, 20262,   256,   146, 10420,   303,   180,   450, 13732,\n",
      "           252,  9525,   148,   117,  1011,   146, 11522, 11551,   117,   146,\n",
      "          8776,   157,  1417,   117,   179,   860, 14619,   210,   123, 13028,\n",
      "           125,   223, 22279,   122,  1941,   176,   229, 22280, 10201,   256,\n",
      "           180,  5907,   117,   180, 13305,   179,   146,  4250,  2650,   124,\n",
      "           529,  3837, 13808, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 153: tensor([[  101,   123,  9586, 20612,   118,   176,   117,   416,  1149,   712,\n",
      "          9361,  1312,   942,   125,   222,  6815, 22280,   117,   179,  3767,\n",
      "         22278,  1021,  1695,   180, 19206,  3282,   292,   125,  2713,  1440,\n",
      "          6940, 22282,   117,   121, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 154: tensor([[  101,  1941,  8301,  1198, 22286,   117,   122,   117,   123,  1018,\n",
      "           180, 22283,  5009,   178,   229, 22280,  8204,  4945,   125,  1342,\n",
      "         13477,   154,   697,   173, 21666,  1105,   102]])\n",
      "DEBUG: Tokenized sentence 155: tensor([[  101,   240,  1921,  1405,   451, 22278,   117,   325,   291,  1528,\n",
      "           117, 19288,   171, 14948,   342,   123, 11489,   151,   125,  5110,\n",
      "           121,   102]])\n",
      "DEBUG: Tokenized sentence 156: tensor([[  101,   163,  2550,   151,   233, 20799,   286,   527,   148,  9301,\n",
      "          1009, 22280, 13920,   102]])\n",
      "DEBUG: Tokenized sentence 157: tensor([[  101,  2010,  2002,   118,  2036,   125, 19170,  6834,   256,   146,\n",
      "          1820,   247,   117,   170,   146,   347,   629,   303,   125, 16064,\n",
      "           260,  8523,   102]])\n",
      "DEBUG: Tokenized sentence 158: tensor([[  101,   262,  1706,   171,  5980, 22280,   117,  6884, 22280, 22279,\n",
      "           117,  1695,   700,   117,  4141,   236,  1449,   157,   180,  7296,\n",
      "           256,   117,  1364, 14021,   125,   599,   183,   117,   785,  9221,\n",
      "           514,  1859,   122,  5510,   788,   117,  8940,  3093,  8156,  2387,\n",
      "           259,   532,  3907,  2698,   122,  1018,  2044,   221, 18615,  1187,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 159: tensor([[  101,  5009,   178,  5948,   256,   118,   146,  2643,   138,   122,\n",
      "         16280,   118,   176,   125,   873,   118,  1041, 22278,  2323,  1177,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 160: tensor([[  101,  1148,  2610,   118,   176,  2745,   221,   123,  4036,   122,\n",
      "          4141,   236, 12401, 13665,   118,   176,   123,   169,  8388,  2954,\n",
      "           173,  1105,   171,   925,   148, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 161: tensor([[  101,   449,   229,  4149,  1657, 22279,   466,   702,  9834,   117,\n",
      "          1011, 16571,   487,   117,   122,   123, 10201,  2028,   298,  1749,\n",
      "           909,  7780,   117,   179,   169,  8388,   246,   176,  3841,  6859,\n",
      "           170,   368,   117,  2364,   146,  1178, 10422,   124,  1971,   102]])\n",
      "DEBUG: Tokenized sentence 162: tensor([[  101, 17760,   118,   176,   122,   662,  1353,   123,  7282,   159,\n",
      "           202,  3147,   117,   123,  5961, 10580,   117, 14466,   117,  2607,\n",
      "           364,   175,   117, 16649, 13431, 13510, 22281,   125,   944,   259,\n",
      "          6615,   102]])\n",
      "DEBUG: Tokenized sentence 163: tensor([[  101,  1676,  1256,  2856,   180, 12495,   117,  5009,   178,   117,\n",
      "          6851,   201,   117,  2113,   117,   125,  1485,   260,  1176,   179,\n",
      "         19994,   256,   117,  1413,  3377,   202,  3147,   171,  9730, 22279,\n",
      "           122,  8362, 22278,   118,  2036,   146,  4081,   298, 10674,  4525,\n",
      "         22279,  1289,   122, 13348,  1228,   358,   117,   122, 16280,   118,\n",
      "          2036,   259,  2510, 13513,   375,   581,  1165,   442,   122,   146,\n",
      "          4410, 22279,   159,  2143,  9370, 22280,   122, 20703,  1212,   117,\n",
      "           229, 22280,   176,   706,   370,   122, 17760,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 164: tensor([[ 101, 1112, 8743, 3933, 5664,  146, 4141,  236,  136,  102]])\n",
      "DEBUG: Tokenized sentence 165: tensor([[  101, 22354, 17662,   368,   117,   173, 18271, 22290,  7831,   118,\n",
      "           176,   202, 16936,  3210,   122, 12086,  7583, 15512, 13793,   102]])\n",
      "DEBUG: Tokenized sentence 166: tensor([[  101,   123,  4303, 16995,   118,   176,   820,   202, 19125,   303,\n",
      "           117,  6540,   118,   123,   125,   256,   702,   122,  3033,   102]])\n",
      "DEBUG: Tokenized sentence 167: tensor([[  101,   146,  4970,   378,   117,   320, 10500,   614,   210,   117,\n",
      "          2927,   118,   176,  1990,  3181,   243,   122,   117,  3951,   170,\n",
      "           146, 20277,   455,  2036,  5808,   151,   123, 15460,   256,   117,\n",
      "          2687,   203,   125,  4332,   942, 19767, 22281,   117,   420,  9247,\n",
      "           382,  6482,   102]])\n",
      "DEBUG: Tokenized sentence 168: tensor([[  101,  5009,   178, 17522,   498,   368,   449, 12822,   179,   176,\n",
      "          2811,   123,  8223,   117,  1941,   146, 12961,   125,   163,  2550,\n",
      "           151,  1021, 11314,   243,  1950, 10490,   671,   246,   202,  1690,\n",
      "         22280,   102]])\n",
      "DEBUG: Tokenized sentence 169: tensor([[  101,  1191,   118,   176,  2044,   222,   739,   390,  2762,   240,\n",
      "          1719,   123,  1105,   117,   179,   495,  3876,   596,   202,  3420,\n",
      "           739,   117,   122,   229, 13398,   259, 11314,  2650,  1058,   171,\n",
      "         19317,   175,   744,   229, 22280, 19575, 22287,   170,   146,  9019,\n",
      "         13793,   102]])\n",
      "DEBUG: Tokenized sentence 170: tensor([[  101,   123,  3264, 22232,   795,   417,  1797,   456, 20466,   117,\n",
      "         13086,   272,  1757,   821,   102]])\n",
      "DEBUG: Tokenized sentence 171: tensor([[  101,  1112,   222,  5490,   285,   118,  1143,  7642,  8167, 22354,\n",
      "         10355,   117,   305,   162,  7039,   243,   259,   598,   474,   122,\n",
      "           781,  4387,  1409,  1143,   171, 15447,   102]])\n",
      "DEBUG: Tokenized sentence 172: tensor([[  101,   964, 22087,   138,   117,   311,  7248, 22281,   125,  1719,\n",
      "           123,  4446, 22279,   117,   506, 10201,   591,   429,   118,   176,\n",
      "           173,  2244,   123,  7930,  2350,   687,   232,   117,   122,   117,\n",
      "           180,   151,   230,  5314,   146, 13453,  3355,   286,  1359,   256,\n",
      "           123,   898,   102]])\n",
      "DEBUG: Tokenized sentence 173: tensor([[  101,   449,   229, 22280,   706, 15568, 22282,   118,   176,  4412,\n",
      "         22278,   785,  4041,  8725,   102]])\n",
      "DEBUG: Tokenized sentence 174: tensor([[  101,   123, 15172,  4149, 22279,  3051,  1556,   118,  2036,   230,\n",
      "         14594, 16353,   117,   179,  6718,  2684,   123,  2954,   117,   625,\n",
      "          2080,   870,   509,   146,  1941,  8301,  1198, 22286,   102]])\n",
      "DEBUG: Tokenized sentence 175: tensor([[  101,   495,   230, 14594,  3598, 11649,   117, 11535,   860,   102]])\n",
      "DEBUG: Tokenized sentence 176: tensor([[  101,   122,   325,   179,   123,  3848,   687,   151,  7854,   151,\n",
      "          4863, 12245,  2010,  1415,  1785, 22279,   339,   125,  5791,   183,\n",
      "          3874,   125, 21595,   252,   117,  1953,  3103, 22279,   117,  3002,\n",
      "         14755,   123, 16302,   304, 22280,   171,  6815, 22280,   117,  8204,\n",
      "           792,   146,  1417,   102]])\n",
      "DEBUG: Tokenized sentence 177: tensor([[  101, 12631,  1353,   118,   146,  7078,  2746,   117,  1996,   118,\n",
      "          2036,   455,  1011,   221,  8111,   102]])\n",
      "DEBUG: Tokenized sentence 178: tensor([[  101,   122,   202,  1342,   644,   744,   125,  9461,   117, 10116,\n",
      "          2598,   118,   146,  5695,   222, 13265,   151, 22280,   117,  1191,\n",
      "         16286,   122,   117,  5334,  2537,   117,  5608,  5009,   178,   221,\n",
      "           347,  1341,   102]])\n",
      "DEBUG: Tokenized sentence 179: tensor([[  101,  2010,  7343,   925,   148, 22280,   117,  9099,   203,   118,\n",
      "          2036,   102]])\n",
      "DEBUG: Tokenized sentence 180: tensor([[ 101,  176, 2779,  344, 1014,  102]])\n",
      "DEBUG: Tokenized sentence 181: tensor([[  101,   146,   179,   122,   668,  4812,   117, 11935,   185,   118,\n",
      "           311,  2044,   146,  3265,  1266,   123,  1105,   171,   766,  1391,\n",
      "           183,   173, 21990,  2204,   102]])\n",
      "DEBUG: Tokenized sentence 182: tensor([[ 101, 3303, 4111, 1112,  179,  146, 4750, 2010,  170,  785, 4945, 2010,\n",
      "          179,  146, 3285, 4368, 1362, 1571, 1699,  125, 2796,  519,  102]])\n",
      "DEBUG: Tokenized sentence 183: tensor([[  101,  9086,   123, 22283,  2780,  3495,   102]])\n",
      "DEBUG: Tokenized sentence 184: tensor([[  101,   229, 22280, 12012,  7482,   125,  3598,   578,   170,   146,\n",
      "           347,  1417,   179,  2036,  2811, 22287,  2350,  1215, 22282,   122,\n",
      "           171,   325,  2135, 22280, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 185: tensor([[  101,  3769,  4486,  4112,   118,   202,  8562,   159,  1941,   944,\n",
      "           259,  5334, 18444,   271,  4807,   117,   122,   117,   954,  1564,\n",
      "           272,   325,  5121,   117,   625,  4141,   236,  2607,   364,   256,\n",
      "           229,   327, 14594,   117,  4169,   173,  1105,   171,  5009,   178,\n",
      "           146, 21824,   303,   171, 14948,   342,  3711,   252,   785,  6358,\n",
      "           373,   117,  4945,   171,  1177,   171,   347,  3695,  4141,   236,\n",
      "          1112,   171,   347,   925,   148, 22280, 22354, 10355,   368,   170,\n",
      "           230,   739,  6720,  1200,   102]])\n",
      "DEBUG: Tokenized sentence 186: tensor([[  101,   122,   180, 22283,   117,   229, 22280, 18370,   256,   123,\n",
      "          1105,   102]])\n",
      "DEBUG: Tokenized sentence 187: tensor([[  101,   543,  5723,   118,   176,   123,   222,  2745,   117,  1312,\n",
      "          2848,   117, 11831,   183,   117,   260,  1176,  5334,   288,  7173,\n",
      "           214,   657,   455,  2036,   873,   285,   692,   123,  3302,   202,\n",
      "          3147,   171,  9525,   283,   102]])\n",
      "DEBUG: Tokenized sentence 188: tensor([[  101,  5009,   178,   122, 22232,   795,   229, 22280,   176, 16463,\n",
      "         11814,   125,  2533,  6658, 22278,  3365,  6358, 12250,   171,  4062,\n",
      "          7148,   117,   146,  3316,   170,   179,   368, 19288,   944,   259,\n",
      "          1564,   221, 10381, 11489,   562,   171,  8588,   339,   102]])\n",
      "DEBUG: Tokenized sentence 189: tensor([[  101, 12119, 14533,   118,  2036,   222,   739, 19903,  1637, 16995,\n",
      "         22287,   118,   202,   311,  1039,   117, 13071,   293,   122, 12060,\n",
      "           713,   102]])\n",
      "DEBUG: Tokenized sentence 190: tensor([[  101,  2010,   122,   222, 11842,  2397, 10355,  5009,   178, 20089,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 191: tensor([[  101, 22232,   795,  9202,   256,   117, 19683,   173,  4410,  4098,\n",
      "          2010,   240,  4183,  3391, 13793,   229, 22280,   122,   117,   144,\n",
      "          2640,   944,  4178, 22287,   179,   146,  7148,   434,   763,   229,\n",
      "         22280,  3804,   125, 12821,  3613, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 192: tensor([[  101,  2010,   122, 11935,   679,   243,   125, 12464,   117,  1502,\n",
      "           229, 22280,   449,   117,  2389,   296,   117,   179,  4178, 13468,\n",
      "          1004,   146,   179,  1776,   102]])\n",
      "DEBUG: Tokenized sentence 193: tensor([[  101, 16246,   118,   176,   230,  3639,   398, 11291,   298,  1405,\n",
      "          5456,  1530,  7406,   256,   909,   180,  1069,   171, 11842,  3459,\n",
      "         17551, 12374,   692,   118,   333,   138,  1289,   125,   472,   514,\n",
      "           421,   304, 22280,   117,  7799,   188, 20154,   138,   123, 13535,\n",
      "          1950, 10490,  2208,   117, 16759,   143,   125,  7605,   138, 10278,\n",
      "           117,  6432,   125,  8588,   852,   272,   122,  6432,   125,   273,\n",
      "          6677,  1249,   102]])\n",
      "DEBUG: Tokenized sentence 194: tensor([[  101,  1112,   222, 11842,   222,  6149, 11842, 22354,   122,  1016,\n",
      "           262,   146,  7148,   434,   763, 12086,   766,   173,  1105,   125,\n",
      "          5009,   178,   122,  2636,   118,   176,  1364,   125,  1084,   102]])\n",
      "DEBUG: Tokenized sentence 195: tensor([[  101,  1941,  7719, 22287,  1988,   368,   221,  1852,  6812,   125,\n",
      "          9480, 14948, 11690, 22287,   118,   202,  1485,   260,  1373, 22281,\n",
      "           170, 19935,   117,   122,   123,  2954,   117,   538,   333, 22280,\n",
      "           143,   180, 14735,  4146,   117,  4170,   122,  2606,   229, 22280,\n",
      "          4363,   923,  2880,   151, 22280,   125,  7283,   260,  7799, 13779,\n",
      "          2662,  1313,   171,  7258,  3459, 17551,   117, 20838,  3838,   118,\n",
      "          2036,   138,  9429, 22281,  7083,   122, 16302,   118,  1340,   260,\n",
      "         12338,   271,   222,  9235,  3695,   122,  4929,   907, 22280, 21839,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 196: tensor([[  101,   222,   679,   117,   173,   179,   368,   117,   271,  1684,\n",
      "           117, 13140,   125,  6358, 12250,   117, 10573,   256,   423,  1112,\n",
      "           347, 12541, 22354, 15024,   118,  2036,   179,  4141,   236,  1009,\n",
      "           256,  3039,   125,   636,  9538,   122,   179,   146, 17671,  1637,\n",
      "          1467,  6104,   170,   123,  4036,   123,  8768, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 197: tensor([[  101,   434,   763, 15823,   172, 22288,   117,  7151,  8174,   788,\n",
      "           449,   117,   176,   614,   210,  2036,  7119,  9226,   146,   179,\n",
      "           398,  4549,  8041,   320,  7433, 22282,   260,  3656,   401,   117,\n",
      "           370,   118,   176,   118,  8544,  8296,   243,   125,  9226,  3769,\n",
      "           122,  1028, 16264,  2010,   644,   492,   102]])\n",
      "DEBUG: Tokenized sentence 198: tensor([[  101, 16003,   792,   179,   744,   229, 22280,   176,  2541,  1014,\n",
      "           117,   146,  3002, 11646,   136,   102]])\n",
      "DEBUG: Tokenized sentence 199: tensor([[ 101,  122, 2779,  117,  179, 1941,  146,  978,  240, 1111,  321, 2034,\n",
      "          243,  102]])\n",
      "DEBUG: Tokenized sentence 200: tensor([[  101,   202,   644,  1457,   117, 10355,   146,  7492,   303,   320,\n",
      "          3753,  4968, 21510,  2010,  4062,   117,  2535,   179,   146,  7275,\n",
      "          2397,   418, 11802,   130,   125,  9538,   117, 21174,   925,   325,\n",
      "           331, 22281,   236,  1655,   221,   123,  7122, 21824,  1936,   102]])\n",
      "DEBUG: Tokenized sentence 201: tensor([[  101,  1941,   229, 22280, 17891,   834,   596,   102]])\n",
      "DEBUG: Tokenized sentence 202: tensor([[  101,   122, 13156,   456,   118,   176,   117,  1364,  7799,  3724,\n",
      "           122, 13449, 19088, 22281,  5450, 15249,   942,   117,  8582,  1676,\n",
      "          7489,   304,   128,   180, 20027,   102]])\n",
      "DEBUG: Tokenized sentence 203: tensor([[  101,  2010,  7258,  3459, 17551,  9247,   654,   118,  2036, 22232,\n",
      "           795,   171, 19271,   956,   180, 19104,   102]])\n",
      "DEBUG: Tokenized sentence 204: tensor([[  101,   229, 22280, 21719,  2535,   271,   259,  6815,   128,   117,\n",
      "           179,   331,  7628,   170,   260,  3848,   687,   562,   102]])\n",
      "DEBUG: Tokenized sentence 205: tensor([[  101,  1547,   329,   125,  1105,  2010,  1214,   252,   125,   576,\n",
      "           173,   625,   117,  7148, 14000,  5009,   178,   102]])\n",
      "DEBUG: Tokenized sentence 206: tensor([[  101,  1183,  8073,   247,   339, 17400,  5926,   246,   117,   122,\n",
      "          3876,   653,   644,  7570,   203,   146,  1151, 14139, 22280,   173,\n",
      "          8603,   180,   327,  1198,  3729,   102]])\n",
      "DEBUG: Tokenized sentence 207: tensor([[  101,  1921,  2954,   117,   529,  7289,   125,  5009,   178,   117,\n",
      "           331,   176, 14580,   203,   498,   260,  7799, 13844,   122,   259,\n",
      "          9361, 21037,   243, 14031,  6347,   171, 14948,   342,   102]])\n",
      "DEBUG: Tokenized sentence 208: tensor([[  101,  4141,   236,   117,   170,  1250,  1519,  6423,   298,   125,\n",
      "          1105,   117,  5572, 16879,   351,   258,  8066,  6044,   246,   102]])\n",
      "DEBUG: Tokenized sentence 209: tensor([[  101,  5009,   178,   122, 22232,   795, 14058,   692,   118,   202,\n",
      "           125,   870,  8150,   117, 16617,   128,   240,  5637,   118,  1340,\n",
      "         20853,   123,   525,   381,  2346,   351,   180, 12495, 16294,   117,\n",
      "           146,  5790,  1499, 15295,   117,  2589,   146,   877,   319,  5607,\n",
      "           180,  3848,   687,   151,   180, 22283,   123,  5664,   125,   222,\n",
      "           454,   117,   146,  5572, 16879,  2649,  9565,  2962,   870, 11837,\n",
      "          3870, 22278,   117,   123, 21736,   366,  2861,   340, 22281,   598,\n",
      "          1313,   180,  8981,  4419,   122,   298, 13790,   171,   925,   148,\n",
      "         22280,   102]])\n",
      "DEBUG: Tokenized sentence 210: tensor([[  101,  2010,   179,  2541, 22281,  1084,  1434,   117,  2397,   125,\n",
      "          4023,   136, 10573,   256,   860,   102]])\n",
      "DEBUG: Tokenized sentence 211: tensor([[  101,   176,   495,   240,  2318,   180,  3400,  1557,   117,   179,\n",
      "           679,   492, 19636,   118,   123,  2477,   146,  1407,   240,   210,\n",
      "           117,   995,   123,   327, 14578,  5012,   151, 22280,   117,  1467,\n",
      "          5308,   118,  1084,  1084,   582,  1011,   102]])\n",
      "DEBUG: Tokenized sentence 212: tensor([[  101,   230,   827,   154,   180,   577,   304,   117,   179,  2364,\n",
      "          5127,   171,  8788, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 213: tensor([[  101,   229, 22280,   229, 22280,   495,  1257,  3989,  1399,   146,\n",
      "          1342,   102]])\n",
      "DEBUG: Tokenized sentence 214: tensor([[  101,   449,   229, 22280,  3569,   221,   123,  2480,   117,   834,\n",
      "           370,  3433,   230,  3122,   121, 22361,  5708, 13793, 14948,   342,\n",
      "          2010,   320,  1528,   229, 22280,  2541,   331,   117,  4141,   236,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 215: tensor([[  101,  2779, 21174, 12079,   118,   437,   102]])\n",
      "DEBUG: Tokenized sentence 216: tensor([[  101,  4141,   236, 13406,   685,   102]])\n",
      "DEBUG: Tokenized sentence 217: tensor([[  101,   179,  1941,  1011, 15363,  4062,   102]])\n",
      "DEBUG: Tokenized sentence 218: tensor([[  101,   122,   117,   173,  1652,   125,  4096,   117,  4207,  7283,\n",
      "           271, 22281, 17269,  1044,   117,   179,  1085,   944,   532,  2217,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 219: tensor([[  101,   122, 10355,   260,   238,  2766,   277,  7348,   179,   978,\n",
      "          2160,  2684,  1369,  7719,  1405,  5456,  1530,   123,  3953,   171,\n",
      "          1151, 14139, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 220: tensor([[  101,  1112,   122,   179,   176,  5308, 22281,  6867,  1659,   229,\n",
      "         22280, 19864,   123,  1434,  7970,  4036,   222,  7395,   268,   125,\n",
      "          1118,  3049,  1149,   102]])\n",
      "DEBUG: Tokenized sentence 221: tensor([[  101,  3841,   272,   792,   179,   117,  1075,   171,  1338,   171,\n",
      "           454,   117,  1011,   368,   125, 20262, 22281,   221, 21990,  2204,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 222: tensor([[  101, 22354,  8383,   102]])\n",
      "DEBUG: Tokenized sentence 223: tensor([[  101,   123,  4036, 17522,   118,  2036, 21964,   328,   117,   271,\n",
      "           125, 12215,  4900,   596,   117,   173,   179,   146, 15273, 13808,\n",
      "          3632,   624,   229, 22280,   978,  8886,   143,   102]])\n",
      "DEBUG: Tokenized sentence 224: tensor([[  101,  3547,   117,   123,   327,  7698,   495,  5533,   117,   785,\n",
      "          1839,   117,   123,  1685,  2653, 20899,   180,  4411,   102]])\n",
      "DEBUG: Tokenized sentence 225: tensor([[  101,  5657,  1096,   117,   240,  1104,  1017,   117,  3174,   900,\n",
      "           118,   176,   123, 22283,  1450,  2856,  1075,   125,  2849, 22282,\n",
      "           118,   176,   202,  8788, 22280,  1847,   117, 19938,   117,  8364,\n",
      "           298, 14431,   441,  5646,  4765, 11358,   304, 22280,   122,  1434,\n",
      "           123,  6629,   326,  9561,   102]])\n",
      "DEBUG: Tokenized sentence 226: tensor([[  101,   259,  3885,  8043,  3833,   170,  2571, 10602,  5267, 22287,\n",
      "          1684,   117,   240,  1851,  1382,   304, 22280,   117,   222,  1112,\n",
      "           372,   537, 22287, 22354,   117,   122,   860, 22280,   655,   179,\n",
      "          1369,  2415, 19649,  3514,   176,   180,   320, 13375,   122,   146,\n",
      "           372,   537, 22287,  1528,  7947,   221, 13375, 22282,   146,  8022,\n",
      "           175,   117,   179,   123,  4441,   285,   122,  3264,   117,   171,\n",
      "           179,   221,  2036,   870, 19328,   796, 22282,   146,  6482,   298,\n",
      "           390,  2227,  3471,   117,   366,  3185,  1149,   122, 15210, 22281,\n",
      "           125,   179, 14936,  1611, 22281,   293,   974,   259,  6310,   171,\n",
      "          1247,   102]])\n",
      "DEBUG: Tokenized sentence 227: tensor([[  101,   229, 22280,   122,   316, 22280,   851,  4194,   243,  6086,\n",
      "          6482,   146,   333,   154, 22280,   180, 11126,   351,   418, 13140,\n",
      "           125,   390,  2227,   483,  1058,   117,   582, 13382,   210,   259,\n",
      "          5976,  4173,   474,   170,   675,  2459,   122,   532,  2292,   117,\n",
      "          6335,   230,   739, 20027,   125,  3002,  8805,   207,   102]])\n",
      "DEBUG: Tokenized sentence 228: tensor([[  101,  3636,   273,   522,  2799,   117,   625,   229, 22280,  1146,\n",
      "           291,   229, 22280, 16003,  5212,   180,   329,   304,   117,   179,\n",
      "           122,   240,  1084,   785, 17569,   122,   975,  3391,   215,  5304,\n",
      "           229,  4411,   117, 18253,  2227,   118,   176,   123,  6372,   387,\n",
      "           705,   122,  5855, 22287,   229,  5675,   259, 19204, 12682,   348,\n",
      "           118,   176,   117,   260,  1176,   117,   420, 14252,   122,   736,\n",
      "           117, 19984, 18732, 22281,   117,   173,   179,  8679,   240,  2480,\n",
      "          1615,  1746,  4464,   102]])\n",
      "DEBUG: Tokenized sentence 229: tensor([[  101,  4141,   236,   180,  7296,   256, 10107,   229,  4411,   146,\n",
      "           179,  2036,  5572,   508,   122,  5793,   117,   834,   372,   537,\n",
      "         22287,   221,   123,  7698,   102]])\n",
      "DEBUG: Tokenized sentence 230: tensor([[  101,   123, 22296,   368, 18574, 15363,  3867,   221,  1495,   102]])\n",
      "DEBUG: Tokenized sentence 231: tensor([[  101,   122,  2523,   138, 18127,   303,   143,   229, 22280,  2036,\n",
      "          7278, 11814, 11665, 18757,  1382,   483,  1402,   969,   931,   562,\n",
      "           117,  5022, 13718,   243,  5578,   140,   793,   122, 22243,  2935,\n",
      "           122,  5022,  8574,  1522, 21228, 22281,   125,  3088,   392,  2523,\n",
      "           138,  1176,   117,  4537,   557,   230, 12164, 22278,   291,   389,\n",
      "           873,   201,   117,   229, 22280,  7570,   203,   368,   117,   123,\n",
      "          3922,  4149, 22279,   117,  5022,  6688, 17189, 19199, 22281,   179,\n",
      "           176,  4363,   923,   180,  5675,   862,   833,   151,   118,  2036,\n",
      "          2535,  4314,  2745,  1257,  9336,   146, 16473, 22280, 16853,   366,\n",
      "          9647,  7649,   102]])\n",
      "DEBUG: Tokenized sentence 232: tensor([[  101,   146, 17910,  7817, 16280,   118,   176,  3725,   117,  8043,\n",
      "           260, 12514, 20004, 22281, 13340, 11314,   555,  1684,  6183, 17675,\n",
      "           555,   117, 19877,  3611,   243, 22278,  4067,  5382,  4623,  7970,\n",
      "         22281,   388,  6254, 16788,   143,   117,   260, 19988,   470,   466,\n",
      "         14960,   942,   138,   180,  7698,   117,   320,  5212,  9621,  2822,\n",
      "           451, 22278,   117,   273,  2848,   303,   117,   146, 11979,  1444,\n",
      "           117,   123,  2551, 14368,   251,   412,  5401,   304, 22280,   765,\n",
      "          1058, 22278,   366, 21021,   117,   146, 11334,  3459,  2472,   240,\n",
      "          5976,   102]])\n",
      "DEBUG: Tokenized sentence 233: tensor([[  101,   122,   978,   125,  4314,  2745,  1257,  1112,   221,   179,\n",
      "          4394, 22282,   136,  1021,   125,  2803,   578,   118,  2036,   785,\n",
      "         22354,  8564,   368,   117,  2636,   418,   934,   146,   347,  6032,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 234: tensor([[  101,  1021,  6952,   243,  1256,  2653, 20899,   122, 11736,  1847,\n",
      "          3933,  5664,   102]])\n",
      "DEBUG: Tokenized sentence 235: tensor([[  101,   202,  2699,   171, 15273, 13808, 22280,   146,  8022,   175,\n",
      "           117,   125,  4189,  2623,   247,   117,  1112,  1074,   375, 22354,\n",
      "           122,   662,   529, 12466,   179,  2541,   194, 14773,   214,   423,\n",
      "          3420,   117,  1971,   179,  1485,  2859,   117,  8730,  1941,   170,\n",
      "          1257,   117,   376,  1684,   271,   442,  4797,   117, 13890,  6856,\n",
      "           712,  9730,   143,   602,  1172,  9258, 22281,   449,   170,  4141,\n",
      "           236,   180,  7296,   256,   117,   179,   117,  1369,   138,  1615,\n",
      "           122,  1615,  4199,   143,   337,   300,   931, 22278,   173,  2070,\n",
      "           122, 18574,   125,  3047,   123,  5627,   292,   298,   532,  9000,\n",
      "           117,   123,  5664,  5896,   256, 19360,   125,  4074,   229, 22280,\n",
      "          4750,   125,   547,  3933, 15515,   123,  4067,   125, 16241,  1537,\n",
      "         22287,   746,  2836,   179,   146,  6677,   157,  1557,  6867,   498,\n",
      "           123,  1386,   180,  2606,   102]])\n",
      "DEBUG: Tokenized sentence 236: tensor([[  101, 19200,  1502, 14890,   653,   320,  3560,   234,   117,   122,\n",
      "          3866,  2044,   327,  4036,   102]])\n",
      "DEBUG: Tokenized sentence 237: tensor([[  101,   229, 22280, 14940,   117,  8544,  1941, 13083, 19356,  1825,\n",
      "           117,   260, 14883,   159,   277, 15777,   379, 21307, 22287,   173,\n",
      "          5553,  8362, 22278,   118,   176,   146, 19068,  5292, 22280, 13779,\n",
      "          5876,   577,   966,   179,   176,   360,   508,   692,   221, 18165,\n",
      "          1719,   123,  3402,   176,   173,   694,   304,   256,   173, 15419,\n",
      "         22281,   117,  1151,   289,  9420,   102]])\n",
      "DEBUG: Tokenized sentence 238: tensor([[  101,   622,   593,   351, 13369,   102]])\n",
      "DEBUG: Tokenized sentence 239: tensor([[  101,   318, 13793,   117,  4141,   236,   180,  7296,   256, 10733,\n",
      "           325, 10105,   240,  1839,   123,   327,  4970,  4199, 10733,   222,\n",
      "           739,  6532,   693,   296,   702,   123,  1105,   117,   449,  4750,\n",
      "          3530,   230,  3264,  9317,   117,   582,   662, 22281,   236,   122,\n",
      "          5167,  1249,   123,  6272,   117,   271,  4654,   555,  1005,   151,\n",
      "           123,   327,  9461,  7453,   117,   125,  7444, 22281,   117,   146,\n",
      "           347, 13850, 22178,   117,   146,   347,   338,  1286,   125,  1105,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 240: tensor([[  101,   123, 22296,  3874,  1659,  3530,   151,   102]])\n",
      "DEBUG: Tokenized sentence 241: tensor([[  101,   146,  3147,   117,   173,   179,   368,   117,   726, 14730,\n",
      "           481,   117, 18165, 22278,  8540,   117, 12444, 11448,  7583,  5314,\n",
      "           222,  2317,   283,   372,  2248,  1212,   123, 13159, 12444,  2765,\n",
      "         13605,   251,   117,   259,  4857,  1408, 16026, 22281,   117,   123,\n",
      "          3428,   154,   362, 22282,  2034,   117,   259,  8735,   143, 20237,\n",
      "           117,   146, 17176,   834,  2606,   455,  3762, 13584,  3632,  2817,\n",
      "           159,   125,  2745,   117, 16280,  2807, 22281, 10428,   513,   180,\n",
      "          2772,   102]])\n",
      "DEBUG: Tokenized sentence 242: tensor([[  101,  2010,   271,   146,  2397,  3804,   125, 20027,   102]])\n",
      "DEBUG: Tokenized sentence 243: tensor([[  101, 19068,   796,   256,   368,   202, 14212,   102]])\n",
      "DEBUG: Tokenized sentence 244: tensor([[  101,   123, 22296,  7148,  6086,  3002, 11646, 15545,   130,   122,\n",
      "           180, 22283,   117,  1977,  4178,   136,   102]])\n",
      "DEBUG: Tokenized sentence 245: tensor([[  101,   176,  2779, 16759,  1875,   136,   102]])\n",
      "DEBUG: Tokenized sentence 246: tensor([[  101,   740,  5787,   176,  3456,  2040,  1249,   122,  7762, 22281,\n",
      "           236,   744,   123,  2822,   230,  3264,  5066,   632,   556,   117,\n",
      "         13823,  1337,   122,   171,  1697,   102]])\n",
      "DEBUG: Tokenized sentence 248: tensor([[101, 122, 368, 136, 102]])\n",
      "DEBUG: Tokenized sentence 249: tensor([[  101,   146, 22296,  2364,   368,  9205,   151,   123,   623,  2100,\n",
      "         14372,   229,  1589,   583,   117,   331,   368,   122,   179,  2779,\n",
      "         12444,   370,  6629,   243, 22279,   700,   125, 21160,   222, 16423,\n",
      "         22279,  2010,   229, 22280,  1075,  1016,  1016,   262,  2980,   154,\n",
      "          5700, 13793,   117, 15700,   251,   331,   423,   347,  5791,   183,\n",
      "          7255,   117,   262,  2590,   125,   222,  2115,  6372,   286,   272,\n",
      "          3664,   138,   102]])\n",
      "DEBUG: Tokenized sentence 250: tensor([[ 101,  146, 8271, 4267,  159,  203,  102]])\n",
      "DEBUG: Tokenized sentence 251: tensor([[  101,  1191,   118,   176,   318, 13793,   222, 13239,   792,   193,\n",
      "          4602,  1212,   117,   173,   179,  4141,   236,   117,  1364,   792,\n",
      "          1655,   498,   260,   747,   117,  9821, 18165,   229,  7476,   171,\n",
      "          3922,  4149, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 252: tensor([[ 101,  449,  117,  125,  695,  373,  117,  598,  456,  260, 2551,  138,\n",
      "          122,  146, 6032,  418, 1353,  102]])\n",
      "DEBUG: Tokenized sentence 253: tensor([[  101,   146, 12810,  8861, 22288,   123,  3049,   304,   117,   606,\n",
      "           296,   348,   123,   223, 22280,  7521,   180, 17722,   252,   102]])\n",
      "DEBUG: Tokenized sentence 254: tensor([[  101,  8940,   125,  5533,   230,   374,   251,  4441, 13808,   125,\n",
      "         12514,  2082, 14473, 11891, 22281,   117,   122,   222,   915,  5587,\n",
      "          4525,   178,   125,  2840,   421, 18182,   102]])\n",
      "DEBUG: Tokenized sentence 255: tensor([[  101,   123,  2954, 19455,  2836,   180,  7321,   102]])\n",
      "DEBUG: Tokenized sentence 256: tensor([[  101, 16280, 22287,   118,   176,   744,   260,  1270,  8008,  1402,\n",
      "         20390,   589,   171,   644,   122,  1941, 14619,   210,   222,  1128,\n",
      "         22281,  2649,  6039,   711,   125, 15419, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 257: tensor([[  101,   123, 13943,  8004,   151,   118,   176,   117,  6788,   214,\n",
      "           170,   123,   313,  6758, 22305,   125,   222,  1160, 10176,   179,\n",
      "          1418,  1326,   484, 22278,   259,   532,  3400,   501,   117,   122,\n",
      "           146,  2992, 22288,   744,  1011,  1364,  5925,   833,  3897,  4409,\n",
      "           180,   879, 22282, 11484,   171,   169, 14116,   969,   117,   179,\n",
      "          7882,  1096,   202, 21228,  8574,   452,   117,   271,   222,  1754,\n",
      "         14133,   122, 19876, 12268,   243,   102]])\n",
      "DEBUG: Tokenized sentence 258: tensor([[  101,  4141,   236,   180,  7296,   256,   117,  8853,  1364,   712,\n",
      "           532,   516,   579,   117,  7635,   151,   117,   834,  2533,  6658,\n",
      "           117,   320,   734,   154,  2177, 19589,  1657, 22279,   222,  3960,\n",
      "          9438,  2177,   125, 20840, 22280,   202,  8887,  1410,   171,  1010,\n",
      "           215,   102]])\n",
      "DEBUG: Tokenized sentence 259: tensor([[  101,   146,   969,   273,  2227,   581,   256,   202,  2880, 22280,\n",
      "           117,  5550,  2746,   125, 14378, 13278,   122,  8773,  2935,   117,\n",
      "           170,   123,  2933,  2698,   292,   125,   222,  6592,   428,  7229,\n",
      "          7973,   117,  2745, 11972,   179,   146,  1384,   256,   102]])\n",
      "DEBUG: Tokenized sentence 260: tensor([[  101,  2811,  1341,   117, 20578,   122, 16817,  2365,   438,   966,\n",
      "           125,  2987,   495,  2745,   391,  4020,   122, 15518,   439,  3897,\n",
      "           201,   320,  6793,   179,   117,   171,  2009,   598,   342,   117,\n",
      "          2036,  2134,  3638,   146, 13943, 22282,   146, 11152,  9266,   180,\n",
      "           327,  3377,   159,  3865,   122, 12837,   304,   117, 18804,  2037,\n",
      "           214,   598,   146, 21228,   146,  8574,   452,   122,   623,  3453,\n",
      "         22280, 10116,   366, 18757,  1382,   483,  1402,   122,   298,  6592,\n",
      "           243,  5578,   102]])\n",
      "DEBUG: Tokenized sentence 261: tensor([[  101,  5809,  5099,   117,   202,  4367, 10527, 22279,   162,  7970,\n",
      "         22281,   924, 15698, 21769,   117,   222,   939,  3002,   118,  7302,\n",
      "         12571,  3972,  1212,   762, 11541,   118,   176,   122,  1664,   351,\n",
      "          6939,  3797,   102]])\n",
      "DEBUG: Tokenized sentence 262: tensor([[  101,   495,   230,  1354, 11114,   125, 14883,  1750,   179,   176,\n",
      "          4621,   256,   102]])\n",
      "DEBUG: Tokenized sentence 263: tensor([[  101,  8940, 13369,   117,   170,   146,  6793,  2143,  9370, 22280,\n",
      "           125,   230,  1151,  4554,   102]])\n",
      "DEBUG: Tokenized sentence 264: tensor([[  101,   229, 12475, 13793,  1480,  7485, 10347,   122, 10978,   322,\n",
      "           180,  7321,   923,   118,   176,  1695,   123,  1695,  7194,   557,\n",
      "         12514,   125, 14378,  2086,   122, 11534, 14533,   118,   176,   939,\n",
      "           125,  2217,   117,  2459, 22279,   854,  2028, 22281,   117,   125,\n",
      "          1485,   260,  4516,   122,   125,  1485,   260, 18516,   117,  2840,\n",
      "          4212,  4929,   907,   128,  3155,   102]])\n",
      "DEBUG: Tokenized sentence 265: tensor([[  101,  7226,  8954,   692,   320,   210,   541, 22280, 10537,  8468,\n",
      "           180,  5294,  8849,   736, 17318, 22287, 14643,   418, 21439,   796,\n",
      "           256,   146,  1417,   117,  7583, 18968, 22278,   260,  7800, 22281,\n",
      "           179,  2662, 14705, 22278,   123,  1956,  1286, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 266: tensor([[  101,  1413, 22287,   118,   176,   390,   942,   117,   125, 14790,\n",
      "         22278,   122,  8184,   324,   117, 13841,  1491,   117,   146,   388,\n",
      "          6738,  3173,   185,   117,   146, 13850, 22178, 13793,  2242,   180,\n",
      "          9463,   117,   146, 11552,  6733, 22280,   122, 13140,   125,   781,\n",
      "          9412, 22278,   117,   320,  1341,   125, 10173,  6943, 22281,  5248,\n",
      "           117, 12909,   591,   171,   969,   117,   170,   260,  1277,   616,\n",
      "           785, 20269,   122, 21990,   138,  3235, 22282, 17026,   498,   123,\n",
      "          2134, 10491,   340,   366,  1632, 18354, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 267: tensor([[  101,  1636, 14533,   118,   176,   123,  7800,   125,   146, 11437,\n",
      "          5548,   594, 22287,   781,  4387,  1509,  5359,  3706,   117,   179,\n",
      "         15724, 22287,   117,   123,   222,   596,   117,   125,   313,   512,\n",
      "           537,   122,   125, 13970, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 268: tensor([[  101,  1450,  4687, 16555, 22287,  2292,   320, 18744,   291,   229,\n",
      "          6742,  2103,   171,  8271,   102]])\n",
      "DEBUG: Tokenized sentence 269: tensor([[  101,   122,   117, 16080,   122, 13661,   246,   117,   123,  1354,\n",
      "         11114,   298, 14883,  1750,   176,  4621,   256,   102]])\n",
      "DEBUG: Tokenized sentence 270: tensor([[  101,  4141,   236,  3235,  2071,   118,   176,   202,  8788, 22280,\n",
      "           117,   221, 22278,   792,  3852,   102]])\n",
      "DEBUG: Tokenized sentence 271: tensor([[  101,   170, 11304,  8940, 13540, 11538,   285,   125,  3933,  7698,\n",
      "           117,  2113,   146,  3795,   117,   222,  4575,  3262, 18271,   243,\n",
      "           117,   125,  1491,  2308,  3141, 11563,   117,  5708,   549,   125,\n",
      "           572,   283,   117,   329,  5519,   122, 10978,  1408,   117,   449,\n",
      "         10984,  8156, 10898, 22281,   122,  9585,   117,  8004,   151,   117,\n",
      "           125,   576,   173, 18187,   117,   146,  4332,   303,   122,  3444,\n",
      "           304,   256,   146,  6792,   175,  2010,  1941, 18752,   437, 13779,\n",
      "         20346,   644,   492, 13910,   243,  5023,  1547, 22281,   229,  9463,\n",
      "           125,   222,   475,  2227,  5636, 22279,   123,  4410, 12165,   304,\n",
      "           122,  6511,   171,  7889,   151, 22280,  4363,   151,   118,   176,\n",
      "           229,  7321,   102]])\n",
      "DEBUG: Tokenized sentence 272: tensor([[  101,  1423, 20933,  1378,   529, 11351,  2461,   117,  1348,  4602,\n",
      "           243,   118,  2036,   123, 11214,   392,   117,   230,  2606, 11791,\n",
      "           117,   146,  1294,  1444,   122, 12837,   303,   117, 18825, 20666,\n",
      "         21990, 22278,   122, 18757,  6915,   117,  2863,   256,   117,   170,\n",
      "           146, 11552,   785,  3848, 22279,   125,   230,   370, 13397,   124,\n",
      "           222,   328,   122, 17479,   117, 11026,   118,  2036, 22278,  1571,\n",
      "           124,   102]])\n",
      "DEBUG: Tokenized sentence 273: tensor([[  101,   122,   123,  1354, 11114,   117, 16188,   251,   954,   169,\n",
      "         14116, 22281, 10414,   180, 20390,   292,  6792,   175,   117,   262,\n",
      "          3891,   102]])\n",
      "DEBUG: Tokenized sentence 274: tensor([[  101,   122,   123,  1695,  1405,   203,   303,   146,  2082, 14473,\n",
      "           900,   366, 12514,   262,   176,  6757,   202,  1480,  7485,   181,\n",
      "           268,   362, 22282,  4643,   342,   366, 21021,   117,   271,   202,\n",
      "         21228, 21333,   140,   679,   123,   169,  8388,   398,  2916,   125,\n",
      "          3377,  9850,   102]])\n",
      "DEBUG: Tokenized sentence 275: tensor([[  101,   173,  5255,   117,  2745, 19726,   456,   202, 22243, 22280,\n",
      "         19262,   117,   122,   123, 13943,   117,   171,  2979,   117,  3568,\n",
      "           272,  2836,   170,   123,   327,  3377, 20004, 22279, 12448,   123,\n",
      "         12475, 13793,   366, 20390,  1908,   102]])\n",
      "DEBUG: Tokenized sentence 276: tensor([[ 101, 4141,  236, 1767, 1031, 1281,  178,  117, 4047,  697,  117, 7955,\n",
      "         1362,  273, 1289,  183, 2401, 4211,  415,  102]])\n",
      "DEBUG: Tokenized sentence 277: tensor([[  101,   146,   734,   154,  2177,  4566,  4575,   492,   210,   247,\n",
      "           117, 12631,  1196,   123,   230,  2606, 19543,   122,   834,   623,\n",
      "          2100, 11293,   117,  1623,   679,   118,   146,   240,  1839,   170,\n",
      "           146,  4336, 22279,   325,  6205,   243,   285,  2401, 22279,   524,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 278: tensor([[  101,  1112,  6086,   117,   222,  5926,  3095,   243,   117,   222,\n",
      "          3061,   371,   415,   117,   834,  2716,   117,   834,  3495,   117,\n",
      "           834,   390,  2420,   320,  1528,   117,   978,  6033,  3660,  1069,\n",
      "           230,  1154,  2890,   179,   146,  1334, 11324,   256,   122, 16246,\n",
      "           271, 17479,   320,  6793,   179,   368,   117,  1369,   117,   655,\n",
      "           247,   171,  2244,   117,  1950,  5066,  1051,   243,   117,  9192,\n",
      "          6969,  1859,   117,  5334,  5630,   117,  2113,  2036, 15700,   490,\n",
      "          2745,   117,  2745,  2010,   123,  1105,   117,   123,  2606,   122,\n",
      "           123, 15685, 22354,   122,   700,   412, 14355,   304, 22280,  2711,\n",
      "           366,  5365,   117,  5078,   252,   118,   176,   123, 13519,  4678,\n",
      "          1042,  1877,   286,   125,   434,   763,   102]])\n",
      "DEBUG: Tokenized sentence 279: tensor([[  101,   123, 21736,   171,   146,   635,   179,  2036,   962,  5723,\n",
      "           117, 16995,   118,   146, 22003,   117,   170,   146,   347, 10881,\n",
      "          1364,  2492,  5872,   117,   146, 13449, 19088,   370,   300,   122,\n",
      "         13779,   430,  1212,   117,  5708,   122, 18908,   501,   125,   230,\n",
      "          9434, 22280, 20957,   122,   320,   653,   596,  5936,   102]])\n",
      "DEBUG: Tokenized sentence 280: tensor([[  101,   860,  9266, 12444,   240,   344,   304, 15397,   159,   260,\n",
      "          2459,   117,  3452,   118, 10497,   954,  9250,   501,   117,   423,\n",
      "         19456, 12854,   218,   415,   102]])\n",
      "DEBUG: Tokenized sentence 281: tensor([[ 101, 4256, 3146,  256,  117, 5334, 5630, 1078,  576,  325,  102]])\n",
      "DEBUG: Tokenized sentence 282: tensor([[  101,  1112,   271,  1061,   229, 22280,   176,  8650,   923,   102]])\n",
      "DEBUG: Tokenized sentence 283: tensor([[  101,  2249, 15537,   229, 22280,  7112, 16102,   487,   102]])\n",
      "DEBUG: Tokenized sentence 284: tensor([[  101,  1112, 16589, 15500,   692,   403,  3286,   256,   118,   176,\n",
      "           320,  7148,   122,   117, 13140,   125, 16001,   117,   125,  2401,\n",
      "         22279,   524,   117,  3009,   151,   118,   176,  5444,   102]])\n",
      "DEBUG: Tokenized sentence 285: tensor([[  101,  1270,  7826,   403,   117,  3429,   118,  2036,   418,  3138,\n",
      "          1112,   122,   176,  2779,   146, 21021,   236,   136,   102]])\n",
      "DEBUG: Tokenized sentence 286: tensor([[  101, 22354, 16278,   456,   118,   123,  2044,   117,   834, 14903,\n",
      "          2798,   320,  1528, 13083,   154,   118,  1084,   449,   123,  3138,\n",
      "           229, 22280,  8544,   122, 18825,  5630,   118,   176,   118,  2036,\n",
      "           320,   943,  2152,   157,   117,   170,   230,  1670,  3878,   304,\n",
      "         22280,   125, 11718,   343,   102]])\n",
      "DEBUG: Tokenized sentence 287: tensor([[  101,   318, 13793,   117,  8198,   118,  2036,   123, 10201,  2028,\n",
      "           117,   425,   230, 10200,  2949, 22281, 12773,   351,   599,  2647,\n",
      "           122, 10428,  1337,  2010,   146,   347,  2982,   117, 19215,  4413,\n",
      "         10557,   382,  8540,   143,   171, 14308,   243,   117,   146, 20401,\n",
      "           125,   163,  2550,   151,   102]])\n",
      "DEBUG: Tokenized sentence 288: tensor([[  101,  2745,  1257,  2364,  2036,  4048, 22288,   316, 22280,  4062,\n",
      "           117,   316, 22280, 15975,  8948,  4812,  3255,  4900,  2182,   102]])\n",
      "DEBUG: Tokenized sentence 289: tensor([[  101,  2535,   117,  3582, 22278,   229,  2606,  9429, 22281,   122,\n",
      "         18636, 13844,   117,   221,   260,  1647,  2364,   352,   796,   124,\n",
      "          4654,   555,   102]])\n",
      "DEBUG: Tokenized sentence 290: tensor([[  101,  1112,  1467,  2779,   146, 19604,   125,  2745,   136,   102]])\n",
      "DEBUG: Tokenized sentence 291: tensor([[  101,   229, 22280,  2471,  7104,   243,   170,   259, 17080, 19254,\n",
      "           125,  4062, 15211, 22280,   136,   102]])\n",
      "DEBUG: Tokenized sentence 292: tensor([[  101,  4717, 10671, 17689,   259, 17080,   505,  2552,   136,   102]])\n",
      "DEBUG: Tokenized sentence 293: tensor([[  101, 22354,  6185,  8041,   368,   123,  2004, 22278,  5594,  3292,\n",
      "           418,  3989,  1399,  4149,   181,   243,   118,  2036,   623,  3593,\n",
      "           179,  1201,   923,  3032,  3391, 22280,   143,   102]])\n",
      "DEBUG: Tokenized sentence 294: tensor([[  101,   368, 15637,   118,   176,   117,  6834,   256,   259,  8446,\n",
      "           117, 12374,   256,  6432,   173,  2748,   117, 10201,   256,   123,\n",
      "           327,  9713,   304, 22280,   122,   123,   327,  8286,   412,   975,\n",
      "          5242, 22278,   449,   123,  3002,  8269,   184,  8149,   833,  3897,\n",
      "           364,   229, 22280,   176, 13851,   285,   256,   432, 13793,  5205,\n",
      "           256, 14245,   143,   102]])\n",
      "DEBUG: Tokenized sentence 295: tensor([[ 101,  122, 4141,  236, 6540,  123, 5334,  900,  271,  222, 7955,  102]])\n",
      "DEBUG: Tokenized sentence 296: tensor([[ 101, 7710, 2071,  118,  176, 3102, 1177, 8204, 7912,  125,  898,  653,\n",
      "          117,  122, 9804, 5458,  260, 3664,  138,  202, 8271,  102]])\n",
      "DEBUG: Tokenized sentence 297: tensor([[  101, 17522, 18206,   117,   123,  2551, 22278,   969,   154,   271,\n",
      "           176,  7912, 22278,  4537,   286,   412,  2004, 22278, 15419,   102]])\n",
      "DEBUG: Tokenized sentence 298: tensor([[  101,  1112,   122,   176,  2779,   146, 21021,   236,   136,   102]])\n",
      "DEBUG: Tokenized sentence 299: tensor([[  101, 22354,   495,   123,  3002,  8269,  3138,   179,  8940,   125,\n",
      "          1160,   123,  3258,  1095, 22279,   298,   532, 16706,   102]])\n",
      "DEBUG: Tokenized sentence 300: tensor([[  101,  1112,   229, 22280,   229, 22280, 22354,   122,   368,   123,\n",
      "         16278,   151,   125,  1160, 13186,  2537,   118,   123,   221,   146,\n",
      "          4707,   180,   327, 12223,  3391, 13793,   117,   271,   146, 12657,\n",
      "         22281, 16257, 22280,   179, 16278, 22279,   202,   528,   146,  1078,\n",
      "           391,   180,   327,  1746,   699,   740, 15478, 15736,   170,   146,\n",
      "         13419,   117,   449,  2044, 18049,   351,   117,  1151, 17598,   102]])\n",
      "DEBUG: Tokenized sentence 301: tensor([[  101,  1112,   122,   176,  2779,   146, 21021,   236,   136,   102]])\n",
      "DEBUG: Tokenized sentence 302: tensor([[  101, 22354,  2010,   229, 22280,   229, 22280,  2638, 14376,   117,\n",
      "          5510, 19927,   214,   222,  9247,   183,   202, 22243, 22280,   180,\n",
      "          7321,   102]])\n",
      "DEBUG: Tokenized sentence 303: tensor([[  101,  1941,  1587,   154,   123,  1858, 22279,   502,  1051,   692,\n",
      "           118,   176,   118,  2036,   259,  2873, 20477,   128,   102]])\n",
      "DEBUG: Tokenized sentence 304: tensor([[  101,  3876,  2182,   230, 18637, 12862, 22278,   123, 13943,   102]])\n",
      "DEBUG: Tokenized sentence 305: tensor([[  101, 13510, 22281,  2366,   923,   202,  3420,  4141,   236,   327,\n",
      "           256,  2337,   684,   151,   498,   123, 13970, 22278,   146,   325,\n",
      "          7211,   311,  2650, 22282,   125,  3922,   683,  2317,   232,   256,\n",
      "           118,  2036,   259, 13841,   102]])\n",
      "DEBUG: Tokenized sentence 306: tensor([[ 101,  202, 1325, 2010, 4063,  151,  102]])\n",
      "DEBUG: Tokenized sentence 307: tensor([[  101,  1695,  2036,  3207,   256,  1941,   221,  3767,   123,  7698,\n",
      "           117,   785,  1695,   117,   230,  3061,   371,   415, 18175,   117,\n",
      "           122,   117,  6033,   117,   325,  2036,  2803,  5723,  1966,  1695,\n",
      "           171,   179,  1364,   146,  4745,   180,  4036,   102]])\n",
      "DEBUG: Tokenized sentence 308: tensor([[  101, 15313,   259,  5708,   122,  2789,   179,   146,  8271, 16165,\n",
      "         22281,   236,   123,   374, 22278,   117,  3922,  4149,   348,  1315,\n",
      "           474,  3514,   229,  2480,   222,   328,   125,   438,  5505,   102]])\n",
      "DEBUG: Tokenized sentence 309: tensor([[  101,   368,   586, 22279,  8041,   117,  1169, 22281, 13550,   240,\n",
      "         20277, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 310: tensor([[  101,  1413,   123,   327,  1746,   699,   117,   170,   123,  9463,\n",
      "           785,  6628,   117,   259,  5708,  5572,  4617,   128,   117,   123,\n",
      "          5961,   118,  2036,  4486, 17253, 22281,  1532,  4410,  3174,   979,\n",
      "           404,   117,   123,  3182, 22278,   125,  1796,   117,  5223,   122,\n",
      "         10105,   117,   420, 13717, 14011,   268,   143,   125,  5052,   102]])\n",
      "DEBUG: Tokenized sentence 311: tensor([[  101,   122,  1413, 14619,   210, 13431,  6086, 15545,   130,   851,\n",
      "         21944,   117,  5818,   118,  2036,   202, 20462,   117,  4849,   118,\n",
      "          2036,   117, 13449, 11252,   117,   222, 21672,   284,   130,   117,\n",
      "          3606,   230,  1109,   232, 22280,   122,  3852,  6345,   123,  3444,\n",
      "           304, 18997,  1112, 15212,   118,   437,   229,   223, 22280,   117,\n",
      "         12961,   176,  8204,   499, 22281,  5078,   307,   118,   311,   117,\n",
      "           420,   339,   118,   437,   123, 13012,   304,  1112,   122,  4141,\n",
      "           236,  9247,   654,   117,   271,   171,   286,   117,  7078,  2746,\n",
      "          2010,   122,  2779,  3289,   185, 22283,   117,   644,   492,  2779,\n",
      "          3289,  5644,  3411,   117,   146,  8271,   417,  9030, 22288,   102]])\n",
      "DEBUG: Tokenized sentence 312: tensor([[  101,   222,  5184,   183,  8018,   762,  3529,   118,   176,   240,\n",
      "           125,   839,   171, 14793,   125,   222,  1486, 11837,   458,   117,\n",
      "           122,   230,   541, 22278,   117,  2590,   412,   125, 11769,   304,\n",
      "         22280,   125,   222,  7520,   117,  8525,   203,   146, 11979,   125,\n",
      "          4141,   236,   180,  7296,   256,   102]])\n",
      "DEBUG: Tokenized sentence 313: tensor([[  101,   259,  7769,   125,   629, 22280,  1010, 12061,  8264,  1084,\n",
      "           146,  6032,   260,   969,   470,   117,   122,  1364,  1390, 16719,\n",
      "           201,   125,  5052,   117,  2365,   203,   857,   222,  7520,   221,\n",
      "           260,  5099,   180,  5675,   117, 13239,   228,   944,  4922, 15512,\n",
      "         13793,   123,  2863,   180,  1746,   699,   102]])\n",
      "DEBUG: Tokenized sentence 314: tensor([[  101,   262,  3400,  1557,  1977,   123,  6920,   117,   122,   117,\n",
      "          1362,  2607,  2032, 22280,   117,  8163,  3529,   118,   176,   598,\n",
      "           146,  1078,   391,   117,   123, 10786,  4765,   118,  2036,   260,\n",
      "           148,   128,   122,   260, 21347,   102]])\n",
      "DEBUG: Tokenized sentence 315: tensor([[  101,  2010,  7343,  7258,  7343,  3189,   286, 17080,  3165,   143,\n",
      "          2638,  4839,   256,   740,   117,   123,  7078,   934,  5572,  4617,\n",
      "          3797,   102]])\n",
      "DEBUG: Tokenized sentence 316: tensor([[  101,   449,   117,  7254,   125,   230,  3138,   695,   343,   117,\n",
      "         15568, 22288,   118,   176,   117,   122,  9247,   654,   117, 12110,\n",
      "           214,  5926,   246,   221,   146,  1341,   180,  4411,   102]])\n",
      "DEBUG: Tokenized sentence 317: tensor([[  101,  2010,   262,   368,   229, 22280,   262,  1342,   262,  6086,\n",
      "          3002,  2525,   262,  6086,  7148,   171,   644,   492, 22279,   429,\n",
      "           118,   176,   123,  1984, 22282,   122,   123,  4654,   934,   117,\n",
      "         18260,  1877,   486,   122, 10714,   102]])\n",
      "DEBUG: Tokenized sentence 318: tensor([[ 101,  495,  123, 7406, 7870,  179, 1359,  256,  102]])\n",
      "DEBUG: Tokenized sentence 319: tensor([[  101,   146,  5846,   262, 16266,   243,   712,   390,  2227,   483,\n",
      "          1058,   122,   146,  1831,   125,  4141,   236,   180,  7296,   256,\n",
      "         11272,  1982,   123,  7223,   406,   180,  6318,  1135,   117,   320,\n",
      "          1341,   180,  5246,   117,   179,   905,   151,   256,   123, 21602,\n",
      "          1885,   159,   170,   123,  1439,   903,   298,  3845,  8170,   102]])\n",
      "DEBUG: Tokenized sentence 320: tensor([[  101,   123,  7698,   712,  3885,   176, 14547,   173, 15978,   124,\n",
      "           117,   122, 14968,   122, 22005,   713,   143,   125,  1364,   146,\n",
      "         16453, 22280, 18661, 22285,  1172,   490,   221,  9718,   118,  2036,\n",
      "           146, 15051,   102]])\n",
      "DEBUG: Tokenized sentence 321: tensor([[  101,   146,  3459, 17551,   171,  1247,   117,  2760, 10671, 22281,\n",
      "           269,   343,   122,  7351, 19927,  1337,   117,  2798,   331,   865,\n",
      "         13642,   148,   256,   146,   179, 10355, 22287,   117,   271, 12910,\n",
      "         15736,   123,   179,   229, 22280,  5327,  1084,   102]])\n",
      "DEBUG: Tokenized sentence 322: tensor([[  101,  8586, 22361, 11972,  1085,  3145,  3330,  2876,   319,   401,\n",
      "         22354,   481,   700,   117,  7719, 22287,   179,   529,  1315,   986,\n",
      "           125,   629, 22280,  1010,  9584,   230, 13305, 18190,   289,   364,\n",
      "           117,   179,   117,   240,  2729, 21856,   117,  8625, 22278,   954,\n",
      "          5097,   123,  1031,   931,   146,  2242,   180,   223, 22279,   118,\n",
      "           180,   118, 13943,   102]])\n",
      "DEBUG: Tokenized sentence 323: tensor([[  101, 16241,  1537, 22287,   176,  2026,   256,   123,  3852,  3047,\n",
      "         17811,   117,   122,   146,  8412,   458, 21868, 10193,   117,   179,\n",
      "           176,  2243, 22281,   236,   173,  2571,  1266,  1495,   117,  1413,\n",
      "         11298, 22282,   146,  5830,  2550,   247,   117,   123,  8032,   122,\n",
      "           123, 10849, 22282,   117,   222,  5184,   183,  2979,   122,  5923,\n",
      "           157,   125,  2606,   117, 14021,   125,  1403,   124,  3103,   102]])\n",
      "DEBUG: Tokenized sentence 324: tensor([[  101,   123,  1386, 14313,   285,   125,  4141,   236,  7493,   739,\n",
      "         17409, 22280,   202,   925,   148, 22280,   122,   744,   325,   173,\n",
      "         22232,   795,   102]])\n",
      "DEBUG: Tokenized sentence 325: tensor([[  101,   646,   314,  5895,   371,   785,   854,  2028,   117,   229,\n",
      "         22280,   123, 12123, 22288,   240,  1966,   596,  2471,   368,  1685,\n",
      "           481,   117,   176,  1971,   102]])\n",
      "DEBUG: Tokenized sentence 326: tensor([[  101, 12232,   288,   118,   202, 18043,   524, 13305,   122, 15024,\n",
      "           118,  2036,   179,  1011,   125,   599,   183,   423,  1568,   102]])\n",
      "DEBUG: Tokenized sentence 327: tensor([[  101,  5009,   178, 20242,   171,  7029, 17551,  1678,   146,   179,\n",
      "          2036,  1353,   483,   122,   325,   123,  2606,   229,  2316, 20798,\n",
      "         12174,  3529,   202,   746, 22287,   118,  2724,  6465,   180, 11126,\n",
      "           351,   146,   179,  7898,   320, 14633, 13793,   122,   117,  2440,\n",
      "           366, 12816,   179, 19284,   221,  8868,   291,  3456, 11647,   123,\n",
      "          7698,   125,   629, 22280,  1010,   117, 16241,  1537, 22287,  8402,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 328: tensor([[  101,  3413,  2160,   117,  2694,  2044,   221, 21990,  2204,   117,\n",
      "         11765, 11199,  3831,   123,  1105,   766,  1391,   183,   117,  4203,\n",
      "           111, 13747, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 329: tensor([[  101,   117,  2779,   148,   576,  1004, 18651,   202,   179, 17090,\n",
      "           117, 11935,  4432,   146,  9931,   221,   222,  1571,  1699,  7970,\n",
      "           651,   102]])\n",
      "DEBUG: Tokenized sentence 330: tensor([[  101,   785, 21875,   123,  7390,   442, 22278, 22232,   795, 14661,\n",
      "           118,   176,   125,   646,   314,  5895,   102]])\n",
      "DEBUG: Tokenized sentence 331: tensor([[  101,   171,   151,  6086, 16450,   304, 22280, 18646,  6476, 22312,\n",
      "           321,  7166,   118,   176,   117,  1016,   117,   316, 22280,   834,\n",
      "           223, 22279,   117,   230,  6754,   854,  2028,   125,  1685,   481,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 332: tensor([[  101,   146,  3265,   117, 17226,   117,   700,  7642,   130,  1266,\n",
      "           243,   170,  1364,   146,   273,   415, 22280,   117,   262,  3285,\n",
      "           286,   117,   123,  5334,   900,   117,  1839,   125,   222,  4449,\n",
      "           117,   122,  8383,   102]])\n",
      "DEBUG: Tokenized sentence 333: tensor([[  101,  8544, 16302,   243,   320,  5278,   122, 19068,   796,   256,\n",
      "           118,   176,   785,   173,  4036,   102]])\n",
      "DEBUG: Tokenized sentence 334: tensor([[  101,   625,  2080,   123, 21990,  2204, 16708, 17251,   125,  2745,\n",
      "           179,   146,  1384,   256,   102]])\n",
      "DEBUG: Tokenized sentence 335: tensor([[  101,  5147,   117,   262,  1684,  1004,  5687,   347,  8620,  9730,\n",
      "           203,   118,   637,   703, 22280,   123,   222, 18369,   117, 20242,\n",
      "           118,   146,   271,  1417,   700,   117,  3285, 13665,   118,   146,\n",
      "          1362,  1571,  1699,   298,  2980,   102]])\n",
      "DEBUG: Tokenized sentence 336: tensor([[  101,   646,   314,  5895, 19876,  2904,   146,  9470,   180,  1105,\n",
      "           117,  1678,   222,  5492, 22280,   117,   122, 13891,   260,  6880,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 337: tensor([[  101,   123,   905,   247,   117,  2044,   179,   146,  5308,   692,\n",
      "         10580,   117,  5078,   252,   118,   176,   123,  5334,   900,   102]])\n",
      "DEBUG: Tokenized sentence 338: tensor([[  101,   978,   785,  7672,   171, 13389,   123,  2954,   117,  2252,\n",
      "           151,   118,   176,   598, 22278,  8130,   117, 12631,  1196,   712,\n",
      "         17343,  1044,   102]])\n",
      "DEBUG: Tokenized sentence 339: tensor([[  101,   229, 22280, 14971,   298,   736, 14582,   117,  2113,  2036,\n",
      "         21612,  1112, 12361, 19161, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 340: tensor([[  101,   495,   437,  3556, 22280,   117, 13140,   125,   853,  6522,\n",
      "           128,   117,  5381,   265,   151,   118,   176,   785,   180,   223,\n",
      "          6974,   304, 22280,   179,   259,  4966,  4250,  2650,   288,   221,\n",
      "           146,  1010,   215,   102]])\n",
      "DEBUG: Tokenized sentence 341: tensor([[  101,   202,  1571,  1699,   495,   146,   877,   319,  8609,   179,\n",
      "           176, 13028,   646,   314,  5895,   122,   259,  8229, 17850, 12812,\n",
      "         16754, 22287,   118,  2036,  3185,  3309,   117,  1112,   646,   314,\n",
      "          5895, 11522, 11551, 15632, 22280, 22354, 10355, 22287,   118,  2036,\n",
      "           117, 14537,   348,   118,  2036,   123, 12486,  5424,   122, 18260,\n",
      "           118,  2036,   229,  3049,   304,   374, 22281,  1936,   285, 22278,\n",
      "          3235,  3711,   252,  2684,   179,   368,   176, 20813,   256, 13586,\n",
      "          2472,   117,   834, 14903,  2962,   320,   570, 21269,   117,   123,\n",
      "          5334,   900,   122,   123, 10420,   900,   179, 17331,  1403,  3849,\n",
      "           221,   123,   327,  2480,   102]])\n",
      "DEBUG: Tokenized sentence 342: tensor([[  101,   449,   117,   170,   146,   596,   117, 11502,   118,  2036,\n",
      "          3667,   122,   123,  1069,   318, 13793,   176,  2036,   870,  1185,\n",
      "           748,  4020, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 343: tensor([[  101,  1941,  9144,   260,   675, 14178,   259,  9288,   229, 22280,\n",
      "           176,   822,   375,   692,   125, 10381,   118,  2036,  1876,   303,\n",
      "           143,   498, 22280,  1010,   215,   102]])\n",
      "DEBUG: Tokenized sentence 344: tensor([[  101,  1112,   271,  1085,   259, 17369,   136,   102]])\n",
      "DEBUG: Tokenized sentence 345: tensor([[  101,   122,   176,   123,  9349,  7339,   117,  1676,  5207,   117,\n",
      "          2459, 12989,   649,   122,   333, 18376, 11788,  2364,  1796, 17935,\n",
      "           243,   240,  3933, 16240,  2034,   298,  6365, 10980, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 346: tensor([[  101, 22354,   222,   644,  1678,   230,  3743,   125, 22232,   795,\n",
      "           122,   117,   412,   681,   576,   117,  2002,   118,   176,   320,\n",
      "         12245,   125,  9767,   173,   898,   102]])\n",
      "DEBUG: Tokenized sentence 347: tensor([[  101,   449,   260,   675, 10200,  2949, 22281, 12773,   784,   229,\n",
      "         22280, 16420,  1202, 22287,   180,  1105,   171,  7392,   202,  1325,\n",
      "           117,  4750, 11639,   118,  2036,   179,   123, 13782, 18304,  2173,\n",
      "           223, 22279,   229, 22280,   495,  7583, 17704,   117,  7583,  8940,\n",
      "           123,   333,   327, 14057,   117,  2113,   495,   123,  2606,   125,\n",
      "           347,  7392, 14351,   122,  2684,   117,   176,  2036,   229, 22280,\n",
      "         10216,   256,   123, 14876,   151,   117,   240,   325,   125,   230,\n",
      "           576,  9226, 22278,  3914,  2004, 22278,  5961,   229,  1858,   117,\n",
      "           529,  3611,  5907,   223, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 348: tensor([[  101,  1112,   449,  1977,  1467,   123,  1858,   136,   271,   176,\n",
      "         13028,   136,   102]])\n",
      "DEBUG: Tokenized sentence 349: tensor([[  101,  2364,   219,   268, 15024,   102]])\n",
      "DEBUG: Tokenized sentence 350: tensor([[  101, 22354,  2249,   123,   347,  1568,   117, 12444,   333,  6086,\n",
      "          2397, 17897,   243,   179,   117,   230,  2954,   117,  2036,  4169,\n",
      "           117,   785,  1877,   286, 22279,   870,  3093,   183,   117,   122,\n",
      "           240,  1977,  1695,   700,   146, 16030,   228,   125,   599,   183,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 351: tensor([[  101,   180,  4187,  2990,  2954, 10201,   256,   118,   176, 15363,\n",
      "           524,  1011, 12401,   286,   117,   506,  3344,   118,  1340,   123,\n",
      "          2551,   122, 18860,   118,   202,   117,  1048,  4549,  2895,   117,\n",
      "           221,   260, 11351,   171,  1815, 10738,   117,   240,  4227,   179,\n",
      "           260,   675, 17897, 22281,  2365,   229,  2880,   151, 22280,  5288,\n",
      "         15682, 10363, 19356,   328,   117,   179,   646,   314,  5895,  2535,\n",
      "         10315,  2836, 11448,  5785,  1676,  1084, 20739,   700,   262,   176,\n",
      "         20933,   578,   122,   229, 22280, 17662,   325,   149,  2291,   102]])\n",
      "DEBUG: Tokenized sentence 352: tensor([[  101, 18127,   256,   118,   176, 14619,   210,   117,   449,   324,\n",
      "         22280,   170,  3541,   252,   599,   218, 15182,   117,   171,   596,\n",
      "           173,   179,  6086,   653,  2397,  3851, 12541,   117, 10201,   256,\n",
      "           118,   176,   125,  3908,  5484,   286,  2461,  1415, 10786,  3103,\n",
      "           122, 12631,   942,   117,   122,   331,  2535,  4428,   256,   179,\n",
      "           944,  3636,   870,  8150,  1085,  1684, 11224,   382,   122,   445,\n",
      "          4757,   442,   117,  5159,   271,   179, 11443,   246,   117,   260,\n",
      "          3235, 13097,   117,   122,  1821,  1684, 20055,   125,  5334,   157,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 353: tensor([[  101,   700,  5809,   122,  1028,   901,   468,  3391, 22280,   143,\n",
      "           423,  3714,   117,   646,   314,  5895,   117,   176,  1004,   179,\n",
      "           785,  1160,   744,   117,  5078,   252,   118,   176,   123,  9767,\n",
      "           122,   259,   873,   249, 17065, 22281,   180,   327,   851,  1601,\n",
      "           151,  1990,  3181,   692,   118,  2036,  1941,   146, 16450,   304,\n",
      "         22280,   170,   230, 12448,   852,   256,   421,   122, 21597,   392,\n",
      "           117,  1532,   337,  3066, 14271,  1076, 13086,   125,   273,  1289,\n",
      "           183,   102]])\n",
      "DEBUG: Tokenized sentence 354: tensor([[  101,  1364,   146,   347,  6532,   495, 13239,   712,  4332,   942,\n",
      "         18513,  1721,   122, 10381,   118,  2036,   179,  2036,  1996, 22281,\n",
      "           236,   117,   240,  3165,   125,  4023,   117,  1977,   870,   509,\n",
      "          8940,   123,   333,   347,  1568,   122,   117,  1953,   117,   327,\n",
      "           223, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 355: tensor([[ 101, 4058,  118,  176,  481,  117,  122,  368, 4177,  432,  191,  201,\n",
      "          529, 7477,  623, 3593,  102]])\n",
      "DEBUG: Tokenized sentence 356: tensor([[  101,  8028,   259,   532,  3058,   428,   501,   117, 12008,  3529,\n",
      "           118,   176,   123,  4270,   221,   123, 14164,   102]])\n",
      "DEBUG: Tokenized sentence 357: tensor([[  101,   122,  1684,   260,  7477, 15756,  7919, 22281,   123,  3953,\n",
      "           180,   327,  5160,  2346,   351,   102]])\n",
      "DEBUG: Tokenized sentence 358: tensor([[  101, 17588,   203,   118,   176,   173,   144,  6562,   102]])\n",
      "DEBUG: Tokenized sentence 359: tensor([[  101,  1065,   318, 13793,   123,   327,  1069,  3171, 12357,   246,\n",
      "          1364,   368,   176,  8915,   701,   532, 12400,   125,   792,   122,\n",
      "         21122,   102]])\n",
      "DEBUG: Tokenized sentence 360: tensor([[  101,   905,  4373,   123,   333, 20073,   102]])\n",
      "DEBUG: Tokenized sentence 361: tensor([[  101,   449,   222,  5532,  1749,   415,  3429,   125,  1160,   318,\n",
      "          2413,   383, 22279,   118,  1340,  2010,   123,  1386,   180,   327,\n",
      "           223, 22279,  2251,   567,   102]])\n",
      "DEBUG: Tokenized sentence 362: tensor([[  101,  5334,   748,   118, 10478, 12674,   122,  8650, 18004,   175,\n",
      "           229, 22280,   331,   240,   740,   117,   449, 14619,   210,   785,\n",
      "           240,   898,  2004, 22280,  6757, 22232,   795,   117,  4363,   151,\n",
      "          2745,   455,   146, 21878,   320,  3714,   122,   123,  9019,   151,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 363: tensor([[  101,  2364,   176,  8564,   316, 22280,   438,  1165, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 364: tensor([[  101, 17226,   117,   170,   146, 13239,   298,  3492,   117,  9694,\n",
      "           490,   118,   176,   118,  2036,   260,  5923,  3632, 22281,   122,\n",
      "           123,   390,  2420, 14439, 22288,   123,   854,  2028,   949,  1601,\n",
      "          8551,   304,  6470,   222, 13254, 13140,   272,  1069,   122,  4062,\n",
      "          6590, 10733,   118,   176,  1004,  1839,   180,   327,  2415, 19649,\n",
      "         22278,  3985,   387,   125,  8609,  3285, 13665,   118,   176,   173,\n",
      "          9196,   272,  1557,  1988,   259,  8229,   598,   456,  2432,  3667,\n",
      "           117,   122,   870,   509, 13429,   203,   179,   978,  9861,   122,\n",
      "           416,   304,  2694,  3343,  1402,   117, 17850, 12812,  4820,   259,\n",
      "          6646,  6267,   321, 18288,  2473,   146,  1530,   122,  8296,  2247,\n",
      "          1023,  1977,   146,   376,  1249,   122,  1023, 20346,   146,  1031,\n",
      "          1025,   236,   102]])\n",
      "DEBUG: Tokenized sentence 365: tensor([[  101,   202,   995,   622,  2002,   221, 13746, 22282,  7304,   748,\n",
      "           118,   176,   712, 11454, 19449, 15575,   117,  9537,   146,  3165,\n",
      "           173,  5179,   259,  2390,   700,  8198,   118,  2036,  5365,  6671,\n",
      "         14348,   117,  3285, 13665,   118,   176,   173,  4021, 19847,  2048,\n",
      "           501,   117, 11234,   785,   117,   122,   262, 18633,  1797,   286,\n",
      "           954,   532,  9288,   102]])\n",
      "DEBUG: Tokenized sentence 366: tensor([[  101,   202,  2402,   622,  1204,   118,   176,  6124, 11538,   117,\n",
      "         20639, 22288,   325,   171,   179,   538,  8776,   444,   117,  1023,\n",
      "         19827,   117,   173, 10894,  3391, 13793,  3429,   118,  2036,   123,\n",
      "         14594,   298,  6349,   117,  2694,   170, 18624,   498,  5179,   259,\n",
      "          6578,   117,  1065,   146,  4319,   125,  4707,  2684,   123,  9106,\n",
      "           232, 12492,   102]])\n",
      "DEBUG: Tokenized sentence 367: tensor([[  101,   202,  3147,   117,   240,   210,   117,  7194,   456,   118,\n",
      "           176,   229,  3391,  3630,   117,  3336, 10303,   412, 13747,  3292,\n",
      "           117,   122,   180, 22283,   173,  4271,  1191,   118,   176,  2397,\n",
      "           117,  5101,  1427,   123,   327,   525,  3239,  1758,   117,  1204,\n",
      "           118,   834,  4838,   183, 19920,   122,   333,   247,   102]])\n",
      "DEBUG: Tokenized sentence 368: tensor([[  101,   532, 16391,  4703,   210,   560,   506,  2533, 13834, 13173,\n",
      "           288,   118,  2036,   123,  9421,   102]])\n",
      "DEBUG: Tokenized sentence 369: tensor([[ 101, 5192,  118,  176,  102]])\n",
      "DEBUG: Tokenized sentence 370: tensor([[  101,  3429,   118,  2036,   318, 13793,   123,  3138,  1434,   230,\n",
      "          4036,   102]])\n",
      "DEBUG: Tokenized sentence 371: tensor([[  101,   173,   144,  6562,   944,   146, 10355, 22287,  8797,   978,\n",
      "          2601,  1546, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 372: tensor([[  101, 19736,   260, 17356, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 373: tensor([[  101,   327,  1310, 16122, 13793,   495,  2314,   307,   118,   176,\n",
      "           117,  2314,   307,   118,   176,   785,   117,  9579, 22282,   123,\n",
      "           636,  3442,   272, 10463,   179,  7119,   122, 16280,   118,   176,\n",
      "         13140,   125, 16151,   221,   123,  1998,   122, 13140,   125, 12908,\n",
      "          2028,   202,   347,   143,   512,   303,   102]])\n",
      "DEBUG: Tokenized sentence 374: tensor([[  101,   260,  1176,   117,   240,   210,   230, 15419,   125, 12448,\n",
      "           852,   454, 11147,   252,   374,  2876,  2836,   118,  2036,   260,\n",
      "         12183,   303,   143,  2010,   229, 22280,  9679,   320,   943,   183,\n",
      "           125,  1977,  5805,   151,   117,   122,   125,   179,  2277,   117,\n",
      "           122,   240,  1977,   117,  1796, 13032,  6086,  3495,   179,  2036,\n",
      "         16386,   151, 14114, 22290,  2245,   483,  1402,   102]])\n",
      "DEBUG: Tokenized sentence 375: tensor([[  101, 11556,   146,   347,  8620,   173, 21990,  2204,   117,  5695,\n",
      "           118,  2036, 11199,  3831,   123,  1966,  3953,  2010,  3874,   146,\n",
      "           766,  1391,   183, 10355,   118,  2036,   117,   173,  5902,   785,\n",
      "         13427,   117,  1112,   179,   146,  1568,   125,   646,   314,  5895,\n",
      "          1021, 14775,  1075,   180,  4532,  6099,   123, 18615,  1187,   117,\n",
      "           122,   146,  7392,   117,   146,  5023,   428,   117,  1966,  1011,\n",
      "           202, 15273, 13808, 22280,   117,  8350,   229,  4768,   180,  4595,\n",
      "           170,   230, 22282,  2470,   210,   125, 12466,   240, 13379, 22354,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 376: tensor([[  101,   125,   327,   223, 22279,  2010,  2798,   230,  3661,   117,\n",
      "          2798,   230, 16266,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 377: tensor([[ 101,  495,  221,  432, 7063,  455,  943, 1112,  449,  117,  870,  509,\n",
      "          117, 1977, 1467,  740,  136,  102]])\n",
      "DEBUG: Tokenized sentence 378: tensor([[  101,  5787,   925,   148,  7970, 19462, 17704,   179, 18305,   221,\n",
      "           368,   230,  1448,   223, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 379: tensor([[  101,   449,   318, 13793,   240,   179,  1971,  9250,   247,   136,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 380: tensor([[  101,  1467,  3933,  4131, 22278,   117,   123,  1815,  2009,   391,\n",
      "          2186,   683, 22278,   117,   179, 16241,  1537, 22287,   176,   361,\n",
      "           130, 10930,   123, 12972,   118,  2036,   136,   102]])\n",
      "DEBUG: Tokenized sentence 381: tensor([[ 101, 1467,  368,  432,  537, 2640,  136,  102]])\n",
      "DEBUG: Tokenized sentence 382: tensor([[  101,   229, 22280,   117,   693,  3671,   117,  2113,   495,  1135,\n",
      "         17892,   125,   347,  1568,   102]])\n",
      "DEBUG: Tokenized sentence 383: tensor([[  101, 22354,   122,   646,   314,  5895,   117,  2249,   325, 17366,\n",
      "           240,   123,  1743,   264,   123,   327,  1622,   340,   117,   325,\n",
      "           122,   325, 21333,   140,   679,   202,   125, 11437, 22280,   366,\n",
      "           317,   537,  1650,   102]])\n",
      "DEBUG: Tokenized sentence 384: tensor([[  101,   366,  6536,   179, 16653,   171,  1010,   215,   117,  2798,\n",
      "           230,   331,  2036, 18402,   202,  3714,   117,   122, 17226,   117,\n",
      "           495,  1971,   146,   347, 21813,  9635,   173, 11092,   436,   118,\n",
      "          1340,   117,   179,   260,  1176,   117,   170,   785,  3803,   303,\n",
      "           125, 14876,   151,   117, 17002, 21554,   122,  8627,   711,  4221,\n",
      "         22293,   579,  9694,   128,   125,  1450, 10200,  2949, 22281, 12773,\n",
      "           784,   117, 14644,  5529,   122, 10398,   117,   180,   327,   851,\n",
      "          1601,   151,   102]])\n",
      "DEBUG: Tokenized sentence 385: tensor([[  101,  7543,  5630, 19356,  3070, 22282,   118,   176,   180,   360,\n",
      "          3863,   508,   117,   179, 17783, 13674,   117,  2251, 22282,   155,\n",
      "         18003,   123,   347,  1341,   117,   229,  1589,   860,   364,   117,\n",
      "          8362,   214,  8032,   657,   121,   102]])\n",
      "DEBUG: Tokenized sentence 386: tensor([[  101, 22232,   795,   146,  1112,  1151,   281,   994,   171,  1273,\n",
      "           603,   117,  3539,  5848, 22282,   872,   514, 22287, 22354, 18127,\n",
      "           256,   118,   176, 14619,   210,   180,   139,   124,   102]])\n",
      "DEBUG: Tokenized sentence 388: tensor([[  101, 22232, 12764,   159,  9095,   117,   123,   331,   522,   125,\n",
      "          5009,   178,   117,   179,  8544,   117,   170,   785,  2778, 14273,\n",
      "           117, 10964,   123, 17962,  3852,  1564,   102]])\n",
      "DEBUG: Tokenized sentence 389: tensor([[  101,   173,  1250,   117,   740, 19288, 22278,  9463,   180,  2954,\n",
      "           117,   202,   347,  2109, 22285,  4083, 21617,   240,   682,  5976,\n",
      "           117, 12232,   285,   125,  5223, 10849,   117, 19547,   693,  1313,\n",
      "           122,  3848, 17714,   143,   117, 12214,   328,   240,   222,  7967,\n",
      "         12920,   125,  5231,  4322, 22282,   123,  4768,   170,   222, 19068,\n",
      "          4234, 22280,   125, 14121,   117, 12700,   243,   117,   924, 20262,\n",
      "         22281,   202,  1997,   102]])\n",
      "DEBUG: Tokenized sentence 390: tensor([[  101,   122,   146,  3174, 13733,   180,  2606,  1684,   123,   646,\n",
      "         22290,  2430,   117,  1684, 11510, 12674,   285,   117, 18260,   538,\n",
      "         12461, 22281,   122,   123,  8893, 22282,   170,   368,   117,   646,\n",
      "           314,  5895,   117,   123,  1977,   117,  1485,   260,  1176,   179,\n",
      "          2036, 10348,   123,   223, 22280,   123, 10786,  4765,   117,  1143,\n",
      "           269,  8041,  1988,   260,  8523,  1014,   230,  9196,  2382,   229,\n",
      "          9463,   102]])\n",
      "DEBUG: Tokenized sentence 391: tensor([[  101,   122, 18127,   256,   118,   176,  1004,   171,  9169, 13376,\n",
      "          3173,   183,   125, 22232, 22278, 17897,   124,   117,  1941,   796,\n",
      "         22280,  1423,   273,   304,   286, 18127,   256,   118,   176,   298,\n",
      "           532,  5708, 17291,   268,   118,  6054, 22281,   117,   125,   532,\n",
      "         12141, 15000, 12632,   143,   117, 13182,  2028,   442, 22278,  9324,\n",
      "           252,   117,   271, 17897,  7280,  9144,  4654,   555,   117,   240,\n",
      "         13702,   117,   260, 17704, 22281,   171, 15273, 13808, 22280,   117,\n",
      "          8934,   173,  7698,   102]])\n",
      "DEBUG: Tokenized sentence 392: tensor([[  101,   646,   314,  5895,   117,   230,   576,   117,   744,   173,\n",
      "           144,  6562,   117, 12183,   214,   146,   765,   397,   125,  6261,\n",
      "          1182,   148, 12909,   285,   117, 10733,   117,   271,   240, 16473,\n",
      "         22280,   117, 13431,   210,   118,  2036, 22278, 14876,   151,  1415,\n",
      "          8446,   125,   179,  2364,   176, 18127,   124,  2684,   318, 13793,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 393: tensor([[  101,  5069,   748,   118,   176,  2044,   171,  5774,   125,  9480,\n",
      "         14948,   123,  1105,  1011,  9257, 22278, 22243,  1337,   122,  4019,\n",
      "         10766,   285,  4566,   146,   947, 22232,   795,  2510,  4322,   202,\n",
      "           347,  3147,  5009,   178,  6952,   256,   117,   125,   222,   221,\n",
      "          1342,  1341,   180, 17935,   404,   117, 11233, 13194, 22280,   122,\n",
      "           273,  2758,   796,   243,   449,   117,   125, 19170,   117,  4169,\n",
      "           229,  4303,   171,  3147,   230,  1124,  7137, 13717,   285,   117,\n",
      "           123,  1977, 21280,   146,  3896,   125,  1112,   238,   252,  1611,\n",
      "         21510, 22354,   117,   122,   418,   117,   179,  8940,  6568,   157,\n",
      "          2382,   117,  5608,   125,   670,   146,  8517,   180,  1105,   117,\n",
      "          1996,   118,  2036,  3933,  5664,   173, 11021,   117,   122,   180,\n",
      "         22283,  1178, 21511,  2072,   944,  8540,   143,   122,  8174,  1789,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 394: tensor([[  101,   122,  8362, 22278,   118,   176,  2477,  1084,   125,  1839,\n",
      "           222,   768, 22285,  4095,   243, 15406,   683, 22280,   117,   179,\n",
      "          9821,   230,  1956,   343,   102]])\n",
      "DEBUG: Tokenized sentence 395: tensor([[  101,   229, 17190,   151, 22280,   117,   646,   314,  5895,  3874,\n",
      "         12123, 22288,   125,  2745,  3413, 15024,   118,  2036,   179, 22232,\n",
      "           795,  3859, 22278,   230,  9586,   125,  1546, 22278,   117,   122,\n",
      "           368,  2547,  3529,  4234,   246,   102]])\n",
      "DEBUG: Tokenized sentence 396: tensor([[  101,  1016,  2036,   417,  1797,   923,  1028, 18127,   303,   143,\n",
      "           240,  1416,   123,   171, 12361, 22281,  8889,   765,  1058, 22280,\n",
      "           117,   318, 13793,   785,   173,  5587,   229, 11126,   351,   117,\n",
      "           170,   179,   121,   102]])\n",
      "DEBUG: Tokenized sentence 397: tensor([[  101, 22232,   795,  2036,  3980,   809,   256,   259, 13841,  1485,\n",
      "           260,  1062,   842,  1075,   171, 19935,   449,   117,  5153,  2745,\n",
      "           117,   171,   179,  1407,   368,   176, 18127,   256,   495,   298,\n",
      "         19068, 14881,   143,   170,   179, 16188, 14533,   123,   651,   102]])\n",
      "DEBUG: Tokenized sentence 398: tensor([[  101,   744, 18253, 13793,  1021,  3598,   117,  2798, 18691,  2615,\n",
      "         22279,   320,  5818,   121, 22361, 15252,   118, 22232,   138,  8940,\n",
      "           146, 10049, 18776,   141,   117,  1950,  5723,   123,  5820,   171,\n",
      "         19068,  4234, 22280,   117,   273,   351,   118,   146,   117,  2751,\n",
      "         22278,   118,   146,   117, 15891,   524,   256,   118,  2036,  1839,\n",
      "          6205,   159,   277,  4682,   285,   170, 15460,   211,   117, 10049,\n",
      "          1399,   118,  2036,   146, 14006, 22280,   117,   866,   624,   256,\n",
      "           118,  3185,  1281,  3514,   221,   146,   347,  1247,   117,   122,\n",
      "         16246, 14339,   102]])\n",
      "DEBUG: Tokenized sentence 399: tensor([[  101,  1112,   122,   179, 11775,   765,   397,   173,  1485,   260,\n",
      "         21734, 22281,   173,   179,  1021,   215,  4612,  3391, 13793,   102]])\n",
      "DEBUG: Tokenized sentence 400: tensor([[  101,   146, 22296,   123,   229, 22280,   333,   179, 10606,   785,\n",
      "         11799,   123,   327, 11126,   351, 12444,   333,  4877,   914, 15558,\n",
      "         22290, 22354,   229, 22280, 14940,   117,  4750,  1084,   925,   102]])\n",
      "DEBUG: Tokenized sentence 401: tensor([[  101, 16280,  3106,   303,   143,   240,  1921,  9019,   151,   117,\n",
      "          1821,   316, 22280, 12431,   221,   368,   271, 22280,   347,  2004,\n",
      "         22280,  5774, 17065,   102]])\n",
      "DEBUG: Tokenized sentence 402: tensor([[  101,  1112,   170,   123,  4036,  7544,   151,  2745,   449,   117,\n",
      "           652,   117,   495,  8911,  2822,   389, 17611,   123,  8768, 22278,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 403: tensor([[  101, 22354,   122,   117, 19508,   117,   262,   320,  4223,   247,\n",
      "           125,   766,  1391,   183,   117,  4203,   111, 13747, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 404: tensor([[  101,   117,   629,  1353,   123, 17088,   125,   179, 11736,   117,\n",
      "         12631,  1353,   128,  3667,   117,   122,  1191,   118,   176,   125,\n",
      "         20262,   221,   123,  1546, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 405: tensor([[  101,  1367,   412,  2570, 22278,   117, 10189,   123,  2545,   151,\n",
      "           117,   262,   123,  7231, 22278,   117,  3851,   229,  1202,  2328,\n",
      "           117, 11298, 22288,   123,  1486,  2752,   117,   122,   117,   202,\n",
      "          1338,   125,  1510, 22281,   481,   125,  4036,   117,  2080,   320,\n",
      "          2187,   125,  1543,   117,   582,  5555,   259,   532,  3845, 16929,\n",
      "           272, 21990,  2204,   102]])\n",
      "DEBUG: Tokenized sentence 406: tensor([[  101, 20680,   118,   176,   222,   622,   229,  3952,   117, 19973,\n",
      "           180,   651,   117,  2089,   203,   118,   176,   117,  1191,  3965,\n",
      "           125,  1069,   122,  9565,  1009,  2604,   140,   123, 22283,   123,\n",
      "           327, 18489,   340,   102]])\n",
      "DEBUG: Tokenized sentence 407: tensor([[  101,  1112,   122,   146, 15273, 13808, 22280,   136,   102]])\n",
      "DEBUG: Tokenized sentence 408: tensor([[  101,   146, 22296,   117,   179, 12361,   285,   449,   229, 22280,\n",
      "          4207,  4314,   125,  1084,   925,   229, 22280,  4207, 15683,   118,\n",
      "           176,   229,  5635,   185,   117,   834,   370, 19480,   652,   123,\n",
      "           327, 11126,   351,   495, 21229, 20383,   178,  8223,   123, 20027,\n",
      "         17558,  5799,   259,   532,  6109, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 409: tensor([[  101, 22354,  2010,  3295,   117,  3295,   117, 10355,   368,   117,\n",
      "         10415,   214,   170,   222,  3695,   117,   123,  1977, 12908,   124,\n",
      "           259,   532,  3965,   117,   123,  5664,   229, 22280,   122,   316,\n",
      "         22280,  2996, 22278,   271,  3189, 11639,   117,  2113,   117,   202,\n",
      "          1338,   125, 11169,   117,   982, 22280,  7422,   214,  1364,   146,\n",
      "          1410, 15277, 17513,   117,  3687,   222,  5995, 22280,   320,   221,\n",
      "           122,   320, 14436,  3681,   117,   179,  6532,   792,   117,   122,\n",
      "           117,   870,   509,   117,   781,   183, 12604, 13550,   221,   329,\n",
      "           170,  4042,   285,   173,  2601,   117,   123,  5594,  3292,  7496,\n",
      "          2127,   251,   122,   146,  1695,   179,  1047, 22280, 10939,   123,\n",
      "          9245,   102]])\n",
      "DEBUG: Tokenized sentence 410: tensor([[  101,   229, 22280, 21174, 19623,  2037, 22282,   118,   311,   180,\n",
      "          7716, 22280, 17611,   123,  8768, 22278,   229, 22280,   331,  2036,\n",
      "         12439,   124,   146,  5791,   183,   117,   271,   146,  1831,   102]])\n",
      "DEBUG: Tokenized sentence 411: tensor([[  101,  1011,   785,   325,  2124,   483, 22287,  6569,  2640,   122,\n",
      "           170,   230, 10428, 22279,  2401, 22279,   524,   415,   102]])\n",
      "DEBUG: Tokenized sentence 412: tensor([[  101,  1956,   581,   256,   118,   176,   125,   370, 13032,   739,\n",
      "          2000,  3292,   171,  1147,   865,  9763,   256,   123,  6272,   498,\n",
      "          1569,  6797,   316, 22280,  1004,  9679,  4270,  1532,  4767,   125,\n",
      "           681,  2601,   271,  2387,   230, 19553,   420, 13254,   143,  1532,\n",
      "         16139,   304, 22280,   125,  1955,   291,   229,  8097,   125,   222,\n",
      "          3822,   102]])\n",
      "DEBUG: Tokenized sentence 413: tensor([[  101,   122,   173,  2038,   125,  6320, 22279, 15061,   117,   229,\n",
      "         22280, 18689,   151,   117,   170,  1364,   146,  2368,   117,   179,\n",
      "         15098,   614,   210,   325,   440,  5345,  1522, 22280,   171,   179,\n",
      "           368,   102]])\n",
      "DEBUG: Tokenized sentence 414: tensor([[  101,   262,  4922, 11791,  5398,   304, 22280,   125,  5791,   183,\n",
      "           117,  8540,   122, 13140,   125,  2521,  2028, 22281,   202,  3753,\n",
      "           179,   646,   314,  5895,  5235, 22280,  1112, 19026, 22354,   122,\n",
      "          8383,   221,   123,  1855,   125,   629, 22280,   599,   145,   171,\n",
      "         15273, 13808, 22280,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 411 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.004613999549334459\n",
      " Coesão Score Final: 0.5023069997746672\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'contudo', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'apesar de', 'todavia', 'nao obstante', 'por conseguinte', 'nem', 'ou', 'ora', 'quer', 'seja', 'como', 'quanto', 'ao passo que', 'para que', 'antes que', 'logo que', 'depois que', 'porque', 'gracas a', 'no entanto', 'por exemplo', 'alias', 'principalmente', 'nao so', 'mas tambem', 'tanto', 'quanto', 'se nao', 'de sorte que']\n",
      " Número de conectivos: 41\n",
      " Número de sentenças: 415\n",
      "======================\n",
      "Resultados para preprocessado_o_mulato_aluisio_azevedo_cap_3.json:\n",
      "{'coesao_score': np.float64(0.5), 'conectivos_encontrados': ['e', 'mas', 'porem', 'contudo', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'apesar de', 'todavia', 'nao obstante', 'por conseguinte', 'nem', 'ou', 'ora', 'quer', 'seja', 'como', 'quanto', 'ao passo que', 'para que', 'antes que', 'logo que', 'depois que', 'porque', 'gracas a', 'no entanto', 'por exemplo', 'alias', 'principalmente', 'nao so', 'mas tambem', 'tanto', 'quanto', 'se nao', 'de sorte que'], 'num_conectivos': 41, 'proporcao_conectivos': 0.005, 'similaridade_media': np.float64(1.0), 'num_sentencas': 415}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  5147,   117,   170,   123,  4532,   125,   646,   314,  5895,\n",
      "           117, 15130,   118,   176,   173,  1105,   125,  5009,   178,   260,\n",
      "         20991,  8286, 22281,   285, 20027,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,  8198,   260,  9344,   579,   170,   259,   532, 14563,  2377,\n",
      "           185,   308,   390,  1149,  2996,   138,   117,   449,   125,  1491,\n",
      "         13841,   117,   785, 19607, 22281,   122,  4088,   229, 11126,   351,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[ 101, 1112, 8196, 1149,  271,  260,  366, 9344,  579,  102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101, 10881, 22003,   271,   146,   591,  9344,   579, 19730, 22281,\n",
      "           271,   259,   366,  9344,   579,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101, 22354,  3769,   122,  1028, 17783, 16264,   176,  3841, 17282,\n",
      "           210, 14501,   721,  2401,   680,   909,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,  1796,   366,  9344,   579,   229, 22280, 18574, 22287,  2476,\n",
      "           125,  3286,   304, 22280,   221, 13841,   122,  2307,   117,  5594,\n",
      "           138,  7970,  6698,   117, 13944, 21206, 22287,  1684,   146,  4947,\n",
      "           125,  2571,  8296,   303,   143,   173,  2377,   185,   308,   445,\n",
      "          4757,  2247,   117,   125, 18577,  6597,  1107,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,  2010, 15212,  7482,   117, 19519,   256,   260,  1176,   121,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,  4351, 14737,  9344,   310,   418,   495, 10420, 10656, 12160,\n",
      "           125,   370,  1971, 10881,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   221,  2184, 18271, 22290,   252,   118,  1340,   122,   222,\n",
      "           528,   779,   247,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   122,   117,   625,   700,   171, 17052,   117,   229, 22280,\n",
      "           311,  2377, 21852,  2044,   117,   291,   625,  1367, 22287,   644,\n",
      "           834, 11967,   159,  2389, 22279, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   123, 22296,   117, 10502,   117,  2798,  2036,  2826, 22280,\n",
      "          3874,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   122,  3456,  1187,  2836,   259,  5708,   122,   629,   830,\n",
      "           679,   123,   717,   581,   117,   271,   176,  7012, 22281,   236,\n",
      "           230,   329,  2382,   125,   447, 22280,   143,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,   123, 20027,  9344,   310,  5048, 13808,   118,   176,   117,\n",
      "          1202, 22287,  1014,   121,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,  4351, 14737,   117,   125,  1858, 10173,  6943,   122,   125,\n",
      "           230, 17704,   693,   147, 10422,   481,   117,   785,  8089,  1337,\n",
      "           117, 14057,   366,   924,   390,  1149,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   123,  7492,   331, 18402,   173,  3848,   687,   562,   122,\n",
      "          9679, 11935,  1530,  1266,  2745,   978,   222, 15618,   293,  1722,\n",
      "           125, 14565,   117,   179,   740,   173,  1250, 16555,   202,  4102,\n",
      "           293,   173,  1105,   230,  2421,  8060, 14908,  3391, 10501,   304,\n",
      "         22280,   125, 10809, 22281,   117, 16317, 10318,   122,   879,   934,\n",
      "           128,  5825,   256,  1684,   260, 15520, 22281,   125, 14857,   117,\n",
      "           125,  2415,   122,   259, 15045,   942,   125,   226,  2426,  1939,\n",
      "         22278,   117,   259,  1647,   117, 10355,  4286,   735,  1780,  1112,\n",
      "          4133,   125,  4023,   117,  1085, 11842, 11935,   635,   221,   260,\n",
      "         19659,   125, 12728, 22354, 13028,   118,   176, 22232, 22278,   171,\n",
      "           505,   283,   117,   122,   260, 17181, 22281, 14915, 22287,   118,\n",
      "           229,   240,  1112, 10256, 22278, 22279,   291,  2577, 13808, 22354,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   495, 19569,   246,  7904,  3388,   255,  1319,   122,  3486,\n",
      "          1802,   125, 20460,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[ 101, 4970,  256,  102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,  3852, 22278,   123,   390,  2420,   202, 12401,  1637,   125,\n",
      "          7308, 17704,   180,  6094,   304, 22280,   122, 11935,  1530,   117,\n",
      "           582,  9046,   371,   146,   347,   652,  1417,   171,  2397,   170,\n",
      "          1977,   700,  3429,   123,  6759,  2010,   146, 10418,   730,   945,\n",
      "         22280,   117, 10418,   171,  6569,   373,   117,   222, 18100,  1165,\n",
      "          3120,   298,  1256,  4203,   442,   117,   179,  6952,   256,  1684,\n",
      "           125,  5546,   285,   122,  2184,   581,   508,   256, 22278,   623,\n",
      "          1471, 12717, 22278,   240,   180,   329,  7583, 19278,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,  7719, 22287,  2461,   179,   117,   222,   644,   117,  1362,\n",
      "         14890,   125,  4939,   117,  6757,   123,  4500,  3292,  1988,   146,\n",
      "         16573,   502,   201,   117,   179,  9821, 16620,   123, 17061,   320,\n",
      "         19125, 17425,   185,   117, 15700, 22278,   171,  1690, 22285,  6923,\n",
      "           268,   122,  3686,  5636,   524,   762,   211,  2817,   125,  9531,\n",
      "           146, 14808,   403,  6032,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101, 14971,   125,  1434,  7672,   260,   854,  2028, 22281,   117,\n",
      "         16044,   557,   179,   260,   466,  1399,   291,   870, 17598,   123,\n",
      "         19068,   387, 15908, 11577,   185,   202,   193,  1286,   326,   171,\n",
      "          1690, 22280,   122,  9086,   785, 21990,   181,   537,   201,   625,\n",
      "          2036, 10355, 22287,   179,   176,  9821,   170,   146,  1449,   157,\n",
      "           254, 22283,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   978,   118,   176,   229,  5371, 22278,   125,   785,  7304,\n",
      "          5872,   122,   123,   944,  7719,   179,  1796,  6447,   173, 13254,\n",
      "          1822,   151,   118,   176,   123,  5899,   623, 12051,   125,   417,\n",
      "           444,  1107, 22279,  8825,   154,  1100,   117,   179,  2036,  3172,\n",
      "           256,   121,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101, 22232, 22278,   171,   505,   283,   117,   202,   347,   596,\n",
      "           125, 12401,   328,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   144,  2640,  2931,   125,   230,  8574,  3870, 22278,   733,\n",
      "         18982,  8849, 22280,   202,   644,  1457,   123,   230,  2992,   151,\n",
      "           117,   744,   325,  8574,  3870, 22278,   117,   229,   615, 15707,\n",
      "         22278,   123,   525,   381,  2346,   351,   125,  1847,   230,  4767,\n",
      "           285,  6650,   125,   766, 18747, 22281,   117,   347,  1174,  5321,\n",
      "          9078,   634,   183,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,   123,  4970,   256, 15641, 22288, 10152,   715,   415,   117,\n",
      "           122,   117,   173,  3902,   123, 14876,   151,   171,   730,   945,\n",
      "         22280,   117,  2364,   325,   662, 22288,  4566,  2653,  2766,   347,\n",
      "          1657,   247, 16621,   118,   176,  4955,   304,   415,   240,  1719,\n",
      "           123, 20027,   171,  3002, 11646,   229, 22280,  8204,  9226,   325,\n",
      "          5961,   125,   223, 14271, 14523,   117,  2798,   272,   472, 12126,\n",
      "          1516, 22281,   117,  2798,   125, 15605,  3198, 14252,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,  2010,   123, 22283,   146,  7343,  8797, 10418, 19068,   796,\n",
      "           256,   118,   176,   740,   625,   614,   210,  2036, 10201,   256,\n",
      "           146, 15211, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   179, 11402,   272,  2397,   179, 16450,   304, 22280,   125,\n",
      "           302,  5870, 11972,   122,   179,   495,   222,  4170,   271,  1790,\n",
      "           173,   644,   229, 22280,   176,   873,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   123,  1858, 17181,   125,   121,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101, 22232, 22278,   171,   505,   283,   117, 13028,   118,   176,\n",
      "          2337,   178, 13359,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,   854,  2426,   508, 19569,   246,  5923,   124,   117,   122,\n",
      "           316, 22280,  8089,  1337,   271,   123, 14057, 17749,   785,  2135,\n",
      "         22280,   739,   122, 13605,   201,   117,   223,   128,   259,  2515,\n",
      "           591,   122, 16224, 22281,   117,  5708,  3260,  2301,   122, 12141,\n",
      "           264, 21510, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,   495,  4343,  8849,   415,   259, 13254,   143,   171,  1847,\n",
      "           523, 21612,   118,  2036,  1112, 19277,   193,  2037, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,  5057,   118,   176,   785,  2415, 19649, 22278,   466,   852,\n",
      "           256,   123,   327,   549,  3428, 15558,  2507,  1877,   328,  4217,\n",
      "           364,   256,   125,  1685,   173,  1685,  3354,   122,  9679, 15872,\n",
      "          4234, 22282, 14977,  4242, 20987,  4306,   320, 14643, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101, 10355, 22287,   117,   173,   388,   785,   333,   247,   117,\n",
      "           179,   740, 18424, 22278,   712, 12530,   481,   230,   928,   328,\n",
      "           415,  1568,  2037, 22280,   240,   389,  4693,   117,  2917,   125,\n",
      "          2242,   117,   146,   615,  7912, 22278,   712,  6884,  2841,   221,\n",
      "           146,   221,   122,   179,   117,  1065,   318, 13793,   117,  2337,\n",
      "           178, 13359,  2364,   325,  5357, 22278,  1831,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,  3794,   118,   176, 14619,   210,   173,  1105,   125,  5009,\n",
      "           178,   123,   139,   124,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101, 22032,   351,  7206, 10372,   966,   117,  7492,   125,   739,\n",
      "         14876,   151,  1266, 15209,  8446,   117, 10995,   122,  3360, 10201,\n",
      "           256,   118,   176,  1684,   171,   360,   595, 17551,  5356,  9258,\n",
      "           298,   532,   238,  2766,   444, 17504, 22281,   117,   122,  3876,\n",
      "           644,   450,  2836,   118,  7707,  4019,   367,  5366,   146, 14890,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,  1011,  1684,   123,  5961,  3002,   180,  1069,   313,  9193,\n",
      "         22278,   117,   123, 15419,   180,   615,  1369,   138,   117,  9584,\n",
      "          8184,  1564,   173,  1105,   125,   230,  8932,   117,   736,  8184,\n",
      "           173,  1105,   125,   222, 18369,   117,   146,   454,  1457,   173,\n",
      "          1105,   125,   222, 18369,   122,  3695,   117,   122,  1016,   240,\n",
      "          4271,  1684,   117,  1684,   125, 17611,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101,  8544,  1474,   162,  1005,   670,   117,  2589,   291,   229,\n",
      "         22280,  2589,  8781,   285,   117,   122,   117,   260,   924,   240,\n",
      "          1510, 22281,   117,   495,   180,  1105,   102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101, 18574,  1364,   146, 15273, 13808,   451,   618,  2836,   117,\n",
      "           834, 10806,   117,   259,  3240,   404,  1159,   179,  2036, 11314,\n",
      "           228,   202, 21115,   117,   122,  6952,   256, 15480,   229,  4768,\n",
      "           117,  3852,   508,   214,   657,  1719,   123,   651,   117,   125,\n",
      "          3043,  1531,   117,  3285,  1825,   146, 17749,   173,  2745,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,   176,  1623,   322,  3179,  1420,   347,   117,  1084,  1011,\n",
      "           740,   117,  7230,   779,   146,  1078,   391,   117,   123, 16633,\n",
      "           118,  2036,   260,   877,   842,   117,   123,  4640,   259,  4488,\n",
      "           118,  4577,   180,  5411,  3391, 13793,   117,   964,   285,   122,\n",
      "         15649,   240,  1415,  1370,  3178,   117,  7656,   122, 12376, 20344,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,   495,  9106,  1780,  2477,   705,   117,   449, 18601,   179,\n",
      "           173,   390,   304,   117, 12670,   124,   785,  2982,  4062,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101, 10348,   118,   176, 22278,  4486,   125,  2567,  9679, 12232,\n",
      "         22282, 17877,   125,  2047,  6501, 22280,   122,  2157,  2836,   259,\n",
      "         13841,   170, 13636,   735,   319,  7967,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[ 101, 4343, 8849,  256,  146, 9769,  102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,  2010,   202,   347,   596,   117, 10355,   740,   170,  3673,\n",
      "           430,  2766,   117,   260, 10687,  2365,   123,   327,  8267,   125,\n",
      "          3766,   124,   221, 17783,  3146, 22281,   122,  3841,   125,   240,\n",
      "           363, 22361,  1369,   146,  1223,   176,   146,  4364,   692,   325,\n",
      "          8545, 16420, 12604,  8889,   136,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101,  7799, 12156,  8257, 14533,   485,   252, 17704, 12156,  8257,\n",
      "         14533,   221,  1434,   125,  1160,   122,  1790,   136,   102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101, 10573,   256,   117,  3951,   222,  5995,   994,   117,   170,\n",
      "           260,   148,   128,   529,  2526, 22282,  1557,  2010,  1790,   122,\n",
      "           146, 20679,   415,   714,   180,   223, 11198,   125,  3766,   124,\n",
      "           180,   118,   176,   230,  8267,   739,   122,   122,   331,  1112,\n",
      "          1757,  8497,   118,  1757,  8497,   118,  1757,  8497, 22354,   122,\n",
      "           418, 13610,   146,  1312,   303,   122,   180, 22283,   117,  2541,\n",
      "           123,   898,  6943,   343,   240,   118,   176,   125,  8092,   538,\n",
      "          6349,   117,  5357,  1284,   243,  4313,   291,   318, 13793,  2541,\n",
      "           221,   123,  1884, 12773,   351,   171,  7670, 22279,  7845,   256,\n",
      "           179,  2267,   327,   229, 22280,  1021,   125, 10698,  4327,  7325,\n",
      "           117,  2113,   260,  1950,   391, 12268,   591,   293, 11666, 11972,\n",
      "           221,  1407, 10415, 22282,   170,   259, 13746, 22281,   117,   834,\n",
      "           179,   259,   736,  2811, 22287,   412,  4286,  8808,   322, 16858,\n",
      "           210, 10355,  3002,   180, 16188,  3391, 13793,   123,  3598,  2010,\n",
      "          4654,   555,   259,  5976,  2365,   179,  1434,  3002, 15724, 22287,\n",
      "           123,  6124,   154, 16420,  1148,  2443,   122, 10049,  1217,   259,\n",
      "           822,   272,  1044,   117, 20933,   578,   118,  7707,  1160,  3673,\n",
      "         20480,   122,  7277,   118, 15887,   202,   347,  1247,   102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[  101,   122,  1790,   136,   122,   331,  3767,   146,  1877,  5137,\n",
      "         21102,   125,  4848,   123,  5782,  2037,   322, 20350,   319,   125,\n",
      "          3598,   122,   102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[  101, 11314, 22278,   118,   176,   229,  9196,   272,   421,  1941,\n",
      "           229, 22280,   607,  8267,  1941,   229, 22280,   607, 16195,   122,\n",
      "           240,  1257,   179,  1061,  6952, 22287,   316,  1657,  3656, 21675,\n",
      "         22281,  1224,   319,   185,  1224,   319,   185,   117,  2684,  4640,\n",
      "          1587,   154,   179,   122,   171,   179,  1061, 11185,   102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101,  6344,  2779,  1415,   117,   179,  7707,   741,   157,   117,\n",
      "           412,  7489,   304, 22280,   125,  7122,  5836, 11324,   117,   179,\n",
      "          7707,  1021,   125, 11076,  5052,   171,  1340,  7052,   486,   123,\n",
      "         22127,   125,   121,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101, 22032,   351,  7206, 10372,   966,   117,   146,   179,   123,\n",
      "         19674,  2251,   844,   178,   221,  8128, 13254,   143,   902,   735,\n",
      "         19458,   240,  1415,  2639,   125, 20027,   179, 16420,   125, 17749,\n",
      "           516,  1859,  2036,  7425, 12338,   122,  1670, 17714,   501,  3916,\n",
      "         22282,   555,   151,   117,   495,   117,   834,   623,  2100,   117,\n",
      "           146,   347,  2600, 19877, 22280,   125,  7283, 15631,   243,   470,\n",
      "          7086,   122, 15618, 15335,   277,   102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101,  1684,  1796,   785,  1111,   492,  2382,   202,  1325,  1089,\n",
      "          1587,   581,  1374,   180,   327, 10849,   117, 10355, 22287,  3914,\n",
      "           117,  1362,  2143,  9370, 22280,   125,  3979, 22280,  1112,   170,\n",
      "           123,   121,   102]])\n",
      "DEBUG: Tokenized sentence 50: tensor([[  101, 22032,   351,   229, 22280,   706,   123,  9349,  2765,  1467,\n",
      "          2010,   146,   644,   492,   180,  7492,   376,   230,   416,   304,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[  101, 22354,  1084,  1011, 14619,   210,   173,  1105,   125,  5009,\n",
      "           178,   123,  2779,  4221, 16257,   252,   117,  4970,   256,   171,\n",
      "          1538,   125,  9719,   102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[  101,  1719, 13586,  1375,  1076, 15530,  2552,   125, 11586,   577,\n",
      "          2037,   117,  1623,   194,   508,  2440,   180,  5776,  3095, 12717,\n",
      "           351,   171,   302,   118,   125,   118, 11902,   260,  2996,   303,\n",
      "           143,   785,  1111, 11291,   591,   123,  3258,  1095, 22279,   171,\n",
      "          9169,   122,   170,   222,  4227,   125, 17407, 21308,   125,  5731,\n",
      "           320,  1341,  9657,   180,  9463,   117, 10151, 20062,   246,  1031,\n",
      "          2640,   171,   125,   230,  4871,   294,   118,  2940,   170,  1977,\n",
      "           740,   176, 10348,   102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[  101,   146,  4227,   495,   221,  4412,   171,  2599,   632, 22280,\n",
      "           125,   230,  5995,   421,   122,  5127,   171,  3846,   122,   171,\n",
      "         18190, 22280,   125,   222,  2996,   524, 22280,   118,  7967,   102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101,  9344,  3391,  3356, 22279,  2836,   118,   176,   117, 13086,\n",
      "          2131, 12683, 22281,   117,  1904,  5974,   118,   176,   125,   576,\n",
      "           173,   625,   117,   221,   925,  4640,   222,  7513,  6436,   268,\n",
      "           320, 12728,   125,  9480, 14948,   117,  1139, 12746,  2382,   246,\n",
      "          2036,  8530,  1235, 11541,   146,  2377,   185,   201, 16613, 17611,\n",
      "         22281,  2389, 15736,   125,   188, 19035,   252,   221,   259, 15050,\n",
      "         22279,   221,   123, 17935,   404,  2010,  3951,  1154,  2010,   122,\n",
      "          1359,   256,   123,   327,  9104,   117, 15796,   214,   118,   176,\n",
      "           123, 16463,   183,   538, 17664, 22281,   180,  4767,   117,  1684,\n",
      "         18206, 12898, 22278,   117, 10984,  8156,  1970,   117, 19608, 16512,\n",
      "           173,  2745,   179,  2036, 10355, 22287,   230,  1905,   304, 22280,\n",
      "          4078,   117,  1510,   537,  4718, 15823, 19088, 22281,   122,   390,\n",
      "         17423,   143, 22206, 22281,   625,   229, 22280,  3486,  1399,   117,\n",
      "           221, 16044,   307,   179, 12943, 22278, 15363,   102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101,   978,   123,  4410,   898,   980,  7909,   122, 19519,   285,\n",
      "           117,  1990,   619,   256,   259,   139, 22281,   117,   122, 10355,\n",
      "          7296, 12764,   401,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101,   146,   958,  1025,   117,   173,  3561,  1105,  9480, 14948,\n",
      "         18424, 22278,   146,   347,   169, 14116,   718,  2056, 22280,   117,\n",
      "         14619,   210,   176, 16995,  2981,   117,  1611,  2267,   117,   123,\n",
      "           327,  3189,   328,  1863,   243,   304,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[  101,   146,   958,  1025,   495,   222,  2397,   273, 10214,   243,\n",
      "           180,  2606,  1112,   179,   176, 20949, 22278,   712,   329,   143,\n",
      "         22354,   117,  6834,   256, 16224,   246,   117,   785,  3689, 22280,\n",
      "           117,  5923,   157,   117,  2979,   117,   170,   146, 13227, 11661,\n",
      "           268,   408,  9539,   202,   347,   739, 20327,   994,   173,   766,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[  101,   229, 22280,   689,  2037,   256,  7355,   162,  1149, 11563,\n",
      "           117,   122,  1956,   581,   256,   118,   176,   171, 11021,   125,\n",
      "         14223,   118, 10497,  1743,  1708,   122,  2787,   703,   401,   726,\n",
      "           230,  2767,   436, 12051,  1684,   117,  2440,   171,  6938,   180,\n",
      "         11126,   351,   117,   146, 20327,   994, 16293,   122,   146, 11979,\n",
      "           180,  7924, 10984,  6375,   255,  4812, 14967,   154,   827,   154,\n",
      "          2010,  2401,   680,  2381,   102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[  101, 14915,   230,  5223,   877,   252,   202, 21959, 14983, 22280,\n",
      "           117,   170,   123,   615, 17003,  2377,   185,   159,   463,  1039,\n",
      "           272,   117,  2160,   125, 10620, 19873,   117,   964,   833,   474,\n",
      "           122, 21990,   128,   117,   179,  2036, 20262,   692,   123,  9463,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101,  9871,  2380, 15500, 22278,   179,  1923,   483,   397, 14710,\n",
      "         22287,  1112,  2036, 18119, 10717,   123,   223, 22280,   202,  9169,\n",
      "         22354,  5057,   368,   653,   123,   327, 17897,   117,   222,   644,\n",
      "          1141,   117,  1342,   229, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101,  3235,  1399, 22278,  1945,   256,   170,   260,   408,  4379,\n",
      "           613,  4464,  5546, 22086,   138,   171, 10881,   117,   785,   730,\n",
      "          1361,   401,   117,   271,   179,   768, 14300,   138,   123,  3746,\n",
      "           148,   118, 11997,  3301, 14820,  1381,   146,  9804, 13733,   102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101,  1598, 13808,   125,   230, 14876,   151,   258,  8066,  6044,\n",
      "           117,  1956, 19685, 22278,   240,  1719,   123,   651,  5057,   118,\n",
      "           176,   739, 11600, 19872,   180,  4131, 22278,  2557,   625, 18402,\n",
      "          3384,   151,  3401,   117,  2863,   256,  1434,  2178,   117,   122,\n",
      "           117,  1684,   179,   333, 22279,   725,   151,   320,  3279, 10355,\n",
      "         17236,  1112,   146,  7275, 13862, 16454, 18711, 22354,   870,  1816,\n",
      "           304,   692,   179,   495, 12008,   474, 22280,   210,   596,  3283,\n",
      "           371,   117,   170,  5747,  4500,  3292,   117,   230,   388,  4056,\n",
      "         20400,   439,   232,   125,   327, 20027,   122, 18024, 22278,   118,\n",
      "           123,  8817,  4009,  6774,   300,  2187,   125,  1543,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[  101,   860,  1223,   262,   785,  2533,  2994,   122, 20783,   243,\n",
      "           229, 11126,   351,   102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[  101,   495, 12531,  1336, 22280,  1021,  4698,   122,  1685,   481,\n",
      "           122,   331,  3207,   124,   123, 13429,   478, 22280,  1510, 22281,\n",
      "          1176,  2010,   240,   230, 10838, 22278,   117,   222,   856, 19806,\n",
      "           117,   122,   202,   644,   171,   347,  3002,  1165,  6859,  2982,\n",
      "          7719,  3413,   123,   944,   117,   170, 20838,   151,   102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[  101,   625,  2097,   151,   380,   193,  1329,   118,   176,   117,\n",
      "         12183,   256, 17649,  2846,   375,   246,   146,  5546, 10848,   171,\n",
      "           317,   252,   455,   102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101,  1112,  3413,   122,   146,  2780,   221,   311,  1434,  1282,\n",
      "         22282, 18010,   183,   102]])\n",
      "DEBUG: Tokenized sentence 67: tensor([[  101, 22354, 18601,   170,   230,  7729, 10766,   340, 13823,  1337,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[  101,   978, 11913,   260,  6536,   122,  9679,  6374, 20390,   789,\n",
      "           185,   117,   449,  2364, 17318,   117,  2113,   146,  6815, 22280,\n",
      "          2036,  1996,   124,  8586, 22361,   229, 22280, 16512,   363,  3992,\n",
      "           175, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101, 19016,   124,   173,   596,   117,   449,   146,  6815,  1657,\n",
      "          4886,   124,   171, 11628,  5567,   146,   653,   179,   171, 20390,\n",
      "           789,   185,   102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[ 101, 2010, 2364,  325,  572, 1427,  102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101,   229, 22280,  4654,   304,   256,   117,   221,   229, 22280,\n",
      "           327, 22282, 22176,   256,   170, 16001,   366,  2459,   122,   117,\n",
      "          2798, 15356,   125, 11062,   117,  1467,  4051,   125,  1847,   123,\n",
      "          2954,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101,  1112,  1202, 22287,   171,  1690,   117,  3874,  3874, 22354,\n",
      "          3947,  5723,   170, 11948,   852, 10606,   582, 10606,   117,  1021,\n",
      "           125,  9215,   118,   176,  4019,   367,  5366, 13501,   151,   118,\n",
      "          2954,   102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101, 11386, 21423,  5874,   128,   117,   125,   661,  1637,   117,\n",
      "           122,  2364,   176,  6969,   351,   171,  1690,  7817,   118,   125,\n",
      "           118,   969,   102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101,  9871,  3456,  2387, 22278,   146,   766,   180,  2526,   125,\n",
      "           629, 22280,   599,   145,   171, 15273, 13808, 22280,   117,  1815,\n",
      "           495,   146,  7672,   179,   978,   171,   528,   102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[  101,  2010,  2798,   221,   925,   123, 20482,  2443, 22278,  7845,\n",
      "           256,   368,   117, 10415,   214,  1921,  2954,   173,  1105,   171,\n",
      "          5009,   178,   102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101,   180,  8156,  2010,   221,   146,  1956,   619, 22280,  3874,\n",
      "           117,  7343, 15045,  7258,   117, 18691,  8111,   229,  7122, 18597,\n",
      "           117,   331, 22281,   236,  1655,   117,  1004,   170,  4023,  2010,\n",
      "           170,  1719,   123,   271, 20258,   272,   117, 10719,   646,   314,\n",
      "          5895,   117,   123,  1984, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[  101,   495,  7427,   183,   944,   259,   481, 17754,   256,   229,\n",
      "          2047,  6501, 22280,   146,  1961,   141,   171, 12935,  6031,  7258,\n",
      "          4062,  9631,   249,   298,  5585,   128,   102]])\n",
      "DEBUG: Tokenized sentence 78: tensor([[  101,   122,   785,  5646,   524,  6436,   268,  1112,   173,  1105,\n",
      "          2461,  1021,   125,  2745,   117,   271,   229, 11967,   232,   102]])\n",
      "DEBUG: Tokenized sentence 79: tensor([[  101, 22354, 10355, 22287,   259,   532, 14353,   128,   102]])\n",
      "DEBUG: Tokenized sentence 80: tensor([[ 101, 1112,  331, 3207, 3495,  102]])\n",
      "DEBUG: Tokenized sentence 81: tensor([[  101, 22354,  5443,   256,   146,   958,  1025,   173,   388, 11831,\n",
      "           183,   125, 13779,  2662,   322,   102]])\n",
      "DEBUG: Tokenized sentence 82: tensor([[  101,   202,   325,  2010,  1684,   146,   653,   268,  5058,  2364,\n",
      "          1796,   125, 15872,  2949,  5344,   653,   173, 13254,   117,   495,\n",
      "          1941,  3285,   286, 11631,   229, 22280, 14971,   125,  2643,   360,\n",
      "          3194,   210, 15745,  2836, 19163, 12301, 10348,  1054,  4149,   352,\n",
      "           151,   125,   416,   304,   117,   712,  3667,   117,   122,   978,\n",
      "           123,  7989,   171,   636, 10448,  7909,   171, 15273, 13808, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 83: tensor([[  101,   123,  1815,  1112,   327,  3189,   328,  1863,   243,   304,\n",
      "         22354,   495,   230,  9586,   125, 12530,   481,   117,  1283,   194,\n",
      "           387,   117,  5762, 13717,   285,   117,  1821, 13722,   404,   117,\n",
      "          7390,  5137, 13808,   125,  2996,   303,   143,   117,  6003,   125,\n",
      "          5365,   117,  4062, 16450,   304, 22280,   122,  2829,   310,  3578,\n",
      "          8012,   102]])\n",
      "DEBUG: Tokenized sentence 84: tensor([[  101,   123,  2337,   178, 13359,  8060, 11448, 22278,   230,   576,\n",
      "           179,   740,  1011,  2787,  3070,   214,  2684,   538, 14689,  5198,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 85: tensor([[  101,  1863,   243,   304,   958,  1025,   229, 22280,  3235,  1399,\n",
      "           146,   347,  6532,   125,  6759,   122,  3330,   256, 19796,  3514,\n",
      "           146,  1568,   117,   123,  1977,   331,   436,  5723,   240,  1112,\n",
      "           149,   268,  6199, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 86: tensor([[  101,  2010, 15212,   222,   273,  1289,   183,  1014, 21322,   102]])\n",
      "DEBUG: Tokenized sentence 87: tensor([[  101, 19068,   796,   256,   118,   176,   740,   260,  9461,  2208,\n",
      "           117,   179,  2036, 13173,   692,   123,  1916,  8102,   672,   151,\n",
      "           602, 12429,   375,   102]])\n",
      "DEBUG: Tokenized sentence 88: tensor([[  101,   176,  2779, 13256, 22281,   236,   125,   222, 11935,   635,\n",
      "           221,   173, 22175,   943,   102]])\n",
      "DEBUG: Tokenized sentence 89: tensor([[  101,  5267,   256,   138, 19763,  2863,   692,  5411, 22278,   118,\n",
      "          1084,  1112,   180,   118,   311, 21322,   179,   437,  2822,   244,\n",
      "           928,   128,   392,  2010, 21322,   122, 10428, 22279, 22354,   449,\n",
      "           123,  3898,   211,  6021,   285,   390,   304,   229, 22280,   176,\n",
      "         18079,  2836,   170,  7583,   273,   522,   304,   102]])\n",
      "DEBUG: Tokenized sentence 90: tensor([[  101,  9584, 12448,   102]])\n",
      "DEBUG: Tokenized sentence 91: tensor([[  101,   260, 10137,   138,  1128, 22281,  6451,   118,  2036,  1078,\n",
      "           576,   325,  1011,  9850,   822,   375,   256,   240,  1685, 10674,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 92: tensor([[  101,   495,   222,   273,  1289,   183,   333,   247,  3461, 22282,\n",
      "           322, 13793,  4778, 22175, 10348,   118,   176,   123, 10620,  6569,\n",
      "          9258, 22281,   412, 17935,   404,   449,   615,  2010,   260, 13540,\n",
      "          1299,   562,  8101,   692,  1684,   102]])\n",
      "DEBUG: Tokenized sentence 93: tensor([[  101,  1863,   243,   304,  1011,  1078,   576,   325, 13722,   404,\n",
      "           117,   325,  4102, 22279,   251,   123,  1105,  1048,   155,   351,\n",
      "          1078,   576,   325,   170,   146,   347,  5274,   128,  5708,  4285,\n",
      "          6451,   118,  2036,   229, 19164,   340,   366,  1151,  2041,  5949,\n",
      "           146,   347, 17749,  9821,   222,  1340,   282,   994,   260,   675,\n",
      "           942,   470,   230, 16960,  1165,   285,   102]])\n",
      "DEBUG: Tokenized sentence 94: tensor([[  101, 10187,  1165,   256,   102]])\n",
      "DEBUG: Tokenized sentence 95: tensor([[  101,  1564,   117,   146, 13779,   430,  1212,   117,   146, 11152,\n",
      "           599,   145,  1564,   117, 14619,   210, 10850, 18003,  7583,  2954,\n",
      "           123,  4767,   171,  9019, 13793,   102]])\n",
      "DEBUG: Tokenized sentence 96: tensor([[  101,  1084,  1011,   117,  3285,   286,   123,   222,  2242,   117,\n",
      "           577,  1825, 18781, 22305,   246,   260,   877,   842,   117,   146,\n",
      "         11552,  1031,  1281,   178,   498,  9480, 14948,   117,   179,   117,\n",
      "           320,  7670,   117,  1598, 13808,   118,   176,   123,  6374,  3933,\n",
      "          5664,   122,  2000,  2349,   256,   260,  1835,   966,   102]])\n",
      "DEBUG: Tokenized sentence 97: tensor([[  101,   173,   230,   366,  9751,   180,  2375,   117, 18119,   720,\n",
      "           598,   123,   629,  2382,   117,  5009,   178,   122,   146,   317,\n",
      "         22279,   339,   434,   763,  8362,   228,  1270, 18376, 11788,   123,\n",
      "          3524,   304, 22280,   173,  4410,  4098,   125,   222, 17611,   125,\n",
      "           332,   145,   123,  7231, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 98: tensor([[  101,   202,  4745,   180,  4767,  4063,   151,   146,  2082, 14473,\n",
      "         14472,   138, 17704, 22281,   117,   179, 10415,   692,   102]])\n",
      "DEBUG: Tokenized sentence 99: tensor([[  101,  2010,   318, 13793, 15060,  3891,   146,  1151, 14139, 22280,\n",
      "           136,  2638, 14376,   146,   958,  1025,   117, 15568,   214,   118,\n",
      "           176,   171,  5039, 22278,   117,   123,   629,   830, 13133,   138,\n",
      "         14790,   138,   117,   221,  4365,   260,  4141,  6069,  1908,   102]])\n",
      "DEBUG: Tokenized sentence 100: tensor([[  101,   122,   117, 12809,   118,   176,   221,   230,   366, 17181,\n",
      "         22281,   125,   121,   102]])\n",
      "DEBUG: Tokenized sentence 101: tensor([[  101, 22232, 22278,   171,   505,   283,  2010,  2826, 22278,  3933,\n",
      "          5664,   117,   121,   102]])\n",
      "DEBUG: Tokenized sentence 102: tensor([[  101,  2337,   178, 13359,   102]])\n",
      "DEBUG: Tokenized sentence 103: tensor([[  101,  2337,   178, 13359, 15568, 22288,   259,  5708,   221,   146,\n",
      "         12639,   122,   969,   654,   222,  4217,   397,   102]])\n",
      "DEBUG: Tokenized sentence 104: tensor([[  101,  2010,   240,  1977,  4217,  1402,   136, 16795,   118,  2036,\n",
      "           117,   173, 17065, 18612,   735, 22279,   117,   123,  7492, 22032,\n",
      "           351,   179,  2036,  9086,   320,  5872,   102]])\n",
      "DEBUG: Tokenized sentence 105: tensor([[  101,  2010,   240, 16241,  1537, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 106: tensor([[  101,  9396,   123, 19277,   193,  2037,   117, 13449, 11252,   949,\n",
      "          1601,  8551,  2227,   403,   170,   259,   329, 19161, 22281,   298,\n",
      "         19629,   102]])\n",
      "DEBUG: Tokenized sentence 107: tensor([[  101,  2010,   368,   229, 22280,   122,  2996, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 108: tensor([[  101,   123, 17704,   229, 22280,  7093,   121,   102]])\n",
      "DEBUG: Tokenized sentence 109: tensor([[  101,  4351, 14737,   136,   102]])\n",
      "DEBUG: Tokenized sentence 110: tensor([[  101,  7513,   285,   256,  1863,   243,   304,   123,  1858, 17181,\n",
      "           125,   121,   102]])\n",
      "DEBUG: Tokenized sentence 111: tensor([[  101, 22232, 22278,   171,   505,   283,   117,  2389,  7831, 16463,\n",
      "          1612,   221,   146,  1341,   125,   646,   314,  5895,   102]])\n",
      "DEBUG: Tokenized sentence 112: tensor([[  101,  2010,  1977,   136,   146,  7365,   121, 22361,  9480, 14948,\n",
      "           136,  2010,  7365,   136,  2779,  3960,   247,   179,   368,   229,\n",
      "         22280,   122,  7365,   117, 10502,  2010,   122, 14830,   203,  4351,\n",
      "         14737,  1821,   170,  3456,   584,   102]])\n",
      "DEBUG: Tokenized sentence 113: tensor([[ 101,  122, 7365,  117, 1141,  117,  240,  670,  125, 1568,  102]])\n",
      "DEBUG: Tokenized sentence 114: tensor([[  101,   122,  2389,   296,   117,  1369,   418,  1977,  2662,  4178,\n",
      "          1004,   123,  4131, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 115: tensor([[  101,   122,  4837,   256,   123, 14057,   170,   146, 10786,   303,\n",
      "          5444,   102]])\n",
      "DEBUG: Tokenized sentence 117: tensor([[  101,   398,  4549,  2904,   123, 13717,   379,  2034,   117,  3891,\n",
      "           123,  8783,   180,  3049,   304,   712,  1143,   146,  4947,   180,\n",
      "          5489, 22281,   375, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 118: tensor([[  101,   240,  1342,  1341,   117, 22232, 22278,   171,   505,   283,\n",
      "          7513,   285,   256,   123, 22032,   351,  7206, 10372,   966,  2010,\n",
      "          1502,   122,   146,   179,  2036,  2826, 22280,   117,   121,   102]])\n",
      "DEBUG: Tokenized sentence 119: tensor([[  101, 22032,   351,   785,  3264, 13305,   102]])\n",
      "DEBUG: Tokenized sentence 120: tensor([[  101, 10105,   271,   860, 12413,   329,   418,  1977, 22278,  6233,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 121: tensor([[  101,   122,  3985,   151,   202,   347, 11979,   834, 17387, 22281,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 122: tensor([[ 101, 2010, 5747,  576,  123, 1976,  202, 5973,  268,  102]])\n",
      "DEBUG: Tokenized sentence 123: tensor([[  101,   254,  2041,  2010, 11032,  1977, 20075, 22278,   125,  4640,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 124: tensor([[  101,   398,  4549,  2904,   123,  1858,   117, 16044,   557,  9392,\n",
      "           900,   180,  1622,   340,   125,  3400,  1557,   117,   221,  9226,\n",
      "           325,   102]])\n",
      "DEBUG: Tokenized sentence 125: tensor([[  101,   230,  5664,  1016,   331,   202, 15273, 13808, 22280,  6884,\n",
      "         22280,  2010,   122,   271,  2036,  2826, 22280,   117,  7122,  8598,\n",
      "           146,  5980,   244,  5321,   262,   344,   157,   123, 13779, 22278,\n",
      "           117,   122,  1790,   117,  2389,   296,   331,   363, 22361, 11972,\n",
      "           418,  1364,  2041,   247,   125, 19016,  1149,   122,   125,   450,\n",
      "          1382,   784,   102]])\n",
      "DEBUG: Tokenized sentence 126: tensor([[  101,  4789,   175,   320,   317, 22279,   339,   117,   179,   418,\n",
      "           320,  1341,  2461,  2010,  3466,   160, 22361,  3456, 22089,   117,\n",
      "           766,   118,   125,   118, 17912, 22279, 22032,   351, 13754,   240,\n",
      "         19877, 22280,   529, 21347,  2787,  6069,   401,   102]])\n",
      "DEBUG: Tokenized sentence 127: tensor([[  101,   149,  3411,   117, 16143,   118,   176,   222,   739,   390,\n",
      "          2762,   117,   179,  8940,   180, 17935,   404,   102]])\n",
      "DEBUG: Tokenized sentence 128: tensor([[  101,  2010,   146,  4127, 11646,  3848, 17714, 22279,   146,  1143,\n",
      "           185,   418,  4678, 21074,   117,   834,   118,   792, 12268,   136,\n",
      "           122,  2044,   146,   418,   326,   125,   230,  1151,   780,  1378,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 129: tensor([[  101,  2010,  3456,   179,  2684,   311,  5637, 22281, 11510, 12674,\n",
      "         22282,   170, 12338,   229,  4767,   102]])\n",
      "DEBUG: Tokenized sentence 130: tensor([[  101,   495, 22232, 22278, 17897,   124,   117,   179,  6952,   256,\n",
      "           260, 17915,   170,   146,  4127, 11646,   102]])\n",
      "DEBUG: Tokenized sentence 131: tensor([[  101,  2010,  2541, 20933,   578,   123,  9317,   171,  1690,  3848,\n",
      "         17714, 21101, 13397,   178, 17522,  2044,   123, 17935,   404,   117,\n",
      "           598, 14996,   102]])\n",
      "DEBUG: Tokenized sentence 132: tensor([[  101,  2010,   146, 17704,   102]])\n",
      "DEBUG: Tokenized sentence 133: tensor([[ 101, 1996,  123,  331,  522,  102]])\n",
      "DEBUG: Tokenized sentence 134: tensor([[  101,   179,  3752,   514,   364,  2389,   296,   179,   418,   123,\n",
      "         22283,  9349,   125,  1796,   102]])\n",
      "DEBUG: Tokenized sentence 135: tensor([[  101,   958,  1025,  1367,   118,   176,   123, 11471,   125,   646,\n",
      "           314,  5895,   117,   122, 17530,   123,  5940,   221, 15891,  4765,\n",
      "           598,   860,   809,  9466, 14004,   123,  3953,   171, 11775,  1312,\n",
      "           303,  2350,   687,   319,  2160,   954,  5976,   102]])\n",
      "DEBUG: Tokenized sentence 136: tensor([[  101,  2010,  3009, 22280,   179,   538,   629, 22280,  1395, 17551,\n",
      "         22281,   117,  3009, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 137: tensor([[  101,   449,   229, 22280,  1146,   333,   325, 15645,   441,   171,\n",
      "          5790, 13793,   102]])\n",
      "DEBUG: Tokenized sentence 138: tensor([[  101,   260, 20269,   117,  1953,   260, 20269,   102]])\n",
      "DEBUG: Tokenized sentence 139: tensor([[  101,   629, 22280, 11368,   362,   381,  2037,  3141,   117,   179,\n",
      "           222,  1568,   125, 20027,   376,   173,  1105,   117,   122,   179,\n",
      "          4678,  5058, 15702,   180,  2551,   366,  8447,   122,   179,  7707,\n",
      "          8091,  4131,   138,  1884,  7378,   122,   230, 15645,  3269, 18376,\n",
      "           404,  1342,   644,   117,   173,  5288,  1105,   117,   230,  9586,\n",
      "           117,   144,  3249,   117,  4169, 13218,   125, 17554, 22290,   683,\n",
      "          1884,  5635,  2935,   117,   179,   269, 12371,   180, 10105, 18661,\n",
      "           125,  1342,  1652,   125,   230, 17479,   179,  1284,  1699, 22288,\n",
      "           123,   230, 20027,  6650,   125,   525, 18982,   255,   902,  2215,\n",
      "           444,   125,  1354,   367,  2996, 22280,   122,   202,   185,   117,\n",
      "         14914,   117,   179,  3413,   122,   146,  1528,   117,   146,  8562,\n",
      "           122,   179,  2859,  8091,   260,   675,  4639,   252,  7248,  7485,\n",
      "          1350,   146,   179,  8230, 22287,   123, 22283,   240,  3867,  5207,\n",
      "          8679,   260,  8699,   390,  1149,  5980,   138,   125,  1831,   122,\n",
      "          8410,   229,  4067,  2184,  6069,  7909,   549,   524,   870,  1816,\n",
      "           303,   118,  2036,   117,  7343, 15045,  7258, 14914,   117,   179,\n",
      "           117,   176,  5249, 22280,  7967, 22281,   320,  7343,  1312,   303,\n",
      "           117,  1405,  4240,   229, 22280, 15212,  1342, 11935,   635,  6033,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 140: tensor([[  101,   262, 19006,   240,  4127, 11646,   179,   117,  1444,   180,\n",
      "         11214,   392,   221,  5530,   122,  1169, 22281, 13550,   412,  7492,\n",
      "         17897,   124,   117,  7570,   203,   123,  4767,   170,   762, 16760,\n",
      "           125, 12361,   303,   102]])\n",
      "DEBUG: Tokenized sentence 141: tensor([[  101,   260, 17704, 22281,  1632,  6463,   118,   176,   117,   449,\n",
      "         19914,  2044,   173,  6742,  1187,  8757,   102]])\n",
      "DEBUG: Tokenized sentence 142: tensor([[  101,   146,  3848, 17714, 22279, 20482, 15690, 22278,   123,  4303,\n",
      "           180, 19104,   122,  7912, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 143: tensor([[  101,   318, 13793,   117,   146,  1564,   117,   179,  2684,   123,\n",
      "         22283,   176, 14223,   124,   163, 13194, 22280,   202,   347,  6916,\n",
      "         22280,   117, 15568, 22288,   118,   176,   125,   222,  5995, 22280,\n",
      "           122, 20933,   654,   123, 13239,  7521,  2461,   102]])\n",
      "DEBUG: Tokenized sentence 144: tensor([[  101, 20760,   228,  2592,   102]])\n",
      "DEBUG: Tokenized sentence 145: tensor([[  101,  4127, 11646,   495,   854,   125, 22232, 22278, 17897,   124,\n",
      "           222,   466,  5321, 13427,   117,  2582,   234,   117,   785,  6751,\n",
      "           298,   644,  3471, 11351,  5066, 13294, 22281,   117, 10786,   942,\n",
      "         14563,   117, 12141,  4332, 22285,  5680, 14908,   128,   102]])\n",
      "DEBUG: Tokenized sentence 146: tensor([[ 101, 6642,  256, 5747, 7406,  304,  122, 4173,  151,  125, 1105, 9442,\n",
      "          102]])\n",
      "DEBUG: Tokenized sentence 147: tensor([[  101,   123,  7492,   418,  5105,   202,  1423,   180,  4767, 18928,\n",
      "          1337,   102]])\n",
      "DEBUG: Tokenized sentence 148: tensor([[  101,  2010,   123, 22283,   117,  9349, 22281,   229, 22280, 13429,\n",
      "           210,   102]])\n",
      "DEBUG: Tokenized sentence 149: tensor([[ 101, 4332, 2404,  102]])\n",
      "DEBUG: Tokenized sentence 150: tensor([[  101,  6086,   229, 22280,   118, 18661,   118,   179,   118,  2826,\n",
      "         22278,   117,  6086,  3002, 11646,  3848, 17714, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 151: tensor([[  101,  1502,   146,  1950,   391, 12268,   243,   229, 22280,  4750,\n",
      "          2477,  8430,  6205, 22278,   229,  4767,   117,   834,   240,   230,\n",
      "          7924,   136,   102]])\n",
      "DEBUG: Tokenized sentence 152: tensor([[  101,  4286,  5477,   123, 22296,   117,   176,  2134, 22279,   339,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 153: tensor([[  101,   449,  5308,  2765,   117,   179,   229, 22280,   260,  2243,\n",
      "         22281,   117,  3002,  2525, 22279, 17586,   123, 11471,  2010,   176,\n",
      "           347,  1564,   229, 22280,   437, 20482, 15690,   117, 10354, 22032,\n",
      "           252,   222,  2238,  2041,   437,  6456,   305,   384,   117,   834,\n",
      "           118,   792, 12268, 22279,  5127,   125,  1160,   221,   123, 17935,\n",
      "           404,   117,   785,  1316,  7585,   285,   117,  9247,  1552,   412,\n",
      "          8792,   328,  2010,   146,  8792,   328, 14619,   210,  3769,  4678,\n",
      "         21074,   117,   347,   644,   492,   136,   229,  4767,   260, 12338,\n",
      "          5489,  2916, 22287, 11989,   243,   123,  4187,   171,  3848, 17714,\n",
      "         22279,   122,   146, 11775,  1677,   247,   125, 22232, 22278, 17897,\n",
      "           124,   117,   449, 14457,   125,  3508,  6774,   123,  4410,   117,\n",
      "          2113,  9480, 14948,   429,   118,   176,   123,  6374,   230,   661,\n",
      "           304,   320,  7670,   102]])\n",
      "DEBUG: Tokenized sentence 154: tensor([[  101,  1695,   700,   117, 16143,   118,   176,   222,  5546,  6923,\n",
      "          2430,   125,  8625,   138,  2787,   703,   401,   117,   122,   173,\n",
      "          2590,  3794,   118,   176,   123,  8792,   328,   117,   230,  1124,\n",
      "          7137,   549,  5031,   796,   123,  1354,  6592,   252,   785,  8196,\n",
      "          2382,   122, 13086,   125,  2968,   117,   222, 12413,   125,  1224,\n",
      "           343,   170,  1510, 22281,  4484,   793,   125, 11254,   117,   746,\n",
      "          2986,   123,  1034,   956, 22288,   102]])\n",
      "DEBUG: Tokenized sentence 155: tensor([[  101,  3058,   256,   118,   176,  4566,  2277,   117,   221,   925,\n",
      "           123,  4767,   117,  9112,  6205, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 156: tensor([[  101,   122,   117, 20373,   170,  4903,   260,   223,   128,   230,\n",
      "          5223,  8951,   125,  5731,   117, 13086,   125, 10798,   128,   117,\n",
      "          2022,   151,   118,   176,   123,   944,   117,   222,   240,   389,\n",
      "           117,   123,   475, 18006, 22279,   159,   260,  7889,   138,   781,\n",
      "          4387,  1509,   102]])\n",
      "DEBUG: Tokenized sentence 157: tensor([[  101,   123,  3293,   705,   125,  5009,   178,   122, 22232, 22278,\n",
      "         17897,   124,   117,  7719,   117,  1202, 22287,   125,  8792,   328,\n",
      "           122,  4127, 11646,   117,   125,   230,   329,  7882,   852,   524,\n",
      "         10788, 22278,   117,  2052,  1616,   232,   117,   179,  3330,  2123,\n",
      "         22278,  9480, 14948,   122, 11348,   256,   123, 11451,   180,  1105,\n",
      "           117,   122,   325,   125,   230, 13305,   331,  1266,  2787,   703,\n",
      "           159,   117,   122,  1858,   331,   221, 13159, 22282,   117,   122,\n",
      "          1858,   331,   221,   629,   830, 13133,   146,   302,   298,   338,\n",
      "          7485,   143,   122,  4915, 19726,   442,   123,  4768,   102]])\n",
      "DEBUG: Tokenized sentence 158: tensor([[ 101, 1502,  117, 2440, 2166, 4012,  117,  146, 1312,  303,  495, 1684,\n",
      "         1258,  635,  122, 3002, 8805,  102]])\n",
      "DEBUG: Tokenized sentence 159: tensor([[  101,  2010,  3769, 17479, 22281,   125,  1790,   376, 13702, 22281,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 160: tensor([[  101, 10719, 22032,   351,   173,  4410,  4098,   123, 22232, 22278,\n",
      "           171,   505,   283,   117, 12110,   214,   170,   146, 11552,   221,\n",
      "           146,  5184,   183,  4136,  4307,  1165,   243,   125,  8792,   328,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 161: tensor([[  101,   122,  8880,   123, 10415, 22282,   498,   146,  3240,   404,\n",
      "           326,   366,  1124,  7796,   176, 12274,   210,   316, 22280,  1004,\n",
      "           271,   260, 17704, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 162: tensor([[  101,  1112,  1941,   176,   229, 22280,  1519, 21206, 22287,   170,\n",
      "           123,   327,  8625, 22278,  6003,   122,  3049,   304, 22280,   125,\n",
      "          6472, 11666, 12413,   125, 11254,   173,   576,   366, 16900,  2307,\n",
      "           117, 11666, 11967,   986,   230,  4286,  8808,   322, 22354,   700,\n",
      "          5961,   228,   538, 11314,  2650,  1058,   117,   179,  6641, 14533,\n",
      "           171,  9019, 13793,   221,   194,  7629,   578,   260,   675, 13718,\n",
      "          2668,   842,   122,   117,   240,   230,   669,   232, 22280,  2711,\n",
      "           117, 18195,   228,   123, 13142,  2684,   712, 17611, 22281,   123,\n",
      "          3883,   117, 22227,  9449,   125, 12399,   122,   259, 21145, 22281,\n",
      "           298,  7967, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 163: tensor([[  101,  2010,   259,  4004, 12886,   255,   117,   271,  7707, 13028,\n",
      "           146,  7343,   975,  5242, 22280,   730,   945, 22280,   117,   417,\n",
      "          1797,   456, 22232, 22278,   171,   505,   283,   117,   818, 22280,\n",
      "          1516,   176,   818, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 164: tensor([[ 101, 2780,  163,  281, 4146, 4435, 5835,  538,  240, 3165, 2866,  102]])\n",
      "DEBUG: Tokenized sentence 165: tensor([[  101,  2010,   122,   230,   834,   118,   792,  2186, 14418,   792,\n",
      "           260, 17479, 22281,  1485,   125,   610,  1609,   151,   117, 15530,\n",
      "           128,   125, 11586,   117,  6205, 22278,   125,   765,   397,   202,\n",
      "          3173,   303,   117,   123, 13564,  1609,   684,   260,  1690,   404,\n",
      "         12674, 22281,   229,  4654,   304,   102]])\n",
      "DEBUG: Tokenized sentence 166: tensor([[  101,  2010,   123, 22296,   117,   222,  4062,  1224,   319,   185,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 167: tensor([[  101, 15024,   260,   924, 20991,   320,   653,   596,   102]])\n",
      "DEBUG: Tokenized sentence 168: tensor([[ 101, 2010,  122, 2859, 4654, 2227, 2368,  136,  102]])\n",
      "DEBUG: Tokenized sentence 169: tensor([[  101, 16795,   123,   171,   505,   283,   117,  2010,   176,  4654,\n",
      "          2227,   102]])\n",
      "DEBUG: Tokenized sentence 170: tensor([[  101,   146,  1312,   303,   122,   179,   229, 22280,  4178, 22287,\n",
      "          1434,   123,   596,   122,   123,  2856,  1084,   221,  4654,   934,\n",
      "           418,  6889,  1566, 20466, 22281,  2798,   146,  4141, 13793, 13540,\n",
      "          1281, 22278, 22278,   733,  4861,   304, 22280,  8742,   256,   118,\n",
      "          2036,   123,  4410,   102]])\n",
      "DEBUG: Tokenized sentence 171: tensor([[  101,  2010,  2684, 10486, 17704, 22281,   117,  4023,   311, 16759,\n",
      "         22279,  1485,   123,   176,  1434,   210,   125,  9349,   259,  7769,\n",
      "           123,  2822,   210,   118,  7707,  1916,  1927,  3292,   102]])\n",
      "DEBUG: Tokenized sentence 172: tensor([[  101,  1112,   122,  2113,  7122, 17704,  1174,   329,   962, 22281,\n",
      "           375,  7258,   151,  1174,  1084, 22354,   122,   230,  7666,   792,\n",
      "         12268,   117,   123, 17704,   324, 22280, 12223, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 173: tensor([[  101,   230,   576,   117,   173,   179,   572, 22283,   730,  2048,\n",
      "           222,  4004, 12886, 22287,   117,  2113,   311, 15024,   179,   146,\n",
      "          7343,   975,  5242, 22280,  1011,   266,  3285,   286,   117, 10692,\n",
      "         19623,  1063,   148,   122,   146,  1407,   122,   179,   259,  7496,\n",
      "           308,   229, 22280,   176,  4647, 22287,   423,   655,  2866,   117,\n",
      "          4647, 22287,   118,   176,   423,  6747,   298,   532, 14415,   102]])\n",
      "DEBUG: Tokenized sentence 174: tensor([[  101,   229, 22280,  4178,   450,  3309,   300,   136,   102]])\n",
      "DEBUG: Tokenized sentence 175: tensor([[  101,  6086,  1124, 14273,   171,  1640,   136,   102]])\n",
      "DEBUG: Tokenized sentence 176: tensor([[  101,  1502,   123,  1966,   331, 21280,  1112,   139, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 177: tensor([[  101,  1640, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 178: tensor([[  101,   736,   629, 22280,  1112,   139, 20477,   102]])\n",
      "DEBUG: Tokenized sentence 179: tensor([[  101,  7259, 10087,   117, 14914,   143,   117,  8933,   143,   122,\n",
      "          5553,   514,   145, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 180: tensor([[  101,   222,  5452,  2309,   455,  4360, 10161,   229,  1877,  6340,\n",
      "           322,   180,   661,  1144,   795, 14948,  3303,   123,   327,   661,\n",
      "           304,   102]])\n",
      "DEBUG: Tokenized sentence 181: tensor([[ 101, 2010, 4332,  378, 4332,  378, 2010,  785, 1004,  117,  121,  102]])\n",
      "DEBUG: Tokenized sentence 182: tensor([[  101,   360,   232, 22279,   418, 14803, 22287,  1877,   486,   102]])\n",
      "DEBUG: Tokenized sentence 183: tensor([[  101,  2010, 10539,   260,   592, 14192, 10661,   102]])\n",
      "DEBUG: Tokenized sentence 184: tensor([[  101,  2010,   229, 22280,  7258,   117,   262,   230,   661,   304,\n",
      "           171, 22232, 21102,   102]])\n",
      "DEBUG: Tokenized sentence 185: tensor([[  101, 13239,   228,   123,  7104,  2123,   123, 14957,   102]])\n",
      "DEBUG: Tokenized sentence 186: tensor([[  101,   146,   958,  1025, 10375, 13450,  2044,  1112,   179,  1369,\n",
      "          1011,   222,   995,  3093,   124, 22354,   646,   314,  5895,   262,\n",
      "           146,   877,   319,   179,   229, 22280,   176, 17409,   203,   102]])\n",
      "DEBUG: Tokenized sentence 187: tensor([[  101,  1011, 19016,   214,   123, 11471,   117,   122, 19016,   214,\n",
      "          2789,   118,   176,  4412,   102]])\n",
      "DEBUG: Tokenized sentence 188: tensor([[  101,  9480, 14948,   117,   834,  2822,   123, 11675,   117, 10733,\n",
      "           240,  1257,   230, 10032, 17497,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 189: tensor([[  101,  3803,  5105,   118,   176,   240,  6374,  1004,   122,   368,\n",
      "           117,  2798,  1016,  1112,  2684,  9821,   229, 22280,   370,  4428,\n",
      "           243,  3874,   102]])\n",
      "DEBUG: Tokenized sentence 190: tensor([[  101,   122,   222,  3002,  1360,   201, 22354,  8028,   740,   117,\n",
      "           125,   898,   221,   898,   102]])\n",
      "DEBUG: Tokenized sentence 191: tensor([[  101,   122,   117,  2508, 22278,  9530,   508,   125, 11775,  6590,\n",
      "           117, 20582, 22288,   118,   176,   320,  1341,   125,  1863,   243,\n",
      "           304,   102]])\n",
      "DEBUG: Tokenized sentence 192: tensor([[  101,  2779,  4221,  1870, 17522,  2044,   221,  1982,   180,  8932,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 193: tensor([[  101,  2010,   179,  1815,   146,  7093, 22281,   136,   102]])\n",
      "DEBUG: Tokenized sentence 194: tensor([[  101, 16795,   173, 11021,   117,  3791,  5974,   118,   176,   117,\n",
      "           170,   785,  3316,   102]])\n",
      "DEBUG: Tokenized sentence 195: tensor([[  101,  2010,  1977,   136,  1996,  9480, 14948,   117, 16044,   557,\n",
      "         22238,   304, 22280,   122,  6251,  8149,   214,   146, 17749,   102]])\n",
      "DEBUG: Tokenized sentence 196: tensor([[  101,   123,  1858, 17861, 20004,   246,   123, 11471,   170,   222,\n",
      "           298, 12804,  4215,   102]])\n",
      "DEBUG: Tokenized sentence 197: tensor([[ 101, 2010, 1016,  117, 1016,  102]])\n",
      "DEBUG: Tokenized sentence 198: tensor([[  101,   122,   123,  2267,   171, 19317,   175,  1191,   222, 21115,\n",
      "           125, 21692,  2028,   102]])\n",
      "DEBUG: Tokenized sentence 199: tensor([[ 101, 2010, 2798,  240, 1257,  102]])\n",
      "DEBUG: Tokenized sentence 200: tensor([[  101,  2010,   222,   766,  1445, 22280,  5012,   203,  2779,  4221,\n",
      "          1870,   170, 18624,   102]])\n",
      "DEBUG: Tokenized sentence 201: tensor([[  101,  2010,  9349, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 202: tensor([[  101,   179,   122,  3413,   117,  2779,  4221, 16257,   252,   136,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 203: tensor([[  101,  2010,   122,   230,   437, 14036, 22279,   123,  4970,   256,\n",
      "          1623,   679,   259, 10786,   942,   102]])\n",
      "DEBUG: Tokenized sentence 204: tensor([[  101,  2010,  1141,   117,   368,   229, 22280,   122,  2996, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 205: tensor([[  101,  1204,  9480, 14948,   117,  4764, 19309,   348,   118,   176,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 206: tensor([[  101,   449, 14619,   210,   229, 22280,   122,  1084,  3867,  4486,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 207: tensor([[  101,  2010,   179,  5708,   179, 13841,   122,   179, 22000, 22281,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 208: tensor([[  101,  2389,   252,   117,  2389,   252,   117,  9586,   271,   368,\n",
      "          5911,   304,   170,   146, 11628,  5567,   102]])\n",
      "DEBUG: Tokenized sentence 209: tensor([[  101,  2389,   252,   271,   368,   176, 18119,   154,   123, 13872,\n",
      "           180, 11471,   102]])\n",
      "DEBUG: Tokenized sentence 210: tensor([[  101,  4048,   222, 10692, 11437,   339,   117,   146,   644,   492,\n",
      "           171,  2397,   102]])\n",
      "DEBUG: Tokenized sentence 211: tensor([[  101,  9480, 14948,   117,   834,  5510,   587,  8149, 22282,   146,\n",
      "         17749,   117,  2587,   828,   256,   259,  5708,   598,   146,  7365,\n",
      "           122, 16280,  1407,   171,   179,   123,  8588,   421,   117,   123,\n",
      "         18374,   171,   179,   418,  2036, 10355,   102]])\n",
      "DEBUG: Tokenized sentence 212: tensor([[  101,  1112,   646,   314,  5895,   495,   170,  3901, 20882,   122,\n",
      "          1004, 22003,   117,   449,   117,   179,   679,   492,   117,  1065,\n",
      "           179,  3767, 22278,   744,  2036,   229, 22280,   978, 20791,   230,\n",
      "           877,   232,  3661,   125,  7523,   304, 22280,   117,   222,   331,\n",
      "         22000,   455,   123,  1199,  2446, 22281,   236,   117,   625,  1369,\n",
      "           117,   202,  1325,   117,   495,   740,   117, 16079,   555,   154,\n",
      "          2381,   117,   123,   325,  1224,  3533,   117,   123,   325, 12060,\n",
      "           478,   117,   122,   117,  1202, 22287,  1659,  2010,   327,  5495,\n",
      "          9480, 14948,  1695,   117,   291,  3874,   117,  9679,   320,  4863,\n",
      "           171,  5032,   171,   347, 10034,   303,   662,   191,   229, 22280,\n",
      "           229, 22280,  1796, 17478,  5961, 22278,   118,  2036,   271,   260,\n",
      "          1028,   117,  6643, 10343,   122, 22121,   229, 22280,  3283,   371,\n",
      "           271,   259,   124,   321,  2271,   171, 15273, 13808, 22280,   117,\n",
      "           179,   117,  3002,   176,  4621,   692,  3914,  2072,  5510,  1789,\n",
      "           173, 14502,   122,  9958,   125,  4157, 22282, 22354,  7583, 21692,\n",
      "          2028,   125,   646,   314,  5895,   171,   151,   118,  2036,   271,\n",
      "           230, 18009,  2805,   304, 16280,   118,   176,  4878,   251,   117,\n",
      "          6641,   251,   117,   538, 10981,  3015,   125,   390,   304, 10984,\n",
      "         12569,  7485,  4812,   102]])\n",
      "DEBUG: Tokenized sentence 213: tensor([[  101,  1112,   222,  7094,   175,   122,   146,   179,   368,   122,\n",
      "           222,  9855,  4408,   243,  4047,   179,  5488,   785,   117,  2113,\n",
      "           176,  5192,   173,   144,  6562,   122, 17522,   123,  8768, 22278,\n",
      "           222,   374,   326,   102]])\n",
      "DEBUG: Tokenized sentence 214: tensor([[  101, 22354,  4922,  2880,   151, 22280,   117,  8880,   229,  4767,\n",
      "           117,   170,  1315,   474,   117,   682,  2432,  3385,  2010,   146,\n",
      "          4141,   236,  8891,  3671,   122,   146,   176,  3141,  2916,   451,\n",
      "          4596,   128,   102]])\n",
      "DEBUG: Tokenized sentence 215: tensor([[  101,   506,  2044, 11924,   123,   646,   314,  5895,   122, 10000,\n",
      "           123,  7104,  2123,   260, 17704, 22281,   117,  3951,   123,  1078,\n",
      "         13398,   230,  8422,   291,   230,  3661,   291,   222, 22000,   125,\n",
      "         22231,   175,   247,  8043,  1112,   121,   102]])\n",
      "DEBUG: Tokenized sentence 216: tensor([[  101,  2779,  4221, 16257,   252,  1684, 11791,   271,   259,  4157,\n",
      "           207,   117,   179,  7482,   333,  2779,  1941,  1798, 12909,   243,\n",
      "          2010,   318, 13793,   117,   121,   102]])\n",
      "DEBUG: Tokenized sentence 217: tensor([[  101,  1863,   243,   304,   117,   582,  2541,   170,  1921, 21322,\n",
      "           136,  2450, 22278, 22278,  3437,   170,  1039,  2010,   625,   176,\n",
      "           662, 11152,  2811,  2982,   117,   121,   102]])\n",
      "DEBUG: Tokenized sentence 218: tensor([[  101,  4351, 14737,   136,   102]])\n",
      "DEBUG: Tokenized sentence 219: tensor([[  101,   122,  2365,  1684,   229,  8993,   285,  3182, 22278,   230,\n",
      "         13779,  2662,   322,   117,   222,  6775,   117,   221, 21595,   307,\n",
      "           170,   260,   390,  1149,  4486,   764,   522,  4851,   122,  5739,\n",
      "          1149,   117,   449,   179, 22227, 11837,   923, 13734,   796, 22282,\n",
      "           125,  3979, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 220: tensor([[  101,  2010,  4023,   259,  1191,   122,   146,   644,   492,   259,\n",
      "          1442,  2610,  2723, 16305,   117,   170,   222,   418,   326,   125,\n",
      "          9463,   117,   123,  7492, 22032,   351,   625,   128,   682,  4058,\n",
      "           240,   740,   102]])\n",
      "DEBUG: Tokenized sentence 221: tensor([[  101,  4141,   236,  8891,  3671,   117,   123,  1977,   331, 14915,\n",
      "         22287,   240,  1112,   347,   504,  5424, 22354,   117,   495,   390,\n",
      "           303,   125,  4698,   122, 14730,   481,  5923,   157,   117,  1623,\n",
      "          1460,   117,   786,  2525,   125,   730,  4242,   117,  5708,   785,\n",
      "          7769,   117,  9463,   173,  1315,   986,   117,   230,  5223, 11753,\n",
      "           556,   117,  8598,   117,  1719,   194,  5105,  4995,   285,   122,\n",
      "         15908, 11577,   185,   125,  2389, 22279, 22280,   765,  1058, 22280,\n",
      "           117, 13305,   117,  1004, 13305,   117,  7398, 10268,   246,   320,\n",
      "          1423,   180,  3049,   304,   102]])\n",
      "DEBUG: Tokenized sentence 222: tensor([[  101, 11386,   599,  2912,   138, 16467,   122,  8954,   256,   320,\n",
      "         14643, 22280,  1261,  4242,   180,   327,  2004, 22278, 16982,   124,\n",
      "           122,   125,   736,   117,   305,  1608, 12317,  3508,  1721,   170,\n",
      "           146, 12682, 22280, 20957,   122, 11997,   483,   298,   599,  8859,\n",
      "         22281, 12714,   102]])\n",
      "DEBUG: Tokenized sentence 223: tensor([[  101,   625, 17318,   117,   978,   146, 22032,   556,   243,   781,\n",
      "          1499, 22063,   243,  1073,  4349,   125, 21734,   792,  8041,   118,\n",
      "           176,  1364,   498,   146,  7325,   117, 12307,   348,   260,  7663,\n",
      "           170,   260,   877,   842,  7479,   272,   442,  9821, 22287,   260,\n",
      "         11351,   125,   222,  1354,   833,  3897,  1286,   171,   286,   117,\n",
      "           291,  3508, 12190,   243,   170,   123,  1877,   148,   180,   223,\n",
      "         22280,   146,  4081,   366, 10286,   117,   179,  2510,  4322, 22287,\n",
      "           122,  5334, 18444,   271,  9349,   102]])\n",
      "DEBUG: Tokenized sentence 224: tensor([[  101,  1903,   171,  1410,   117, 13380,   117, 13140,   125,  6251,\n",
      "         17932,   138,   117,   170, 17251,   320,  3495,   117,   785, 17275,\n",
      "           293,   122, 15079,   286, 14773,   259,  4966,   117,   123,  1977,\n",
      "          4537,   151,   170,   260,   675,  9886,  1690,   266,  1149,   117,\n",
      "          1031,  4718,   118,  7707,   146,   331,   154,   455,   117,   146,\n",
      "          1403,   159,   122,   259, 22000, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 225: tensor([[  101,   978,  3933,   144, 15266, 13808,   125,   347,   122,  9882,\n",
      "           240, 15872,   387,   102]])\n",
      "DEBUG: Tokenized sentence 226: tensor([[  101, 14971,   366,   333,   912,   470,   117,   366,  7039,   272,\n",
      "          1557,   170,   390,  1149, 18720,   214,  4654,   304,  2010,   229,\n",
      "         22280,  4363,   151,  1896,  2901,  2798,  5995,   251,   117,   449,\n",
      "           202,   644,  1457,  9086,   272,  9461,   117, 15872, 10490,   243,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 227: tensor([[  101,  1021,   785,   179,  4141,   236,  8891,  3671,  2863,   256,\n",
      "         15397,   159,   123,  9480, 14948,   418,  1684,   146, 16278,   151,\n",
      "           117,   123,  1984, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 228: tensor([[  101, 14619, 21813,   203,   942,   146,  5267,   692,   123,   333,\n",
      "           247,  1112,   222,  9196,  2382, 22354,   117, 10355, 22287,   449,\n",
      "         11666,   118,  2036,  1004,   102]])\n",
      "DEBUG: Tokenized sentence 229: tensor([[  101,   146,   176,  3141,  2916, 22280,  5097,   117,  1966,   495,\n",
      "          4970,   378,   180,   681,  2267,   125, 22232, 22278, 17897,   124,\n",
      "           122,   117,   271,  6086,   117,   222,  1903,   191,  2245,  3608,\n",
      "           171, 15273, 13808, 22280,  3874,   117,   240,   210,   117,   978,\n",
      "           171,  1342,  3133, 13793,   146, 17275,   122,   123,  4351, 22282,\n",
      "           124,   712,  4966,   117,   123,  1977,   324, 12298,   340,   331,\n",
      "         13028,  1112, 17531,  2010,   879,  1149,  2010, 13588,  1289, 22354,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 230: tensor([[  101,  7258,   125,  3815, 22280,   117,   125,   222,  3815, 22280,\n",
      "           125, 12588,   117,  1084,   221,   260,  5099,   171, 11522,   314,\n",
      "           117,   582,  9882,  1510, 22281,  1056,   143,   202,   596,   180,\n",
      "         13301,  1375,   146,  4745,   171,   622,  9882,   118,   146,   229,\n",
      "           651,   102]])\n",
      "DEBUG: Tokenized sentence 231: tensor([[  101, 12444,   370,  1821,   146, 10053,   180,  2169,   125,  3103,\n",
      "         22279,  8891,  3671,   117,  3378,   185,   117,   785,  3791,   201,\n",
      "           117,   449,   170,   123, 11451,  1684,  3002,  7629,   154,   102]])\n",
      "DEBUG: Tokenized sentence 232: tensor([[  101, 11386, 14790,   138, 10647,   117,   173,  1250, 16509,   138,\n",
      "           117,  4513,  8264,   117,  1065,   146,  3443, 14837, 22280,   117,\n",
      "           259,   532,   766,  6199, 22281, 17850,  2951,  3514,  4385,   122,\n",
      "          9726,  2935,  2308,   581,   738,   671,   117,   744, 13305,   117,\n",
      "           122, 10881,   123,  3235,  3711,   252,  5708,   125,  3852, 22280,\n",
      "           117,  9585,   122, 10497,   218,  2370,   117, 17749,   125,   854,\n",
      "          2028,  2337,  1009,  5223,   230,   739,  3049,   304,   117, 15914,\n",
      "           657, 12966,   171,  1831,   117, 10786,   942, 15618,  1409,   122,\n",
      "         19613,   117,  8094, 22278,  4336,  4850, 14689,  1797,   508,   122,\n",
      "          3598,   154,   117,   240,   210,   785,  1004, 18051,   117, 18051,\n",
      "           123,   949,   125,   572,   283,   125, 15445,   117,   179,   495,\n",
      "           170,   455,   368,  3791,  2836,   123,  9463,   102]])\n",
      "DEBUG: Tokenized sentence 233: tensor([[  101,  2384,  4741,   117,  1257,   320,   169, 14116,  2009,   123,\n",
      "          2745,  5674,   151,   146,   179,  2589,  1772,   102]])\n",
      "DEBUG: Tokenized sentence 234: tensor([[  101,  1112,   229, 22280,  4708,   256,   123,   327,  3264, 12220,\n",
      "           118,   853,   314,  2010,   122,   146,   347, 10832,   125,   329,\n",
      "           741,   240,  2249, 22281, 15839,   324, 18980,   122, 10832, 22281,\n",
      "           171,  5457,  1021,   240,   123, 22283,  2798,   146,   347,  1289,\n",
      "           382, 22280,   122,   765,  1058, 22280,   572,   283,   125,  3848,\n",
      "           268,   117, 19738,   202, 15273, 13808, 22280,   117,   423,  1407,\n",
      "           316, 19240, 10334,   117,   291,   653,  7814,   890,   243,   366,\n",
      "          1028, 11126,   784,   291,  1004,   179,   176,   495, 15273, 22285,\n",
      "         21817,   291,  1004,   179,   176,   229, 22280,   495, 22354,   229,\n",
      "         22280,   144,  5689, 21307,   170,   259,   532,  5976,   102]])\n",
      "DEBUG: Tokenized sentence 235: tensor([[  101,   229,   577,   304,   495,   376,   286,  2684,   423,  2160,\n",
      "         22282,   117,   222,  1695,  7427,   183,  4256,  9193, 22280,   125,\n",
      "           440,  5345,  1522,   125,   646,   304,   102]])\n",
      "DEBUG: Tokenized sentence 236: tensor([[  101,  1112,  7967,   122,  7967,  4712,   122,  4712,  3848, 17714,\n",
      "         22279,   122,  3848, 17714, 22279,  9778,   122,  9778, 22354,   122,\n",
      "          1011,  1684,   123, 20206,   179,   146,  1010,   215,  2471, 12650,\n",
      "           785,   117,   176,  2243, 22281,   236,   123,  1658,   298,   866,\n",
      "           186,   124,  2817,   102]])\n",
      "DEBUG: Tokenized sentence 237: tensor([[  101,  2010,   123,  7308,   273,   522,   304,   117,   184,   852,\n",
      "           256,   368,   117,   122,  3401, 11314,   243,   529,   223,   128,\n",
      "          5809,  5294,  8849, 22281,  7226,  4878,   486,   230,  9349,  6867,\n",
      "          9769,   117,   179,   331, 18545,   125, 16386,   140,   146,  5562,\n",
      "         22280,   122,  4613, 22282,  3157,  2430,  3495,  1165,  6254,   117,\n",
      "           125,  1977,  3189,   179,  2589,   117,   229, 22280,   259,  5205,\n",
      "           256,  1112,   179,   229, 22280,  4750,  2643, 18237,   303,   143,\n",
      "           123,  3484,  3252,   268,   180,   223, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 238: tensor([[  101, 22354,   449, 14619,   210,   117,   625, 10348,   221,  3285,\n",
      "           140,   260, 11967,   138,   173,  1569,  2760,  2010,   495,  7583,\n",
      "          1111,   522,   304,   229, 22280,   978,  5848, 22281,   229,  3182,\n",
      "         22278,   495, 14466,   122,  8748, 14971, 17226,   125,  9784,   291,\n",
      "         10415, 22282,   117,  3240, 22282,   672,  2895,   229,  2551,   726,\n",
      "          2856,  6969,  7345,   117,   173,   738,  6278,   138,   117, 19016,\n",
      "           214,   146,   347, 13850, 22178,   125,  3049,  8193, 16418,   117,\n",
      "         19738,   229, 11126,   351,   102]])\n",
      "DEBUG: Tokenized sentence 239: tensor([[  101,   229,  4768,   117, 12063,   118,   202,   125,   498,  1149,\n",
      "          3391, 22278,  6628,   117,  1571,  5321,   125,  3196,   326,   185,\n",
      "           117,  7924, 18995,   285,   117, 11294,  9991,   240,  1510, 22281,\n",
      "         16855,  1491,   320, 13227,   303,   117,   466,  2986,   146,  2992,\n",
      "          1309, 13793,   117,   222,  8196,  1927,   314, 18206,   408,  9539,\n",
      "           117,   125,  2987, 13376,   303,   117,  1706,  2557,   117,   170,\n",
      "          3714, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 240: tensor([[  101,  2251,  5630,   259,  3980,  7362, 11242,   117,   260,  4141,\n",
      "           562,   122,  7355,  2841, 20609,   221,   368,   117,  3874,  1021,\n",
      "           117,   240,   210,   117,   271,   222, 17611,   320, 20975,   247,\n",
      "          4565,  1196,   117,   123, 12837,   304,   180, 12495,   117, 19938,\n",
      "           232,   214,   146,   347, 12424, 22280,   125, 13850,  3391, 22278,\n",
      "           122, 13779,  1552,   146,   347,   572,   283,   171,  9691, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 241: tensor([[  101,   173,  1105,   785,  1670, 17714, 22139,   102]])\n",
      "DEBUG: Tokenized sentence 242: tensor([[  101,  9882, 22278,  5546,   154,   102]])\n",
      "DEBUG: Tokenized sentence 243: tensor([[  101,   170,   123, 13109,  4461,   682,   117,   123,  2259,   151,\n",
      "         22280,  1204,   118,   176,   325, 16248,   102]])\n",
      "DEBUG: Tokenized sentence 244: tensor([[  101,   570, 14376,   118,   176,  2044,   146, 14643, 22280,   117,\n",
      "           122,   347,  1149,  5424,   117,   700,   125,   785,   577,  1655,\n",
      "           117,   870,  2961,   146,  7325,   122,   905,  4373,   123,  8032,\n",
      "          3746,  2028, 22290,   933,  1564,  1112,   176,  3189,   143,  4945,\n",
      "           146,  1423,   657,   179,   260,  1176,   311,  3456,  4776,  3503,\n",
      "         10180,   171,  6314, 22278,  7812,   316, 22280,   416,   154, 22354,\n",
      "           149,  3411,   117, 13734, 14995,   230, 15445,   171, 14643, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 245: tensor([[  101,  2010, 11032, 16534,   966,   102]])\n",
      "DEBUG: Tokenized sentence 246: tensor([[ 101,  398, 4549, 2904,  146, 1073, 4349,  102]])\n",
      "DEBUG: Tokenized sentence 247: tensor([[ 101,  122, 9247,  654, 2010,  146,  121,  102]])\n",
      "DEBUG: Tokenized sentence 248: tensor([[  101,   360,   232,   123, 17704,   229, 22280,  8743,   230, 15941,\n",
      "           136,  9480, 14948,   262,   792,   176,   978,   117,  1961,   203,\n",
      "         11935,  2650,   214,  1084,   240,  1839,   180,  1105,   117,   122,\n",
      "          2927,   170,   230,  1448,   102]])\n",
      "DEBUG: Tokenized sentence 249: tensor([[ 101, 1112,  495,  146,  179, 1021,  102]])\n",
      "DEBUG: Tokenized sentence 250: tensor([[  101, 22354,   146,   504,  5424, 15580, 22288,   118,   176,   170,\n",
      "           123,  1448,   122,  8480,   456,   117,   700,   125, 20206,   259,\n",
      "         11454,  1941,  6916,   308,   320,  6793,   179,   146,   958,  1025,\n",
      "           117,   229, 11471,   117,  3229,  2666,   256,   646,   314,  5895,\n",
      "           117,   123, 19284,   373,   171,  1368,  7970,   264,  6632,   122,\n",
      "           125,   736,  5184,   382,  4428,   909,   171, 15273, 13808, 22280,\n",
      "          1112,   180,   327, 12171,   138,  2509, 22354,   271,   123, 10842,\n",
      "           256,   368,   102]])\n",
      "DEBUG: Tokenized sentence 251: tensor([[  101,   637,  1857,   339, 11579,  2044,   221,   123, 17935,   404,\n",
      "           117,   144,  9990,   210,   403,   117,   170,  7672,   123,  8742,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 252: tensor([[  101,  2010,   229, 22280,  7206,  2384,  4741,   117,   229, 22280,\n",
      "          7258,   102]])\n",
      "DEBUG: Tokenized sentence 253: tensor([[  101, 10355,   146, 12361,   175,   117,   449,   146,  7275, 15273,\n",
      "         13808, 22280,  6199,   122,   222,   516,   124,  4149,   172, 11591,\n",
      "          1096,   243,   102]])\n",
      "DEBUG: Tokenized sentence 254: tensor([[  101,   122, 12374,   256,   117,   170, 17275,   117,  1112,   259,\n",
      "          8981,   252,   117,   259,   146,   947,   319,  1462,  1111,   117,\n",
      "           259, 13718,  2387, 22279,   122,   259,   331,  5312,  2337,  2992,\n",
      "          4623,  3527,   735,   371, 22361, 22361,   146,   347,  2277,   125,\n",
      "          4640,  2337,  2992,  4623,   495,   730,  3173,  1125,   243,  2010,\n",
      "          7273,   259, 10811,   395, 11011, 22281,   117,  7273,  5585,   203,\n",
      "           318, 13793,   123,  5961,   529,  7828, 22281,   180,   327, 12171,\n",
      "           138,   202,   434,   455,   366,  5886,   143,   117,  1112,  1011,\n",
      "           173,   893,   304, 22280,   117,   449,  1021,   125,  4412,  1706,\n",
      "           785,   125,   176,   792,   122,  5971, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 255: tensor([[  101, 22354,   870,  1816,   304,   256,   368, 13140,   125, 22000,\n",
      "         22281,  3953,  1409,   102]])\n",
      "DEBUG: Tokenized sentence 256: tensor([[  101, 11234,   171,  1944,   180,  5261, 10780, 13793,   117,  1112,\n",
      "         14619,   210,   229, 22280,  1011, 17800,   243, 22354,   117,   298,\n",
      "          1717,  5934,   117,  1112, 16420,  4270,   173, 18486,   183, 22354,\n",
      "           117,   229,  2567,  1950,   234,   856,  8723, 22280,   117,  1112,\n",
      "          2364,  5063,   123,  5651,   118,  1084,   117,   449,   176,   146,\n",
      "          1104,  9280,   117,  1467,   222, 15152,  5639, 22354, 10789, 18206,\n",
      "           146,  3822,   629, 22280,   599,   145,   102]])\n",
      "DEBUG: Tokenized sentence 257: tensor([[  101,  1112, 10355,   146,   317, 22279,   339,   179,   495,   146,\n",
      "           629, 22280,   505,  1159,   125, 21990,  2204,   117,   173,  2009,\n",
      "          3265, 22354,  5069,   748, 16624,  1789,  3514,   123,  4067, 19449,\n",
      "          8253,   171,  6924,   404,   117,   146,  2873,  1471, 22283,   117,\n",
      "           146,  1744,   141,  1112,  8111, 22278,   125, 14594, 15403,   117,\n",
      "           700,   272,   333,   785, 18633,  1797,   286,   229,  2510, 12288,\n",
      "         22278,   125,   792,  2245,   102]])\n",
      "DEBUG: Tokenized sentence 258: tensor([[  101,   123, 22296,   117,   271,  7583,   117,  7845,   256,   229,\n",
      "         22280, 16974,  1858,  4067,   320,  2265, 13808, 22280,   449,   179,\n",
      "           117,   653,   229, 11126,   351,   117,  1021,   390,   942,   125,\n",
      "           739,  7349,   102]])\n",
      "DEBUG: Tokenized sentence 259: tensor([[  101, 22354,  1822,   151,   118,   176,   123, 11368,   451,  9025,\n",
      "          2754,   117,   125, 12898,   128,   102]])\n",
      "DEBUG: Tokenized sentence 260: tensor([[  101,  1112,  2365,   347, 13071,   117,  1141,  7258, 22354,   122,\n",
      "           117,  2787,  7447,   348,   123,  4410,   117,   170,  5747, 21012,\n",
      "           292,  1112, 15010, 22287,   259,  1118,  4092,   143,   125,  2716,\n",
      "         22278,  2010,   259,  5302, 22279,  3895,  2010,   146,  2397,   180,\n",
      "           449,  5105,   514,   522,   117,   122,  1028,  6514,   138,   125,\n",
      "          4209,  9607,   951,  2365,   123,   327,  4867,   221,   123,  5664,\n",
      "           117,  2365,   102]])\n",
      "DEBUG: Tokenized sentence 261: tensor([[  101,   229, 22280,   176,   706,   514,   702,   102]])\n",
      "DEBUG: Tokenized sentence 262: tensor([[  101, 22354,   122,  1990,  2836,   118,   176,   117,  1462, 22279,\n",
      "           348,   123,  3049,   304,   117, 20089,   102]])\n",
      "DEBUG: Tokenized sentence 263: tensor([[  101,  1112,  1953,   123, 22186,   102]])\n",
      "DEBUG: Tokenized sentence 264: tensor([[  101,  1141,   146,   390,   303,   179,  1165, 12051,   125, 22186,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 265: tensor([[  101,   229, 22280,  1021,   179,  8781, 22282,  2010,   146, 14438,\n",
      "           171,   447,   455,   117,   146,  1465,   364, 22282,   298,  5708,\n",
      "           117,  8128, 13564, 15546,   117,  8206,  6726,  2032,  5344,   102]])\n",
      "DEBUG: Tokenized sentence 266: tensor([[  101, 17878,   117, 14415,   117,   495, 13380,   117, 13380,   117,\n",
      "         13380, 22354,   646,   314,  5895,  1151,   289,   524,   256,   102]])\n",
      "DEBUG: Tokenized sentence 267: tensor([[ 101,  122,  146,  958, 1025, 2798, 2803, 4234,  102]])\n",
      "DEBUG: Tokenized sentence 268: tensor([[  101,   417,  1797,   923,   118,  2036,  8446, 13187,  2799,   498,\n",
      "           146,  8851,  6812,   969,  5723,   260, 15631,   243,   470,   173,\n",
      "           130,  5319,   268,   117,   834, 16462,   102]])\n",
      "DEBUG: Tokenized sentence 269: tensor([[  101,   646,   314,  5895,  1941,   229, 22280, 16995,  3602,   304,\n",
      "         22280,   229, 11471,  5401,   256,   118,   176,   180,  4573,   117,\n",
      "           180,  5065,   117, 16315,   256,   118,   176, 11032,  1532, 14409,\n",
      "           117, 11032,   229,  1858,  4513,   870,   509, 15231,   140,   123,\n",
      "          3049,   304,   122,  2389,  7831,   221,   259,  1143,   117,   318,\n",
      "          2413,  5387,   423,   437,   635,   102]])\n",
      "DEBUG: Tokenized sentence 270: tensor([[  101,  1112,   179, 12361,   175,   102]])\n",
      "DEBUG: Tokenized sentence 271: tensor([[  101, 22354, 20552,   102]])\n",
      "DEBUG: Tokenized sentence 272: tensor([[  101,  5147,   117,   146,   958,  1025,   123,   629,   830, 13133,\n",
      "           118,  2036,   123,  8037, 22278,   171, 19042, 22279,   117,   179,\n",
      "           646,   314,  5895,  5980,   186,   229,  1945,   232,   180, 11471,\n",
      "           117,  8544, 15982,   348,   179,  1112,  2072,   173,  7695,  7909,\n",
      "           125,  7465, 12411, 22281,   179,   123,   327, 22238,   304, 22280,\n",
      "           877,   232,   495,   329,   256,   455,   159,   222,   492,  1196,\n",
      "           170,   259,  3667,   102]])\n",
      "DEBUG: Tokenized sentence 273: tensor([[  101, 22354,  2010,   123, 22296,  2638, 14376,   117,  1439,   183,\n",
      "          1439,   183,   607,   230,  4939,   940,  2010,   123,   125, 19462,\n",
      "           450,  3309,   324,   449,   229, 22280,   333,  3391,   703, 22280,\n",
      "           123,   298, 11935,  1530,   117,  1257,   117,  5900,  4500,  3292,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 274: tensor([[  101,  2010,  1141,   117,   693,  3671,   117,  3568,   694, 13211,\n",
      "           646,   314,  5895,   117, 16044,   557, 14829, 12171,   304, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 275: tensor([[ 101,  122,  730, 2127, 4838, 1353,  118,  176,  102]])\n",
      "DEBUG: Tokenized sentence 276: tensor([[  101,  2010,   123,  4939,   298, 11935,  1530,   102]])\n",
      "DEBUG: Tokenized sentence 277: tensor([[  101, 21289,   146,  1342,   117,   418,  8409,   259, 17084,   122,\n",
      "          1990,   619,   214, 14051,  2065,   117,   271,  1977,  1331,  1112,\n",
      "          2541,  5533, 22354,   646,   314,  5895,  1048,   155,   685,   117,\n",
      "          1767, 13605,   201,  2684,   123, 10957,   298, 13841, 13053,  7583,\n",
      "          8574,  3870, 22278,  3444,   304,   173, 19181, 22288, 16589, 15500,\n",
      "           692,   403,   123,  2847,   180, 11471,   117,   271,   176,  5775,\n",
      "         19181, 10717,   230,  8931,   102]])\n",
      "DEBUG: Tokenized sentence 278: tensor([[  101,  2010,   146,  7275,  4141, 13793, 21990,  2204,   102]])\n",
      "DEBUG: Tokenized sentence 279: tensor([[ 101, 1996,  146,  958, 1025,  102]])\n",
      "DEBUG: Tokenized sentence 280: tensor([[  101,   122,  3285, 13665, 11258,   260,   223,   128,   529,   313,\n",
      "          2245,   483,  1402,   366,  2848,  1149,   102]])\n",
      "DEBUG: Tokenized sentence 281: tensor([[  101,   146,  7275,  4141, 13793, 21990,  2204,  1941,   117,   173,\n",
      "           222, 20105,  2762,  3565,   202,  5492, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 282: tensor([[  101, 11032,   615,   122,   146,  5492, 22280,   171,  3565, 22282,\n",
      "          2265, 22285, 21817,   136,   102]])\n",
      "DEBUG: Tokenized sentence 283: tensor([[  101, 14657, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 284: tensor([[  101,   122, 10692,   654,   146, 12639,   102]])\n",
      "DEBUG: Tokenized sentence 285: tensor([[  101,  2010, 19810, 22335,  2010,  1141, 19810, 22335,   117,   125,\n",
      "           997,   125,  1511,   125,  3692, 22302,   102]])\n",
      "DEBUG: Tokenized sentence 286: tensor([[  101,  1502,  3876, 20105,  2762,  7012,   368,   117,  7892,  2148,\n",
      "         12755,   246,   122,   170,   785, 10502,  1235,   122, 14350,   634,\n",
      "          6471,   125,  2178,   117,   123,  7308,  2018,   122, 13779,  1102,\n",
      "           304,  4939,   298,   684, 19181,   128,   102]])\n",
      "DEBUG: Tokenized sentence 287: tensor([[  101,   646,   314,  5895,   117, 13038,   825,   117, 17400,   117,\n",
      "           425,  3661,   125,  6320,   117,  9784,   146,  1815, 20105,  2762,\n",
      "           229,   681,  2880,   151, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 289: tensor([[ 101,  781, 1352, 1749,  415,  146,  958, 1025,  117,  122,  179,  740,\n",
      "         1790,  122, 1858, 5664,  102]])\n",
      "DEBUG: Tokenized sentence 290: tensor([[  101,  1790,   229, 22280,   176,  3286,  2010,   607, 18206,   325,\n",
      "         13702,   117,   449,   785, 22279,   117, 20373,   170,  4903,   260,\n",
      "           223,   128,   123,  1716, 22278,   171, 19042, 22279,   125,   646,\n",
      "           314,  5895,   122,  5643,   214,   118,  2036,   173,  5530,   298,\n",
      "           211,   683,  3456,  1187,   308,   117, 14000, 13363,  1780,  2010,\n",
      "          3960,   151,   117,  7343, 14914,   117,  3285, 22279,  7482,   146,\n",
      "          5750, 14418, 22280,   179,  6292, 11028,  6621,  4939,   659,   171,\n",
      "           792,   260, 17678, 22281,   117,   259,  2189, 20225,   117,   260,\n",
      "          9480,  2635,   125,  6472,   117,  9721, 18509,   118,   176,   412,\n",
      "          2480,   391,  2290,   298, 11935,  1530,   102]])\n",
      "DEBUG: Tokenized sentence 291: tensor([[  101,   646,   314,  5895, 20045,   203,   123,  3049,   304,   271,\n",
      "         11610,  3138,  4621,   285,   102]])\n",
      "DEBUG: Tokenized sentence 292: tensor([[  101,  2010,   615,   615,  2660,  4500,  3292,  7343,  3695,   117,\n",
      "           229, 22280,   122,   668,  4812,   122,   958,  1025, 16278,   456,\n",
      "           170,   344,   304,  4042,  2896,   102]])\n",
      "DEBUG: Tokenized sentence 293: tensor([[  101, 11972,   331, 16649,   122, 21128,   117,   139, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 295: tensor([[  101,   646,   314,  5895,  4141,   236,   180,  7296,   256, 22279,\n",
      "          6424,  2933, 15633,   246,   123,   549,   117,   123,   233, 21072,\n",
      "           852,   180,  2480,   271,   123,  3002,  8269,  9922, 15736,   146,\n",
      "          1247,   173,   179, 19009,   271,   176,   601,   838,  2836,  1676,\n",
      "          3766,   277,   298, 21616,   117,   366, 11967,   138,   117,   529,\n",
      "         21861,   298,  1690,  7817, 22281,   117,   529,   223, 11198, 22281,\n",
      "           298, 19010,  4829,   271,   176,  3513,   151,   423, 17749,   117,\n",
      "           412,  9463,   117,  1676,   877,   842,   117,   240,   944,   259,\n",
      "           240,   128,  2010, 11972,   117,  7343, 15045,  3695,   102]])\n",
      "DEBUG: Tokenized sentence 296: tensor([[  101,   646,   314,  5895,   179,  1391, 22288,   118,   176,  6767,\n",
      "         16094,  9468,   125,   179,   978,   785,  6938,   102]])\n",
      "DEBUG: Tokenized sentence 297: tensor([[  101,   958,  1025,  2628,   118,   146,   423,  4332,   303,  2684,\n",
      "           123, 17935,   404,  2002,   118,  2036,   230,   466, 14960,   942,\n",
      "         22278,   117,  1367,   118,  2036,   230,  9427,   159,   715,   125,\n",
      "          2042,  7485,   211,   117, 19736,   118,  2036,   230,  2727, 14004,\n",
      "           117,   122,   117,   700,   125,   607,   301,   118,  1340,   494,\n",
      "           840,   243,  1004,   117,   271, 19712,   176,  5057,   271, 22281,\n",
      "          5440, 15443, 22281,  1075,   171, 10262,  2063,   247,   117,   125,\n",
      "           766,   117,  4955,   304,   415,   117,  6149,   505,   277,   303,\n",
      "           173,  6726,   171, 10268,   117, 15891,  5254,  1017,   364,   230,\n",
      "          3524,   304, 22280,   171,   644,   180,  4939,   298, 11935,  1530,\n",
      "           117,  7253,   214,   123,   944,   259,  9250,   501,   180, 10697,\n",
      "           117,  6627,   214,  4484, 20383,   277,   122, 22000, 22281,   117,\n",
      "         18968,   214,   260, 16264,   117, 10310,  1145,   214,   259,  3401,\n",
      "           117,  3898,  1145,   214,   146,   179,  2036,  9821,   125,   325,\n",
      "          3316,   117, 13140,   125, 15952,   271,   176,  5489, 20477,  1875,\n",
      "           221,   222,   739, 15511,  2758, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 298: tensor([[  101,   905,  4373, 21346,   243,  2933, 15633,   246,   146, 12399,\n",
      "           298, 11935,  1530,   117,   170,   123,   327,  2317, 11540,  1719,\n",
      "          7352,   117,   532,  9213,   173,  1270,  4691, 22282,  1415,   388,\n",
      "          2032, 22281,   117,  5747,  5486,   117,   785, 16805,   117,   785,\n",
      "         15699,   125, 17875,   102]])\n",
      "DEBUG: Tokenized sentence 299: tensor([[  101,  6424,  1988,  1990,   974,   146, 13702, 14155, 10565,   173,\n",
      "           179,   176, 20254,   944,   117,   944,   221,   123, 13544,   366,\n",
      "          2139,   122,   221,  1052,  6501,   366,  1027,   117,   529,  1647,\n",
      "           117, 10355,   368,  7892,  8354,   246,   117,  1112,  2259, 22279,\n",
      "           118,   176,   123,   229,   154,   180,  7308, 11394,  6044,  2707,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 300: tensor([[  101, 22354,   495,  2745,   173, 14121,   117,   122,   171,   325,\n",
      "         15045,   117,   122,   171,   325,  2135, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 301: tensor([[  101,  3876,   644,   944,   599,  2037,   692,   117,  1065,   146,\n",
      "         20177,  2684, 13040, 22279, 11314,  2650,   397,   125,  3568,   304,\n",
      "         22280,  4575,   291,   390,   303,   117,  4712,   291,  7967,   117,\n",
      "         16241,  1537, 22287,  1084,  8544,   117,   834,   176,  5110, 14959,\n",
      "           180, 17845,   304,   712,  1143,   229, 22280,   176,  7339, 11451,\n",
      "          7492,   117,  2798, 16450,   304, 22280, 12448,  2010,   260,  1256,\n",
      "          2856,   180,  1373,   117, 14000,   146, 17278,   117,  2976,   118,\n",
      "           176,   146, 12399,   123, 16386,   140,   102]])\n",
      "DEBUG: Tokenized sentence 302: tensor([[  101,  9767, 22278,  5787, 17331, 13665,  3695,   179, 12424,   228,\n",
      "           123,  1589,   395,  3783,   154,   180,  1062,   252,   102]])\n",
      "DEBUG: Tokenized sentence 303: tensor([[  101,  2010, 12931,   102]])\n",
      "DEBUG: Tokenized sentence 304: tensor([[  101,  2010,  1502, 13441, 22278,   118,   176,   122,  2745,  1858,\n",
      "           576,  1160,   629, 22280,  2432, 21616,   117,  2534, 14790,   138,\n",
      "           117,  2534,   102]])\n",
      "DEBUG: Tokenized sentence 305: tensor([[ 101, 2010, 3527,  102]])\n",
      "DEBUG: Tokenized sentence 306: tensor([[ 101,  117, 3527,  102]])\n",
      "DEBUG: Tokenized sentence 307: tensor([[  101, 19821, 14339,   102]])\n",
      "DEBUG: Tokenized sentence 308: tensor([[  101,  2010, 10859,  1089,  6504,   102]])\n",
      "DEBUG: Tokenized sentence 309: tensor([[  101,   122,  4111,  3413, 15212,  6775,  2745,   102]])\n",
      "DEBUG: Tokenized sentence 310: tensor([[  101,   179,   229, 22280,   607,   117,   173,   670,  3933,   243,\n",
      "          1147,  4939,   125,   325, 13702,   102]])\n",
      "DEBUG: Tokenized sentence 311: tensor([[  101,   122,   123,  4410,   171, 12361,   175,  5267,   256,   123,\n",
      "           969,   194,   292,   125,   222, 18974,   102]])\n",
      "DEBUG: Tokenized sentence 312: tensor([[  101,  2010,   146,   179,  2036, 21174,   870,  1816,   934,   117,\n",
      "         14914,   117,   122,   179,   229, 22280,   607,   854,  2028,   179,\n",
      "           117,  4922,  1373,   117,   229, 22280,  2660,   123,   327,  7164,\n",
      "         14497,  8650,   671,   229,  8993,   171, 16936,   303,   102]])\n",
      "DEBUG: Tokenized sentence 313: tensor([[  101,  7628,  9262,  2191, 13717,   591,   117, 10027, 15403, 22281,\n",
      "          4708,   118,   176,  3495, 19623,   148, 22287,   118,   176, 11628,\n",
      "          5567, 22281, 15045, 22281,   117,   202,   475,  4803,   607,   222,\n",
      "           475,  4803,   260,   466,  1705, 19423, 22287,   123,   222, 14701,\n",
      "          3240,   404,  1159, 22280,  2826, 22280,   118,  2036,  2332,  3876,\n",
      "           644,   229, 22280,   607,  2397,   117,   240,   325, 12307,  8916,\n",
      "          3194, 22279,   117,   179,   229, 22280,  3598,   185,   347,  9463,\n",
      "           243,   538,  2241,   326,   143,   117,   529,  6688,  1149,   117,\n",
      "           538, 20860, 22281,   125, 11152,   291,   529,  4103,   125,  7716,\n",
      "          2798,   607,  2606,   117, 17704,   291,   390,   304,   118, 22186,\n",
      "           117,   179,   229, 22280,  7096,  1541,   587, 15182, 22278,   117,\n",
      "           423,  1528,   347, 12232,  6436,   268,  1160,   125,   815, 15249,\n",
      "           324,   102]])\n",
      "DEBUG: Tokenized sentence 314: tensor([[  101,   873,   210,   118,   176, 14563,  5359,  3706,   125, 11152,\n",
      "         13427,   117, 16450,   303,   143,  4444,   128,   125,   144,  2382,\n",
      "           117,  4439,   125,  3334,   170,   449,  1186,  3391, 13793,   125,\n",
      "           313, 19717,   314,   117,  7845,   277, 21321, 22281,   117,  1034,\n",
      "         11742, 13586,  1375,   591, 18901,   157,   180,  1956,   247,   266,\n",
      "           117,   302,  7052, 22281, 13140, 22281,   125, 18878,   117, 13437,\n",
      "           942,   125,   408, 11538,   125,   362,  1653, 22283,   117,   475,\n",
      "         16055, 22283,   117,  9066,  5137,   117,   146,   644,   492,   117,\n",
      "          7343,   934, 22280,  7258,   260, 20364,   118, 10802,   117, 14744,\n",
      "           138,   291,   344,   277,   117, 13467,   170,   259,   532,  2987,\n",
      "         22281,   117,   260,   675, 16388,  4117,   842,   125,   578, 15673,\n",
      "           421,   117,   260,   675, 16388,   374,  3613, 22281,   125,  6472,\n",
      "         22281,   117,   675, 18636,  8625,   138,   125,  2189,  1350,   117,\n",
      "           675,  4004,  2307,   125,   661,  1637,   117,   532,  2492,   145,\n",
      "           173,   944,   259, 17084,   117,   712,   682,   122,   712,  1510,\n",
      "         22281,   173,  1078,   222,   102]])\n",
      "DEBUG: Tokenized sentence 315: tensor([[  101,   122,   860,  2049,   454, 15439,   243,   117, 14021,   125,\n",
      "         13702,   117,  7612,   175,   117,   170,   123,  1923,  6943, 10686,\n",
      "          1378,   122,   146, 16450,   304, 22280,  1519,   175,   117,  7282,\n",
      "           151,   117, 18451,   118,   176,   117,  7889,   268,   125,   898,\n",
      "           117, 18648,   608,  9468,  6542,   123, 12171,   304, 22280,   125,\n",
      "           944,   117,   625,  1369,   138,  1078,   615,   331,  4047,   122,\n",
      "         11562,   173,   898,  2004, 22280,   122,   229,   327, 20233, 22278,\n",
      "         11451,   124,   314,  5895, 18419,   118,   176,   240, 15020,   261,\n",
      "           852,   117,   122,   730,  2127,  4838,   304,   256,   118,   176,\n",
      "           229,  9104,   117,  1151,   289,  9420,   102]])\n",
      "DEBUG: Tokenized sentence 316: tensor([[  101,  2010,   123,  2954,   117,  3449,   146,   958,  1025,   117,\n",
      "         16188, 22278,   118,   176,  1364,   146, 12399,   102]])\n",
      "DEBUG: Tokenized sentence 317: tensor([[  101,  4857, 22287,   118,   176,  1491,   122, 17579,  2755, 11891,\n",
      "          8889,   942, 19408,   358,   117,   170,   123,  3294,   180, 19462,\n",
      "           122,   259, 13724,   486,   171,  1847,   523,   122,   180,  3480,\n",
      "           421,   304, 22280,   117,   179, 16594, 17516, 22278,   298, 11935,\n",
      "          1530,   122, 20889,   171,  1847,   523,   117,   122,   122,   860,\n",
      "           179,  2036,   180,   123,  4939,   102]])\n",
      "DEBUG: Tokenized sentence 318: tensor([[  101,   449,  1004,   117,   659,   118,   176,   123, 16188,  3391,\n",
      "         13793,  2010,  3191,  7649,   117,  4425,   117, 15091,   853,  6522,\n",
      "          2935,   117,   146,   655,   180, 19462,   117,  2745,   123, 21115,\n",
      "           125,  3598,   117,   229, 22280,  8730,   809,  9211,   292,   125,\n",
      "          3568, 22280,   413,  2552, 10896,   117,   179,  6788, 22287,   240,\n",
      "           420,   260, 15063,   117,   259, 12252, 22280,   143,   259,   388,\n",
      "          2032, 22281,   117,  7355,  7591, 22281,   125,  2293, 22278,   173,\n",
      "           230,  3661,  1968,  2745,   117,  2745,   117,  6054,   271,   146,\n",
      "           644,   124,   314,  5895,   969,   654,   222,  4217,   397, 12186,\n",
      "           122,  3171,   125,  3602,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 319: tensor([[  101,  2010,   607, 14619,   210,   117,   221,   259,  3848, 17714,\n",
      "           143,   117,   222, 16341,   118,   125,   118,   176,   492,   117,\n",
      "         17772, 17189,   122,  2840,  2552,   102]])\n",
      "DEBUG: Tokenized sentence 320: tensor([[  101,   122,  3295,   146, 14914,   375,   483,   146,   179,   122,\n",
      "           222, 16341,   118,   125,   118,   176,   492,   136,   102]])\n",
      "DEBUG: Tokenized sentence 321: tensor([[  101,  2010, 15363,   102]])\n",
      "DEBUG: Tokenized sentence 322: tensor([[  101,  2660,   123,  7390,  1076,   125,   229, 22280,  9718,   102]])\n",
      "DEBUG: Tokenized sentence 323: tensor([[  101,  2010,   170,  6251, 17932, 22278,   176,   229, 22280,  4178,\n",
      "           117,  2826, 22278,   117,   179,  2779, 21174,   102]])\n",
      "DEBUG: Tokenized sentence 324: tensor([[  101,  2010, 11032,   117,   240,  3165,   125,  4023,   659,   118,\n",
      "           311,   146,  2748,   173,   229, 22280,   176, 20990,  2387,   117,\n",
      "          3436, 22280,   118,  2036, 12044,  4764, 12172,  1440, 22280,  2453,\n",
      "           180,  4939,   102]])\n",
      "DEBUG: Tokenized sentence 325: tensor([[  101,  1236, 22279,  2010,  1502,  1141,   117,  7258,   102]])\n",
      "DEBUG: Tokenized sentence 326: tensor([[  101,   180, 22280,  3003,  2856,   102]])\n",
      "DEBUG: Tokenized sentence 327: tensor([[  101,   123, 22296,   117,  7343, 15045,  3695,   318, 13793,  7872,\n",
      "           125,   944,   259, 18694,   180,  2420,   230,  5231,   619, 22280,\n",
      "           558,  1817,   415,   125, 20027, 22281,   117,   125, 12301,   117,\n",
      "           390,   942,   117, 14582,   117,  1124,   352,  4242,   122,  2128,\n",
      "         11324, 22281,   117,   179,   194,  2041, 22287,   146, 12399,   179,\n",
      "          2798,   222, 18181,  7967, 22281,   125,  2592,   259,  5497, 22281,\n",
      "           122,   125,  1485,   260, 18516,  1065,   146,  3848, 17714, 22279,\n",
      "          2684, 22280,  7392,  4575,   117,  1169,   769,   117, 10940, 14208,\n",
      "          2208,   529,  3049,  1149, 16031, 22281, 18720, 22281,   125, 14371,\n",
      "           117,   122,   117,   170,  3769, 14371,   117,  7035,   118,   176,\n",
      "          1491, 12446,   653,   229,  1174,   304,   117,   320,   388,  3039,\n",
      "           117,   122,   260, 20027, 22281,   117,   291,  8679,   123, 22283,\n",
      "          3791, 12317,   117,   291,   117,   123,  4222,   452,   272, 17611,\n",
      "           117,  1169,   183,  3139, 22287,   118,   176,   420,   146,  2049,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 328: tensor([[  101,  4366,   118,   176,  2201,   117,   123,  9349,  1984,   117,\n",
      "          5489,   185,   117, 13142,   117, 20141,   117, 11510, 12674,   118,\n",
      "           176,   117,   646, 22290,   252,   102]])\n",
      "DEBUG: Tokenized sentence 329: tensor([[  101,  2010,   646, 22290,   252,   136,  2010, 11032,  1941,  3363,\n",
      "           230, 17704,   179,   504, 11512, 22288,   222,  3848, 17714, 22279,\n",
      "           123,  1224,   319,   185,   117,  1084,   653,   202, 12399,  2010,\n",
      "           123,  1224,   319,   185,   136,  2010,  1141,   117,   123,  1224,\n",
      "           319,   185, 11972,   117,  7343, 15045, 14914,   117,   122,   230,\n",
      "          4446, 22279,   125,  2415,   322,   260, 20027, 22281, 11215,  7888,\n",
      "          1039,  3806, 22281,   125,  6205, 22278,   117,  2803,   830, 22305,\n",
      "           117, 17291,   842,   502,   401,   117,  7949,   303,   721,   122,\n",
      "           146,   325,   102]])\n",
      "DEBUG: Tokenized sentence 330: tensor([[  101,   122,  2745,  3413,   320,  4081,   273,  3939,   201,   285,\n",
      "          9196,  2382,   322,   125,  1510, 22281,  5099,   125,  2293, 22278,\n",
      "           117,   298,  9247,   382,   171,  2241,   326,   458,   122,   180,\n",
      "         11233,  8397,  1282,   415,   313, 10441,   159,   124,   171,  2049,\n",
      "           124,   314,  5895,  8204, 14748,   118,   176,   146,  1342, 16787,\n",
      "           118,   146,   123,  4412, 21541,   117,   302,   214,   118,  2036,\n",
      "           260,   223,   128,   538, 20462, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 331: tensor([[  101,  2010, 15060,   202,  1178,  7382,   180,  4939,  2638, 14376,\n",
      "           146, 12361,   175,   102]])\n",
      "DEBUG: Tokenized sentence 332: tensor([[  101,  2010,   123, 22296,  2510, 14649,   646,   314,  5895,   102]])\n",
      "DEBUG: Tokenized sentence 333: tensor([[  101,  2010,   969,  2599,   118,   176,  3568, 22280,   143,   125,\n",
      "          1798,  2135, 22280,  9993, 22287,   118,   176,   390,  1149,   712,\n",
      "         10115,  9296, 22287,   712, 10115,   259,  6124, 11538, 22281,   987,\n",
      "           769,   118,   176,   577, 11057,   125, 12588,   117, 13449, 15957,\n",
      "         22281,   117,  2727,   321,   117, 18099,   117, 20460,   117,  1063,\n",
      "          5934,   117,  4273,  1708,   125, 14857,  5440, 22287,   118,   176,\n",
      "           388,   545, 18659,  5567, 22281,   125,   822,   747,  3598,  2599,\n",
      "           118,   176,   259,   169, 14116, 22281, 20197,  5082,   188,   256,\n",
      "         12051, 22287,   118,   176,   125,  1364,   260,   313,  2245,   483,\n",
      "          1402,   122,   117,  3448,   117,   170,   739,   717,   980, 22280,\n",
      "          1250,   388,   272,   146,  2401,   680,   415,  4848,   125,  7468,\n",
      "          9258,   102]])\n",
      "DEBUG: Tokenized sentence 334: tensor([[  101,   318, 13793, 13734,   796, 22287,  1485,   260,  5099,   125,\n",
      "          2293, 22278, 22278,   222,   331,   596,   117,  1904,   539,   118,\n",
      "           176,   230, 19016,   671,  4051,   125,  9697, 18356,   222,  3370,\n",
      "         22279,   117,   122,   117,   202,  1423,   171,  3254,   191,  4765,\n",
      "           366, 13388, 22279,   171,   851,  2412, 22279, 18624,   180, 18760,\n",
      "         13793,   117,  3379,   202,  5001,   117, 17579,  2755, 11891,   125,\n",
      "         15698,   117,   123,  3294,   125, 16594, 17516, 22278,   298, 11935,\n",
      "          1530,   102]])\n",
      "DEBUG: Tokenized sentence 335: tensor([[  101, 18817,   125,  1084, 20739,   962,   228,   712,  6171,   423,\n",
      "          1632,   303,   146,  2992, 22288,  4081, 22279,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 336: tensor([[  101,   944, 21829,   303, 15455,   117,   173, 12171,   304, 22280,\n",
      "           123, 19462,   117,   122,  7674, 22287,   146,  1690,  7817,   118,\n",
      "           125,   118,   969,   170,  7672,   366,   316,   492,  1149,   102]])\n",
      "DEBUG: Tokenized sentence 337: tensor([[  101,   607,   230,  9856,  2607,  2855,   143,  5070,  7459, 22281,\n",
      "          2745,   176, 16188, 22278,  6597,  3107,   944,   259,  2201,   117,\n",
      "          1485,   260, 15578,  4275,  4322, 22281,   117,  1485,   260,  4103,\n",
      "           117,  5267, 22287, 17372,   260, 15888,   679,   303,   143,   171,\n",
      "           369, 22281,   148,   102]])\n",
      "DEBUG: Tokenized sentence 338: tensor([[  101,   726,   418,  1178, 18137,   236,   146,  2049,   176, 14777,\n",
      "          1532, 16201,   350,   266,   304, 22280,  3061,   478,   117,  5651,\n",
      "           285,   123,   615,   117,   418,  5651,   285,   123,  4939, 22279,\n",
      "           958,  1025,  5235,  3370, 22279,   339,   102]])\n",
      "DEBUG: Tokenized sentence 339: tensor([[  101,   646,   314,  5895,  8544,  5961,   117,   368,  1316, 22290,\n",
      "          2598,  2010,   125, 19170,   117,   146,  2049, 19994,   122,  3189,\n",
      "          5197,  1820,   117,  8163,   343,   118,   176,   173,  3334,   123,\n",
      "          4768,   298, 11935,  1530,   117, 11621,   124,   118,   176,   117,\n",
      "          3623,   259,  6401,   117,  1174,  1537,   524,   117,   502,  1051,\n",
      "           118,   176,  1078,   222, 18047,   179,  1981,  3767,   652, 12733,\n",
      "         22278,   607,   338,  7052, 22290,   268,   143,   117,  1432, 13644,\n",
      "          1650,   117,  9247,   382,   117,  6742,  1187,  8757,   117,  2510,\n",
      "         13513, 22281,   117, 11989,  5082,   125,  9701,   117, 20860, 22281,\n",
      "         21959,   289,  1270,  3626,   442,   117, 21616, 16776,   128,   117,\n",
      "          1143, 12681,  3895,   117,   854,  2028, 22281, 13745, 22281,   117,\n",
      "          2217,  5167,   308,   449,   117,   125,   695,   373,   117,   271,\n",
      "           240, 16473, 22280,   117,   188,   256, 12051,   118,   176,   146,\n",
      "         12399,   122, 20378,   123, 18760, 13793,  2010,   271,   136,   240,\n",
      "           179,   136,  2010,   180, 22283,   123,  1695,   418, 22280,   944,\n",
      "         12401,   474,   117,  2448,  7831,  1941,   170,   123,  4939,   171,\n",
      "           622,  1457,   117, 10315,   348, 12535,  2445, 22281,   117, 18648,\n",
      "           173,  5076,  3495,   117,   221,   229,  1858,  1434,   744,  1407,\n",
      "          4074, 22279,   146,   958,  1025,   398, 13193, 22279,  2904,  4041,\n",
      "          8725,   117,   170,   123,  3182, 22278,  8742,   102]])\n",
      "DEBUG: Tokenized sentence 340: tensor([[  101,  2010,   449,   240,   179,   644,   492,   176, 20813, 22287,\n",
      "           316, 22280,  7642,  8167,   136,   102]])\n",
      "DEBUG: Tokenized sentence 341: tensor([[  101, 16795,   646,   314,  5895,   102]])\n",
      "DEBUG: Tokenized sentence 342: tensor([[  101,   958,  1025,  2787,  8551, 22288,  2588, 18004,   175,  1510,\n",
      "         22281, 11166, 22281,   125,  6205, 22278,   122,  2927,   118,   176,\n",
      "          2044,   102]])\n",
      "DEBUG: Tokenized sentence 343: tensor([[  101,  2010,   122,  2113,   860,   302,  3711,   268,   117,   240,\n",
      "          4848,   125,  3122,   117,   122,  8562,   179, 12361,   303,   240,\n",
      "          2764,   795,   964,   684,   118,  2036,   125,  1084,   586,   763,\n",
      "           179, 16241,  1537, 22287,   176, 17409,   186,   125,  1105,  2010,\n",
      "           170,  3901,   122,   122,   785,  2557,   418,  4939,   117,  4178,\n",
      "           136,  2010,  2780,   102]])\n",
      "DEBUG: Tokenized sentence 344: tensor([[ 101,  740, 1941,  376,  347,  596,  102]])\n",
      "DEBUG: Tokenized sentence 345: tensor([[  101, 11032, 14657, 22279, 22279,   146, 14876,   151, 22280,  7304,\n",
      "           748,  2044,   146, 11552,   221,   146, 12639,   102]])\n",
      "DEBUG: Tokenized sentence 346: tensor([[  101,  2010,   202,   596,   298, 17448,  4966,   117,  1996,   117,\n",
      "           700,   125,   230, 18827,   117,   495,  1369,   146, 11455,  1950,\n",
      "         22280, 19552, 22280,  1257,   262,   102]])\n",
      "DEBUG: Tokenized sentence 347: tensor([[ 101, 2686,  333,  102]])\n",
      "DEBUG: Tokenized sentence 349: tensor([[  101,   173,   592,   117,  1118, 18384,   102]])\n",
      "DEBUG: Tokenized sentence 350: tensor([[  101,   122, 20881, 13028,   118,   176,   318, 13793, 12110,   117,\n",
      "           179,   547,  1790,   146, 12399,   298, 11935,  1530,   117,  1112,\n",
      "          8993,   171,  5272, 13665, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 351: tensor([[  101, 11032,   117,   259,  1925,  1111, 21761,   228,  1966,  5856,\n",
      "         22278,   222,  1815,  9796,   397,   125,   505,  5505,   117,   179,\n",
      "          1191,   123,  2317, 11540,   117,   271,   176,   706, 15816,   117,\n",
      "           202,  8788, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 352: tensor([[  101,   230,  2880,   151, 22280,   117,   240,   210,   117,   222,\n",
      "          7967,  4173,   286, 10077,  3876,  1247,   146,   347,  7258,   117,\n",
      "           122,   259,  5272,  1044,   117,   179,  1084, 16420,  9442,   117,\n",
      "          9336,   228, 19356, 22279,  2935,   123,  7427,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 353: tensor([[  101,   331,   700,   125, 11330,   122,  2139,   481,   117,   122,\n",
      "           179,   146,  3261,  4141, 22278,  4083,   125,  9606,   122,  2049,\n",
      "           138,  3684, 22288,  8925,   230,  3264,  5675,   117,   123,   615,\n",
      "          3539,   123,   333,  1790,   123,  7308, 13779,  1102,   304,  4768,\n",
      "           298, 11935,  1530,   102]])\n",
      "DEBUG: Tokenized sentence 354: tensor([[  101,   123,  2317, 11540,   304,   456,   173,  1315,   986,   117,\n",
      "           449,   146,  2317,  8476, 13793,   117, 19552, 22280,  3043,  7326,\n",
      "           140,   117,  9673,   117,   173,  5002, 22330,   117,  4902,   123,\n",
      "           179,  1084,   418,  2981,   246, 22279,   180, 22283,  2788,   123,\n",
      "          4939,   117,   179,  4435, 22279,   123,  6320,   122,   146, 10303,\n",
      "           125,  9917,   118,  2036,   102]])\n",
      "DEBUG: Tokenized sentence 355: tensor([[  101,  2010,   125,  2745,  1257,   117,  5429, 18417,   646,   314,\n",
      "          5895,   117,   146,   179,   325,   311,  8296,   122,   123,   327,\n",
      "         14876,   151,   146,  7258,   662,  8805,   376,   230, 14876,   151,\n",
      "           125, 18718,   102]])\n",
      "DEBUG: Tokenized sentence 356: tensor([[  101,  2010, 11032,   146,  7258,   744,   229, 22280,  4970,  3874,\n",
      "         17891,  7283,   118,  2036,   102]])\n",
      "DEBUG: Tokenized sentence 357: tensor([[  101,   146,  1342,  8544, 10907,   578,   834,   325,   996,   303,\n",
      "           143,   117,   625,   117,  8540,   246,   117,   417,  1797,  1429,\n",
      "           944,   123, 17935,   404,   102]])\n",
      "DEBUG: Tokenized sentence 358: tensor([[ 101, 3336, 8410,  940,  102]])\n",
      "DEBUG: Tokenized sentence 359: tensor([[  101,  2010,  2533,  1996,   646,   314,  5895, 11631,   117,  9815,\n",
      "           214,   102]])\n",
      "DEBUG: Tokenized sentence 360: tensor([[101, 122, 125, 681, 344, 304, 102]])\n",
      "DEBUG: Tokenized sentence 361: tensor([[  101,  5083,   118,   176,   146,  5334, 19334,   102]])\n",
      "DEBUG: Tokenized sentence 362: tensor([[  101,   146,   317, 22279,   339,  8940,   123, 11831,   185,   159,\n",
      "           221,  5009,   178,   173,  4410,   331, 14442, 22278,  2010,  1502,\n",
      "           122,   146,   179,  2036,  2826, 22280,   117,  4968, 21510,   117,\n",
      "         10692,   455,  2354, 22279,   170,   260,  4103,   122,  2450, 22278,\n",
      "           118,   260,   173,  5899, 22281,   118,  9824,   591,   117,  3189,\n",
      "         10338, 22287,   136,   102]])\n",
      "DEBUG: Tokenized sentence 363: tensor([[  101,  2010,  7093,   318, 13793,   179, 17891,  1004,   117,  3951,\n",
      "          1256,  9342,   125,  7911,   240,  1078,   230,   102]])\n",
      "DEBUG: Tokenized sentence 364: tensor([[  101,  2010,   693,  3671,   117,   629, 22280,   125,   416,   304,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 365: tensor([[  101,  2397, 11972,   122,  5028,   122,  1945,  2010,   893,   304,\n",
      "         22280,  2557,  2010, 20933,   154,   176,  2456, 21788,  1659,   117,\n",
      "           260,   504,  4242,   376,  4062,  6151, 22290,   117,  4062,   302,\n",
      "           303,   122,   229, 22280,   629, 22280, 10336, 20733, 22281,   412,\n",
      "         13219,  2028,   102]])\n",
      "DEBUG: Tokenized sentence 366: tensor([[  101,  3295,   777, 22279,   229, 22280, 16457,   125,   333,   222,\n",
      "          9463,  6436,   268, 13278,   117,   449,   102]])\n",
      "DEBUG: Tokenized sentence 367: tensor([[  101,  2010,  7674, 22287,   118,   176,   118,  2036,  9751,   221,\n",
      "           146, 16972,   117,  8028,   146, 19317,   175,   102]])\n",
      "DEBUG: Tokenized sentence 368: tensor([[  101,   122,   117,  1016,   117, 10415,   214,   117,  5063,   123,\n",
      "         17935,   404,   117,   582,  1941,  2072,   123,  9317,   102]])\n",
      "DEBUG: Tokenized sentence 369: tensor([[  101,  4141,   236,  8891,  3671,   122,   176,  3141,  2916, 22280,\n",
      "          5097, 15724, 22287,   260, 17704, 22281,   117, 19179,   170,   230,\n",
      "         13779,  2662,   322,  1078,  7164,   183,   179,  7707,  2218,  6451,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 370: tensor([[  101,   646,   314,  5895,  5695, 12119, 22278,   171,  1690,   117,\n",
      "           170,  7672,   171,   958,  1025,   179,  2036,  8925, 22278,   222,\n",
      "           411,   702,   320,  1341,   171,   347,   102]])\n",
      "DEBUG: Tokenized sentence 371: tensor([[  101, 16143,   118,   176,   449,   193,   702,   260,   516,  2208,\n",
      "           122, 13449,   391,   117,   712,  1716,  2552,   117,   146,  5334,\n",
      "         19334,  8833,   102]])\n",
      "DEBUG: Tokenized sentence 372: tensor([[  101,  2010, 14914,   117,  2638, 14376,   146,   317, 22279,   339,\n",
      "           117, 12625,   734,   578,   170,   146,  6742,  1783,   230,   395,\n",
      "          2916,   125,   222,  4102, 22280,   125,   316, 14881,   304,   102]])\n",
      "DEBUG: Tokenized sentence 373: tensor([[  101, 19100,   320,  1528,   171,  7275,  1112,  4102, 22280,   171,\n",
      "         15273, 13808, 22280, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 374: tensor([[  101, 14619,   210,   146, 17221,   240,   123, 22283,  1112,  4102,\n",
      "         22280,   302, 21510, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 375: tensor([[  101, 19100,   117,   179,  3411,   229, 22280,   607,  1796,   125,\n",
      "           329,   102]])\n",
      "DEBUG: Tokenized sentence 376: tensor([[  101,   122,   230, 22127,   180,  2480,  2010,   229, 22280,   122,\n",
      "         11775,   102]])\n",
      "DEBUG: Tokenized sentence 377: tensor([[ 101, 1996,  646,  314, 5895,  117, 2636,  118, 2036,  123, 6272,  102]])\n",
      "DEBUG: Tokenized sentence 378: tensor([[  101,   785, 15107,  1212,   117,   449,  4048,   118,   311,   222,\n",
      "          1597, 12518,   102]])\n",
      "DEBUG: Tokenized sentence 379: tensor([[  101,  2010,   122,   125, 11764,   340,  2010, 14000, 22232, 22278,\n",
      "         17897,   124,   102]])\n",
      "DEBUG: Tokenized sentence 380: tensor([[  101,   659,   118,   176,   125,   316, 14881,   304,   125,  4783,\n",
      "         22280,   122, 10168,   102]])\n",
      "DEBUG: Tokenized sentence 382: tensor([[  101,  4351, 14737,  5608,  9480, 14948,   117, 12110,   214,   221,\n",
      "           259, 10786,  6544,   102]])\n",
      "DEBUG: Tokenized sentence 383: tensor([[  101,   629, 22280, 12837, 19161, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 384: tensor([[  101, 22032,   351,   117,   170,   123,  9463, 13086,   117, 10355,\n",
      "          3378,   123, 22232, 22278,   171,   505,   283,  2010,  1502,   117,\n",
      "          7122,  8932,   117,   625,  3804, 22282,   125, 13544,   170,  4569,\n",
      "          7595,   117,   229, 22280,   376,   325,   171,   179,   176,  9050,\n",
      "          1988,   146,  7148,   179,  2036,  2826, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 385: tensor([[  101,   122,   785,  6829,   162,   122,  1519,   539,   118,   176,\n",
      "           170,   146,   179,   123,  9349,  2036,   180,   334, 22361,  1335,\n",
      "         22361,   146,   644,   117,   305, 16397,   118,   311, 13417,   592,\n",
      "           118,  7911,   240,   230,  3061, 16257,   252, 18493,   117,   449,\n",
      "         14619,   210,  4207,   176,   792,   123,  1706,   179,   146,  2397,\n",
      "          7904,   207, 14995,   102]])\n",
      "DEBUG: Tokenized sentence 386: tensor([[  101,  1502,   318, 13793,   607,   125,  2822,   230, 15799,   532,\n",
      "         12206, 21102, 22281,   117,   179,  1971,  2803,  2599,   123,  8488,\n",
      "           117,   123,   785, 15545,   130,   117,   271,   607,   240,   123,\n",
      "         22283,   117,  4438,   179,   117,  3002, 10142,   320, 10169,   117,\n",
      "           418, 22280, 18648,   202, 16960,   303,   122,   229,  1611, 21510,\n",
      "           136,   102]])\n",
      "DEBUG: Tokenized sentence 387: tensor([[  101,  4023,   437,  3039,   117,  6884, 22280,  2684, 22269,   229,\n",
      "          5594,  3292,   125,   222,  7045, 22280,  2010,   271,   146,  7148,\n",
      "           362, 22282,   154,   102]])\n",
      "DEBUG: Tokenized sentence 388: tensor([[ 101, 5069,  748,  123, 1858,  102]])\n",
      "DEBUG: Tokenized sentence 389: tensor([[  101,  2010,   146, 22296,  1966,   117,  2798,   176,  3887,   260,\n",
      "          1176,   117,  4023,   311, 16759, 22279,   538,  3486, 22282,   444,\n",
      "           117,  2684,   176,  2064,  5167,   201, 22279, 22232, 22278,   171,\n",
      "           505,   283, 13754,   229,  9463,  2010,   329,   418,   117, 14000,\n",
      "           117,  1977,  1941,   146,  4970,   123,  1364,   146, 12617, 19385,\n",
      "           157,  1831,   125,  4141,   236, 15045,  1604,   102]])\n",
      "DEBUG: Tokenized sentence 390: tensor([[  101,  2010,   229, 22280,   179,   588, 22314, 22361,   173,   644,\n",
      "           123,  9349,  2243,   123,  1154,   102]])\n",
      "DEBUG: Tokenized sentence 391: tensor([[ 101, 1257,  418,  176, 3285, 1825,  954, 5708,  102]])\n",
      "DEBUG: Tokenized sentence 392: tensor([[  101,   449,   122,   146,   179,  6124, 13793,   376,   146,  1342,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 393: tensor([[  101,  4303,   118,   176,   785,  1004,   785,  1004,  7242,   243,\n",
      "           785,  7104,   947,   366,   675, 18237,   303,   143, 14837,  1212,\n",
      "           180,  1768,   151, 22280,  2547,   593,   117,  7122,  8932,   117,\n",
      "           179,   659, 10303,   102]])\n",
      "DEBUG: Tokenized sentence 394: tensor([[  101, 10801,  2684,   102]])\n",
      "DEBUG: Tokenized sentence 395: tensor([[  101,   122, 22032,   351, 11021, 22288,  3933,  5664,   123, 13219,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 396: tensor([[  101, 22232, 22278,   171,   505,   283,  3378, 22288,   259,  5708,\n",
      "           117,   122,   398,  4549,  2904,   483,   352,  1780,  2010,  4023,\n",
      "          2036,  7211,   173,  1284,   117,   144,  2640,  2598,   301,   222,\n",
      "          7257, 22282,   125, 14371,   179,   176,  9721,  2599,   102]])\n",
      "DEBUG: Tokenized sentence 397: tensor([[  101,   259,   662,  2524,   145, 16083,   228,   118,   176,   298,\n",
      "           532,  4488,   102]])\n",
      "DEBUG: Tokenized sentence 398: tensor([[ 101, 2010, 9317, 2850,  102]])\n",
      "DEBUG: Tokenized sentence 399: tensor([[ 101, 4067, 5510, 1375,  102]])\n",
      "DEBUG: Tokenized sentence 400: tensor([[  101,  9247,   654,  2044,  4141,   236,  8891,  3671,   117,  4273,\n",
      "          7039,   243,   259,  8197,   298, 12141,   102]])\n",
      "DEBUG: Tokenized sentence 401: tensor([[  101,  2337, 21308, 22288,   125,  3866,   260, 17704, 22281,   117,\n",
      "           179,   176, 16924, 14533, 22243,  3832,   221,   123,  4767,   102]])\n",
      "DEBUG: Tokenized sentence 402: tensor([[  101,   149,  3411,   117,  3033,   146,  1564,   117, 10940,   146,\n",
      "          4127, 11646,   423,  2252,   102]])\n",
      "DEBUG: Tokenized sentence 403: tensor([[  101,  8940,   123, 20933,   578,   259,  1151,  6644,   412,  9463,\n",
      "           122,   117,  1821,  6867,   926,  5961,   117,  5222,   179,  1112,\n",
      "          3866, 22278,   146, 13503,   124, 22280,  2684,   146,  1338,   180,\n",
      "          4768,   739,   117,   122,   179,   146, 13503,   124, 22280, 14195,\n",
      "         22278,   221,  2389,   159,   339,   298,  1717,  5934,   122,  1821,\n",
      "           179, 20482,  2028,   146,  8788, 22280,   180,   610,  2204, 22354,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 404: tensor([[  101,  6775,  3413,   117, 14747,   368,   653,   146,  3848, 17714,\n",
      "           747,   221,  1839,   102]])\n",
      "DEBUG: Tokenized sentence 405: tensor([[  101,  1112,  6952,   117,  1143,   185,  2541, 19866,   146,   423,\n",
      "           117,   179,   744,  1790,   437,  3285,   143,   173,  5973,   268,\n",
      "         22354,  2533,  6658,   228,   785,   146,  1312,   303,   171,  1564,\n",
      "           117,   122, 10415,   288,   498,  6086,  5291,   125,  9713,   304,\n",
      "         22280,   117, 13173,   214,   146, 14837, 22280,   171,  4062,  3695,\n",
      "           122, 11314,  2650,   397,   125,  5009,   178,   102]])\n",
      "DEBUG: Tokenized sentence 406: tensor([[  101,   180, 22283,   123,   230,  5314, 13156,   923,   118,   176,\n",
      "           260,   390,  1149,   117,   420,   739, 11037,  7456,   261, 10786,\n",
      "          3103,   122, 12631,   942,   102]])\n",
      "DEBUG: Tokenized sentence 407: tensor([[  101,  2010,  1863,   243,   304,  9247,  5723,  9480, 14948,   117,\n",
      "          2535,   229, 22280,   388,   172,   483,   125,  1160,   117, 16143,\n",
      "           136,   102]])\n",
      "DEBUG: Tokenized sentence 408: tensor([[  101,  2010,  1141,   117,  7122,  1069,   117,  2678, 22283,   125,\n",
      "          8264,   102]])\n",
      "DEBUG: Tokenized sentence 409: tensor([[  101,  2389,   252, 22279, 10996,   682,  9671,   249,   221,  2036,\n",
      "          4640,   222,  7513,  6436,   268,   102]])\n",
      "DEBUG: Tokenized sentence 410: tensor([[  101,  2010,  1141,   117,  1141,  2779,  4221, 16257,   252,   117,\n",
      "          3605,   249,   121,   102]])\n",
      "DEBUG: Tokenized sentence 411: tensor([[  101, 22232, 22278,   171,   505,   283,   117,   229, 22280,  1114,\n",
      "         22279,   125,  4915,  3867, 10687,   123,  6151,   300,   644,   125,\n",
      "           629, 22280,  4141, 13793,   102]])\n",
      "DEBUG: Tokenized sentence 412: tensor([[  101,  7273,   516,   154,   125,  1354,   833,  3897,  3103,   117,\n",
      "          2389,   296,  1084,  2010,  3605,   249,   117, 16450,   304, 22280,\n",
      "          2010,  2337,   178, 13359,   117,   229, 22280,   176,  6969,   304,\n",
      "           180, 14900,   102]])\n",
      "DEBUG: Tokenized sentence 413: tensor([[  101,  2010,  4351, 14737,   117, 15891,   304,   118,   176,   180,\n",
      "          9349,   102]])\n",
      "DEBUG: Tokenized sentence 414: tensor([[  101,  8098, 22279,   532,  1256,  4698,   255,   102]])\n",
      "DEBUG: Tokenized sentence 415: tensor([[  101,  2010,  2389,   296,   117, 10719,   146,   176,  3141,  2916,\n",
      "         22280,  5097,   117,   179,   260,  2571,   390,  1149,   117,   221,\n",
      "           176, 13156,  8165,   102]])\n",
      "DEBUG: Tokenized sentence 416: tensor([[  101,   629, 22280,   376,  1086,   145,  2010,  1112,  7119,   230,\n",
      "           331, 21403,  1519,   118, 10497,  1485,   102]])\n",
      "DEBUG: Tokenized sentence 417: tensor([[  101, 22354,  8825,   654,   146,   958,  1025,   117,   144,  2746,\n",
      "           146,  4351,   339,   272,   170,   123,   327,   877, 16582, 22279,\n",
      "          5948,   304, 22280,   117,  1112,   122,   146,  5253,  2589,  2779,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 418: tensor([[  101, 14439,  2337,   140,   300,   102]])\n",
      "DEBUG: Tokenized sentence 419: tensor([[  101, 22354,   122,   117,  4230,   230,  6742,  1187,  4419,  8742,\n",
      "           117,  2927,   118,   176,  9079, 18376, 11788,   122,  9939,   118,\n",
      "          2036,   170,   388, 20494,  5092,  1112,   222,  1815,  1135,   229,\n",
      "           327,  5134, 22278,  9317, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 420: tensor([[  101,  2010,  1447, 14914,   117,  1447,   240,  7583,  5334,  2103,\n",
      "           324,   117,  1996,   102]])\n",
      "DEBUG: Tokenized sentence 421: tensor([[  101,  1447, 10363, 19356,   140,   118,   176,   222,  1695,   102]])\n",
      "DEBUG: Tokenized sentence 422: tensor([[  101,   646,   314,  5895, 17400, 22238, 12505,   102]])\n",
      "DEBUG: Tokenized sentence 423: tensor([[ 101, 1151,  289,  524,  256,  102]])\n",
      "DEBUG: Tokenized sentence 424: tensor([[  101,   240,  1707, 22278, 15020,   261,   852,   117, 16795,   176,\n",
      "          3933,   366,  2615,  3146, 22281,  1112,  4750,   222,  2724,   221,\n",
      "          6082,   118, 10497,   123,  1105, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 425: tensor([[  101,   260,  9344,   579,  8800,   228,  2044,   117,   170,  1415,\n",
      "          1510,   537,   721,   125, 10089,   151,   102]])\n",
      "DEBUG: Tokenized sentence 426: tensor([[  101,   368,   117,  2699,   246,   598, 14996,   117,  2628,   118,\n",
      "           260,  2684,   260,  5886,   143,   117,   582, 19575, 22287,   117,\n",
      "          1369,   653,   117,  3047,   102]])\n",
      "DEBUG: Tokenized sentence 427: tensor([[ 101, 2927, 1695,  700,  102]])\n",
      "DEBUG: Tokenized sentence 428: tensor([[  101,  2010, 12401, 22278,   118,   176,   117, 14914,   117,   338,\n",
      "           185,   125, 21615,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 429: tensor([[  101, 12910,  2598,   118,  2036,  5009,   178,   117,   179,   146,\n",
      "         11690,   125,   766,   102]])\n",
      "DEBUG: Tokenized sentence 430: tensor([[  101,   259, 17516,  1981,  2765,   170,   146,  1831,   123, 10381,\n",
      "         21158,   102]])\n",
      "DEBUG: Tokenized sentence 431: tensor([[  101,   646,   314,  5895, 15982,   203,   179,  1141,   117,  7837,\n",
      "           654,   118,  2036,   123,   223, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 432: tensor([[  101,  1112,  7799, 13674,   117,   122,  9773, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 433: tensor([[  101,  2010,  2684, 22032,   252,  2389,   296,   176,  3804, 22282,\n",
      "           125,  1569,  5664,   117,   849, 22279,   423,  4127, 11646,   117,\n",
      "           368,  4678,   155,   229, 17935,   404,   102]])\n",
      "DEBUG: Tokenized sentence 434: tensor([[  101,   449,  1981,  2765,  4552,  1084,   123,  8792,   328,   122,\n",
      "          8170, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 435: tensor([[  101,  7282,  1004,   124,   314,  5895, 15313,   118,   176,   202,\n",
      "          3147, 12989,   456,   118,   176,   117, 10049,  2071,   222, 14883,\n",
      "         19450,   122, 20933,   654,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 436: tensor([[  101,  6540,   240, 19877, 22280,   222, 11802,   157,   449,   117,\n",
      "           202,  1338,   180,   681, 19651,   387,   117,   260,  1877,   269,\n",
      "          4323,   176,  2036,  3030, 15736, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 437: tensor([[  101,   331,  1703, 22288,   123, 20262,   102]])\n",
      "DEBUG: Tokenized sentence 438: tensor([[  101,   318, 13793, 10733,   222,   483, 22287,   118,  2765, 18522,\n",
      "           117, 11258, 15397, 20383,   178, 12631,  1353,   118,   176,   712,\n",
      "         17343,  1044,   122,   117,  1075,   179,  3179,   298,  3391,   618,\n",
      "           383,  3820,  2811,   644,  2036, 14408, 10717,   146,  5791,   183,\n",
      "           117,  2251, 22282,   155,   685,   102]])\n",
      "DEBUG: Tokenized sentence 439: tensor([[  101, 17226,   117,   123,  7666, 18175, 17811,   117,   614,   210,\n",
      "         20262,   256,   117, 18648,  8807,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 434 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.005489915263696155\n",
      " Coesão Score Final: 0.5027449576318481\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'contudo', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'uma vez que', 'quando', 'enquanto', 'se', 'caso', 'apesar de', 'alem disso', 'isto e', 'todavia', 'nem', 'ou', 'ora', 'quer', 'seja', 'senao', 'bem como', 'como', 'quanto', 'igualmente', 'ao passo que', 'desde que', 'antes que', 'porque', 'uma vez que', 'por outro lado', 'no entanto', 'com efeito', 'alias', 'a proposito', 'principalmente', 'nao so', 'bem como', 'mas tambem', 'tanto', 'quanto', 'se nao', 'por isso']\n",
      " Número de conectivos: 46\n",
      " Número de sentenças: 440\n",
      "======================\n",
      "Resultados para preprocessado_o_mulato_aluisio_azevedo_cap_4.json:\n",
      "{'coesao_score': np.float64(0.5), 'conectivos_encontrados': ['e', 'mas', 'porem', 'contudo', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'uma vez que', 'quando', 'enquanto', 'se', 'caso', 'apesar de', 'alem disso', 'isto e', 'todavia', 'nem', 'ou', 'ora', 'quer', 'seja', 'senao', 'bem como', 'como', 'quanto', 'igualmente', 'ao passo que', 'desde que', 'antes que', 'porque', 'uma vez que', 'por outro lado', 'no entanto', 'com efeito', 'alias', 'a proposito', 'principalmente', 'nao so', 'bem como', 'mas tambem', 'tanto', 'quanto', 'se nao', 'por isso'], 'num_conectivos': 46, 'proporcao_conectivos': 0.005, 'similaridade_media': np.float64(1.0), 'num_sentencas': 440}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,   607,   481,  9849, 22288,   202,  2992, 22288,  2922,   485,\n",
      "          1451,   230,   940,  4595,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,  1065,   146,  2182,   125,   327, 10681,  2524, 22280, 16241,\n",
      "          1537, 22287,  2036,  5998,   146,  2992,   552,   262,  2047, 16588,\n",
      "           123,  4685,   298, 10557, 22280,   143,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,  1204,   118,   176,   123, 12563,   298, 21145, 22281,   123,\n",
      "          3270, 22278,   298, 14720,   122,   146, 19480,   326,   298, 20743,\n",
      "         22281,   173, 16232,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[ 101,  495, 8598,  122,  928, 1337,  102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   924,  2134, 10491,   340, 22281,   117,   179,   176,  1752,\n",
      "          2227,   271,   123, 12252,   173,  4372, 22280,   125,  6603,  3141,\n",
      "           552,   682,   730,  3173,  2247,   179,   333, 22279, 20728,  2097,\n",
      "           117,   271,   146,  9849,   125,   969,   202,   369, 22281,   148,\n",
      "           171,   644,  9833,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,  1977,   229, 22280,   176, 18127,   180,   527,  3004,   151,\n",
      "          9461, 22282,   339,   117,   179,  7570,   203,   146, 16315,   310,\n",
      "           180,  3952,   271,  9745, 18377, 22279,  2309,   117,   122, 17896,\n",
      "           203,   118,   176,   125, 19170,   202,  1423,   171, 17579,  2755,\n",
      "         11081,   179,  4787, 22278,   146,   347,   118,   572, 22290,  5967,\n",
      "           136,   978,   740, 13417,   481,   625,  4169,   123,   681,   576,\n",
      "           229,  2707,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   229, 22280,   123, 18574, 22287,   122,  2044,  5984,  4298,\n",
      "           944,   170,  4042, 15182,  1876,   303,   143,  9365,   180,   739,\n",
      "         19173,   171,   644,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101, 10355,   118,   176,  5747,  5664,   179,   229, 22280, 20206,\n",
      "           244,  2535,   117,  1502,   123,   347,   596,  4945,  5835,   123,\n",
      "          3295,   117,   834,   259, 16272,   850,  3002,  8990,  5198,   125,\n",
      "           179,  7632, 12232,   118,  1084,   259, 21528,  1044,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[ 101,  527, 3004,  151,  495,  438, 1165,  122,  978,  173,  327, 4067,\n",
      "          230, 7492, 1517,  539,  117, 4970,  256,  117,  121,  102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,  5101,  1817,   449, 17738, 13808, 22281,   117,  5790, 21813,\n",
      "           130,   123,  6082,   256,   229,  2707,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   449,  1921,  1517,   539,   229, 22280,  9882,   125,   223,\n",
      "         22279,   125, 19385,   117,   221,  4917, 22281,  3824,   140,   170,\n",
      "           259,   440,  5345,  1522,   366,   451,  9025,  2509,   117,   179,\n",
      "          4900,   596,   229, 22280,   978, 18689,   286,   744,  5288,  8491,\n",
      "          8308,   304, 22280,  8179,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,  5825,   214,   170,   123,  4970,   256,   260,  2455,  2412,\n",
      "           784, 13240, 22281,   123,  2169,   117,   123,   390,   304,   229,\n",
      "         22280,  2287,   387,   256,   222, 16423, 22279,   171, 11948,  1703,\n",
      "           459,   373,   125, 16971,   327,  1105,   122,  9732,   675,  1169,\n",
      "           143,   271, 18047, 22281,   236,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  8051,   256, 14619,   210,   179,   527,  3004,   151,   978,\n",
      "           222,  5023,   428,   449,  1921,  6790, 12431,   117,   123, 21122,\n",
      "           423,  1354,   367,   285,   879,  6720,   266,   117,   229, 22280,\n",
      "         12444, 10659,   636,  8045,   173,   327,  6272,   117,   171,   179,\n",
      "           123,  7492,   372,   118,  5302,   154,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   123, 18164,   304, 22280,  1250,   495,   179,   146,  3753,\n",
      "           180,   390,   304, 10090,   151,  6856,   125,   675, 11417,   387,\n",
      "           303,   143,   291,   125,   347,  8193,  6522, 22280,   122,   240,\n",
      "          1257,  1485,   260,  2251, 10780, 22280,   143,   176, 16420,  4041,\n",
      "         14754,   712,  2004,   128,  1143,   171, 19480,   326,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101, 14408,  1378,   240,   230,  1356,   581,   125, 11179,   358,\n",
      "           179,   123,  3623,   692,   271,   146, 11050, 22280,   180, 10673,\n",
      "           151,   117,   527,  3004,   151,   117,   170,   375,   421,  2420,\n",
      "          8296,   415,   173,   327,  2169,   117,  3548,  4373,   180,  8121,\n",
      "           304, 22280,   760, 17241,   173,   179,   176, 16995,   117,   122,\n",
      "           298, 12033,   179, 22278,  3444,   304,   692,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   180, 22283, 11126,   252,  5787,   123,  9434, 22280, 13086,\n",
      "           125,  1065, 22287,   122,   222,  4863,   388, 20588, 22282,   117,\n",
      "           179,  2317,   232,   692,   123,   327,  2274,   852,  1369,   138,\n",
      "           316, 22280, 14581,   122,  1348, 14837,   251,   221,   123,   311,\n",
      "           945,   122,   333,   912,  5439,  2524, 22280,   121, 22361,  8410,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   176,   146,  1863,   243,   834, 10130,   175,   229, 22280,\n",
      "           176,  4019, 10766, 22281,   236,  9442,   117,   744,   538,  6353,\n",
      "           125, 17097,   148,   902,   295, 10780, 13793,   117,  2990, 17681,\n",
      "           125,  9344,  1149,   283,   117, 16241,  1537, 22287,   792,   151,\n",
      "          9918,   123,  5907, 15578,  4275,  4322,   125,   527,  3004,   151,\n",
      "           117,   122,  1141,  3330, 22281,  5105,   125,  3933,  6511, 17497,\n",
      "           304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,   271, 13031,   179,   123,  3402, 15098,   338,  1196,   260,\n",
      "          3637,   316, 22280, 10276, 22281,   122,  1743,  2487, 22281,  4566,\n",
      "         10116,   221,   455,  1609, 22282,   118,  7707,   123, 14676,   170,\n",
      "           146,  3979, 22280,   125,   230,  5078, 11259, 18447,   151,   136,\n",
      "           259,  5708,  1491,   122,  5874,  3895,   117,  4023,   229, 22280,\n",
      "           259, 15252,   411,  2387,   151,   170,   123,   325, 13502,  1165,\n",
      "           415,   370, 13397,   124,   117,   176,   259, 19464, 22281,   236,\n",
      "          1266, 11744,   900,  1224,  5543,   138,   125,  3240, 22282, 13733,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,   221,   179,   123, 15156,   304, 22280,  6958,   680,   171,\n",
      "          1815,   296,   125,  7296, 11429,   272,   117,   176,   173,   576,\n",
      "           125,   388,  6774,   320, 13782,  1554,  1604,   171,  3165,   117,\n",
      "           368,   272,   619,   333,   762,  2640,   954,  1990,   793,   171,\n",
      "         10968,  2256,   136,   229,  4767,   117, 19547,   125,  2251,  3603,\n",
      "           117,   202,  1423,   366,   730,  3173, 20258, 22281,  7871,   604,\n",
      "          3391, 22280,   143,   125,   327,  7828,   117,   527,  3004,   151,\n",
      "          1004, 12440,   640,   125, 13502, 15824, 22282,   118,   176,   180,\n",
      "          2251, 10780, 13793,  5785,   240,   327,   928,   128,   392,   117,\n",
      "           122,   171,  7267,   179,  2036, 10971,   923,   320, 14773,   342,\n",
      "          9821, 19387,  1776,   285,   125,   733,  4861,   304, 22280,   240,\n",
      "          1921,  1356,   581,  4661,   122,   472,   537,   154,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   229, 22280,   495,   222, 14439,   179,   740, 19399, 22281,\n",
      "           236, 12856, 22280,   125,   898,   117,   123,   516,   269, 20955,\n",
      "           304, 22280,  2990,  9349,  3221,   327,  8869,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   495,   222,  9898,   117,   179, 18253,   304,   256,   320,\n",
      "          1147, 17275,   375,   125, 12681,   421,   118,  1340,   425,   123,\n",
      "          5540,   117,   271,   123,   222,  3898,   630,   987,  2726, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,   122,   146,  1147,   122,  1016,  2160,   179,   262,   146,\n",
      "           572, 22290,  5967,  3343,  7839,   303,   180,  7828,  2990,  2606,\n",
      "           117,   123,   327,   636,   176,   379,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   229, 20626,  1665, 22278,   873,   210,  3292,   180,  8410,\n",
      "          6838,   117,  7525,   265,   923,   118,   176,   472,  4601,   125,\n",
      "          1568,  2037, 22280,   122,  7622,   151,   118,   176,   179,  5160,\n",
      "           966,  9865,  9412, 22278,  1021,   125,   370,   146,  3165,   180,\n",
      "          2477,   705,   475, 12153,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,   176,   146,  4639, 13287,  1003,   411, 20732,   176, 17896,\n",
      "          1875,   125,   695,   373,   117,  4513,   123,   928,  1337,  6958,\n",
      "         22278,   229,  2377,  2755,   124, 13782,   180,  2733,  9217,   122,\n",
      "         14808,  3292,   117,   146, 18718,   504,   183,   122, 15119,   179,\n",
      "          1021,  6621,   117,   271,   607,   173,  1485,   260,   390,  1149,\n",
      "           117,  5787, 15394, 22281,   236,  1950,   526,  5484,   286,   423,\n",
      "          1356,   980,   252, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,   260, 17464,   325,  6554, 18711,  1509,   125,   527,  3004,\n",
      "           151,  1085, 13776,   598,   123,  8869,   179,  2036, 15724,   125,\n",
      "          4899,   117,   188,   210,   123,   615,  2364,   240,  4863,   117,\n",
      "          2440,   125,   675,   466,  1705,   117,  3859,   151,   271,  4685,\n",
      "          1065, 21102,   375,   117,  1938, 12657, 15976,   179,  2036, 10971,\n",
      "           923,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   240,  1257,   653, 11614,   740,   146,  2987,   117,   222,\n",
      "          4661,  4437,   179, 10468,  2836,   259,  2217,   122,   202, 14353,\n",
      "         22280, 16280,   236, 11258, 20955,   285, 18648,   179,   221,  1719,\n",
      "          1921,  9349,   179,   123,  1384,   256,   117,   740,   117,   123,\n",
      "           327,  2760,   117,   229, 22280,  9607,   351,   230,   331,   366,\n",
      "           475,   741,   266,   303,   143,   179,  6942, 11814,   123,  1078,\n",
      "           222,   125,   532,   592,  9342,   125,  7911,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,  2364,   180,  7482,   125,  3179,  1690,  9491,   897, 10846,\n",
      "          5197,   228,   325,  4793,   351,   358,  4230,   552,  6644,   598,\n",
      "           146,  6436, 16510,   117,   171,   179, 11744,  5630,  1615,  1176,\n",
      "           146, 18908,   247,  3980,   809,   243,  2990, 18190,   289,   364,\n",
      "          9586,   117,   202, 17387,   125,   327,  4149, 10491,   340,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,   222,   338,   303,  1587,   154,   221, 16592, 22278,   118,\n",
      "          1084,   425,   418,  6726,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,  3038,  2647,   125,   179,   944,   259,   532,   238,  2766,\n",
      "           444, 16283, 22281,   117,   834,  2869,   304, 22280,   125,   222,\n",
      "           117,   123, 11099,  4549,  1780,   412,  8869,   117,   527,  3004,\n",
      "           151, 10194,   151,   598,  1921,  9620,   539,   117,  4084,   214,\n",
      "           123,  3636,  3789,  9030, 22281,   146,   653,  1009,   266, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,  1016, 17003,   740, 13144,   146,  9607,   951,   125,  1078,\n",
      "           222,   298, 11179,   358,   117,  3951,   118,  7707,  4863,  2261,\n",
      "          3942,  1970,   342,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,   173,  4616,  7500,   117,   527,  3004,   151,   144,  5723,\n",
      "           259,   532,  2251,  3603,   423, 14701,  3189, 11837,  3632,  2381,\n",
      "          5628,  4457,   202,  2918, 15813,  3942,   460,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101,   230,  2954,   117,   202, 13039,   517,   117,   123, 21990,\n",
      "           151,   331,  4335,   117,   179,  5057,   118,   176, 14353, 22278,\n",
      "           170,   740,   117,   122, 17090,   388,  9238,   246,   873,   118,\n",
      "         15530,  7591,   285,   117,  7809,   118,  2036,   222,   416,   289,\n",
      "          1286,  9365,   171,   313, 20132,  1623,   556,   117, 13254, 20882,\n",
      "           179,  3767, 22278,  5688,   285,  8768, 22278,   118,   122,   222,\n",
      "           390,   303,   785, 17706,   117,  9396,   527,  3004,   151, 13449,\n",
      "         11252,  5488,  1004,   271, 20743,  5830,  9342,   125,  7911,   486,\n",
      "          2779, 15212,  3495,   221,  7198,   222,  4170,   125,   636, 14701,\n",
      "           117, 21990,   151,   229, 22280,   311,  1519,   234,   170,  1966,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101, 18419, 22287,   118,   176,   944,  4461,  6775, 22281,   125,\n",
      "           527,  3004,   151,   117,   122,   259, 18253,   304,   692,   123,\n",
      "          1284,   125,   416, 11661,   842,   125,   390,   304,  5791, 18711,\n",
      "           375,   657,   210,   123,   636,   670,   366, 17704, 22281,   117,\n",
      "          5084, 11665,   179,  2365,  8447,   390,  1149,   117,   229, 22280,\n",
      "           822,   375,   692,   693,  1296, 18775,  4438, 12400,   919,   382,\n",
      "           117,  8407,  1302,   128,   125, 10687,  1004,   118,  6974,   591,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   259,  2251,  3603,   125,   527,  3004,   151, 22070,   117,\n",
      "          1502,   740,   229, 22280,  5057,  9250,   247,   117,   171, 14701,\n",
      "           125,   327,   144,   154,   304, 22280,   202,   577, 22290,   180,\n",
      "           283,   304,   122,  5533,   125,   176,   762, 11028,   684,   170,\n",
      "           123,  6251, 17932, 22278,   117,  7465,  2916, 22287,   118,   176,\n",
      "           170,   146,  1130,   179,  1615,  1176,   207,  8521,  2836,   171,\n",
      "           762,   247,   125,   675,  1169,   143,  6621,  1799,  1444, 22291,\n",
      "           550,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101, 10348,   118,   176,  3413,   625,  1569,   298, 16283, 22281,\n",
      "           978,   123, 15685,   125,  1434,  3933,   144,  5424,   123,  1519,\n",
      "           234,   285,   390,   304,   122, 21619,   118,  2036,   260, 12112,\n",
      "         22281,  2113,  3876,  1652,   740,   368,   256,   256,   118,  2036,\n",
      "           123,   144,   154,   304, 22280,   117,  1016,   271, 12764,  4567,\n",
      "          2836,   123,  4566,   179,   123,   598,   322,   256,   291, 18933,\n",
      "           322,   173,   347, 16550,   825,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,   785, 12444,   123,  8087,   232,   173, 18271,  8948,   140,\n",
      "          3636,  2217,   117,   291,  2992,   421,   118, 15887,   123,  1568,\n",
      "          2037, 22280,   117,   221,   229, 22280,  6476, 22287,   146, 10343,\n",
      "          3656,  8320,   247,   170,   179,   527,  3004,   151,   259,   599,\n",
      "          1125, 15824,   256, 16613,  5911,   942, 17850,  2951,   128,   117,\n",
      "           179,  1061,  5267,   692,   240,  6742,  4379,  7238,   272,  9586,\n",
      "           117,   122,   229, 22280,  1085,  3133, 13793,  6554,   382,   125,\n",
      "           230, 12239,   154,   304, 22280, 14353, 22278,   122,  5787,  1623,\n",
      "          3301,   285,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101,   123,  3295,   122,   179,   944,   240,  1323,   692,   117,\n",
      "           260,  1176, 13301,   474,   240,  1950,  4029,   283,  1425,   640,\n",
      "           397,   117,   449,  2044, 18768, 22281,   657,   230,  2521,  2028,\n",
      "          1670,  3878,   285,   117,  3484,   176,  2709,   619,   123,  9336,\n",
      "           146,  2244,   122,   785,  1528,   146,   313, 20132,  6693,   364,\n",
      "           179,  9821,  4074, 22282,   123,  3049,   304,   171,   577, 22290,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,   229, 22280, 12079,   244,   527,  3004,   151,   173,   327,\n",
      "          3968, 18955,  4251,   954,  1390, 22280,   143,   180,  3952,   117,\n",
      "           582,  4970,   117, 17267,  2245,   243,   123,   347,   934,   157,\n",
      "           125, 14439,   117,  2745,   179,   123,  7308,  2707,   978,   125,\n",
      "           325,  6390,   122,  9745,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,  2671,   181,   268,   118,   311, 19387,   123,  9375,   146,\n",
      "          5866, 14353, 22280,   122, 14848,   179,  3368,   171,  5831,  2990,\n",
      "          2606, 16257,   225,  7177,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 39 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.023156089173965617\n",
      " Coesão Score Final: 0.5115780445869829\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'assim', 'logo', 'pois', 'porque', 'quando', 'se', 'caso', 'apesar de', 'sobretudo', 'ou', 'ora', 'quer', 'seja', 'senao', 'assim como', 'bem como', 'como', 'em vez de', 'para que', 'porque', 'alias', 'sobretudo', 'bem como', 'por isso']\n",
      " Número de conectivos: 27\n",
      " Número de sentenças: 39\n",
      "======================\n",
      "Resultados para preprocessado_senhora_jose_de_alencar_cap_1.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'porem', 'assim', 'logo', 'pois', 'porque', 'quando', 'se', 'caso', 'apesar de', 'sobretudo', 'ou', 'ora', 'quer', 'seja', 'senao', 'assim como', 'bem como', 'como', 'em vez de', 'para que', 'porque', 'alias', 'sobretudo', 'bem como', 'por isso'], 'num_conectivos': 27, 'proporcao_conectivos': 0.023, 'similaridade_media': np.float64(1.0), 'num_sentencas': 39}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[ 101, 4717, 4167, 2856,  171,  644,  102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   222,   969,   388,  9238,   125, 10567,   188,  2427,   118,\n",
      "           176,   529, 12669,  8353,   179,  4677,   210,   260,   629,  4851,\n",
      "           125,   230,  4767,   117,   529, 14803, 22285,   537,  1402,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   123,  3377,   144,   251,  1676, 14049,  4136, 18416, 22281,\n",
      "         18804,  2037,   170,   123,   327, 12683,   171,   149, 22178,   146,\n",
      "           416, 19565,  2231,   183,   125,   527,  3004,   562, 12126,   130,\n",
      "           146, 15252,   411,  6859,  3240, 22282,   266,   185,   171,  1798,\n",
      "           179,   344,   124,   146, 10862,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   570,  3552,   251,   229, 10415, 17124,   170,   259,  5708,\n",
      "           123,  5926, 22282,   423,  3960,  9438,  2177,   171,  6169, 22280,\n",
      "           117,   123,   390,   304,  4048,  1608, 20477, 22278,   173,  8137,\n",
      "         15839,   343,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   146, 12401, 22280, 17896, 22278,   118,  2036,   202,   834,\n",
      "         10130,   175,   117,   271,   202,  8507,   117,   123,  7871,   604,\n",
      "          3391, 13793,  3972,   285, 22305,   179,   125,  4189,  2623,   247,\n",
      "           740,  5510,   499,   125,   898,   117,   271,   123,  3196,  1567,\n",
      "          7882,   952,   125,   222,   689, 10490,   339,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   449,   123,   333,   194,   292,   179,   176,  1270,  3626,\n",
      "           240,  1719,   123,   327,  2760,   117,   176,   125,  3933,  7716,\n",
      "         12156,   151,   123, 11214,  1228,   304, 22280,   273,  3611,  7828,\n",
      "           117,   123,   173,   483,   483,   125,   222, 16148, 13502,  1165,\n",
      "           415,   125,   311,  1185,  1156,   122,   505,   994,   117,   179,\n",
      "           123,  2976, 10984,   118,   898,  7485,  4812,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   532,  5708,  1941,   229, 22280,   376,  5022,   572, 22290,\n",
      "          2370, 19068,   269,  3103,   117,   179, 13156,   210,   538,  1390,\n",
      "         22280,   143,   117,   122,   179,   117,   123,  4209,  2350,  1667,\n",
      "           303,  1664,  2599,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   538,  1084,   118,  4248, 22281,   117,   173,   576,   171,\n",
      "          1248,   713, 13449, 19088,   117, 10527,  6605,   252,  2535,   123,\n",
      "         12252,   121, 22361,  8410,   123,  7871,   259,  1129,  3556,   432,\n",
      "           191,  2370,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101, 10978,  4116,   146,   928,  1212,   834, 10130,   175,   230,\n",
      "         17681,   125,   949,  1601, 21563,   179,   229, 22280,  2036,   122,\n",
      "         17527,  1065,  4863,   596,   117,   777, 22279,   229, 22280, 14940,\n",
      "           176, 13105, 22278,   146,  8788,   281,   325,  2004, 22280,   366,\n",
      "          2996,   303,   143, 15020,   401,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   607,  2459,  1016,   117,   123,  1977,   389,  3980,  2766,\n",
      "           125, 12448,   852,  7503,  2446,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   260,   325, 16353, 22281,  1568,  1604,   143,   629, 22280,\n",
      "         11597, 22281,   240,  3636, 17877,   125, 10638,   247,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   527,  3004,   151, 14777,   118,   176,   125,  1364,  1839,\n",
      "           125,   898, 16241,  1537, 22287,   320,   792,  1921, 14350,   215,\n",
      "          9586,   117,   229,  1183,   340,   316, 19413,   148,   122, 20885,\n",
      "         22278,   117, 13031,   151,   179,  3876,  2182,   740,   762,   343,\n",
      "           122, 14209,   146,  3350,   125,   327,  1622,   340,  1405,   130,\n",
      "          1266,   118,   176,   221, 19978,   159, 10984, 12488,  2381,  1364,\n",
      "           146,   347,  3753,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,   614,   210,   179,  3837,   256,   202, 10862,  3429, 15700,\n",
      "           159,   123,   928,  1337,  4047,   567,   123,   327,  3639, 13750,\n",
      "           154,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[101, 495, 121, 102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,  5101,  1817,   449, 17738, 13808, 22281,   117,   123, 17704,\n",
      "           179,  6569,   151,  1982,   125,   527,  3004,   151,   146, 13833,\n",
      "           247,   125,  5825,   118,   390,   304,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   123,  4970,   256, 21161,   118,   176,   180, 10415, 17124,\n",
      "           221,   418,  7177,   222, 10786,  1286,   229,  6726,   180,  9586,\n",
      "           117,   179,   331,  4922, 17190,   151, 22280,  1365, 22288,   180,\n",
      "          6511, 22238,   304, 22280,   173,   179,  1011,  6618,   154,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   527,  3004,   151, 17522,   123,  3122, 10013,   423,  6169,\n",
      "         22280,   122,  6185,  2904,   230, 21348,   406,   125,  5973,   439,\n",
      "           247, 11629,   123, 11214,   392,   657,   230,  7476,   125,  2987,\n",
      "          2360,   303,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[ 101, 5147,  121,  102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,  5101,  1817,   117, 13851, 12717,   243,   123,   327, 21322,\n",
      "          7004,   118, 16788,   173,   230,   366, 11281, 22281, 14371,   125,\n",
      "          1609,   942,   179, 19001,   320,  1341,   180, 10415, 17124,   117,\n",
      "          1598, 13808,   118,   176, 12449,   423, 16960,   303,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   118,   418,   395,   193,  3281,   125,  3185,  2097,   136,\n",
      "         16795,   123,  4970,   256,   170,   123,  9434, 22280,   125, 19519,\n",
      "           285,   370, 13397,   124,   179, 19764,   146,   347,   934,   339,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   118,  2798,   240,  1257,   449,  6804,   118,   311, 18253,\n",
      "         14960,   285,   607,   125,   333,   146,  6938,   118,  9396,   123,\n",
      "           390,   304,   221,  2822,   230,  3083, 13793, 13398,  1005,   125,\n",
      "           327,  9110,  4047,   567,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,   118,  2983, 21145, 22281,   179, 11893,   316, 22280,  1373,\n",
      "           229, 22280,  1146,   333,  9361,   221,   123, 10428, 22279,   240,\n",
      "          1257,   122,   179,   202,  2187,   125, 17675,   458,   607, 19020,\n",
      "           390,   304,  5923,   124,   122, 15403,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101, 11032,   117,  3185,  2097,   117,   625, 18496,   123,  2992,\n",
      "           151,  1695,  3207,   256,   221,  6961,   159,  8788,   986,   173,\n",
      "         19462,   370,   828,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,   176,   123,   681,  1896,  2901,   662,  1353,   170,   146,\n",
      "         15699,   171, 11997,   421, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,  3841,  4838,   154,   915,  5424, 22280,   146,  1312,   303,\n",
      "           229, 22280,  3851, 11775,   117,   449,  1961,   203,   316, 22280,\n",
      "         21055,  2895,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,  5101,  1817,  3449,   240,   123, 22283,  1202, 22287,   123,\n",
      "          9917,   675, 16085,   143,   171, 21145,   180,  5693,   574,   117,\n",
      "           834, 11076,   259,  5708,   243,   834, 10130,   175,   125,   527,\n",
      "          3004,   151,   117,   582,   730,   151,   256,   146,  3901,   125,\n",
      "           675,  3724,   117, 20466,   123,   273, 20222,   140,   118,   176,\n",
      "           125,  1569, 12126, 11448,   256,   304, 22280,   117,   320,  2892,\n",
      "          2884,   247,   125,   598,  7771,  1076,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,  2789,   118,   123,   123,   390,   304,  5961,   117, 16617,\n",
      "         22278,   125, 10968,  1217,   118,   176,   125,   675,   466, 22280,\n",
      "           118,  1034,   321,   303,   143,   122, 14368,   159,   118,   176,\n",
      "           320,  7257, 18304,  2061,  4410,   179,  8362, 22278,   117,   834,\n",
      "         12943,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,  9679,   179,   123,  4970,   256, 10415,   256,  9365,   171,\n",
      "         21145,   449,   229,  3632,  5066,  1051,   256,   146,   179,   740,\n",
      "         10355,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,   125, 19170,   117,   240,   210,   117,  8082, 13665,   118,\n",
      "           123,   118,   179,  1815, 12524,   123,  8650,   162,  7248,   117,\n",
      "           121,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,  5101,  1817,   136,   123,  7492,  1191,   834, 10130,   175,\n",
      "           125, 18127, 22282,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[ 101,  118,  123, 8650,  162, 7248,  136,  102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101,   122,  7583,   390,   304,  1719,   125,  5580,   136,   118,\n",
      "           170,   730,  3670,   125,  5731,   538, 13841,   122,   538,   305,\n",
      "          1051,   442,   180,  8625, 22278,  2281,   122,   125,   785,  4062,\n",
      "         10303,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,   118, 10201,   118,   311,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   122,   230,  9586,  1004, 22231,   175,  3415,   123,   118,\n",
      "          4970,   256,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[ 101,  118,  122, 1004,  118, 6974,  285,  102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101, 10801,   179,  7384,  7670, 15363,   117,   122,   179,   376,\n",
      "           230,  4410,   785, 15397, 20383,   178,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101,   118,   449,   229, 22280,  6230,  8264,   229,  2707,   102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,   122,   123,   681,   576,   179,   123, 20590,   229, 22280,\n",
      "           311,  5069,   157,   125, 13038,  3382,  1075,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,   118,   262,   123,   681,   576,  1703, 13397,   340,   214,\n",
      "          3769,  3724,   117,   123,   390,   304,  9821,   125,  1160, 10500,\n",
      "           327,  8410,  1314, 13552, 22282,   118,   176, 20514,   285,  7814,\n",
      "         19927,  7424,   240,  1966,  6314,  1689, 11646,   179,   123,  6618,\n",
      "           619,   102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,   449, 10194,   456,   598,  1921,  5811,   304, 22280,   122,\n",
      "          7809,   118,   176,   123,  4970,   256,   173,  5902,  3361,   122,\n",
      "         16423, 22279,   118,  2826, 22278,   118,   311,   230,   144,  5424,\n",
      "           117,   121,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,  5101,  1817,   118,   146,   179,   122,   117,   527,  3004,\n",
      "           151,   136,   118,   449,   607,   125,   333,  1546, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101, 19790,   118,   311,   136,   118,  1546, 22278,   136,   325,\n",
      "           171,   179,  2779,  7206,   117,  9586,   136,   176,   122,   860,\n",
      "           146,  7343,  2455,   373,   102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,   123,   390,   304, 18540, 11541,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101,   118,  2000,   246,   117, 17704,   118,  1977,  7093,   123,\n",
      "         17704,   325, 19543,   117,   123,  8650,   162,  7248,   291,  2779,\n",
      "           136,  1996,   870,   509,   527,  3004,   151,   117,  4136,  3093,\n",
      "           272,  2146,  2461,   301,   102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101,   118, 11032,   117, 11032,   417,  1797,   456,   123,  4970,\n",
      "           256,   123,  1984, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[  101,   418, 10273,  5870,   214,   117,   527,  3004,   151,   102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[  101,  1502,   117,   123,  8650,   162,  7248,   122,   221,   176,\n",
      "          3286, 22282,  1988,  2354, 22279,   136,   118,  1547, 15172,   371,\n",
      "           118,  1028,   785,   325, 19543, 22281,   179,   740,   229, 22280,\n",
      "         10142,   123,   532,  1143,   102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101,   123,  4970,   256, 19398,  1256,   291,  1685,  3360,   125,\n",
      "           390,  1149,   179,   318, 13793,  6952,   692,   202, 22231,  3198,\n",
      "           122,   298,  1647,   229, 22280,  9607,  5635,   243,  2535,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101,   118,   122,   316, 22280, 20882,  1996,   527,  3004,   151,\n",
      "           271,   176,  5443, 22281,   236,   230, 10857, 13793, 14353, 22278,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101,   118,   629, 22280, 10303, 22281,   118,   173,  1364,   146,\n",
      "          1652,   122,   325,  1004,   118,  6974,   285,   171,   179,  2779,\n",
      "           136,   118,   171,   179,  2354, 22279,   117,   527,  3004,   151,\n",
      "           136,   607,   125,   333,   760, 17241,   179,   176,   778,   130,\n",
      "           173,  1364,   146,  2187,   125,  1543,  1858,   390,   304,   179,\n",
      "          1476,   252,   327,  6974,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 50: tensor([[ 101, 1084,  653,  117,  240,  332,  145,  117,  125,  179, 1971,  176,\n",
      "         3887,  117,  623,  857,  179, 9966,  102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[  101,   118, 20616,   122,   418,   123,   327,  6251, 17932, 22278,\n",
      "           117,   121,   102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[  101,  5101,  1817,   136,   118,  1141,   117, 17704,   123,  7122,\n",
      "          6251, 17932, 22278,   418,   173,  4640,   123,  3295,   117,   122,\n",
      "           229, 22280,   173,  3235,   323,   118,  1084,   102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[ 101, 3547,  117, 1257,  122,  146,  455,  944,  873,  210,  122, 4024,\n",
      "         2097,  102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101,  2354, 22279,  7384,  7670,   271,   146,   388,  8696, 22284,\n",
      "           117,  8954,   271,   230,  5495,   118, 10502,   117,  1452,  9763,\n",
      "           229,  4767,   170,   259,  9256,   122,   259, 19678, 22281,   117,\n",
      "           179,  1061,  8679,   944, 13586,   244,   478,   442,   102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101,   122,   271,   229, 22280,   252,   125,   333,  1016,   136,\n",
      "           625,  2354, 22279,  3189,   117,   527,  3004,   151,   117,  3887,\n",
      "           179,  4048,   230,  4784,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101,   118,  1941,   873,  1286,   179,   123, 17704,   229, 22280,\n",
      "           122,  3874, 21990,   181,   537,   364,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[  101,   418,   273,   580,   383,  1825,   259, 17080,   171,   555,\n",
      "           117,   417,  1797,   456,   123, 10687,  1939,  3552,  7831,   123,\n",
      "           169,  8388,  3661,   170,   222,  2135, 22280, 13449, 19088,   125,\n",
      "         18447,   151,   102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[  101,   318, 13793,   229, 22280,  4178,   117,   121,   102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[  101,  5101,  1817,   117,   179,  2779,  1476,   268,   222,  2178,\n",
      "           125,  2987,   117,   146,   325, 18563,  1608,   125,   944,   259,\n",
      "          7820,   117,   123,  3561,  4129,   455,   340,  3456,  4776,  2017,\n",
      "           229, 22280,  6102,   544,   136,   260,   179, 14936,   271,   230,\n",
      "          4784,   117,   173,  4661, 19489,   117,   629, 22280,  3867,   390,\n",
      "          1149,  2415, 19649,   138,   122,  1877,   649,   179,   176,  1403,\n",
      "           228, 10693, 16163,   214,   173,  4217,  1058,  2779,   988, 22280,\n",
      "           271,   222,  9276,  7206,   123,  7812,   179,  6788,   122, 17579,\n",
      "          2755,   124,   118,  3486,   214,   146,   179,  2354, 22279,  3189,\n",
      "          4640,   146,  3495,   659,   171,  2996, 22280, 22003,   117,   122,\n",
      "           180,  2745,   117,  2684, 10428, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101,   449, 13429,  2152,   210,   117,   259,   532,  2202,  8296,\n",
      "          2247,   629, 22280, 13776,  5022,   179,   229, 22280,  1146, 11179,\n",
      "         22282,   327,  8869, 14252,  7444, 22281,   117,   736,  1941, 12301,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101,   118,   625,   412,   681,   576, 19016,   288,  3047,   180,\n",
      "         17704,   117,   229, 22280, 10733,  3933,   144,  5424,   117,   230,\n",
      "           428,   243,  1845,   136,   102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101,  1502,   146,  2987,   376,   230, 19016,   304,  2401, 15266,\n",
      "           415,   117,   179, 13640,   468, 22278,   744,   325,   171,   179,\n",
      "           123,   171, 18659,  5567,   125,   607, 11114,   117,   122,  2684,\n",
      "           653,   171,   179,   123,  2811,   202,   537,   234, 14883, 19450,\n",
      "           125,  1798,   117,   170,   179,   259, 13254,   143,   125, 12932,\n",
      "           176,   563,  9853, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[  101,  1719,  1921,  9349,   179, 11287,   151,   222,  4575,  8598,\n",
      "           303,   117, 12058,   117, 14990,   122, 10692, 11437,  1289,   117,\n",
      "           693,  3671,   179,   229, 22280,  2521,  6759,   118,   176,   170,\n",
      "           123,  9066,   124,   171, 10738,   449,  2588,   123,  3106,   304,\n",
      "         22280,   171,  3495,   102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[  101,   118,  2535,   653,   117,   527,  3004,   151,   117,   418,\n",
      "          2354, 22279,   311,  3951,  3083, 13793,   122,  8094,   327,  2314,\n",
      "           304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[  101,  1977,   607,   125,  4640,   455,   230,  9586,   125,   327,\n",
      "          2169,  4178,   325,   125,   179,  1415,  2217,   179, 10698,   228,\n",
      "           529, 14164, 22281,   136,   122, 19392, 22287,   122,  4062,  2113,\n",
      "          3133, 13793,   117,   170,   123,  8869,   179,  2036,  2789,   347,\n",
      "          1938, 22280,   117, 15480,   202,  1147,   117,   240,   344,   304,\n",
      "           455,  1021,   125,   333, 13441,   251,   102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101,   118,  1075,  2589,   362, 22282,  4643,   748,   123,   390,\n",
      "           304, 19726,   557,   173,   327, 13750,   154,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[  101,  5101,  1817,   744, 17782,   456,  1450,  3724,   173,  4390,\n",
      "           304, 22280,   180, 10415,   449, 15415,   179,   123,   390, 12220,\n",
      "         22280,  2036,   543,  5723,   123,  2892, 12171,   304, 22280,   117,\n",
      "          1075,  9821,  3686,  1319, 22282,   118,   176,   123,  1569, 20615,\n",
      "         22280,   294,   185,   118,  2187, 22282,   117,   221,  2332, 11258,\n",
      "          1689,  2853,   900,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101,   318, 13793,   170,   146,   316,  2175,  4756, 18544,  4330,\n",
      "           221,   123,  2350,   687,  4625,  6496,   117, 15568, 22288,   118,\n",
      "           176,   122,  4708,   214,  1089,  5585,   128,   412,  4767,   117,\n",
      "         12746,  1353,   123, 11562, 22282,   529,  6958, 15268,   125,  6603,\n",
      "          3141,   552,   122, 15091,   125,   240,  1927,   795,  8978,   202,\n",
      "           956,  6693,  6367,   298,  5411,   128,   102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[  101,  1016,   125,  8523,   221,   123, 10415, 17124,   117, 15245,\n",
      "           118,   176,  1950,   526,  5484,   328,  4566,   432,   191,   378,\n",
      "           125,   527,  3004,   151,   117, 10017, 22287,   125,  4863,  1021,\n",
      "           125,   598, 18250,   117,   625, 17915,   236,   180, 22238,   304,\n",
      "         22280,   123,   543,   194,   304,   125,   230,  2760,   123,  3656,\n",
      "           381,   578,   118,  2036,   538, 22000, 22281,   146, 11021,   298,\n",
      "         16706,   102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101,   229, 22280,  7112,  3916, 22282,  9539,  1685,  3354,   625,\n",
      "          8362, 22278,   121,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101,  5101,  1817,   222,  4081,  1510, 16489,   122, 10357,   517,\n",
      "           117,   179,   740,   483, 22287, 18574,   240,   437,   118,  1340,\n",
      "          1615,  1176, 13083,   487,   102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101,  2927,   118,   176,   122,  4970,   527,  3004,   151,   117,\n",
      "          7479, 18908,   501,   125,   229,   934,   419,  1609,   692,   744,\n",
      "           170,   146,  6967,   269,  1286,  4566,  3979, 16489, 13449, 19088,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101,   123, 14350,   215,  9586, 13431, 22278,   125,   327,  4047,\n",
      "           567, 18253, 14960, 15182,   117,   271,   230,  6958, 22278,   125,\n",
      "         21340,   179,  2153,  3239,   214,   118,   176,   210,  1941, 22281,\n",
      "           269,   125, 19170,   117,   176, 16629,  4886,   313,   567,   122,\n",
      "          1065, 21102,   375,   117,  5510, 19927,   214,   125,   898,   259,\n",
      "           955,   474,   122,   572, 22290,  2370, 19673, 22281,  2350, 14505,\n",
      "          5507,   661,   286,   102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[  101,   740,  3420, 22288,   221,   260,  9751,   117,   122,   170,\n",
      "           766,   226,  1053,   351,  8089,  1337, 16322,  2071,  6554, 18711,\n",
      "           375,   246,   260,   924,   987,   413,  8353,   117,   179,  9821,\n",
      "         22287,   222,  5274,  9983,  1430,   221,   327,   223, 22280, 16907,\n",
      "           122,  9726,  1337,   102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101,   123,  5982,   175,   180,  3377,  8163,  4718,   118,   176,\n",
      "           412,  3903,   366,  9751,   117, 16386, 13665,   146,  6169, 22280,\n",
      "           122,   123,   390,   304,  8684,  2610,   236,  2684,   123,   629,\n",
      "          2382,   117,   221, 10137,   159,   118,   176, 16149, 15520,   470,\n",
      "           125,   969,   117,   179,  2036, 20607, 11814,   498,   123,   494,\n",
      "           151,  3317,  5635,  3632,   285,   171,   644,   769, 22278,   125,\n",
      "         13841, 17291,   683,   117,   122,   273,   243,  1609,   692,   118,\n",
      "           176,  1676,   928,  3832,  1632, 18354, 22281,   271,   230,  3563,\n",
      "           232,   125,  2987,   102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[ 101,  173,  483, 4456,  118,  176,  125, 3377,  102]])\n",
      "DEBUG: Tokenized sentence 78: tensor([[  101,  1977,   123,  1003,   236,  3876,  2182,  1016,  1428,  2001,\n",
      "           383,   403,   117,  2686, 13031,   179,   425,   138,   466,  1557,\n",
      "           171, 11451, 22280,   125,   610,  1609,   151,  1011,   123, 17098,\n",
      "           711,   781,  1499, 18711,   375,   246,   123, 16241,  1165,   366,\n",
      "         19493,   117,   123, 10497,   218,  1671,   840,  7027,   124,   117,\n",
      "           173,   179,   176,  9351, 22278,   125,  5334,  1198,   123, 22229,\n",
      "         22278, 16473,   251,   102]])\n",
      "DEBUG: Tokenized sentence 79: tensor([[  101,   700,   125,  3343, 20973,   118,   176,   125,   969,   271,\n",
      "           123, 21672, 22278,  5562,  6278, 22278,   117,   179,   176, 16450,\n",
      "           712, 10786,  3103,   125,   347,  1752, 11003,   117,   123,   390,\n",
      "          8073,  2032,  2245, 22288,   118,   176,   320,  7670,   122, 12044,\n",
      "          2581,   246,   146,  6540,   102]])\n",
      "DEBUG: Tokenized sentence 80: tensor([[  101,   298,  1356,   980,   268,   143,   180,  1048, 19147,  1337,\n",
      "          7658,  6010,   148,   478,   117,   179,  4697,   619,   146, 16768,\n",
      "           117, 10968,  2071,   118,   176,   870,   509,   123, 18563,  1608,\n",
      "          4019,   304,   304, 22280,   180,  2234,   117,   625,  1315,  4602,\n",
      "           243,   218,  2766,   117,   572, 22290,  1817,   123,  3980,  7975,\n",
      "         22278,   125,   661,   151, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 81: tensor([[  101,  2133,   348,   259,  7096,  3103,  2990,  2314,  2349,   304,\n",
      "         22280,   792,   193,  4602,  1337,   117,   221,  1434,   146, 15201,\n",
      "           117,   123,   390,   304, 16272,  1353,   123,  8032,   449,   260,\n",
      "          2796,  7663,   117, 21128,   118,   176,   374, 22290,  4095,   285,\n",
      "           412,  3602,   304, 22280,   117,  9093,   146, 13779, 22278,   118,\n",
      "           202,   117,   122,   210,   766,   117,   202,  1423,   180,  4767,\n",
      "           117,   577,   304,  4212,   123,  8625, 22278,   171, 11451, 22280,\n",
      "           271,   176,  2589,   123, 11254,   171,  1877,   247, 21407,   143,\n",
      "           117,   740,   130,  1703,  1320,   456,   170,   123,  4410,   122,\n",
      "           146, 22000,   117,  7583,  1405,  4149,   244, 22278,   171, 16450,\n",
      "           304, 22280,   338,   286,   117,   179, 17783,  1176,   978,  3382,\n",
      "           130,  8799,   285,   240,  1084,   522, 20010,   102]])\n",
      "DEBUG: Tokenized sentence 82: tensor([[  101,   123, 18781,  2420,   180,  2606, 13441,   251,   117,  2903,\n",
      "           252,   180,   447,  3632,  1718,   328,   117,  2364,  1023,   221,\n",
      "          2468,  3198, 22283,   118,  1084,   117,  2798,   653,   229,  1916,\n",
      "           314,   151,  2940,   117,   230,  4410,   325,  4332, 11540,   117,\n",
      "           222, 22000,   325, 18563,  1608,   102]])\n",
      "DEBUG: Tokenized sentence 83: tensor([[  101,   260,  7663,   179,  1950, 11814,   118,   176,   298,   266,\n",
      "         16198,   125,   527,  3004,   151,   117,  6022,   358,   125,  8773,\n",
      "           122, 14676,   117,  5308,   692,  4230,   898,   222,   958,  8476,\n",
      "         22280,   117,   179, 10201,   256,   259, 12753, 22280,   180,   333,\n",
      "         15407,   117,  5084,   625,   860,  4332,   303,  9726,  1212,   122,\n",
      "           745, 22279,   201,  1002,  3870,   151,   118,   176,   125, 19170,\n",
      "           170,   222,   283,   953,  4163, 22282,   183,   221, 11744,   900,\n",
      "           146, 21688, 10968,  2256,   102]])\n",
      "DEBUG: Tokenized sentence 85: tensor([[  101,  5101,  1817,   117,  2440,   125, 19877,  3611,   285,  1065,\n",
      "           785,   320,  1354,   367,  2869,   129,  7265,   125,   527,  3004,\n",
      "           151,   117,  4108, 21307,   118,   123,   170, 14473,  7089,  3876,\n",
      "          2182,   122, 18224,   256,   179,  3933,   144,  5424,   125, 10376,\n",
      "          2623,   247,  7736, 22278,   229,  1069,   180,   283,   304,   117,\n",
      "           179,   123,  2962, 22278,   123,   905,   247,   316, 22280,  4047,\n",
      "           567,   117,   122,  1271,   151,  2535,  1966,  2831, 20987,  2409,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 86: tensor([[  101,  5147,   740,   170,   123,  1589,   781,  1939, 16760,   179,\n",
      "           123,  5357, 22278,   320, 15568, 22282,   118,   176,   180, 10415,\n",
      "         17124,   117, 17522,   221, 22284,   102]])\n",
      "DEBUG: Tokenized sentence 87: tensor([[  101,  5101,  1817,   117, 12682,   203,   118,  2036,   171,  5995,\n",
      "           293,  2636,   118,   123,   125,   661,   151, 22280,   117,   122,\n",
      "          2002,  5300,   222, 13071,   170,   319,   123,  4187,   455,  3303,\n",
      "           173,  3979,   401,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 86 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.01320858557335796\n",
      " Coesão Score Final: 0.506604292786679\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'entretanto', 'assim', 'pois', 'porque', 'quando', 'se', 'caso', 'apesar de', 'nao obstante', 'sobretudo', 'nem', 'ou', 'ora', 'quer', 'seja', 'senao', 'como', 'porque', 'sobretudo', 'tanto', 'por isso']\n",
      " Número de conectivos: 24\n",
      " Número de sentenças: 88\n",
      "======================\n",
      "Resultados para preprocessado_senhora_jose_de_alencar_cap_2.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'porem', 'entretanto', 'assim', 'pois', 'porque', 'quando', 'se', 'caso', 'apesar de', 'nao obstante', 'sobretudo', 'nem', 'ou', 'ora', 'quer', 'seja', 'senao', 'como', 'porque', 'sobretudo', 'tanto', 'por isso'], 'num_conectivos': 24, 'proporcao_conectivos': 0.013, 'similaridade_media': np.float64(1.0), 'num_sentencas': 88}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,   495,   123,  5314,   171, 16960,   303,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   260,   924, 17704, 22281,   879, 11583,   118,   176,   123,\n",
      "          9317,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   527,  3004,   151,  7194,   151,   118,   176,   412,   425,\n",
      "          7771,  1076,   117,   179,   495,  9918,   123,  5395,   194,   118,\n",
      "         13747, 22278,   125,  2829,   310,   122,  6974,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   229, 22280,  3189,  3413,  4640,   179,  2589,  2990, 14738,\n",
      "         19773,   125,   390,  1149,  5562,  9760,   716, 18200,   179,   176,\n",
      "         21033,   171, 12804, 22285,   366,  2968,   117,   122,   221,  1977,\n",
      "           146,  1847,   122,   230,   183,   273,   522, 19565,   122, 19489,\n",
      "           319,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,  1004,   320,   598,   342,   117,   740,  9679,   179,   123,\n",
      "         12874,   304, 22280,   180,   123, 18661,   256,   125,  7828,   117,\n",
      "           834,   123,   615,   260,  4516, 12156,   923,   529,  8630, 22281,\n",
      "           122,   259, 13449, 19088, 22281,   538, 18908,   501,   117,   271,\n",
      "           260,  3968,  7322,   122,  1877,   649, 14888,   303,   143,   125,\n",
      "           230,  4991,   556,   118,  2337,   232,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,  1016,   229, 22280,   978,   792, 12268,   125,  1847,   122,\n",
      "           834,  2541,  1076,  8347,   179,   146, 12681, 22290,   185,   125,\n",
      "           532, 12141,   229, 22280,  1085,  2726,   416, 19565,   625,  1061,\n",
      "           176,  1480, 22281,   304,   692,   271,   123,  3960, 19147,  3391,\n",
      "         13793,   125,   222, 20327,   125,   337,  6684,  2798,   146,  8788,\n",
      "           281,  2677,   249, 18908,   501,  1528, 15107,  1212,   625,  4273,\n",
      "           321,   692,   230,  4502,   154,   117,   291,   176,   420, 12764,\n",
      "          5283,   221,  3859,   146,  8397,   310,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,  4922,  2880,   151, 22280,   117,   123,   390,   304,  1191,\n",
      "          2869,   304, 22280,   123,   532, 19877,   128,   125,   425,  7771,\n",
      "           118,   180,   272,   740,   179,   229, 22280, 14971,   125, 14738,\n",
      "          6658,   562,   117,   122,   331,   125,  5533,   173,  5533,  5167,\n",
      "           151,  1450,  3746,   470,   125,  4073,   141,   117,  8204, 21066,\n",
      "          2249,  3848,   268,  1452,  1125,   310, 12307,  7909,  1021,   173,\n",
      "          1105,   122,   221, 15297,   185,  5167, 13665,   222,  1945,  1156,\n",
      "           125,  3043,   499, 22305,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,  5101,  1817,   834, 20853,   146, 16960,   303,   117, 14372,\n",
      "           123, 10332,   125,   670,   123,  9586,   117,  1078,   576,   325,\n",
      "           865,   987,  2647,   180,  1622,   340,   125,   222, 14611,  1848,\n",
      "           179,  1021, 12225,   123,  1945,   148, 10003,   118,  5023,   162,\n",
      "           180,   283,   304,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,  1966, 14611,   117,   229,  5012,   151, 22280,   180,  4970,\n",
      "           256,   117,   229, 22280,  4207,   333,  1342,  3133, 13793,  6086,\n",
      "           179,  3541,   252,  8045,  1916,   140,   289,   529, 10687,   125,\n",
      "         13417,   481,   117,  5084,   176,   229, 22280, 17201,   125, 16241,\n",
      "          1537, 22287,   221,  4267,   141,   125,   898,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,  5101,  1817,   978,  1502,   271,  4863,   179,   527,  3004,\n",
      "           151,   117,   123,  1065, 21102,   375,   117, 10500, 22278,   870,\n",
      "           509,   230, 11417,   387,   304, 22280,   122,  1011,  3723,  6044,\n",
      "           123,  4970,   256,   117,   221,  8223,   146,  8540,   179, 18424,\n",
      "         22278,   146,   926,   125, 14744,   159,   123,   313,   567,  4685,\n",
      "           298,  1390, 22280,   143,   117,   316,  3632,  5271,   285,   117,\n",
      "          2249, 16224,   122, 21692,   175,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  4697,   619,   229,  7573,   260, 18127,   303,   143,   180,\n",
      "          2954,  2095,   221,  7659,   159,   118,   176,   179,   229, 22280,\n",
      "          8264, 22278,   202, 21145,   514, 22285,  6021, 22287,   390,   303,\n",
      "         10846,   125,  1977,   527,  3004,   151,   176,  7119, 16498, 22282,\n",
      "           125,   695,   373,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101, 12444,   333,  1502, 13398,  1005,   298,  3845,  2251,  3603,\n",
      "           117,   298,   179,   740,  3240, 22282,   514,   351,   117,   179,\n",
      "           240,  3933,  7892,  2148,   351, 10349,  9877,  3139, 22290,  2733,\n",
      "          5105, 17500,   118,  2036, 17878,   146, 16450,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   229, 22280,   176,   706,  8984,   123,  4970,   256,   173,\n",
      "          5121,   125, 16550,   671, 22282,   123,  9586,   117,  7809,   118,\n",
      "          2036,   230, 10162,   154,   170,   179, 21333,  7755,  3638,   123,\n",
      "           318, 12764,   711,   123, 10415,   117,   122,  4762,   123,  4299,\n",
      "          2022, 22283,   118,  1084,   221,   146,  2009,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   118,   229, 22280, 18661,   179,  2036, 17386,  1790,   117,\n",
      "           527,  3004,   151,  4048,   118,   311,   316, 22280,  1519,   175,\n",
      "           117,   122,  2684,   325, 19543,   117,   176,   122,   668,  4812,\n",
      "           117,   171,   455,   125, 12215,   118,  2643,   138,   118,   229,\n",
      "         22280,   122, 14155,   371,   304, 22280,   117,   229, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,  2389,   296,   136,   260,   390,  1149,   625,   176,  4677,\n",
      "           210,   221,   222, 21145,   582,  2521, 22287,  3530, 14710,   210,\n",
      "           117,  8679,   325, 19543, 22281,   171,   179,   629, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,   449,  2354, 22279,   418,  1790,   744,   325, 19543,   171,\n",
      "           179,   538, 21145, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[ 101, 2364, 2036, 1976, 1016,  102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,  5863,  6952,  1359,   125,  3179,  7513,  6436,   268,   118,\n",
      "          3189,  4945,   615,   122,   136, 16795,   527,  3004,   151,   170,\n",
      "           222, 13449, 19088,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   118,   229, 22280,  7206, 12898, 22278,   117,  3898, 10283,\n",
      "         22288,   123,  4970,   256, 21128,   146,  5078,  2245, 22282,  4566,\n",
      "         13449, 19088,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,   118,  2709,   419,   333,   958,   364,   118,   418,  4062,\n",
      "           118,   449,   146,  7343, 11455,   607,   125,   333,   860,   653,\n",
      "          1147,   173,   179,  7873,   128,   117,   179,  3484,  1342,  2471,\n",
      "           325,  2062, 21569,   138,   122,  1623, 10503,   303,   143,   221,\n",
      "          9726,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   273,   377,   557,  2044,  4230,   123, 11547,  5809,  3724,\n",
      "           170,   230,  3979,   251,  3922,   268,  7743,   117,   527,  3004,\n",
      "           151,  2789,   529,   840,   125, 14890,   121,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  5101,  1817,   117,  1632,  8440,   125,   179,   230,  9586,\n",
      "         16031,   246,  8598,   122,   928,  1337,   117,  8781,   285,  5457,\n",
      "           442,   117,  7119,   370,  6572, 16706,   117,   744,   653,   240,\n",
      "           416,   289,  1286,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,   527,  3004,   151,   179,   176,  9732, 22278,   320,   347,\n",
      "          8589, 12721,   117,  1636,   203,   118,   176,   123,   230,  1484,\n",
      "          3027,   508,   125, 11997,   172,   581, 11294,  5387,  1270,   583,\n",
      "          2370,   125,  6659, 21321,   122,  2694,   230,  3743,   125,  6387,\n",
      "          3637,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   123,   944,   259,   240, 11483,   207,  2990,   662,  7248,\n",
      "          1577,   304, 22280,   117,   202, 15277, 22282,   123, 14121,   125,\n",
      "          1798,   117,  4434,   118,  1084,   229,  1400,   117,  1270, 20509,\n",
      "         22282,   146, 15530,   130,   122,   525,  3198,   307,   146,  4639,\n",
      "           735, 22279,   117,   123,   390,   304, 15495,  9468,  4084,   256,\n",
      "           123,   636, 12171,   304, 22280,   122,   188,  7506,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   291,  1921,  3743,   495, 11826,   123,  1977,  2745,  2036,\n",
      "          9607,   351,   117,   291,  3876,   305,   791,   122, 12245, 21452,\n",
      "           527,  3004,   151,  8060,  6774,   934,   123, 18540,   343,   304,\n",
      "         22280,   179,   123,  7710,  1217, 22278,   202,  2182,   125,  3995,\n",
      "           230,  3138,  4870,  3791,  8440,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,   700,   125,   498, 22281, 17024,   123,  3743,   117,   123,\n",
      "           390,   304, 15399,   171, 11021,   180, 11098,   322,   222,   144,\n",
      "          1198,   125,  2903, 11437, 22280,   173,   694,  1672,   272,   528,\n",
      "          7439,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,  1021,  1369,   420,  6536,   122,  2968,   362, 22282,  5949,\n",
      "           222,  3743, 22280,   125,  3248,   117,  1941, 11229,   117,   179,\n",
      "           740,  3235,  2071,   202,  4102,   293,   243, 11451, 22280,   117,\n",
      "           700,   125,  5825,   243,   229,   327,   505,   185,  2032, 13808,\n",
      "           125,  2189,  1350,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,   320,  4081,   171,   964, 10490,   300,  4169,   222,  2724,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,   527,  3004,   151, 13206,   118,  2036,   123,  3743,   170,\n",
      "           222, 22000,  3361,   122,   123,  4410,  1381,   301,   117,   271,\n",
      "           746,  1337,   125,   695,   373,  3456,  2040,  1637,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101,   118,   221,   146,   139, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,   447,   793,  7642,  8167, 19962,   456,   318, 13793,   527,\n",
      "          3004,   151,  1921,   163, 13194,  3992,   179,  4897, 22279,   260,\n",
      "          8021,   171, 16450,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   740,   978,   870,   509, 19508,  2134,   157,  1087,   148,\n",
      "         10349, 11649,   415,   125,   327,  1069,   122,   173,   576,   125,\n",
      "          9336,   118,   176,   320, 19937,   122,  4314,   118,   176,  4915,\n",
      "           423,  9605,  5822, 22280,   171,  1147,   117, 16512, 22278,   173,\n",
      "           327,  8410,   123,   344,   304,  3804,   221,  9732,   259,  7937,\n",
      "           122, 15384,   586,  1973,   791,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,   180, 22283, 11126,   252,   123,  1945,   148,   125,   179,\n",
      "         18763, 22278,   118,   176,   320,  4314,   146,  8589, 12721,   122,\n",
      "           179,  1858,   576,   525,  3198,   151,   123,   327,  7828,   809,\n",
      "         11152,  9434, 22280,   125,   949,  1601, 21563,   122,   398,  4861,\n",
      "           304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101,  5101,  1817,   271,   125, 12215,   117, 11690,   179,   527,\n",
      "          3004,   151,  1598,  1932,   236,   123,  2821,   240,   179, 15706,\n",
      "         22287, 22032,   252,   117,  1502,   123,  4970,   256,   229, 22280,\n",
      "           978,  1858,  3045,   304, 22280,   179,   229, 22280,  2589, 15397,\n",
      "           159,   123,  9586,   117,  1434,   118,  2036,  4067, 22279, 14829,\n",
      "           118,   176,   123,  1485,   260,   675,  6272, 22281,   122,   853,\n",
      "          6522,   128,   102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,   221,  3413, 16653,  1202, 22287,   171,  3896,   230,  3264,\n",
      "          9317,   285,   179,  8544,  6039, 14324,   221,   259,  3492,   760,\n",
      "          1156,   145,   117,   271,  1941,   259,  1021,  3714,  2044,   700,\n",
      "           180,  5112,   171,  4170,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,   118,  2354, 22279,   229, 22280,  8625,  1790,   117,   527,\n",
      "          3004,   151,   136,   118,   706,   333,   102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,   449,   229, 22280,   176,   380,   436, 22285,   524,   240,\n",
      "          7343,  3953,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,   118,   607,   125,  4412, 15480,   136,   118, 15212,   173,\n",
      "           179, 19226, 22282,   146,   596,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,   222,  3907,   523,  6814,  1204,   123,  9586, 13449, 11252,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,   118,   122,  1941,  3933,  2377, 21569, 11837,   508,   136,\n",
      "           118,   744,   229, 22280,   122,   123,  2211, 13793,   125,   891,\n",
      "           232,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101,  4922,  2880,   151, 22280,   122,   202,  1423,   366,  3979,\n",
      "           401,   180,  9586,   117, 16519,   146,   139, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101,   447,   793,   117,   179,   262,  5300,  1129, 14472,  2855,\n",
      "           286,   229,  4767,   102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[  101,   118,  2663, 22283,   123,   327,  3743,   173,  3420,  8544,\n",
      "           320, 11967, 10953,   763,   146,  4141,   236,  5555,   118,   311,\n",
      "           202, 12399,   171, 17721,   201,   102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[  101, 12044,   260,   675,  6107,   117,   527,  3004,   151,   102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101,   495,   146,   139, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101,   447,   793,   222,  4575,   125,  3113,  6958,   124,   117,\n",
      "           229, 22280,   785, 13717,   243,   117,   449,   577, 22290,   268,\n",
      "           122,  1151,   741,   243,   271,   222,  1671, 22280,  4004,   143,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101,  2440,   125,   347,  1831,   570,  9811,  6284,   243,   978,\n",
      "          5288, 12043,  2420, 21595,   560, 22278,   122,  1390, 17373,  7909,\n",
      "           179,  2036,   285,   256,   766,   226,  1053,   351,   125, 13254,\n",
      "           117,   122,  1105,   256, 15363,   170,   259,  2389, 13732,   683,\n",
      "           125,  3673,   203,  1537,   102]])\n",
      "DEBUG: Tokenized sentence 50: tensor([[  101,  2044,   123,   681,  2064,   304, 22280,  3009,   151,   118,\n",
      "           176,   146,  1903,  4438,  3370, 10441, 22280,   143,   179, 21351,\n",
      "          1684,   222,  1703,   953,   125,  7799,  3979,   401,   170,   179,\n",
      "           176, 17337,  2872,   123,   898,  5397,   102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[  101,   625,   146,   447,   793,   229,  3322,   125,  7392,  1796,\n",
      "           423,  7472,   125,   438,  1165,   128, 12920,   180, 22006,   125,\n",
      "           527,  3004,   151,   117,  4023, 22279,   222,  9667,   179,  1065,\n",
      "          2044, 13865,   123,  3402,   366,   689,   303,   143,   420,   146,\n",
      "          5023,   428,   122,   327,   879,  6720,   266,   102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[  101, 11099,   146,  4575,  4915,   123,  9586,   221,   123,  4067,\n",
      "           125,   327, 20027,   102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[  101, 13638,   118,   176,  9911,   527,  3004,   151,   117,   122,\n",
      "          4492,   179,   495,   327,  4388,   304, 22280,  5212,   173,  1105,\n",
      "          2004, 22278,   117,   229,  4067,   272,   121,   102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101,  5101,  1817,   449, 17738, 13808, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101,   118,   449, 12171,   285,   117,  7122,  9586,   117,   179,\n",
      "           744,   122,  2892,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101,   118, 15212, 13417,   481,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[  101,   118,   331,   712,  4698,   122,   222,   122,   179,   926,\n",
      "         22278,  5212,   498,   898,   122, 16971,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[  101,   118,   122,   123,   327,  5012,   151, 22280,   136, 17891,\n",
      "         10381,   320,  7472,   179,   311,   125,  1342,  5023,   428,   325,\n",
      "          4917, 22281,  3824,   403,   102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[  101,   118,   271,  1331,   136,   118,   122,  2571, 14211,  2036,\n",
      "          4849,   244,   117,   179,   368,   607,   125,  9386,   118,   311,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101,   123,  3122,  2811,  5902, 10928,   117,   146,   447,   793,\n",
      "          3929,  1940,   117,   122,  4031,  2904,   325,   363,  3992,   175,\n",
      "           229, 22280,   598, 18250,   123,  6272,   180,  2614,   387,   102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101,  7583,  3138,   171,  4794,   320,  7472,   221,  2873,   304,\n",
      "         22280,   180, 22006,   229, 22280,  2036, 15397,   186,   102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101, 20552,   368,   179,   260,  6318,  2210, 16388,   122, 19543,\n",
      "         22281,   229, 22280,  3207, 22287, 21839,   143,   125,  8045,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[  101,  2044,   700,   298, 15322, 22281,   117,   121,   102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[ 101, 5101, 1817, 9883,  118,  176,  221, 4314,  123,  390,  304,  173,\n",
      "         4676,  102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[  101,  1004,  1111, 22279,  3103,   978,   123,  4970,   256,   125,\n",
      "         11825,   123,  3867, 19224,   340, 22281,   179,   146,   447,   793,\n",
      "         17003,   370,   125,   576,   173,   625,  1988,   123,   879,  6720,\n",
      "           266,  9365,   125, 11169,   180, 22006,   449,  3102,  2009,   527,\n",
      "          3004,   151,   495,   125,  4493,  6105,   122,   229,   763,  8849,\n",
      "           256,   179, 16241,  1537, 22287, 18047, 22281,   236,   170,   146,\n",
      "           179,   740, 13028,   532,  3907,  2698,   102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101,   118, 21719,  2748,   117,  7343,  7392,  1996,   123,   390,\n",
      "           304, 14094,   230,  4303,  8945,   102]])\n",
      "DEBUG: Tokenized sentence 67: tensor([[  101,  1921,  4303, 10348,   221,   222, 10862, 20882,   246,  8556,\n",
      "          2472,   146,  1997,   495,  9955,   240,   230,  9284, 13793,   810,\n",
      "           117,   271,   146,  4745,   298,   338,  7485,   143,   125,   495,\n",
      "          1087,   122, 13218,   170,   222, 12617,  5580,   125,  6251,  2460,\n",
      "          3240, 22282,   266,   555,   102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[  101,   498, 13501,   375,   117,   173,  8951,   125,  5731,   117,\n",
      "          1021,   146,   964,   175,   397,   122,   325,   466, 10319, 22281,\n",
      "           125,  4766,   102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101,   202,  2182,   173,   179,   527,  3004,   151,   117,   700,\n",
      "           125,  3852,   146,   447,   793,   117,  8544,   240,   327,   576,\n",
      "          4270,   202, 10862,   117,  4169, 22278,  4303,   180,  1390,  1970,\n",
      "           123, 10420, 10656, 12160,   117,  7492,   123,  1977,   123,  9586,\n",
      "          4165,   151,   170,   188, 20154,   138,   102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[  101,   123, 22076,  9079, 22278,  2508,  2277,   964, 13513,   117,\n",
      "         14034,  3128, 22281,   375, 22280,   221,  8684,  2443,   118,   176,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101,   527,  3004,   151, 21161,   118,   176,  3914,   170,   222,\n",
      "         22000,   125,  6185,   421,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101,   118,  8204,  2477,  3185,  2097,   117, 11021, 22288,   123,\n",
      "         10420, 10656, 12160,   449,   229, 22280, 10340, 22279,   117,   179,\n",
      "         11350,   118,   311,   146,   184,   809,  7961,   102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101,   495,   221, 20222,   140,   179,   368,  2080,   102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101,   118,  1941,  9679,   118,   123, 22296,  1977,  2036,  5222,\n",
      "           136,  1502,   262,  3185,  2097,   117,  1021,   125,   333,   325,\n",
      "           125,  1423,   118,   644,   102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[  101,   118,   420,  1382,  3004,   151,   549,   654,   146,   644,\n",
      "          6345,   117, 12418,   123,  7492,   146, 14148,   179, 16403,   221,\n",
      "           146,  2699,   122,  3891,   320,   421,  8090,   738,   748,   123,\n",
      "          4303,   498,   898,   102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101,   229, 22280, 20954,   860,   240, 11483, 22282,   320,   447,\n",
      "           793,   117,   179,   412,   969,   194,   292,   180, 19224,   340,\n",
      "         11175,   256,   125,   327,  7814,   890,   340,   102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[  101,   118,   170,   179,  4131, 22278,  5401,   740,  1790,   136,\n",
      "         10355,   420,   898,   146, 20073,  2189, 13732,   268,   102]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\turevs\\AppData\\Local\\Temp\\ipykernel_8244\\167791990.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(12, 8))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Tokenized sentence 78: tensor([[  101,   527,  3004,   151,  1636,   203,   118,   176,   123,  9317,\n",
      "           125,   495,  1087,   117, 16635,   214,   146,  5023,   428,   123,\n",
      "          9166,   123,   661,  5843, 22278,   179,  2036,  9086,   975,  1885,\n",
      "           185,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 76 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.016983695640636075\n",
      " Coesão Score Final: 0.5084918478203181\n",
      " Conectivos encontrados: ['e', 'mas', 'assim', 'logo', 'pois', 'quando', 'se', 'caso', 'apesar de', 'sobretudo', 'nem', 'ou', 'ora', 'quer', 'seja', 'senao', 'como', 'quanto', 'em vez de', 'para que', 'ao contrario', 'sobretudo', 'nao so', 'quanto', 'se nao']\n",
      " Número de conectivos: 25\n",
      " Número de sentenças: 79\n",
      "======================\n",
      "Resultados para preprocessado_senhora_jose_de_alencar_cap_3.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'assim', 'logo', 'pois', 'quando', 'se', 'caso', 'apesar de', 'sobretudo', 'nem', 'ou', 'ora', 'quer', 'seja', 'senao', 'como', 'quanto', 'em vez de', 'para que', 'ao contrario', 'sobretudo', 'nao so', 'quanto', 'se nao'], 'num_conectivos': 25, 'proporcao_conectivos': 0.017, 'similaridade_media': np.float64(1.0), 'num_sentencas': 79}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  1977,  5521, 22281,   236,   527,  3004,   151,  4900,  2182,\n",
      "           117,   229, 22280,  4314,   151,   125, 14623,   123,   940, 15578,\n",
      "          4275,  4322,   179,  5357, 22278,   259, 13665, 15152,   834, 10130,\n",
      "           175,   122,   179,  1554,   151,   173,  1719,   123,   327,  2760,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   495,   230,  9434, 22280, 16224,   117, 18827,   285,   117,\n",
      "           851,  5988,  4812,   117,   179,  1941, 22281,   269,  2836,   327,\n",
      "          7828,   117,  3951,   118,  2036,  1821,   123, 13605,  8973,   180,\n",
      "          1009,  4408,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   449,   202, 19068,   269,  1286,   125,   532,  1491,  5708,\n",
      "           332,   442,  6788,   692,   260, 15888,   679,   303,   143,   180,\n",
      "          5128,  3292,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,  1577,   256,   118,   176,  9918,   230,  4697,  8497, 13793,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   146,   905,   247, 11914,   180,  2606, 18370,   256,   347,\n",
      "          7407,  2711,   117, 13883,  3391, 13793,   117,   221, 16810,   118,\n",
      "           176,   202,  7549,   551,   117,   582, 18489, 22287,   260, 16543,\n",
      "         20519,   352,  4521,   171,  2397,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101, 16149,  2880,   247,   143,   347,  5791,   183,  5055, 22278,\n",
      "          1815,   599,   218, 15182,   179,  5057, 13239,   222,  1945, 10953,\n",
      "           342,   412,  1298,   565,   171,  9604,   128,   117,  2440,   171,\n",
      "          1340,  7052, 13376,   303,   125,   179,   123,  3402,  1021,   344,\n",
      "           825,   202,   577, 10283,  2189, 13732,   268,   146, 14793,   298,\n",
      "          5419, 14466,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   495,  5034,   221,  8408,  1063,   283,   712, 14848, 22281,\n",
      "           122,  2082,   183,   123,   222,  5023,   428,   117,   123,  4588,\n",
      "         16719,  3391,   151,   170,   179,  1921,   390,  9629, 13417,   481,\n",
      "          2533,   351,   118,  1447,   260,  5790,   183,   143,   325, 12543,\n",
      "           401,   146, 13380,  3482,   179, 15245,   442,  3907,  2698,   117,\n",
      "           122,   123, 12143,   170,   179,  5057,   117,  1615,  1176,   125,\n",
      "         14876,   151,   117,  1569,  1577,   304, 22280, 18784,   735,   232,\n",
      "           657,   785,   760, 17241,   122,  1800,  1471,  2382,   179,  2589,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   229, 22280,  1021,   240,   210,   173,   527,  3004,   151,\n",
      "          2798, 15419,   171, 17850,  2951, 22280,  7094, 18075,   125,  8206,\n",
      "           390,  1149,   179,   117,  1226,  3210,  4095,   243,   173,  8092,\n",
      "         22281,  3258,  2704,  1450,   202,   303,   143, 10398,   117,   176,\n",
      "          3285,   210,   123,   316,   702, 16605,   125,  2745,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,  1004,   320,   598,   342,   117,   740, 19726,  5723,   327,\n",
      "          2000,  3292,   117,   125,   179,   331,  5057,  1700,   117,   625,\n",
      "           146, 19764, 22287,   532,  2004,   128,  6677, 10297,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,  1796,   180, 22283, 16241,  1537, 22287,  2036,  8362, 22278,\n",
      "          5961,   125,  3907,  2698,   122, 20643,  5012,   151, 22280,  9365,\n",
      "           125,   144,  5424, 22281,   179,   229,  4149, 20057, 21517,  6867,\n",
      "           123,   327, 22127,   125,   390,   304,   969,  7700,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   146,   447,   793,   229, 22280,  1011,   123, 10303,   978,\n",
      "          7955,  7583,  1824,   460,   292,  1390, 17373,  7909,   117,   179,\n",
      "          2036, 10348,   222,   416, 19565,   159,   125, 13779, 19622,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   229, 11547,   273,  5424,   285,  2990, 19224,   340,   117,\n",
      "           368,   117,  2397, 21552,   122, 14419, 22305,   117,  7622,   562,\n",
      "         17627, 12543,  3391, 22280,   143,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  1016,   495,  1364, 22073,   117, 12171,   183,   260,  3724,\n",
      "           180,   390,   304,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   118,  5902,   244,   123,  4676,   125, 20990,   285,   118,\n",
      "          1340,   117,  7343,  7392,   117,   221,  5961,   118,  2036,   125,\n",
      "          4947,   785,  1848,   221,  9726,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   118,   123, 22296,   785,  1848,   136,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101, 21289,   146,  4575, 18260,   123,  3049,   304,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   118,   125,  7343,  2982,  1996,   527,  3004,   151,   170,\n",
      "           123,   636, 10310,   794,   852,   122,   333,   194,   292,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,   146,  2189, 13732,   268, 11602, 22288,   229,  9104,   271,\n",
      "           222, 17772, 22280,  2859,   713,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,   221, 12746,   934,   327,   271,   304, 22280, 15518, 17170,\n",
      "         22288,   260,   223,   128,   124,  3274,   230,   229,  1858,   117,\n",
      "         22000,   179,  4837,   256,  8807,   739,   762,   343,   304, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   118,   229, 22280,  7093,   179,  1941, 12044,   173,  2169,\n",
      "           125,  9767,   149,  2291,   136, 16795,   123,   390,   304,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   118, 11412, 13417,   481,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,   118, 20881,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   118, 20881,   136,  4450,   244,   179,   744,   229, 22280,\n",
      "           259,   978,  2160,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  1615,  1105, 22287,   118,   176,  1014,  2169,   117,   122,\n",
      "          2684,   325,   390,  1149,   657,   210,   122,   625,   376,   146,\n",
      "          1568,  6199,   291,   123,   223,   413,   508,   221,  8671,   222,\n",
      "          4062, 20743,   122,  3456,  2387,  8128, 14738, 22282,   477,   268,\n",
      "           143,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,   230,  9586,   438,  1165,   117, 10349,  1840,   403,   117,\n",
      "          2779,   229, 22280,  2036, 12910,  2430,   151,   179,   176,  4103,\n",
      "           236,  3133, 13793,   700,   285,   636,   292,   117,   625,  7422,\n",
      "         22281,   236,  1004,   146,  1147,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   118,  1941,   146,   818, 22280,  3547,   117,  1204,   123,\n",
      "           390,   304,   170,   146,   653,  5902,   333,   247,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   118,   318, 13793,   418,  2121,   328,   136,   118,   316,\n",
      "         22280,  2121,   328,   179,  2036,  1449, 22283,   418, 19224,   340,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,   118,  1941, 18661,  8781,   179,  2779,  1178,   175,   614,\n",
      "           210,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,   179,  2779,  2036,  8183,   130,   222, 20743,   529,  1109,\n",
      "           319,   143,  3804, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,   122,   760, 17241,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101,   222, 10738,   202,  1652,   125, 11179, 22282,   230,   390,\n",
      "           304,   271,  2354, 22279,   117,   527,  3004,   151,   136, 17878,\n",
      "           607,   125,   176,  1165,  2964,   123, 13880, 18982,   340,   118,\n",
      "           229, 22280,  3804,   117,  7343,  7392,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,  1941,   146,   417,  9193, 16708,   146,   447,   793,  1342,\n",
      "           498, 22281, 10557,   183,   179,   146,  1191,   125,  1160,  5995,\n",
      "           159,   229,  9104,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[101, 118, 271, 136, 102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,   376,   614,   210,   125,  9834,   136,   118,  5112, 22280,\n",
      "           117,  7343,  7392,   117,   229, 22280,  3486,   214,   327,  4616,\n",
      "          4074,   285,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,  2826, 22280,   118,  2036,   179,  3384, 22283,   146,  2397,\n",
      "           170,  1977,   155,  2678, 22283,   125,  6759,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[ 101,  118, 1941, 4620,  214,  102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[ 101,  449, 1004,  873,  102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,   271,  5023,   428,   117, 15212,   125,  2822,   123,  7122,\n",
      "          3550,  3391, 13793,   102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,   118,   125,  4863,   117,  7343,  5023,   428,   449,  1921,\n",
      "          3550,  3391, 13793,   146,  7258,   229, 22280,   607,   125,   333,\n",
      "           316, 22280, 15608,   179,   123,  2128,  3897,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,   176,   146,  3283,   140,   117,   146,   179,  2779,   229,\n",
      "         22280, 14657, 22280,   117,   146,  7472,   125,   438,  1165,   128,\n",
      "           123,  7648,   124,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[ 101,  118,  146, 7472,  136,  102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,   179,  4131,   138,   629, 22280,  3867,   179,  2036,  6952,\n",
      "         22287,  3285,  1825,   229,  3049,   304,   117,   527,  3004,   151,\n",
      "           136,   118,   139, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101,   447,   793,   117,  1996,   123,   390,   304, 18827,  9468,\n",
      "           122,   338, 22281, 15394,   214,   170,   222, 11552, 10343,   123,\n",
      "          3122,   337,  3066,  2037,   171,   415,   268,   117,  1137,   185,\n",
      "         22283, 20881,   481, 21174,  7854,   140,   222, 10329,   310,   125,\n",
      "          2169,  8094,   179, 15212,  6637,  2420,   221,   494,   140,  7122,\n",
      "          2760,   122,  6109,   170,  1634,   125,  3083, 13793,  4457,   244,\n",
      "           171,  7472,   125,   438,  1165,   128,   117,  2440,   272,   327,\n",
      "         13638,   232, 22280,   117,   222, 21672,   186,   125,  4073,   194,\n",
      "           304,   221,  6759,   118,   311,   170,  1977,  2779,  8204,   140,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101,   176,  2983, 14211,   741,  4379,   560,   229, 22280,  2036,\n",
      "         12458,  1182, 22287,   117,  4849,   118,  2036,   118,   122, 22283,\n",
      "           222,   179,   311,   122,  4012,   102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[  101,   118, 19821,   123,   792,   417,  1797,   456,   146,  4575,\n",
      "           221, 14195,   146, 22243, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[ 101,  118,  122,  123, 7122, 6272,  102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101,   146,  7258,   229, 22280,  4178,   146,   179,   740,  5488,\n",
      "           117,   449,  3436, 22280,   118,  2036,   179,   221,   123,  4915,\n",
      "           123,  3901,   229, 22280,   834, 22279,  2822, 22278,   125, 19978,\n",
      "           159,   123,  2316, 20798,   125,  7343,  1938, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101,   118,   122,  2004, 22280,   180,  2169,   629, 22280,  5365,\n",
      "           179,  2529,   176,   376,   712, 20881,   481,   122,  1257,   653,\n",
      "          1941,  2541,   660,   900, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101,   118,  6969,   289,   179,  4438, 20881,   481,   117, 13417,\n",
      "           259,  2569, 22283,   229,  4493, 10908,   122,   222,   202, 17387,\n",
      "           180,  8869,  1266,   582,   572, 22283,  4636,  1378,   125, 19170,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 50: tensor([[  101, 15212,   260,   924,  1491,  4073, 22280,   143,   171,  1147,\n",
      "           123,   180,  3061,  1994,   122,   123,   180,  4149, 10491,   340,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[  101,   818, 22283, 19888,   146,  3495,   271,   222,  4551,   300,\n",
      "          1790,   146,   818, 22280,   271,   222, 14744, 22280,   695, 20691,\n",
      "           293,   102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[  101,   240,  1104,  1017,  7427,   333,   325,  7492,   171,   179,\n",
      "           146,  7258,   179,  2364,   262,  2798,   316, 22280,  6754,   117,\n",
      "           271,  2779,   572, 22283,   117,  2798,   316, 22280,  8797,   117,\n",
      "           271,  2779,  7206,   102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[  101,   146,   447,   793,  2389, 15736,   170,  1063,   283,  1921,\n",
      "           390,   304,   179,  2036, 18402,   170,   316, 22280,  6511,  4073,\n",
      "         13793,   171,  1147,   122,   230,  3252,  4002,   221,   368, 12431,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101,   118,   118,   229, 22280,  1201,   151,   123,  7482,   370,\n",
      "          1971,  3495,   117,  3449,   527,  3004,   151,   117,   176,   368,\n",
      "           229, 22280,  1312, 22281,   236,   221,  6759,   118,   311,   123,\n",
      "          7343,  1289,   183,   744,   179,   221,  3413,  1547,  1395, 17551,\n",
      "          3598,   578,  1089,  3061,   371,   909,  9342,   125,  7911,   102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101,   118,   123, 22283,   122,   179,   418,   123,  7614,   117,\n",
      "           417,  1797,   456,   146,   447,   793,   117,   179,  1065,   785,\n",
      "           730,  8393,  5723,   230,  6619, 13793,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101,  1004,  4178,  1382,  3004,   151,   117,   179,  2779,   271,\n",
      "          5023,   428,   229, 22280, 21174, 15891,  1217,   222,  4698, 22287,\n",
      "           834,  1368,  2446,   304, 22280,   171,  7472,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[  101,   118,   146,  7258,   229, 22280,   311,  3189,  9050,   117,\n",
      "          7343,  5023,   428,   117,  3898, 10283, 22288,   123,   390,   304,\n",
      "           170,   222,  1744,  3897,  1990,   283, 20933, 10490, 19773,   340,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[  101, 18661,  1659,   117,   122, 18661, 14619,   210,  1615,   144,\n",
      "          5424, 22281,   179, 16241,  1537, 22287, 12223, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[  101,   240,  1416, 18661,   146,  7426, 13283,   243,   366,  1178,\n",
      "         17377, 22281,   117,   123,  4581,   171,  3436, 22280,   117,   260,\n",
      "           144,   154,   303,   143,   180,  1174,   304,   117, 18661,   179,\n",
      "           395,   303,   230,  1284,   125, 11050,   128,  5066,  1042, 22281,\n",
      "           170,   123,  5583,  7919,   122,  3899,  3679, 22280,   125,   230,\n",
      "         14038, 22278,   125,   610,  5547,   102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101,   146,   447,   793,  1011, 18010,   183,   102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101,   118,   122,   240,   169, 14116, 18661,   179, 15212,   230,\n",
      "           689,   304, 22280,   125,  2745,  2249,  1776, 22278,  7343,  1938,\n",
      "         22280,   117,  3321,   240,   347,  2004,  4149,  1264,   122,   179,\n",
      "           311,   262,  5180,   240,   368,   653,   102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101,  1014,   576,   146,   879, 22282, 19302,   517,  2189, 13732,\n",
      "           268,  4136,  3093,   272,   685,   117,  6804,   148, 17154,  5592,\n",
      "           125,   316, 22280,  5443,   122, 13376,   304,   934,   324,  9217,\n",
      "           117,   271,   123,   179,  2036,  9623,  2014,  2836,   260, 14790,\n",
      "          4242, 18478,  2208,   122,   146, 19042, 22279,  7967,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[  101,   118,  3413,  3189,  4640,   179,   176,  2779,  6344,   222,\n",
      "          5023,   428,   179,   311,   598,  1313,   236,   122, 11314, 22281,\n",
      "           236,   173,  7343, 16550,   825,   117,   320,  2041,   702,   123,\n",
      "          7122,   636,   292,   229, 22280,  2036, 16314,   163,   343,   304,\n",
      "         22280,   117,   834,   652,  3852,   222, 10423,   529, 11169,   273,\n",
      "          3611,  3595,   304, 22280,   221,   146,   179,  8540,   246,   229,\n",
      "         22280, 12210,   303,   125,  6669,  2798,   125,  5825,   118,  2978,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[  101,   118,  1141,   117, 17704,   418,   173,   347,  2368,   117,\n",
      "          1204,   146,  4575,  3531,   373,   102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[  101,   118,  3049,   214,   118,   311,   240,   210,   123, 12464,\n",
      "           125,   370,   222,  5023,   428,  7343,  3695,   117,   179,   311,\n",
      "           659,  1485,   260,  6272, 22281,   117,   271,   259, 17516,   117,\n",
      "          7343,  7392,   102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101,   118,  1084,  1257,   122,  3295,   118,  3102,  1652,   117,\n",
      "           173,   576,   125,  6074,   123,  4500,  3292,   122, 10363, 19356,\n",
      "           140,   118,   311,   170,  1846, 22281,   122, 11169,   117,  3687,\n",
      "          2745,   240,  1004,  8805,   102]])\n",
      "DEBUG: Tokenized sentence 67: tensor([[  101,   744,   325,   117, 18661,   179,   123, 22006,   122, 20030,\n",
      "           117,   449,  1016,   229, 22280,  1981,   333,   625,   259,   438,\n",
      "          1165,   128,   376,   125,   425, 10780,   703,   179, 16109, 22282,\n",
      "           146,  1223,   179,   180, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[  101,   118,  1084,  1257,   229, 22280,   117,   527,  3004,   151,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101,   860,  4619,   339,   122,   230,  2450, 22278,  5261,   671,\n",
      "           117,   179, 12659,   123, 14876,   151,   125,   327,   223, 22279,\n",
      "           117,  1052,   508,  3264,   122,  1684,  5334,   671,   925,   148,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[  101,   146,   447,   793, 13540, 19328,   203,   202,  2242,   171,\n",
      "          9834,   230,  1084, 12092,   148,   179,   368,  4914, 22278,   730,\n",
      "           684,   140,   117,   176,   122,   179,   229, 22280,   123,   978,\n",
      "           147,  1172,   201,   271,  4048,   325,  2310,   415,   102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101,   122,   123,   390,   304,   173, 14399,   123, 14876,   151,\n",
      "           125,   327,   223, 22279, 16117,  2382,   423,   415,   268,   117,\n",
      "         15568, 22288,   118,   176,   222, 16423, 22279,   123,  4252,  2122,\n",
      "           125, 11552,   412, 11471,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101,   625,  2927,   123,   347,  1247,   117,   146,   447,   793,\n",
      "          1011,   125,  1364, 17671,   286,   298, 13200, 22281,   240,   179,\n",
      "          1021, 15394,   243,   122, 15245,   118,   176,   320,  2711,   117,\n",
      "         12837,   303,   117,  4222,  1228,   175,   122,  3979,   181,   268,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101,   118, 15060, 21894, 22281,   136, 16795,   123,  9586,   170,\n",
      "           123,   898,  2515, 15182,   179,   229, 22280,  4314, 22278,   173,\n",
      "          1364,   860,   644,  6345,   102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101,   118,  2354, 22279,   122,   230, 18190,   289,   364,  7248,\n",
      "           117,   527,  3004,   151,   659,   125,  9726,   146,   179,  3189,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[ 101,  118, 1314, 3093,  154, 1004,  117, 7343, 7392,  102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101, 17891, 12908, 22282,   118,  2036,  7343, 11021,   117,   222,\n",
      "         11021,   179,   123, 16241,  1537, 22287,  3102,  1147,   262,   130,\n",
      "          3139,   243,   117,   122,   179,   331,  4023,  4178,   102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[  101,   176,   700,   125,  7422,   118,  1340,   117,   146,  7258,\n",
      "           229, 22280,   311,  8204,   140,  6202,   117,   291,   229,  1212,\n",
      "          8102, 22282,   117,  2779,  9871,  2036, 16759,   823, 22283,   102]])\n",
      "DEBUG: Tokenized sentence 78: tensor([[  101,   118,   706, 12908, 22282,   173,  9726,   834,  2082,   183,\n",
      "           146,   347, 11021,   117,   527,  3004,   151,   117,   179,  6515,\n",
      "           118,   311,   118,   122, 22283, 12856, 22280,  2990, 12908,  2028,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 79: tensor([[  101,   118,  3960,   247,   117,   139, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 80: tensor([[  101,   447,   793,   117,   122,   221, 11076,   118,  2036,  1569,\n",
      "           440,  5345,   452,   179,   240, 19937,   146, 14408,   185,   117,\n",
      "          2036,  3436, 22280,   412,  5058,  1140,   125,  7122,   223, 22279,\n",
      "           117,   179,   176,   607,   221,  9726, 15685,  3102,  1147,   117,\n",
      "           122,  2529,   418,   179,   146,  7258,   155,   706,  2822,   102]])\n",
      "DEBUG: Tokenized sentence 81: tensor([[ 101,  118, 2586,  252,  125, 9726,  102]])\n",
      "DEBUG: Tokenized sentence 82: tensor([[  101,   527,  3004,   151, 13683,   222, 16423, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 83: tensor([[  101,   118,  7422,   146,  8650,   162,   136,   118,   615,  2866,\n",
      "           136, 16795,   146,  4575,   222,  1971,  1334, 13808,   243,   102]])\n",
      "DEBUG: Tokenized sentence 84: tensor([[  101,   118,  5009,   178,   316, 12309,   171,  8650,   162,   117,\n",
      "         12531,   180,  6261,   323,   421,  1996,   123,   390,   304, 16700,\n",
      "           214,   327,   505,   185,  2032, 13808,   102]])\n",
      "DEBUG: Tokenized sentence 85: tensor([[ 101, 2660,  123, 7390, 1076,  125, 5357, 4428,  102]])\n",
      "DEBUG: Tokenized sentence 86: tensor([[  101,   229, 22280,   122,  8797,   117,   449,  1776,  3933,   144,\n",
      "          5424, 10010,   654,   146,  2982,   180,  2267, 17930, 18376,   272,\n",
      "           170,   222,   390,   303,   179,  3851, 18670,   171,  2187,   125,\n",
      "          1543,   117,   122,   123,  1977,   368,  9939,   125,   171,   185,\n",
      "          7187,  5371,   128,   125,  7911,   102]])\n",
      "DEBUG: Tokenized sentence 87: tensor([[  101,   320, 17782,   307,  3769,  3724, 10733,   118,   176,   222,\n",
      "          8931, 22305,  8574,   141,   229,  4410,  1684,   316, 22280,  1743,\n",
      "          2487,   180,   390,   304,   117,   179,  2044,  7904,   128,  5235,\n",
      "           222,   964, 20732,  3979, 16489,   102]])\n",
      "DEBUG: Tokenized sentence 88: tensor([[  101,   146,   447,   793,  4412, 22278,   577,  1604,   125,  6367,\n",
      "           179,  1941,   495,   122,   221, 12746,   934,   146,   347,   873,\n",
      "          2037,   155, 11935, 15135,   123,  3049,   304,   362,  3915,   147,\n",
      "          8156, 10898,   117,   170,   146, 21959,   123,  7729,  2037, 22282,\n",
      "           122,  9456,   702,   146, 20327,   994,   117,   271,   176,   860,\n",
      "           146,  9697, 17190,   236,   102]])\n",
      "DEBUG: Tokenized sentence 89: tensor([[  101,   527,  3004,   151, 20680,   222, 16423, 22279,   146,   347,\n",
      "         10343, 11552,   202,   834, 10130,   175,   171,  4575,   700, 16946,\n",
      "           214,   408,   266,   218, 15182,   123,  3122,   221, 11586,   118,\n",
      "          1084,   229, 19651,   387,  6628,   125,   327,   505,   185,  2032,\n",
      "         13808,   117,  2002,   596,   320,  7392,   125,  7305,   578,   118,\n",
      "           176,   117,   146,   455,   262,  5255,   102]])\n",
      "DEBUG: Tokenized sentence 90: tensor([[ 101,  146,  447,  793,  978,  146,  338,  455, 1286,  171, 1147,  102]])\n",
      "DEBUG: Tokenized sentence 91: tensor([[ 101,  118, 7187, 9342,  136,  102]])\n",
      "DEBUG: Tokenized sentence 92: tensor([[  101, 10719,   368,   102]])\n",
      "DEBUG: Tokenized sentence 93: tensor([[  101,  1941,   229, 22280,   122, 11775,   662, 14679,  3750,   584,\n",
      "          3449,   118,   122,  8911,  2249,  1075, 12156,  8257,   159,   860,\n",
      "          2982,   102]])\n",
      "DEBUG: Tokenized sentence 94: tensor([[  101,   123,  3605,   266,   878,  1981,  6759,   170,   146,   121,\n",
      "         22282,   102]])\n",
      "DEBUG: Tokenized sentence 95: tensor([[  101,   516, 20095,   428,  7057,   397,   125,  1977,   740,  5971,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 96: tensor([[  101,   368,   122,  6754,   122,   240,  1257,   146,  1568,   146,\n",
      "           376, 21094,   117,   449,   176,   146, 17704, 22281,   871, 10982,\n",
      "           320,  8650,   162,   179,  1966,   390,   303,   376,   125,   347,\n",
      "          7226, 11330,  9342,   125,  7911,   117,  7093,   179,   368, 19356,\n",
      "          5424,   322,   136,   118, 10262, 10347,   179,  2779,  5888, 10982,\n",
      "          1257,   102]])\n",
      "DEBUG: Tokenized sentence 97: tensor([[  101,   171,   323,  5197,   151,  1966,  3495,   136,   118,  2779,\n",
      "           146,  2822,   244,   170,   146,   636, 15537,   102]])\n",
      "DEBUG: Tokenized sentence 98: tensor([[  101,   118,   449,   117,  7122,  9586,   117,   221,   179,   538,\n",
      "         19821,   538,  2655, 18377,   140,   538,  3907,  2698,   313,  9193,\n",
      "           128,   136,   118,   146,  7258,   122,  2780,  4588, 16719, 11837,\n",
      "           221, 11675, 11972,   179,   125,   541,   272,  2036, 10469,   151,\n",
      "         11224,   578,   102]])\n",
      "DEBUG: Tokenized sentence 99: tensor([[  101,  2355,   397,   865,  1323, 22282,   118,   311,   834, 10806,\n",
      "           123,   327, 15061,   102]])\n",
      "DEBUG: Tokenized sentence 100: tensor([[ 101,  123,  390,  304, 1191,  222, 3803,  303,  102]])\n",
      "DEBUG: Tokenized sentence 101: tensor([[  101,   118,  1966,   390,   303,   117,   179,   418, 20198,   170,\n",
      "           123,  3605,   266,   878,  8650,   162,   117,   122,   146,  2397,\n",
      "           123,  1977,  2779,  3384, 22283,   221,  7343,   956,   286,   102]])\n",
      "DEBUG: Tokenized sentence 102: tensor([[  101,  1941,   176,   873,   179,   117,   229, 22280,  5735, 17805,\n",
      "           123,   924,   117,   122,  1395, 17551,   179,  2779,   146,  1598,\n",
      "           185,   102]])\n",
      "DEBUG: Tokenized sentence 103: tensor([[  101,   118,  1519,   170,  1039,   417,  1797,   456,   146,  4575,\n",
      "         15518,  2127,   348,   260,   223,   128,   117,   271,  1977,  7622,\n",
      "           151,   259, 14312,   128,   179,  1921, 21103,  2037, 22280,  6853,\n",
      "          2916,   123,   222,  5023,   428, 12008,   102]])\n",
      "DEBUG: Tokenized sentence 104: tensor([[ 101,  118, 1966,  390,  303,  102]])\n",
      "DEBUG: Tokenized sentence 105: tensor([[  101,   118,   146,   655,   136, 16795,   146,  4575,  3848,  7831,\n",
      "           123,  7482,   102]])\n",
      "DEBUG: Tokenized sentence 106: tensor([[  101,   527,  3004,   151,  1191,   222, 10049,   300,   125,  2521,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 107: tensor([[ 101,  118,  860,  390,  303, 2080, 3185, 2097,  122, 2711,  179,  338,\n",
      "          185, 2535,  298, 3058, 1100,  221,  146, 2982,  179,  418, 6544,  183,\n",
      "          607, 3047,  125,  222,  622,  102]])\n",
      "DEBUG: Tokenized sentence 108: tensor([[ 101,  146, 7258, 1981, 2863,  118, 1340, 2249, 1075,  102]])\n",
      "DEBUG: Tokenized sentence 109: tensor([[ 101,  118, 1790,  653,  102]])\n",
      "DEBUG: Tokenized sentence 110: tensor([[ 101,  118,  122, 1434,  118, 2036,  327, 4562,  102]])\n",
      "DEBUG: Tokenized sentence 111: tensor([[  101,  2983, 13692,   629, 22280,   785,  4577,   202,  2187,   125,\n",
      "          1543,   102]])\n",
      "DEBUG: Tokenized sentence 112: tensor([[  101,   118,   418, 22280,   118,   176,  2636,   944,   259,  1564,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 113: tensor([[  101,   118,   146,  7258,  4178,  1407,   171,   179,  2779,   271,\n",
      "           176,  4042,   228,  3769, 19385, 22281,   125, 20743, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 114: tensor([[  101,   118, 11032,   117, 11032,   118,  9670,   517,   118,   146,\n",
      "           125,   179,  7343,   655,   229, 22280,  1981,  4074, 22282,   173,\n",
      "          2745,  3413,   102]])\n",
      "DEBUG: Tokenized sentence 115: tensor([[  101,   118,   123, 22296,  3189, 14223, 22282,   146, 19456, 13093,\n",
      "           183,   136,   118,  2684,   146,  2182,   180,  2064,   304, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 116: tensor([[  101,  5147,   706,  4640,  2249,  1587,   185,   221,   179,   229,\n",
      "         22280, 10262, 10347, 22287,  5790, 22279,  4647,   125,  3933,  7492,\n",
      "           291,  1202,  6933,   251,   102]])\n",
      "DEBUG: Tokenized sentence 117: tensor([[  101,   118, 11094, 22280,  2638, 14376,   146,  4575, 11989,   243,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 118: tensor([[  101,   222,  2982,  2415, 19649, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 119: tensor([[  101,   118,   229, 22280,   117,  7258,  3874,   125, 14155,   371,\n",
      "           303,   143,   102]])\n",
      "DEBUG: Tokenized sentence 120: tensor([[  101,   331,   376,  4073,   194,   304,   221, 11855,   179,   123,\n",
      "         14308,   229, 22280,   122,  7492,  2798,  2996, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 121: tensor([[  101,   118,  3189, 12274,   123, 10013,   136,   118,  5787,   102]])\n",
      "DEBUG: Tokenized sentence 122: tensor([[ 101,  259, 3401,  180, 4562,  102]])\n",
      "DEBUG: Tokenized sentence 123: tensor([[  101,   118,   170,  4073,   194,   304,  1065,   179,  8781, 14223,\n",
      "         22282,   146, 19456, 13093,   183,   117,   229, 22280,  7427,  8264,\n",
      "           136,   527,  3004,   151,  3929,  1940,   222, 16423, 22279,   118,\n",
      "           229, 22280, 18691,   179,  3413,  7282,   171,  7258,   102]])\n",
      "DEBUG: Tokenized sentence 124: tensor([[  101,  1652,   368,   146,  3009, 22278,   271,  7343,  7392,   122,\n",
      "          5023,   428,   117,   229, 22280,  2686,   259, 17516, 20497,   118,\n",
      "          1340,   179,  2779,   229, 22280, 15212,   149,  2291,   123, 14983,\n",
      "         22278,   670,   136,   179,   122,   222,  3907,   523,   180, 20027,\n",
      "           291,   298,  2424,   358,   136,   118,  1004, 18890,  2779,   329,\n",
      "           311, 15580,   229, 22280,  2660, 12245,   102]])\n",
      "DEBUG: Tokenized sentence 125: tensor([[  101,   118,   259,  3401,   180,  4562,  3921,   333,  2983, 12171,\n",
      "           285,  1004,   102]])\n",
      "DEBUG: Tokenized sentence 126: tensor([[  101,   123, 20027,   180,  1815,   390,   304, 20004,  8781,  4672,\n",
      "         22278,   170,  2531,   304, 22280,   125,  6109,   117,  3951,   320,\n",
      "         20743,   123, 17088,   125,  5830,  9342,   125,  7911,   125,   171,\n",
      "           185,   102]])\n",
      "DEBUG: Tokenized sentence 127: tensor([[  101,   176,   229, 22280,  1587, 18509,  2812,   122,   368, 18965,\n",
      "           325,   117,   333, 22278,   146,   171,   185,   125,   623,   118,\n",
      "           118,  1757,  5292,   102]])\n",
      "DEBUG: Tokenized sentence 128: tensor([[  101,   118,   607, 22280,   125,  1587,   578,   102]])\n",
      "DEBUG: Tokenized sentence 129: tensor([[  101,   229, 22280,  2660,   623,  2100,   102]])\n",
      "DEBUG: Tokenized sentence 130: tensor([[  101,   118,   173,  1364,   146,  1652, 18691,   179,   146,  7258,\n",
      "          4620,   404,  1004,   146,  7343,  6314,   102]])\n",
      "DEBUG: Tokenized sentence 131: tensor([[  101,  6532,   117,   271,   122,  2711,   117,  4457,   146,   179,\n",
      "          4252,   214,   117,   146,   325, 11037,   183,   668,  4812,   449,\n",
      "           146, 10346,   122,  4457,   122,  3553,  2684,  3437,   171,   179,\n",
      "           459,  2515, 22280,   117,   229, 22280,   395,   303,  5790,   154,\n",
      "         22280,   125, 14701,   102]])\n",
      "DEBUG: Tokenized sentence 132: tensor([[  101,   122,   123,  7122, 15685,   179, 17891,  8977,   102]])\n",
      "DEBUG: Tokenized sentence 133: tensor([[  101,  3769,   169,  8388, 22281,  3724,   117,   123,   390,   304,\n",
      "         17782,   456,   118,   260,   170,   230,  1884,   118,  2135,  4812,\n",
      "          9434, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 134: tensor([[  101,   118,   229, 22280,   333, 22278, 15045,   136,   118,   146,\n",
      "         22296,  2638, 14376,   527,  3004,   151,   117,  2779, 16314,   240,\n",
      "           740,  1719,   123,  7122,  8869,   102]])\n",
      "DEBUG: Tokenized sentence 135: tensor([[  101,  1028,   123,   376,   125,   416,   304,   117,   179,  7707,\n",
      "          3539,  8219,  1121,   171,  2992, 22288,   102]])\n",
      "DEBUG: Tokenized sentence 136: tensor([[  101,   449,   229, 22280,   311, 21174,   179,  1445, 22282,   117,\n",
      "          1502,  4394,   214,   118,   311,  1966,  1004,   117,  4023,  4968,\n",
      "           272,   685,   118,  2496,  9726,   117,   122,  6084,   118,   311,\n",
      "           625,  1528, 11690,  3541,   252,  2316, 20798,   221,   179,  2779,\n",
      "          6022,  3995,   123,   138,  2079,   304, 22280,   125,  7122,  1069,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 137: tensor([[  101,   229, 22280, 10801,   179,   146,  3495,  3889,  1485,   260,\n",
      "          9427,  3082,   136,   118,   123,   636,  9427,   392,   179,   180,\n",
      "           146,  3495,   122,  1776,   118,  1340,   260,  1028,   629, 22280,\n",
      "          5217, 14348,   117,  1996,   146,   447,   793,   271,   403,  2828,\n",
      "           229,  2013,   151,   102]])\n",
      "DEBUG: Tokenized sentence 138: tensor([[  101,   527,  3004,   151,   117,   179,   222, 16423, 22279,   176,\n",
      "          4314, 22278,  3456,  4776,   159,   423, 10289,   117,  1359,   256,\n",
      "           320,  5902, 10343,   122,  3929,  1672,   170,   455,  1021,  5489,\n",
      "          1672,  2684,  1369,   123,  5790,   154, 22280,   125,   347,  3753,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 139: tensor([[  101,   118,  3207,   118,   311,   744,   117,  7343,  7392,   117,\n",
      "         16302, 22282,   118,  2036,   222,  2009,   102]])\n",
      "DEBUG: Tokenized sentence 140: tensor([[  101,   123,  3661,   117,  1202, 22287,   125, 20853,   117,   418,\n",
      "         22076,   123, 17714,  1430,   942,   102]])\n",
      "DEBUG: Tokenized sentence 141: tensor([[  101,   229, 22280,  1467,   668,  4812,  8364,   860,  3907,   523,\n",
      "           240,  2685,   136,   118,  3852,   146, 10738,   222,  1798,   136,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 142: tensor([[  101, 11412,   117,   449,   176,   368,   577,   140,   123, 15445,\n",
      "           117,   229, 22280,   607,  1423,   125, 18237,   118,  1340, 12733,\n",
      "           159,   102]])\n",
      "DEBUG: Tokenized sentence 143: tensor([[  101,   118,   229, 22280,  1168,   102]])\n",
      "DEBUG: Tokenized sentence 144: tensor([[  101,  2779,  2355,   397, 12908, 22282,   118,   311,   123,  6320,\n",
      "          2990,  2760,   117,  1075,   171,   179,   712, 16270,   102]])\n",
      "DEBUG: Tokenized sentence 145: tensor([[  101,   170,   230, 12126,  6943,   304, 22280,   173,   179,   368,\n",
      "         20045, 22279,   327,  3661,  4412,   244, 20885, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 146: tensor([[ 101,  118,  607,  125,  176, 5646, 4765,  102]])\n",
      "DEBUG: Tokenized sentence 147: tensor([[  101,   118,   122,   145,   146,   179, 14657, 22280,   125,   327,\n",
      "          8286,   117,  7343,  7392,   102]])\n",
      "DEBUG: Tokenized sentence 148: tensor([[  101,   146,   447,   793,  2789,  3852,   123, 18447,   151,   179,\n",
      "         11534,   186,   123,  3661,  8286,   117,   122,  1241,  1353,   123,\n",
      "           363,  4387,  4271,   298,   211,   683,   122,   598,   123,  3377,\n",
      "           117,   123, 14121,   125,  1798,   173,   179,  5357, 22278,   675,\n",
      "          7663,   102]])\n",
      "DEBUG: Tokenized sentence 149: tensor([[ 101,  118,  873, 2872,  128,  102]])\n",
      "DEBUG: Tokenized sentence 150: tensor([[  101,   316, 12309,   171,  8650,   162,   117, 12531,   180,  6261,\n",
      "           323,   421,   102]])\n",
      "DEBUG: Tokenized sentence 151: tensor([[ 101,  123, 2267,  121,  102]])\n",
      "DEBUG: Tokenized sentence 152: tensor([[  101,  3605,   266,   878,   117,  7187,  9342,  1270,   244, 22281,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 154: tensor([[  101,   516, 20095,   183, 13436,   458,   102]])\n",
      "DEBUG: Tokenized sentence 155: tensor([[  101,  6598, 11330,   102]])\n",
      "DEBUG: Tokenized sentence 156: tensor([[ 101,  146, 1342,  102]])\n",
      "DEBUG: Tokenized sentence 157: tensor([[  101,   125,  5830,  2684,   623, 15048,   102]])\n",
      "DEBUG: Tokenized sentence 158: tensor([[ 101,  331,  311, 3207, 3185, 3309,  102]])\n",
      "DEBUG: Tokenized sentence 159: tensor([[  101,   527,  3004,   151, 15399,   180,   505,   185,  2032, 13808,\n",
      "           146,  7645,   185,   125,  3248,   122,  3794,   118,   146,   320,\n",
      "          5023,   428,   102]])\n",
      "DEBUG: Tokenized sentence 160: tensor([[  101,   271,   860,   176,  3058, 22281,   236,  9079,  7826,   735,\n",
      "           307,   173,  2729,  4410,   146,   655,   117,   740,   146,  1316,\n",
      "         22290,  2598,   170,   123,  3661,  5255,   122,  8413,   567,   179,\n",
      "           260,  1176,  2036,  1360, 22281,   321,   256,   259, 18908,   501,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 161: tensor([[  101,   118,  1633,   256, 22280,  2189, 13732,   268, 10798,  4373,\n",
      "           260,  4837,   303,   143,   179,  1021,   202,  3743, 22280,   122,\n",
      "           146,   398,   569,   456,   102]])\n",
      "DEBUG: Tokenized sentence 162: tensor([[  101,   118,  3874,   325,   136,   118,  3874,   117,  3133, 13793,\n",
      "         20206,   118,  2036,   744,   230,   576,   179,  8853, 22283,   173,\n",
      "           675,   223,   128,   123,   877,   232, 15685,   179,  4023,   155,\n",
      "          6105,  3102,  1147,   102]])\n",
      "DEBUG: Tokenized sentence 163: tensor([[  101,   123,   390,   304, 17782,   456,  3769,  3724,   170,   222,\n",
      "          5902,   125,  6511, 18164,   304, 22280,   179, 11092,  4250,   146,\n",
      "          7390,  4645,   451,   735, 16731,   171,  4575,   102]])\n",
      "DEBUG: Tokenized sentence 164: tensor([[  101,   118,   607,   125,   333,   785,  8540,   117,  2779,  2036,\n",
      "          3162, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 165: tensor([[  101,   118,   125,   118,   311,   418, 15685,   117,   179,  2779,\n",
      "          1971,  2401, 22279,  1286,  2779,  2036,  2822,   244,   180,   179,\n",
      "           311,   425,   124,   102]])\n",
      "DEBUG: Tokenized sentence 166: tensor([[ 101,  118, 1519,  170, 1039,  117,  527, 3004,  151,  102]])\n",
      "DEBUG: Tokenized sentence 167: tensor([[  101,   146,  2189, 13732,   268,  7837,   654,   123,   223, 22280,\n",
      "           180,   390,   304,   117,   179,  2036,  6374, 22278,   146, 16450,\n",
      "           304, 22280,   170,   123,   169,  8388, 13246,   122,  9883,   118,\n",
      "           176,   102]])\n",
      "DEBUG: Tokenized sentence 168: tensor([[  101,   625,  2080,   123,  1105,   117,   744,   146,   447,   793,\n",
      "           229, 22280,  1011,   125,  1364, 17671,   286,   171,  3620,   243,\n",
      "          1845,  5790,  6288,  6070, 22278,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 167 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.013629283483789221\n",
      " Coesão Score Final: 0.5068146417418946\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'entretanto', 'portanto', 'assim', 'logo', 'pois', 'ja que', 'uma vez que', 'quando', 'se', 'caso', 'ainda que', 'por conseguinte', 'nem', 'ou', 'ora', 'quer', 'seja', 'senao', 'como', 'quanto', 'em vez de', 'desde que', 'para que', 'uma vez que', 'ja que', 'ao contrario', 'realmente', 'por exemplo', 'tanto', 'quanto', 'se nao', 'por isso']\n",
      " Número de conectivos: 35\n",
      " Número de sentenças: 169\n",
      "======================\n",
      "Resultados para preprocessado_senhora_jose_de_alencar_cap_4.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'porem', 'entretanto', 'portanto', 'assim', 'logo', 'pois', 'ja que', 'uma vez que', 'quando', 'se', 'caso', 'ainda que', 'por conseguinte', 'nem', 'ou', 'ora', 'quer', 'seja', 'senao', 'como', 'quanto', 'em vez de', 'desde que', 'para que', 'uma vez que', 'ja que', 'ao contrario', 'realmente', 'por exemplo', 'tanto', 'quanto', 'se nao', 'por isso'], 'num_conectivos': 35, 'proporcao_conectivos': 0.014, 'similaridade_media': np.float64(1.0), 'num_sentencas': 169}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer  # Or BertTokenizer\n",
    "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
    "\n",
    "model = AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n",
    "\n",
    "def get_sentence_embeddings(sentences, model):\n",
    "    \"\"\"\n",
    "    Obtém o embedding médio para cada sentença usando um modelo pré-treinado.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): Lista de sentenças.\n",
    "        model: Modelo de embedding.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapeamento de sentença para vetor de embedding.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if not isinstance(sentence, str) or len(sentence) < 5:\n",
    "            continue\n",
    "        sentence_embedding = tokenizer.encode(sentence, return_tensors='pt')\n",
    "        print(f\"DEBUG: Tokenized sentence {i}: {sentence_embedding}\")  # DEBUG\n",
    "        embeddings[i] = sentence_embedding\n",
    "    return embeddings\n",
    "\n",
    "def analisar_coesao_sentences(sentences, words, embeddings):\n",
    "    \"\"\"\n",
    "    Analisa a coesão e coerência do texto de forma detalhada.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): Lista de frases do texto.\n",
    "        words (list): Lista de palavras do texto.\n",
    "        embeddings (dict): Embeddings das sentenças (mapeando texto -> vetor).\n",
    "\n",
    "    Returns:\n",
    "        dict: Resultados de análise de coesão, conectivos e similaridade semântica.\n",
    "    \"\"\"\n",
    "    # --- Análise de conectivos ---\n",
    "    conectivos = [\n",
    "        'e', 'mas', 'porem', 'contudo', 'entretanto', 'portanto', 'assim', 'logo',\n",
    "        'pois', 'porque', 'ja que', 'uma vez que', 'quando', 'enquanto', 'se', 'caso',\n",
    "        'embora', 'apesar de', 'alem disso', 'ademais', 'ou seja', 'isto e', 'todavia',\n",
    "        'nao obstante', 'ainda que', 'de modo que', 'de forma que', 'por conseguinte',\n",
    "        'dessa forma', 'desse modo', 'conquanto', 'sobretudo', 'inclusive', 'nem',\n",
    "        'tampouco', 'ou', 'ora', 'quer', 'seja', 'senao', 'assim como', 'bem como',\n",
    "        'como', 'tal como', 'tanto quanto', 'quanto', 'do mesmo modo', 'igualmente',\n",
    "        'em vez de', 'ao passo que', 'desde que', 'a fim de que', 'para que', 'antes que',\n",
    "        'logo que', 'assim que', 'tao logo', 'depois que', 'porque', 'porquanto', 'visto que',\n",
    "        'posto que', 'uma vez que', 'ja que', 'em virtude de', 'em razao de', 'gracas a',\n",
    "        'apesar disso', 'mesmo assim', 'de qualquer forma', 'de qualquer maneira', 'por outro lado',\n",
    "        'em contrapartida', 'em contraste', 'ao contrario', 'pelo contrario', 'no entanto',\n",
    "        'de fato', 'efetivamente', 'realmente', 'com efeito', 'por exemplo', 'alias', 'a proposito',\n",
    "        'inclusive', 'alem do mais', 'acima de tudo', 'principalmente', 'sobretudo', 'nao so',\n",
    "        'como tambem', 'bem como', 'nao apenas', 'mas tambem', 'tanto', 'quanto', 'se nao', 'caso contrario',\n",
    "        'a medida que', 'a proporcao que', 'de maneira que', 'de sorte que', 'de tal forma que',\n",
    "        'de tal modo que', 'de forma que', 'de jeito que', 'de maneira que', 'de modo que',\n",
    "        'em funcao de', 'em vista de', 'por causa de', 'por conta de', 'por motivo de', 'por razao de',\n",
    "        'em consequencia', 'em decorrencia', 'em resultado', 'em virtude', 'em vista disso', 'por isso',\n",
    "        'por essa razao', 'por esse motivo', 'por essa causa', 'por essa razao', 'por esse motivo'\n",
    "    ]\n",
    "    conectivos_encontrados = []\n",
    "    for c in conectivos:\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.lower()  # Normaliza para minúsculas\n",
    "            if c in sentence:\n",
    "                conectivos_encontrados.append(c)\n",
    "                break\n",
    "    num_conectivos = len(conectivos_encontrados)\n",
    "    num_tokens = len(words)\n",
    "    proporcao_conectivos = num_conectivos / (num_tokens + 1e-6)  # Evitar divisão por zero\n",
    "\n",
    "    # --- Análise de coesão semântica ---\n",
    "    # Garante que todos os vetores de embeddings sejam válidos\n",
    "    vetores = []\n",
    "    for vetor in embeddings.values():\n",
    "        if vetor is not None and len(vetor) > 0:\n",
    "            vetor = vetor.T\n",
    "            vetores.append(vetor)\n",
    "    if not vetores:\n",
    "        print(\"Nenhum vetor de embedding válido encontrado. Retornando coesão padrão.\")\n",
    "        return {\n",
    "            'coesao_score': 0.0,\n",
    "            'conectivos_encontrados': conectivos_encontrados,\n",
    "            'num_conectivos': num_conectivos,\n",
    "            'proporcao_conectivos': round(proporcao_conectivos, 3),\n",
    "            'similaridade_media': 0.0,\n",
    "            'num_sentencas': len(sentences)\n",
    "        }\n",
    "    \n",
    "    if len(vetores) < 2:\n",
    "        similaridade_media = 1.0  # Texto curto, assume alta coesão\n",
    "        print(\"isso aqui rodou.\\n\") # DEBUG\n",
    "    else:\n",
    "        similaridades = []\n",
    "        for i in range(len(vetores) - 1):\n",
    "            try:\n",
    "                sim = cosine_similarity(vetores[i], vetores[i+1])[0][0]\n",
    "                similaridades.append(sim)\n",
    "            except Exception as e:\n",
    "                print(f\"Vetores possuem os tamanhos diferentes: {len(vetores[i])} e {len(vetores[i+1])}. Erro: {e}\")\n",
    "        similaridade_media = np.mean(similaridades)\n",
    "\n",
    "    # --- Cálculo do Score Final ---\n",
    "    # Peso 50% conectivos, 50% semântica\n",
    "    score_conectivos = min(1, proporcao_conectivos)  # Ideal: 10% de conectivos\n",
    "    score_semantica = similaridade_media      # Garante entre 0 e 1\n",
    "\n",
    "    coesao_score_final = (score_conectivos + score_semantica) / 2\n",
    "    print(f\"DEBUG ======================\\n len vetores {len(vetores)} \\nSimilaridade média: {similaridade_media}\\n Proporção de conectivos: {proporcao_conectivos}\\n Coesão Score Final: {coesao_score_final}\\n Conectivos encontrados: {conectivos_encontrados}\\n Número de conectivos: {num_conectivos}\\n Número de sentenças: {len(sentences)}\\n======================\")  # DEBUG\n",
    "    return {\n",
    "        'coesao_score': round(coesao_score_final, 2),\n",
    "        'conectivos_encontrados': conectivos_encontrados,\n",
    "        'num_conectivos': num_conectivos,\n",
    "        'proporcao_conectivos': round(proporcao_conectivos, 3),\n",
    "        'similaridade_media': round(similaridade_media, 3),\n",
    "        'num_sentencas': len(sentences)\n",
    "    }\n",
    "\n",
    "# Iterar sobre os textos processados\n",
    "caminho_saida_pasta = \"data/caps_processados\"\n",
    "for arquivo_nome in os.listdir(caminho_saida_pasta):\n",
    "    caminho_arquivo = os.path.join(caminho_saida_pasta, arquivo_nome)\n",
    "    if os.path.isfile(caminho_arquivo) and arquivo_nome.endswith('.json'):\n",
    "        with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo:\n",
    "            dados = json.load(arquivo)\n",
    "            sentences = [\" \".join(tokens) for tokens in dados['tokens']]\n",
    "            tokens = [token for sublist in dados['tokens'] for token in sublist]\n",
    "            titulo = dados.get('titulo', 'Desconhecido')\n",
    "            # Obter embeddings e analisar coesão\n",
    "            embeddings = get_sentence_embeddings(sentences, model)\n",
    "            resultados = analisar_coesao_sentences(sentences=sentences, words=tokens, embeddings=embeddings)\n",
    "            \n",
    "            print(f\"Resultados para {arquivo_nome}:\")\n",
    "            print(resultados)\n",
    "            print(\"\\n\")\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            # Criando um gráfico consolidado para todas as métricas por livro\n",
    "            plt.figure(figsize=(12, 8))\n",
    "\n",
    "            # Adiciona os resultados do livro atual à lista\n",
    "            # Verifica se o texto já está presente no DataFrame\n",
    "            if titulo in df_resultados['titulo'].values:\n",
    "                # Atualiza a linha correspondente com os novos resultados\n",
    "                df_resultados.loc[df_resultados['titulo'] == titulo, ['proporcao_conectivos', 'similaridade_media', 'coesao_score', 'num_conectivos', 'num_sentencas']] = [\n",
    "                    resultados['proporcao_conectivos'],\n",
    "                    resultados['similaridade_media'],\n",
    "                    resultados['coesao_score'],\n",
    "                    resultados['num_conectivos'],\n",
    "                    resultados['num_sentencas']\n",
    "                ]\n",
    "            else:\n",
    "                # Adiciona uma nova linha com os resultados, incluindo as colunas ausentes\n",
    "                df_resultados = pd.concat([\n",
    "                    df_resultados,\n",
    "                    pd.DataFrame([{\n",
    "                        'titulo': titulo,\n",
    "                        'proporcao_conectivos': resultados['proporcao_conectivos'],\n",
    "                        'similaridade_media': resultados['similaridade_media'],\n",
    "                        'coesao_score': resultados['coesao_score'],\n",
    "                        'ttr': None,\n",
    "                        'score_ttr': None,\n",
    "                        'num_types': None,\n",
    "                        'num_tokens': None,\n",
    "                        'proporcao_hapax': None,\n",
    "                        'score_diversidade': None,\n",
    "                        'score_palavras_unicas': None\n",
    "                    }])\n",
    "                ], ignore_index=True)\n",
    "\n",
    "# Salva os resultados atualizados em um arquivo CSV\n",
    "caminho_csv = os.path.join(caminho_saida_pasta, 'resultados.csv')\n",
    "df_resultados.to_csv(caminho_csv, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "50b1b179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Contagem por arquivo:\n",
      "\n",
      "Arquivo: preprocessado_bras_cubas_machado_cap_1.json\n",
      "  DET: 0.14573643410852713\n",
      "  NOUN: 0.2682170542635659\n",
      "  VERB: 0.14263565891472868\n",
      "  PRON: 0.07751937984496124\n",
      "  ADP: 0.11162790697674418\n",
      "  CCONJ: 0.06666666666666667\n",
      "  ADJ: 0.05426356589147287\n",
      "  AUX: 0.021705426356589147\n",
      "  NUM: 0.018604651162790697\n",
      "  SCONJ: 0.03875968992248062\n",
      "  ADV: 0.05116279069767442\n",
      "  PROPN: 0.0015503875968992248\n",
      "  X: 0.0015503875968992248\n",
      "\n",
      "Arquivo: preprocessado_bras_cubas_machado_cap_2.json\n",
      "  ADP: 0.1362126245847176\n",
      "  NOUN: 0.28903654485049834\n",
      "  NUM: 0.009966777408637873\n",
      "  VERB: 0.15614617940199335\n",
      "  SCONJ: 0.05647840531561462\n",
      "  DET: 0.14950166112956811\n",
      "  PRON: 0.06312292358803986\n",
      "  AUX: 0.013289036544850499\n",
      "  ADV: 0.046511627906976744\n",
      "  CCONJ: 0.019933554817275746\n",
      "  ADJ: 0.046511627906976744\n",
      "  PROPN: 0.009966777408637873\n",
      "  X: 0.0033222591362126247\n",
      "\n",
      "Arquivo: preprocessado_bras_cubas_machado_cap_3.json\n",
      "  CCONJ: 0.050473186119873815\n",
      "  PROPN: 0.015772870662460567\n",
      "  PRON: 0.06309148264984227\n",
      "  VERB: 0.138801261829653\n",
      "  ADP: 0.138801261829653\n",
      "  DET: 0.14826498422712933\n",
      "  NUM: 0.012618296529968454\n",
      "  NOUN: 0.2807570977917981\n",
      "  ADV: 0.03785488958990536\n",
      "  ADJ: 0.05993690851735016\n",
      "  AUX: 0.031545741324921134\n",
      "  SCONJ: 0.022082018927444796\n",
      "\n",
      "Arquivo: preprocessado_bras_cubas_machado_cap_4.json\n",
      "  DET: 0.15800865800865802\n",
      "  NOUN: 0.23160173160173161\n",
      "  ADV: 0.05411255411255411\n",
      "  ADP: 0.09090909090909091\n",
      "  VERB: 0.15367965367965367\n",
      "  ADJ: 0.0735930735930736\n",
      "  PRON: 0.08008658008658008\n",
      "  AUX: 0.017316017316017316\n",
      "  PROPN: 0.023809523809523808\n",
      "  CCONJ: 0.06277056277056277\n",
      "  SCONJ: 0.047619047619047616\n",
      "  NUM: 0.006493506493506494\n",
      "\n",
      "Arquivo: preprocessado_dom_casmurro_machado_cap_1.json\n",
      "  DET: 0.09411764705882353\n",
      "  NOUN: 0.22058823529411764\n",
      "  PRON: 0.10294117647058823\n",
      "  VERB: 0.20294117647058824\n",
      "  ADP: 0.11764705882352941\n",
      "  ADJ: 0.052941176470588235\n",
      "  ADV: 0.03823529411764706\n",
      "  CCONJ: 0.05\n",
      "  PROPN: 0.03529411764705882\n",
      "  AUX: 0.026470588235294117\n",
      "  SCONJ: 0.047058823529411764\n",
      "  NUM: 0.0058823529411764705\n",
      "  X: 0.0058823529411764705\n",
      "\n",
      "Arquivo: preprocessado_dom_casmurro_machado_cap_2.json\n",
      "  ADV: 0.050824175824175824\n",
      "  SCONJ: 0.027472527472527472\n",
      "  VERB: 0.1510989010989011\n",
      "  DET: 0.11538461538461539\n",
      "  NOUN: 0.24587912087912087\n",
      "  ADJ: 0.06456043956043957\n",
      "  PRON: 0.10164835164835165\n",
      "  PROPN: 0.017857142857142856\n",
      "  ADP: 0.11675824175824176\n",
      "  CCONJ: 0.08104395604395605\n",
      "  AUX: 0.017857142857142856\n",
      "  NUM: 0.008241758241758242\n",
      "  X: 0.0013736263736263737\n",
      "\n",
      "Arquivo: preprocessado_dom_casmurro_machado_cap_3.json\n",
      "  AUX: 0.02702702702702703\n",
      "  VERB: 0.19369369369369369\n",
      "  ADP: 0.07357357357357357\n",
      "  NOUN: 0.22672672672672672\n",
      "  SCONJ: 0.08408408408408409\n",
      "  DET: 0.12462462462462462\n",
      "  CCONJ: 0.04804804804804805\n",
      "  PRON: 0.07507507507507508\n",
      "  ADJ: 0.06906906906906907\n",
      "  NUM: 0.0045045045045045045\n",
      "  ADV: 0.03903903903903904\n",
      "  PROPN: 0.03153153153153153\n",
      "  SYM: 0.0015015015015015015\n",
      "  PUNCT: 0.0015015015015015015\n",
      "\n",
      "Arquivo: preprocessado_dom_casmurro_machado_cap_4.json\n",
      "  NOUN: 0.2795031055900621\n",
      "  PROPN: 0.018633540372670808\n",
      "  VERB: 0.14906832298136646\n",
      "  DET: 0.14285714285714285\n",
      "  AUX: 0.043478260869565216\n",
      "  SCONJ: 0.049689440993788817\n",
      "  ADJ: 0.055900621118012424\n",
      "  PRON: 0.031055900621118012\n",
      "  ADP: 0.13664596273291926\n",
      "  ADV: 0.037267080745341616\n",
      "  CCONJ: 0.043478260869565216\n",
      "  NUM: 0.012422360248447204\n",
      "\n",
      "Arquivo: preprocessado_iracema_jose_de_alencar_cap_1.json\n",
      "  NOUN: 0.2865853658536585\n",
      "  ADJ: 0.11890243902439024\n",
      "  ADP: 0.1524390243902439\n",
      "  DET: 0.14939024390243902\n",
      "  PRON: 0.0701219512195122\n",
      "  VERB: 0.14329268292682926\n",
      "  PROPN: 0.006097560975609756\n",
      "  ADV: 0.024390243902439025\n",
      "  CCONJ: 0.027439024390243903\n",
      "  SCONJ: 0.012195121951219513\n",
      "  AUX: 0.006097560975609756\n",
      "  NUM: 0.003048780487804878\n",
      "\n",
      "Arquivo: preprocessado_iracema_jose_de_alencar_cap_2.json\n",
      "  VERB: 0.14003944773175542\n",
      "  ADV: 0.03944773175542406\n",
      "  ADP: 0.14990138067061143\n",
      "  PROPN: 0.01972386587771203\n",
      "  PRON: 0.0670611439842209\n",
      "  NOUN: 0.2978303747534517\n",
      "  ADJ: 0.07692307692307693\n",
      "  DET: 0.14792899408284024\n",
      "  CCONJ: 0.03944773175542406\n",
      "  AUX: 0.007889546351084813\n",
      "  SCONJ: 0.009861932938856016\n",
      "  NUM: 0.0019723865877712033\n",
      "  X: 0.0019723865877712033\n",
      "\n",
      "Arquivo: preprocessado_iracema_jose_de_alencar_cap_3.json\n",
      "  DET: 0.13884007029876977\n",
      "  NOUN: 0.2741652021089631\n",
      "  VERB: 0.17926186291739896\n",
      "  ADP: 0.14586994727592267\n",
      "  ADV: 0.029876977152899824\n",
      "  CCONJ: 0.0421792618629174\n",
      "  ADJ: 0.06678383128295255\n",
      "  PRON: 0.05799648506151142\n",
      "  PROPN: 0.01757469244288225\n",
      "  SCONJ: 0.026362038664323375\n",
      "  AUX: 0.014059753954305799\n",
      "  NUM: 0.005272407732864675\n",
      "  X: 0.0017574692442882249\n",
      "\n",
      "Arquivo: preprocessado_iracema_jose_de_alencar_cap_4.json\n",
      "  DET: 0.15254237288135594\n",
      "  NOUN: 0.2919020715630885\n",
      "  VERB: 0.17702448210922786\n",
      "  CCONJ: 0.03389830508474576\n",
      "  ADP: 0.14124293785310735\n",
      "  SCONJ: 0.030131826741996232\n",
      "  ADJ: 0.06591337099811675\n",
      "  ADV: 0.022598870056497175\n",
      "  PROPN: 0.02071563088512241\n",
      "  PRON: 0.047080979284369114\n",
      "  AUX: 0.015065913370998116\n",
      "  X: 0.0018832391713747645\n",
      "\n",
      "Arquivo: preprocessado_o_cortico_aluisio_azevedo_cap_1.json\n",
      "  NOUN: 0.24814888933360016\n",
      "  PROPN: 0.01380828497098259\n",
      "  AUX: 0.015609365619371623\n",
      "  ADP: 0.1658995397238343\n",
      "  NUM: 0.01280768461076646\n",
      "  CCONJ: 0.048028817290374226\n",
      "  VERB: 0.155893536121673\n",
      "  DET: 0.12767660596357816\n",
      "  PRON: 0.0748449069441665\n",
      "  ADJ: 0.04922953772263358\n",
      "  ADV: 0.05123073844306584\n",
      "  SCONJ: 0.03622173303982389\n",
      "  X: 0.00020012007204322593\n",
      "  INTJ: 0.00040024014408645187\n",
      "\n",
      "Arquivo: preprocessado_o_cortico_aluisio_azevedo_cap_2.json\n",
      "  ADP: 0.14877847927532253\n",
      "  NUM: 0.005764479824320615\n",
      "  NOUN: 0.22920669777655778\n",
      "  DET: 0.1284655503705737\n",
      "  VERB: 0.16387592643425747\n",
      "  CCONJ: 0.04831183090859182\n",
      "  ADJ: 0.050782322261872084\n",
      "  ADV: 0.05050782322261872\n",
      "  PRON: 0.09607466373867692\n",
      "  SCONJ: 0.04721383475157837\n",
      "  PROPN: 0.012077957727147955\n",
      "  AUX: 0.01784243755146857\n",
      "  X: 0.0008234971177600879\n",
      "  INTJ: 0.0002744990392533626\n",
      "\n",
      "Arquivo: preprocessado_o_cortico_aluisio_azevedo_cap_3.json\n",
      "  AUX: 0.015888778550148957\n",
      "  NUM: 0.007696127110228402\n",
      "  NOUN: 0.25049652432969216\n",
      "  ADP: 0.15789473684210525\n",
      "  CCONJ: 0.04791459781529295\n",
      "  DET: 0.13505461767626614\n",
      "  VERB: 0.1641012909632572\n",
      "  SCONJ: 0.03450844091360477\n",
      "  PRON: 0.07274081429990069\n",
      "  ADV: 0.04344587884806356\n",
      "  PROPN: 0.013902681231380337\n",
      "  ADJ: 0.05561072492552135\n",
      "  X: 0.0004965243296921549\n",
      "  INTJ: 0.00024826216484607745\n",
      "\n",
      "Arquivo: preprocessado_o_cortico_aluisio_azevedo_cap_4.json\n",
      "  NUM: 0.01092690278824416\n",
      "  NOUN: 0.24642049736247174\n",
      "  ADV: 0.05124340617935192\n",
      "  PROPN: 0.01921627731725697\n",
      "  PRON: 0.07950263752825923\n",
      "  VERB: 0.17030896759608138\n",
      "  AUX: 0.016201959306706856\n",
      "  ADP: 0.1522230595327807\n",
      "  DET: 0.12735493594574226\n",
      "  CCONJ: 0.04822908816880181\n",
      "  SCONJ: 0.031650339110776186\n",
      "  ADJ: 0.04634513941220799\n",
      "  X: 0.00037678975131876413\n",
      "\n",
      "Arquivo: preprocessado_o_mulato_aluisio_azevedo_cap_1.json\n",
      "  AUX: 0.016695576756287946\n",
      "  NUM: 0.0075888985255854295\n",
      "  NOUN: 0.2489158716392021\n",
      "  ADJ: 0.07914137033824804\n",
      "  CCONJ: 0.04856895056374675\n",
      "  VERB: 0.16500433651344318\n",
      "  DET: 0.12402428447528187\n",
      "  ADP: 0.1513443191673894\n",
      "  PROPN: 0.021032090199479617\n",
      "  ADV: 0.05008673026886384\n",
      "  SCONJ: 0.03577623590633131\n",
      "  PRON: 0.051387684301821335\n",
      "  X: 0.0004336513443191674\n",
      "\n",
      "Arquivo: preprocessado_o_mulato_aluisio_azevedo_cap_2.json\n",
      "  ADV: 0.04764838775929349\n",
      "  VERB: 0.18566440747586774\n",
      "  PROPN: 0.025261860751694395\n",
      "  ADP: 0.13514068597247894\n",
      "  NOUN: 0.2423495584308893\n",
      "  DET: 0.12672006572191416\n",
      "  PRON: 0.0722941055658246\n",
      "  ADJ: 0.05730129390018484\n",
      "  SCONJ: 0.038817005545286505\n",
      "  CCONJ: 0.04374614910659273\n",
      "  AUX: 0.017868145409735057\n",
      "  NUM: 0.005956048469911686\n",
      "  X: 0.0012322858903265558\n",
      "\n",
      "Arquivo: preprocessado_o_mulato_aluisio_azevedo_cap_3.json\n",
      "  ADV: 0.04790649884407912\n",
      "  ADP: 0.1492422296429489\n",
      "  NOUN: 0.2422296429488826\n",
      "  DET: 0.12214230670434112\n",
      "  ADJ: 0.06267659902388903\n",
      "  VERB: 0.18250706396095556\n",
      "  PRON: 0.06139224248651426\n",
      "  AUX: 0.018880041099409196\n",
      "  CCONJ: 0.051245825841253534\n",
      "  PROPN: 0.020935011559208835\n",
      "  SCONJ: 0.03467762650911893\n",
      "  INTJ: 0.0005137426149499101\n",
      "  NUM: 0.005137426149499101\n",
      "  X: 0.0005137426149499101\n",
      "\n",
      "Arquivo: preprocessado_o_mulato_aluisio_azevedo_cap_4.json\n",
      "  CCONJ: 0.04941338854382333\n",
      "  ADP: 0.14575569358178053\n",
      "  DET: 0.12657004830917876\n",
      "  NOUN: 0.2492753623188406\n",
      "  VERB: 0.1746031746031746\n",
      "  PROPN: 0.022774327122153208\n",
      "  ADJ: 0.07094547964113182\n",
      "  ADV: 0.045548654244306416\n",
      "  PRON: 0.06363008971704624\n",
      "  AUX: 0.014906832298136646\n",
      "  SCONJ: 0.028157349896480333\n",
      "  NUM: 0.00800552104899931\n",
      "  INTJ: 0.00027605244996549346\n",
      "  X: 0.00013802622498274673\n",
      "\n",
      "Arquivo: preprocessado_senhora_jose_de_alencar_cap_1.json\n",
      "  NOUN: 0.2557924003707136\n",
      "  VERB: 0.1556997219647822\n",
      "  ADP: 0.16404077849860982\n",
      "  ADJ: 0.06765523632993513\n",
      "  DET: 0.13067655236329936\n",
      "  PRON: 0.0778498609823911\n",
      "  AUX: 0.011121408711770158\n",
      "  CCONJ: 0.03614457831325301\n",
      "  NUM: 0.009267840593141797\n",
      "  PROPN: 0.012048192771084338\n",
      "  SCONJ: 0.03707136237256719\n",
      "  ADV: 0.04170528266913809\n",
      "  X: 0.0009267840593141798\n",
      "\n",
      "Arquivo: preprocessado_senhora_jose_de_alencar_cap_2.json\n",
      "  AUX: 0.010968921389396709\n",
      "  NUM: 0.0042656916514320535\n",
      "  NOUN: 0.238878732480195\n",
      "  ADP: 0.1480804387568556\n",
      "  ADV: 0.04692260816575259\n",
      "  VERB: 0.17184643510054845\n",
      "  PRON: 0.07373552711761121\n",
      "  DET: 0.13040828762949422\n",
      "  ADJ: 0.07495429616087751\n",
      "  SCONJ: 0.04143814747105423\n",
      "  PROPN: 0.014625228519195612\n",
      "  CCONJ: 0.043875685557586835\n",
      "\n",
      "Arquivo: preprocessado_senhora_jose_de_alencar_cap_3.json\n",
      "  AUX: 0.017673048600883652\n",
      "  DET: 0.12812960235640647\n",
      "  NOUN: 0.24594992636229748\n",
      "  ADP: 0.14285714285714285\n",
      "  NUM: 0.003681885125184094\n",
      "  VERB: 0.18335787923416788\n",
      "  PRON: 0.07290132547864507\n",
      "  CCONJ: 0.04491899852724595\n",
      "  ADJ: 0.0508100147275405\n",
      "  PROPN: 0.020618556701030927\n",
      "  SCONJ: 0.045655375552282766\n",
      "  ADV: 0.04344624447717231\n",
      "\n",
      "Arquivo: preprocessado_senhora_jose_de_alencar_cap_4.json\n",
      "  PRON: 0.09748700173310225\n",
      "  VERB: 0.19410745233968804\n",
      "  NOUN: 0.22227036395147315\n",
      "  ADP: 0.10875216637781629\n",
      "  SCONJ: 0.05459272097053726\n",
      "  DET: 0.1412478336221837\n",
      "  ADJ: 0.05459272097053726\n",
      "  CCONJ: 0.0402946273830156\n",
      "  AUX: 0.01559792027729636\n",
      "  ADV: 0.04246100519930676\n",
      "  PROPN: 0.02123050259965338\n",
      "  NUM: 0.004766031195840554\n",
      "  INTJ: 0.0008665511265164644\n",
      "  X: 0.0017331022530329288\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "caminho_textos_processados = \"data/caps_processados\" \n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "arquivos_json = [f for f in os.listdir(caminho_textos_processados) if f.endswith('.json')]\n",
    "\n",
    "def contar_pos_tags(texto):\n",
    "    doc = nlp(texto)\n",
    "    contagem = Counter()\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_space:\n",
    "            contagem[token.pos_] += 1\n",
    "    total_tokens = sum(contagem.values())\n",
    "    if total_tokens > 0:\n",
    "        proporcoes = {pos: qtd / total_tokens for pos, qtd in contagem.items()}\n",
    "    else:\n",
    "        proporcoes = {pos: 0 for pos in contagem}\n",
    "    return proporcoes\n",
    "\n",
    "contagens_por_arquivo = {}\n",
    "contagem_total = Counter()\n",
    "\n",
    "for nome_arquivo in arquivos_json:\n",
    "    caminho_completo = os.path.join(caminho_textos_processados, nome_arquivo)\n",
    "    with open(caminho_completo, 'r', encoding='utf-8') as f:\n",
    "        dados = json.load(f)\n",
    "        texto = \" \".join([token for sublist in dados['tokens'] for token in sublist])\n",
    "        contagem = contar_pos_tags(texto)\n",
    "        contagens_por_arquivo[nome_arquivo] = contagem\n",
    "        contagem_total.update(contagem)\n",
    "\n",
    "# Carregar o DataFrame existente\n",
    "caminho_csv = os.path.join(caminho_textos_processados, 'resultados.csv')\n",
    "df = pd.read_csv(caminho_csv)\n",
    "\n",
    "# Para cada arquivo, adicionar/atualizar as features POS no DataFrame\n",
    "for nome_arquivo, contagem in contagens_por_arquivo.items():\n",
    "    # Remover prefixo \"preprocessado_\" e sufixo \".json\" para bater com o campo 'titulo'\n",
    "    titulo = nome_arquivo.replace(\"preprocessado_\", \"\").replace(\".json\", \"\")\n",
    "    for pos, qtd in contagem.items():\n",
    "        # Adiciona a coluna se não existir\n",
    "        if pos == \"SYM\" or pos == \"PUNCT\" or pos == \"INTJ\":\n",
    "            continue\n",
    "        if pos not in df.columns:\n",
    "            df[pos] = 0.0\n",
    "        # Atualiza o valor na linha correspondente ao título\n",
    "        df.loc[df['titulo'] == titulo, pos] = qtd\n",
    "\n",
    "# Salvar o DataFrame atualizado\n",
    "df.to_csv(caminho_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\n Contagem por arquivo:\")\n",
    "for nome_arquivo, contagem in contagens_por_arquivo.items():\n",
    "    print(f\"\\nArquivo: {nome_arquivo}\")\n",
    "    for pos, qtd in contagem.items():\n",
    "        print(f\"  {pos}: {qtd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f868719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             titulo  cluster\n",
      "0          bras_cubas_machado_cap_1        2\n",
      "1          bras_cubas_machado_cap_2        2\n",
      "2          bras_cubas_machado_cap_3        2\n",
      "3          bras_cubas_machado_cap_4        2\n",
      "4        dom_casmurro_machado_cap_1        0\n",
      "5        dom_casmurro_machado_cap_2        2\n",
      "6        dom_casmurro_machado_cap_3        0\n",
      "7        dom_casmurro_machado_cap_4        2\n",
      "8     iracema_jose_de_alencar_cap_1        2\n",
      "9     iracema_jose_de_alencar_cap_2        2\n",
      "10    iracema_jose_de_alencar_cap_3        2\n",
      "11    iracema_jose_de_alencar_cap_4        2\n",
      "12  o_cortico_aluisio_azevedo_cap_1        1\n",
      "13  o_cortico_aluisio_azevedo_cap_2        1\n",
      "14  o_cortico_aluisio_azevedo_cap_3        1\n",
      "15  o_cortico_aluisio_azevedo_cap_4        1\n",
      "16   o_mulato_aluisio_azevedo_cap_1        1\n",
      "17   o_mulato_aluisio_azevedo_cap_2        1\n",
      "18   o_mulato_aluisio_azevedo_cap_3        1\n",
      "19   o_mulato_aluisio_azevedo_cap_4        1\n",
      "20    senhora_jose_de_alencar_cap_1        2\n",
      "21    senhora_jose_de_alencar_cap_2        1\n",
      "22    senhora_jose_de_alencar_cap_3        1\n",
      "23    senhora_jose_de_alencar_cap_4        1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Carregar o DataFrame\n",
    "df = pd.read_csv('data/caps_processados/resultados.csv')\n",
    "\n",
    "# Selecionar apenas as colunas numéricas (ignorando 'titulo')\n",
    "X = df.drop(columns=['titulo'])\n",
    "\n",
    "# Padronizar os dados\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Definir o número de clusters (exemplo: 3, ajuste conforme necessário)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Adicionar o resultado ao DataFrame\n",
    "df['cluster'] = clusters\n",
    "\n",
    "# Exibir os clusters\n",
    "print(df[['titulo', 'cluster']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427b0d7",
   "metadata": {},
   "source": [
    "### Conclusão\n",
    "Primeiramente, um fator essencial que notamos é que teremos que tratar a sensibilidade que existe à diferença de tamanho entre os textos, essa é uma questão na qual vamos precisar pensar com cuidado, evitar usar números absolutos será relevante para tornas os parâmetros mais comparáveis. Por fim, nessa próxima etapa nossa intenção é de aumentar a quantidade de features e principalmente o escopo dos dados, utilizar mais autores e textos para nossa análise será essencial mais para frente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
