{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63cfcefb",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "Nosso grupo enfrentou algumas dificuldades na definição do projeto, equilibrando a busca por uma proposta relevante com o receio de estabelecer objetivos além de nossas capacidades. Após diversas discussões e ideias, decidimos desenvolver uma solução que compara dois textos e, por meio de análises semânticas e sintáticas, calcula a similaridade entre eles em forma de porcentagem. Dessa maneira, buscamos inferir se os textos foram ou não produzidos pelo mesmo autor. \n",
    "Após a definição da nossa ideia, iniciamos pesquisas sobre o plano de ação para lidar com o problema proposto. Primeiramente, percebemos que o pré-processamento é uma etapa extremamente relevante e que difere consideravelmente de outros casos de PLN. Isso ocorre porque um autor pode expressar seus maneirismos linguísticos e estilísticos de maneira sutil, o que torna características geralmente descartadas — como o início e o fim de sentenças, a pontuação e o uso de stopwords — potencialmente relevantes para a análise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb403de",
   "metadata": {},
   "source": [
    "## Coleta de dados \n",
    "Considerando o problema abordado, optamos inicialmente por buscar textos de autores em comum, mas com temáticas distintas. Dessa forma, conseguimos testar a funcionalidade da nossa solução minimizando possíveis interferências decorrentes da similaridade temática entre os textos. Para isso, utilizamos o site Domínio Público, onde foi possível encontrar obras de diversos autores em língua portuguesa.\n",
    "Selecionamos, nesta etapa, quatro textos: Dom Casmurro, de Machado de Assis, e O Cortiço, de Aluísio Azevedo e Iracema de José de Alencar. Atualmente, estamos utilizando esses 3 textos para testes, mas nossa intenção é expandir consideravelmente o conjunto de dados ao longo do desenvolvimento. Cada texto teve 4 capítulos extraídos e separados para análise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1805bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo: data/caps_processados\\dom_casmurro_machado_cap_1.txt\n",
      "Arquivo salvo: data/caps_processados\\dom_casmurro_machado_cap_2.txt\n",
      "Arquivo salvo: data/caps_processados\\dom_casmurro_machado_cap_3.txt\n",
      "Arquivo salvo: data/caps_processados\\dom_casmurro_machado_cap_4.txt\n",
      "Arquivo salvo: data/caps_processados\\iracema_jose_de_alencar_cap_1.txt\n",
      "Arquivo salvo: data/caps_processados\\iracema_jose_de_alencar_cap_2.txt\n",
      "Arquivo salvo: data/caps_processados\\iracema_jose_de_alencar_cap_3.txt\n",
      "Arquivo salvo: data/caps_processados\\iracema_jose_de_alencar_cap_4.txt\n",
      "Arquivo salvo: data/caps_processados\\o_cortico_aluisio_azevedo_cap_1.txt\n",
      "Arquivo salvo: data/caps_processados\\o_cortico_aluisio_azevedo_cap_2.txt\n",
      "Arquivo salvo: data/caps_processados\\o_cortico_aluisio_azevedo_cap_3.txt\n",
      "Arquivo salvo: data/caps_processados\\o_cortico_aluisio_azevedo_cap_4.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Diretórios de origem e destino\n",
    "origem = \"data/caps\"\n",
    "destino = \"data/caps_processados\"\n",
    "os.makedirs(destino, exist_ok=True)\n",
    "\n",
    "for nome_arquivo in os.listdir(origem):\n",
    "    caminho_origem = os.path.join(origem, nome_arquivo)\n",
    "    caminho_destino = os.path.join(destino, os.path.splitext(nome_arquivo)[0] + \".txt\")\n",
    "    if os.path.isfile(caminho_origem):\n",
    "        with open(caminho_origem, 'r', encoding='utf-8') as f_origem:\n",
    "            conteudo = f_origem.read()\n",
    "        with open(caminho_destino, 'w', encoding='utf-8') as f_destino:\n",
    "            f_destino.write(conteudo)\n",
    "        print(f\"Arquivo salvo: {caminho_destino}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46a5c5",
   "metadata": {},
   "source": [
    "## Limpeza dos textos\n",
    "\n",
    "Na etapa de pré-processamento definimos uma pipeline de limpeza de dados, essa é responsável por tornar os dados mais homogêneos, sem perder a complexidade. Além disso, aplicamos tokenização no texto, tomando cuidado para manter a divisão entre sentenças e manter pontuação e stopwords que podem ser relevantes para análise.\n",
    "Abaixo definimos as configurações e funções usadas para limpeza do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6798610",
   "metadata": {},
   "source": [
    "A célula abaixo corrige um problma com a biblioteca nltk que estávamos tendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "95fc800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ./.venv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./.venv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.data.path.append('./.venv/nltk_data')  \n",
    "\n",
    "nltk.download('punkt', download_dir='./.venv/nltk_data')\n",
    "nltk.download('stopwords', download_dir='./.venv/nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6a99cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def limpeza_texto(texto):\n",
    "\n",
    "    # 1. Corrigir quebras de linha excessivas\n",
    "    texto = texto.replace('\\n', '')\n",
    "    \n",
    "    # 2. Corrigir múltiplos espaços\n",
    "    texto = re.sub(r'[ \\t]+', ' ', texto)\n",
    "    \n",
    "    # 3. Remover pontuação desnecessária (mas manter . , ? — e aspas)\n",
    "    texto = re.sub(r'[!;:()*[\\]{}<>]', '', texto)\n",
    "    \n",
    "    # 4. Converter para minúsculas\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    # 5. Remover acentos\n",
    "    texto = unicodedata.normalize('NFKD', texto)\n",
    "    texto = ''.join(c for c in texto if not unicodedata.combining(c))\n",
    "    \n",
    "    return texto\n",
    "\n",
    "def limpar_e_tokenizar_texto(texto):\n",
    "    # Limpa o texto primeiro\n",
    "    texto_limpo = limpeza_texto(texto)\n",
    "    \n",
    "    sentences = sent_tokenize(texto_limpo, language='portuguese')\n",
    "    # Remove sentenças vazias\n",
    "    sentences = [s for s in sentences if s.strip()]\n",
    "    # Tokeniza as sentenças e achata a lista de listas\n",
    "    tokens = [token for s in sentences for token in word_tokenize(s, language='portuguese')]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def extrair_titulo_autor(nome_arquivo):\n",
    "    # Remove \"preprocessado_\" e \".txt\" ou \".json\"\n",
    "    nome_limpo = re.sub(r'^preprocessado_', '', nome_arquivo)\n",
    "    nome_limpo = re.sub(r'\\.(txt|json)$', '', nome_limpo)\n",
    "    \n",
    "    # Separar título e autor\n",
    "    match = re.match(r'(.+?) \\((.+?)\\)', nome_limpo)\n",
    "    if match:\n",
    "        titulo, autor = match.groups()\n",
    "        return titulo.strip(), autor.strip()\n",
    "    else:\n",
    "        return nome_limpo.strip(), None  # Se não bater, só retorna o título"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d6134",
   "metadata": {},
   "source": [
    "### Tokenização e Armazenamento\n",
    "\n",
    "Na célula abaixo realizamos a tokenização e armazenamento dos textos em um json com título do texto, nome do autor, e conteúdo tokenizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cd992666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processando frase original (após split e strip): 'Uma noite destas, vindo da cidade para o Engenho Novo, encontrei num trem da Central um rapaz \n",
      "aqui do bairro, que eu conheço de vista e de chapéu'\n",
      "Tokens gerados: ['uma', 'noite', 'destas', ',', 'vindo', 'da', 'cidade', 'para', 'o', 'engenho', 'novo', ',', 'encontrei', 'num', 'trem', 'da', 'central', 'um', 'rapaz', 'aqui', 'do', 'bairro', ',', 'que', 'eu', 'conheco', 'de', 'vista', 'e', 'de', 'chapeu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cumprimentou-me, sentou-se ao pé de mim, \n",
      "falou da lua e dos ministros, e acabou recitando-me versos'\n",
      "Tokens gerados: ['cumprimentou-me', ',', 'sentou-se', 'ao', 'pe', 'de', 'mim', ',', 'falou', 'da', 'lua', 'e', 'dos', 'ministros', ',', 'e', 'acabou', 'recitando-me', 'versos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A viagem era curta, e os versos pode ser \n",
      "que não fossem inteiramente maus'\n",
      "Tokens gerados: ['a', 'viagem', 'era', 'curta', ',', 'e', 'os', 'versos', 'pode', 'ser', 'que', 'nao', 'fossem', 'inteiramente', 'maus']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sucedeu, porém, que, como eu estava cansado, fechei os olhos \n",
      "três ou quatro vezes; tanto bastou para que ele interrompesse a leitura e metesse os versos no bolso'\n",
      "Tokens gerados: ['sucedeu', ',', 'porem', ',', 'que', ',', 'como', 'eu', 'estava', 'cansado', ',', 'fechei', 'os', 'olhos', 'tres', 'ou', 'quatro', 'vezes', 'tanto', 'bastou', 'para', 'que', 'ele', 'interrompesse', 'a', 'leitura', 'e', 'metesse', 'os', 'versos', 'no', 'bolso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '-- Continue, disse eu acordando'\n",
      "Tokens gerados: ['--', 'continue', ',', 'disse', 'eu', 'acordando']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '-- Já acabei, murmurou ele'\n",
      "Tokens gerados: ['--', 'ja', 'acabei', ',', 'murmurou', 'ele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '-- São muito bonitos'\n",
      "Tokens gerados: ['--', 'sao', 'muito', 'bonitos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vi-lhe fazer um gesto para tirá-los outra vez do bolso, mas não passou do gesto; estava amuado'\n",
      "Tokens gerados: ['vi-lhe', 'fazer', 'um', 'gesto', 'para', 'tira-los', 'outra', 'vez', 'do', 'bolso', ',', 'mas', 'nao', 'passou', 'do', 'gesto', 'estava', 'amuado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No \n",
      "dia seguinte entrou a dizer de mim nomes feios, e acabou alcunhando-me Dom Casmurro'\n",
      "Tokens gerados: ['no', 'dia', 'seguinte', 'entrou', 'a', 'dizer', 'de', 'mim', 'nomes', 'feios', ',', 'e', 'acabou', 'alcunhando-me', 'dom', 'casmurro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os \n",
      "vizinhos, que não gostam dos meus hábitos reclusos e calados, deram curso à alcunha, que afinal \n",
      "pegou'\n",
      "Tokens gerados: ['os', 'vizinhos', ',', 'que', 'nao', 'gostam', 'dos', 'meus', 'habitos', 'reclusos', 'e', 'calados', ',', 'deram', 'curso', 'a', 'alcunha', ',', 'que', 'afinal', 'pegou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nem por isso me zanguei'\n",
      "Tokens gerados: ['nem', 'por', 'isso', 'me', 'zanguei']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Contei a anedota aos amigos da cidade, e eles, por graça, \n",
      "chamam-me assim, alguns em bilhetes: \"Dom Casmurro, domingo vou jantar com você'\n",
      "Tokens gerados: ['contei', 'a', 'anedota', 'aos', 'amigos', 'da', 'cidade', ',', 'e', 'eles', ',', 'por', 'graca', ',', 'chamam-me', 'assim', ',', 'alguns', 'em', 'bilhetes', '``', 'dom', 'casmurro', ',', 'domingo', 'vou', 'jantar', 'com', 'voce']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '\"--\"Vou \n",
      "para Petrópolis, Dom Casmurro; a casa é a mesma da Renania; vê se deixas essa caverna do \n",
      "Engenho Novo, e vai lá passar uns quinze dias comigo'\n",
      "Tokens gerados: ['``', '--', \"''\", 'vou', 'para', 'petropolis', ',', 'dom', 'casmurro', 'a', 'casa', 'e', 'a', 'mesma', 'da', 'renania', 've', 'se', 'deixas', 'essa', 'caverna', 'do', 'engenho', 'novo', ',', 'e', 'vai', 'la', 'passar', 'uns', 'quinze', 'dias', 'comigo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '\"--\"Meu caro Dom Casmurro, não cuide que \n",
      "o dispenso do teatro amanhã; venha e dormirá aqui na cidade; dou-lhe camarote, dou-lhe chá, dou-\n",
      "lhe cama; só não lhe dou moça'\n",
      "Tokens gerados: ['``', '--', \"''\", 'meu', 'caro', 'dom', 'casmurro', ',', 'nao', 'cuide', 'que', 'o', 'dispenso', 'do', 'teatro', 'amanha', 'venha', 'e', 'dormira', 'aqui', 'na', 'cidade', 'dou-lhe', 'camarote', ',', 'dou-lhe', 'cha', ',', 'dou-lhe', 'cama', 'so', 'nao', 'lhe', 'dou', 'moca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '\" \n",
      "Não consultes dicionários'\n",
      "Tokens gerados: ['``', 'nao', 'consultes', 'dicionarios']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Casmurro não está aqui no sentido que eles lhe dão, mas no que lhe pôs \n",
      "o vulgo de homem calado e metido consigo'\n",
      "Tokens gerados: ['casmurro', 'nao', 'esta', 'aqui', 'no', 'sentido', 'que', 'eles', 'lhe', 'dao', ',', 'mas', 'no', 'que', 'lhe', 'pos', 'o', 'vulgo', 'de', 'homem', 'calado', 'e', 'metido', 'consigo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dom veio por ironia, para atribuir-me fumos de \n",
      "fidalgo'\n",
      "Tokens gerados: ['dom', 'veio', 'por', 'ironia', ',', 'para', 'atribuir-me', 'fumos', 'de', 'fidalgo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tudo por estar cochilando! Também não achei melhor título para a minha narração - se não \n",
      "tiver outro daqui até ao fim do livro, vai este mesmo'\n",
      "Tokens gerados: ['tudo', 'por', 'estar', 'cochilando', 'tambem', 'nao', 'achei', 'melhor', 'titulo', 'para', 'a', 'minha', 'narracao', '-', 'se', 'nao', 'tiver', 'outro', 'daqui', 'ate', 'ao', 'fim', 'do', 'livro', ',', 'vai', 'este', 'mesmo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O meu poeta do trem ficará sabendo que não \n",
      "lhe guardo rancor'\n",
      "Tokens gerados: ['o', 'meu', 'poeta', 'do', 'trem', 'ficara', 'sabendo', 'que', 'nao', 'lhe', 'guardo', 'rancor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E com pequeno esforço, sendo o título seu, poderá cuidar que a obra é sua'\n",
      "Tokens gerados: ['e', 'com', 'pequeno', 'esforco', ',', 'sendo', 'o', 'titulo', 'seu', ',', 'podera', 'cuidar', 'que', 'a', 'obra', 'e', 'sua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Há \n",
      "livros que apenas terão isso dos seus autores; alguns nem tanto'\n",
      "Tokens gerados: ['ha', 'livros', 'que', 'apenas', 'terao', 'isso', 'dos', 'seus', 'autores', 'alguns', 'nem', 'tanto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_dom_casmurro_machado_cap_1.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Agora que expliquei o título, passo a escrever o livro'\n",
      "Tokens gerados: ['agora', 'que', 'expliquei', 'o', 'titulo', ',', 'passo', 'a', 'escrever', 'o', 'livro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Antes disso, porém, digamos os motivos que \n",
      "me põem a pena na mão'\n",
      "Tokens gerados: ['antes', 'disso', ',', 'porem', ',', 'digamos', 'os', 'motivos', 'que', 'me', 'poem', 'a', 'pena', 'na', 'mao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vivo só, com um criado'\n",
      "Tokens gerados: ['vivo', 'so', ',', 'com', 'um', 'criado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A casa em que moro é própria; fi-la construir de propósito, levado de um \n",
      "desejo tão particular que me vexa imprimi-lo, mas vá lá'\n",
      "Tokens gerados: ['a', 'casa', 'em', 'que', 'moro', 'e', 'propria', 'fi-la', 'construir', 'de', 'proposito', ',', 'levado', 'de', 'um', 'desejo', 'tao', 'particular', 'que', 'me', 'vexa', 'imprimi-lo', ',', 'mas', 'va', 'la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um dia'\n",
      "Tokens gerados: ['um', 'dia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'há bastantes anos, lembrou-me \n",
      "reproduzir no Engenho Novo a casa em que me criei na antiga Rua de Mata-cavalos, dando-lhe o \n",
      "mesmo aspecto e economia daquela outra, que desapareceu'\n",
      "Tokens gerados: ['ha', 'bastantes', 'anos', ',', 'lembrou-me', 'reproduzir', 'no', 'engenho', 'novo', 'a', 'casa', 'em', 'que', 'me', 'criei', 'na', 'antiga', 'rua', 'de', 'mata-cavalos', ',', 'dando-lhe', 'o', 'mesmo', 'aspecto', 'e', 'economia', 'daquela', 'outra', ',', 'que', 'desapareceu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Construtor e pintor entenderam bem as \n",
      "indicações que lhes fiz: é o mesmo prédio assobradado, três janelas de frente, varanda ao fundo, as \n",
      "mesmas alcovas e salas'\n",
      "Tokens gerados: ['construtor', 'e', 'pintor', 'entenderam', 'bem', 'as', 'indicacoes', 'que', 'lhes', 'fiz', 'e', 'o', 'mesmo', 'predio', 'assobradado', ',', 'tres', 'janelas', 'de', 'frente', ',', 'varanda', 'ao', 'fundo', ',', 'as', 'mesmas', 'alcovas', 'e', 'salas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na principal destas, a pintura do tecto e das paredes é mais ou menos igual, \n",
      "umas grinaldas de flores miúdas e grandes pássaros que as tomam nos blocos, de espaço a espaço'\n",
      "Tokens gerados: ['na', 'principal', 'destas', ',', 'a', 'pintura', 'do', 'tecto', 'e', 'das', 'paredes', 'e', 'mais', 'ou', 'menos', 'igual', ',', 'umas', 'grinaldas', 'de', 'flores', 'miudas', 'e', 'grandes', 'passaros', 'que', 'as', 'tomam', 'nos', 'blocos', ',', 'de', 'espaco', 'a', 'espaco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nos quatro cantos do tecto as figuras das estações, e ao centro das paredes os medalhões de César, \n",
      "Augusto, Nero e Massinissa, com os nomes por baixo'\n",
      "Tokens gerados: ['nos', 'quatro', 'cantos', 'do', 'tecto', 'as', 'figuras', 'das', 'estacoes', ',', 'e', 'ao', 'centro', 'das', 'paredes', 'os', 'medalhoes', 'de', 'cesar', ',', 'augusto', ',', 'nero', 'e', 'massinissa', ',', 'com', 'os', 'nomes', 'por', 'baixo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não alcanço a razão de tais personagens'\n",
      "Tokens gerados: ['nao', 'alcanco', 'a', 'razao', 'de', 'tais', 'personagens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando fomos para a casa de Mata-cavalos, já ela estava assim decorada; vinha do decênio anterior'\n",
      "Tokens gerados: ['quando', 'fomos', 'para', 'a', 'casa', 'de', 'mata-cavalos', ',', 'ja', 'ela', 'estava', 'assim', 'decorada', 'vinha', 'do', 'decenio', 'anterior']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Naturalmente era gosto do tempo meter sabor clássico e figuras antigas em pinturas americanas'\n",
      "Tokens gerados: ['naturalmente', 'era', 'gosto', 'do', 'tempo', 'meter', 'sabor', 'classico', 'e', 'figuras', 'antigas', 'em', 'pinturas', 'americanas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O \n",
      "mais é também análogo e parecido'\n",
      "Tokens gerados: ['o', 'mais', 'e', 'tambem', 'analogo', 'e', 'parecido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tenho chacarinha, flores, legume, uma casuarina, um poço e \n",
      "lavadouro'\n",
      "Tokens gerados: ['tenho', 'chacarinha', ',', 'flores', ',', 'legume', ',', 'uma', 'casuarina', ',', 'um', 'poco', 'e', 'lavadouro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uso louça velha e mobília velha'\n",
      "Tokens gerados: ['uso', 'louca', 'velha', 'e', 'mobilia', 'velha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Enfim, agora, como outrora, há aqui o mesmo contraste \n",
      "da vida interior, que é pacata, com a exterior, que é ruidosa'\n",
      "Tokens gerados: ['enfim', ',', 'agora', ',', 'como', 'outrora', ',', 'ha', 'aqui', 'o', 'mesmo', 'contraste', 'da', 'vida', 'interior', ',', 'que', 'e', 'pacata', ',', 'com', 'a', 'exterior', ',', 'que', 'e', 'ruidosa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O meu fim evidente era atar as duas pontas da vida, e restaurar na velhice a adolescência'\n",
      "Tokens gerados: ['o', 'meu', 'fim', 'evidente', 'era', 'atar', 'as', 'duas', 'pontas', 'da', 'vida', ',', 'e', 'restaurar', 'na', 'velhice', 'a', 'adolescencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois, \n",
      "senhor, não consegui recompor o que foi nem o que fui'\n",
      "Tokens gerados: ['pois', ',', 'senhor', ',', 'nao', 'consegui', 'recompor', 'o', 'que', 'foi', 'nem', 'o', 'que', 'fui']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em tudo, se o rosto é igual, a fisionomia é \n",
      "diferente'\n",
      "Tokens gerados: ['em', 'tudo', ',', 'se', 'o', 'rosto', 'e', 'igual', ',', 'a', 'fisionomia', 'e', 'diferente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se só me faltassem os outros, vá um homem consola-se mais ou menos das pessoas que \n",
      "perde; mais falto eu mesmo, e esta lacuna é tudo'\n",
      "Tokens gerados: ['se', 'so', 'me', 'faltassem', 'os', 'outros', ',', 'va', 'um', 'homem', 'consola-se', 'mais', 'ou', 'menos', 'das', 'pessoas', 'que', 'perde', 'mais', 'falto', 'eu', 'mesmo', ',', 'e', 'esta', 'lacuna', 'e', 'tudo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O que aqui está é, mal comparando, semelhante à \n",
      "pintura que se põe na barba e nos cabelos, e que apenas conserva o hábito externo, como se diz nas \n",
      "autópsias; o interno não agüenta tinta'\n",
      "Tokens gerados: ['o', 'que', 'aqui', 'esta', 'e', ',', 'mal', 'comparando', ',', 'semelhante', 'a', 'pintura', 'que', 'se', 'poe', 'na', 'barba', 'e', 'nos', 'cabelos', ',', 'e', 'que', 'apenas', 'conserva', 'o', 'habito', 'externo', ',', 'como', 'se', 'diz', 'nas', 'autopsias', 'o', 'interno', 'nao', 'aguenta', 'tinta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma certidão que me desse vinte anos de idade poderia \n",
      "enganar os estranhos, como todos os documentos falsos, mas não a mim'\n",
      "Tokens gerados: ['uma', 'certidao', 'que', 'me', 'desse', 'vinte', 'anos', 'de', 'idade', 'poderia', 'enganar', 'os', 'estranhos', ',', 'como', 'todos', 'os', 'documentos', 'falsos', ',', 'mas', 'nao', 'a', 'mim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os amigos que me restam \n",
      "são de data recente; todos os antigos foram estudar a geologia dos campos-santos'\n",
      "Tokens gerados: ['os', 'amigos', 'que', 'me', 'restam', 'sao', 'de', 'data', 'recente', 'todos', 'os', 'antigos', 'foram', 'estudar', 'a', 'geologia', 'dos', 'campos-santos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quanto às \n",
      "amigas, algumas datam de quinze anos, outras de menos, e quase todas crêem na mocidade'\n",
      "Tokens gerados: ['quanto', 'as', 'amigas', ',', 'algumas', 'datam', 'de', 'quinze', 'anos', ',', 'outras', 'de', 'menos', ',', 'e', 'quase', 'todas', 'creem', 'na', 'mocidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Duas \n",
      "ou três fariam crer nela aos outros, mas a língua que falam obriga muita vez a consultar os \n",
      "dicionários, e tal freqüência é cansativa'\n",
      "Tokens gerados: ['duas', 'ou', 'tres', 'fariam', 'crer', 'nela', 'aos', 'outros', ',', 'mas', 'a', 'lingua', 'que', 'falam', 'obriga', 'muita', 'vez', 'a', 'consultar', 'os', 'dicionarios', ',', 'e', 'tal', 'frequencia', 'e', 'cansativa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, vida diferente não quer dizer vida pior, é outra cousa a certos respeitos, aquela vida \n",
      "antiga aparece-me despida de muitos encantos que lhe achei; mas é também exato que perdeu muito \n",
      "espinho que a fez molesta, e, de memória, conservo alguma recordação doce e feiticeira'\n",
      "Tokens gerados: ['entretanto', ',', 'vida', 'diferente', 'nao', 'quer', 'dizer', 'vida', 'pior', ',', 'e', 'outra', 'cousa', 'a', 'certos', 'respeitos', ',', 'aquela', 'vida', 'antiga', 'aparece-me', 'despida', 'de', 'muitos', 'encantos', 'que', 'lhe', 'achei', 'mas', 'e', 'tambem', 'exato', 'que', 'perdeu', 'muito', 'espinho', 'que', 'a', 'fez', 'molesta', ',', 'e', ',', 'de', 'memoria', ',', 'conservo', 'alguma', 'recordacao', 'doce', 'e', 'feiticeira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em \n",
      "verdade, pouco apareço e menos falo'\n",
      "Tokens gerados: ['em', 'verdade', ',', 'pouco', 'apareco', 'e', 'menos', 'falo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Distrações raras'\n",
      "Tokens gerados: ['distracoes', 'raras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O mais do tempo é gasto em hortar, jardinar \n",
      "e ler; como bem e não durmo mal'\n",
      "Tokens gerados: ['o', 'mais', 'do', 'tempo', 'e', 'gasto', 'em', 'hortar', ',', 'jardinar', 'e', 'ler', 'como', 'bem', 'e', 'nao', 'durmo', 'mal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ora, como tudo cansa, esta monotonia acabou por exaurir-me também'\n",
      "Tokens gerados: ['ora', ',', 'como', 'tudo', 'cansa', ',', 'esta', 'monotonia', 'acabou', 'por', 'exaurir-me', 'tambem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quis variar, e lembrou-me \n",
      "escrever um livro'\n",
      "Tokens gerados: ['quis', 'variar', ',', 'e', 'lembrou-me', 'escrever', 'um', 'livro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Jurisprudência'\n",
      "Tokens gerados: ['jurisprudencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'filosofia e política acudiram-me, mas não me acudiram as forças \n",
      "necessárias'\n",
      "Tokens gerados: ['filosofia', 'e', 'politica', 'acudiram-me', ',', 'mas', 'nao', 'me', 'acudiram', 'as', 'forcas', 'necessarias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois, pensei em fazer uma \"História dos Subúrbios\" menos seca que as memórias do \n",
      "Padre Luís Gonçalves dos Santos relativas à cidade; era obra modesta, mas exigia documentos e \n",
      "datas como preliminares, tudo árido e longo'\n",
      "Tokens gerados: ['depois', ',', 'pensei', 'em', 'fazer', 'uma', '``', 'historia', 'dos', 'suburbios', \"''\", 'menos', 'seca', 'que', 'as', 'memorias', 'do', 'padre', 'luis', 'goncalves', 'dos', 'santos', 'relativas', 'a', 'cidade', 'era', 'obra', 'modesta', ',', 'mas', 'exigia', 'documentos', 'e', 'datas', 'como', 'preliminares', ',', 'tudo', 'arido', 'e', 'longo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi então que os bustos pintados nas paredes entraram \n",
      "a falar-me e a dizer-me que, uma vez que eles não alcançavam reconstituir-me os tempos idos, \n",
      "pegasse da pena e contasse alguns'\n",
      "Tokens gerados: ['foi', 'entao', 'que', 'os', 'bustos', 'pintados', 'nas', 'paredes', 'entraram', 'a', 'falar-me', 'e', 'a', 'dizer-me', 'que', ',', 'uma', 'vez', 'que', 'eles', 'nao', 'alcancavam', 'reconstituir-me', 'os', 'tempos', 'idos', ',', 'pegasse', 'da', 'pena', 'e', 'contasse', 'alguns']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Talvez a narração me desse a ilusão, e as sombras viessem \n",
      "perpassar ligeiras, como ao poeta, não o do trem, mas o do Fausto: Aí vindes outra vez, inquietas \n",
      "sombras?'\n",
      "Tokens gerados: ['talvez', 'a', 'narracao', 'me', 'desse', 'a', 'ilusao', ',', 'e', 'as', 'sombras', 'viessem', 'perpassar', 'ligeiras', ',', 'como', 'ao', 'poeta', ',', 'nao', 'o', 'do', 'trem', ',', 'mas', 'o', 'do', 'fausto', 'ai', 'vindes', 'outra', 'vez', ',', 'inquietas', 'sombras', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fiquei tão alegre com esta idéia, que ainda agora me treme a pena na mão'\n",
      "Tokens gerados: ['fiquei', 'tao', 'alegre', 'com', 'esta', 'ideia', ',', 'que', 'ainda', 'agora', 'me', 'treme', 'a', 'pena', 'na', 'mao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sim, Nero, Augusto, \n",
      "Massinissa, e tu, grande César, que me incitas a fazer os meus comentários, agradeço-vos o \n",
      "conselho, e vou deitar ao papel as reminiscências que me vierem vindo'\n",
      "Tokens gerados: ['sim', ',', 'nero', ',', 'augusto', ',', 'massinissa', ',', 'e', 'tu', ',', 'grande', 'cesar', ',', 'que', 'me', 'incitas', 'a', 'fazer', 'os', 'meus', 'comentarios', ',', 'agradeco-vos', 'o', 'conselho', ',', 'e', 'vou', 'deitar', 'ao', 'papel', 'as', 'reminiscencias', 'que', 'me', 'vierem', 'vindo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deste modo, viverei o que \n",
      "vivi, e assentarei a mão para alguma obra de maior tomo'\n",
      "Tokens gerados: ['deste', 'modo', ',', 'viverei', 'o', 'que', 'vivi', ',', 'e', 'assentarei', 'a', 'mao', 'para', 'alguma', 'obra', 'de', 'maior', 'tomo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eia, comecemos a evocação por uma \n",
      "célebre tarde de novembro, que nunca me esqueceu'\n",
      "Tokens gerados: ['eia', ',', 'comecemos', 'a', 'evocacao', 'por', 'uma', 'celebre', 'tarde', 'de', 'novembro', ',', 'que', 'nunca', 'me', 'esqueceu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tive outras muitas, melhores, e piores, mas \n",
      "aquela nunca se me apagou do espírito'\n",
      "Tokens gerados: ['tive', 'outras', 'muitas', ',', 'melhores', ',', 'e', 'piores', ',', 'mas', 'aquela', 'nunca', 'se', 'me', 'apagou', 'do', 'espirito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É o que vais entender, lendo'\n",
      "Tokens gerados: ['e', 'o', 'que', 'vais', 'entender', ',', 'lendo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_dom_casmurro_machado_cap_2.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Ia entrar na sala de visitas, quando ouvi proferir o meu nome e escondi-me atrás da porta'\n",
      "Tokens gerados: ['ia', 'entrar', 'na', 'sala', 'de', 'visitas', ',', 'quando', 'ouvi', 'proferir', 'o', 'meu', 'nome', 'e', 'escondi-me', 'atras', 'da', 'porta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A casa \n",
      "era a da Rua de Mata-cavalos, o mês novembro, o ano é que é um tanto remoto, mas eu não hei de \n",
      "trocar as datas à minha vida só para agradar às pessoas que não amam histórias velhas; o ano era de \n",
      "1857'\n",
      "Tokens gerados: ['a', 'casa', 'era', 'a', 'da', 'rua', 'de', 'mata-cavalos', ',', 'o', 'mes', 'novembro', ',', 'o', 'ano', 'e', 'que', 'e', 'um', 'tanto', 'remoto', ',', 'mas', 'eu', 'nao', 'hei', 'de', 'trocar', 'as', 'datas', 'a', 'minha', 'vida', 'so', 'para', 'agradar', 'as', 'pessoas', 'que', 'nao', 'amam', 'historias', 'velhas', 'o', 'ano', 'era', 'de', '1857']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--D'\n",
      "Tokens gerados: ['--', 'd']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Glória, a senhora persiste na idéia de meter o nosso Bentinho no seminário? É mais que tempo, \n",
      "e já agora pode haver uma dificuldade'\n",
      "Tokens gerados: ['gloria', ',', 'a', 'senhora', 'persiste', 'na', 'ideia', 'de', 'meter', 'o', 'nosso', 'bentinho', 'no', 'seminario', '?', 'e', 'mais', 'que', 'tempo', ',', 'e', 'ja', 'agora', 'pode', 'haver', 'uma', 'dificuldade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Que dificuldade? \n",
      "--Uma grande dificuldade'\n",
      "Tokens gerados: ['--', 'que', 'dificuldade', '?', '--', 'uma', 'grande', 'dificuldade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Minha mãe quis saber o que era'\n",
      "Tokens gerados: ['minha', 'mae', 'quis', 'saber', 'o', 'que', 'era']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José Dias, depois de alguns instantes de concentração, veio ver se \n",
      "havia alguém no corredor; não deu por mim, voltou e, abafando a voz, disse que a dificuldade \n",
      "estava na casa ao pé, a gente do Pádua'\n",
      "Tokens gerados: ['jose', 'dias', ',', 'depois', 'de', 'alguns', 'instantes', 'de', 'concentracao', ',', 'veio', 'ver', 'se', 'havia', 'alguem', 'no', 'corredor', 'nao', 'deu', 'por', 'mim', ',', 'voltou', 'e', ',', 'abafando', 'a', 'voz', ',', 'disse', 'que', 'a', 'dificuldade', 'estava', 'na', 'casa', 'ao', 'pe', ',', 'a', 'gente', 'do', 'padua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--A gente do Pádua? \n",
      "--Há algum tempo estou para lhe dizer isto, mas não me atrevia'\n",
      "Tokens gerados: ['--', 'a', 'gente', 'do', 'padua', '?', '--', 'ha', 'algum', 'tempo', 'estou', 'para', 'lhe', 'dizer', 'isto', ',', 'mas', 'nao', 'me', 'atrevia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não me parece bonito que o nosso \n",
      "Bentinho ande metido nos cantos com a filha do Tartaruga, e esta é a dificuldade, porque se eles \n",
      "pegam de namoro, a senhora terá muito que lutar para separá-los'\n",
      "Tokens gerados: ['nao', 'me', 'parece', 'bonito', 'que', 'o', 'nosso', 'bentinho', 'ande', 'metido', 'nos', 'cantos', 'com', 'a', 'filha', 'do', 'tartaruga', ',', 'e', 'esta', 'e', 'a', 'dificuldade', ',', 'porque', 'se', 'eles', 'pegam', 'de', 'namoro', ',', 'a', 'senhora', 'tera', 'muito', 'que', 'lutar', 'para', 'separa-los']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Não acho'\n",
      "Tokens gerados: ['--', 'nao', 'acho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Metidos nos cantos? \n",
      "--É um modo de falar'\n",
      "Tokens gerados: ['metidos', 'nos', 'cantos', '?', '--', 'e', 'um', 'modo', 'de', 'falar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em segredinhos, sempre juntos'\n",
      "Tokens gerados: ['em', 'segredinhos', ',', 'sempre', 'juntos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bentinho quase que não sai de lá'\n",
      "Tokens gerados: ['bentinho', 'quase', 'que', 'nao', 'sai', 'de', 'la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A pequena \n",
      "é uma desmiolada; o pai faz que não vê; tomara ele que as cousas corressem de maneira, que'\n",
      "Tokens gerados: ['a', 'pequena', 'e', 'uma', 'desmiolada', 'o', 'pai', 'faz', 'que', 'nao', 've', 'tomara', 'ele', 'que', 'as', 'cousas', 'corressem', 'de', 'maneira', ',', 'que']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Compreendo o seu gesto; a senhora não crê em tais cálculos, parece-lhe que todos têm a alma \n",
      "candida'\n",
      "Tokens gerados: ['compreendo', 'o', 'seu', 'gesto', 'a', 'senhora', 'nao', 'cre', 'em', 'tais', 'calculos', ',', 'parece-lhe', 'que', 'todos', 'tem', 'a', 'alma', 'candida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Mas, Sr'\n",
      "Tokens gerados: ['--', 'mas', ',', 'sr']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'José Dias, tenho visto os pequenos brincando, e nunca vi nada que faça desconfiar'\n",
      "Tokens gerados: ['jose', 'dias', ',', 'tenho', 'visto', 'os', 'pequenos', 'brincando', ',', 'e', 'nunca', 'vi', 'nada', 'que', 'faca', 'desconfiar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Basta \n",
      "a idade; Bentinho mal tem quinze anos'\n",
      "Tokens gerados: ['basta', 'a', 'idade', 'bentinho', 'mal', 'tem', 'quinze', 'anos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Capitu fez quatorze à semana passada; são dous criançolas'\n",
      "Tokens gerados: ['capitu', 'fez', 'quatorze', 'a', 'semana', 'passada', 'sao', 'dous', 'criancolas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não se esqueça que foram criados juntos, desde aquela grande enchente, há dez anos, em que a \n",
      "família Pádua perdeu tanta cousa; daí vieram as nossas relações'\n",
      "Tokens gerados: ['nao', 'se', 'esqueca', 'que', 'foram', 'criados', 'juntos', ',', 'desde', 'aquela', 'grande', 'enchente', ',', 'ha', 'dez', 'anos', ',', 'em', 'que', 'a', 'familia', 'padua', 'perdeu', 'tanta', 'cousa', 'dai', 'vieram', 'as', 'nossas', 'relacoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois eu hei de crer?'\n",
      "Tokens gerados: ['pois', 'eu', 'hei', 'de', 'crer', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mano \n",
      "Cosme, você que acha? Tio Cosme respondeu com um \"Ora!\" que, traduzido em vulgar, queria \n",
      "dizer: \"São imaginações do José Dias os pequenos divertem-se, eu divirto-me; onde está o gamão?\" \n",
      "--Sim, creio que o senhor está enganado'\n",
      "Tokens gerados: ['mano', 'cosme', ',', 'voce', 'que', 'acha', '?', 'tio', 'cosme', 'respondeu', 'com', 'um', '``', 'ora', \"''\", 'que', ',', 'traduzido', 'em', 'vulgar', ',', 'queria', 'dizer', '``', 'sao', 'imaginacoes', 'do', 'jose', 'dias', 'os', 'pequenos', 'divertem-se', ',', 'eu', 'divirto-me', 'onde', 'esta', 'o', 'gamao', '?', \"''\", '--', 'sim', ',', 'creio', 'que', 'o', 'senhor', 'esta', 'enganado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Pode ser minha senhora'\n",
      "Tokens gerados: ['--', 'pode', 'ser', 'minha', 'senhora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Oxalá tenham razão; mas creia que não falei senão depois de muito \n",
      "examinar'\n",
      "Tokens gerados: ['oxala', 'tenham', 'razao', 'mas', 'creia', 'que', 'nao', 'falei', 'senao', 'depois', 'de', 'muito', 'examinar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Em todo caso, vai sendo tempo, interrompeu minha mãe; vou tratar de metê-lo no seminário \n",
      "quanto antes'\n",
      "Tokens gerados: ['--', 'em', 'todo', 'caso', ',', 'vai', 'sendo', 'tempo', ',', 'interrompeu', 'minha', 'mae', 'vou', 'tratar', 'de', 'mete-lo', 'no', 'seminario', 'quanto', 'antes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Bem, uma vez que não perdeu a idéia de o fazer padre, tem-se ganho o principal'\n",
      "Tokens gerados: ['--', 'bem', ',', 'uma', 'vez', 'que', 'nao', 'perdeu', 'a', 'ideia', 'de', 'o', 'fazer', 'padre', ',', 'tem-se', 'ganho', 'o', 'principal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bentinho há de \n",
      "satisfazer os desejos de sua mãe e depois a igreja brasileira tem altos destinos'\n",
      "Tokens gerados: ['bentinho', 'ha', 'de', 'satisfazer', 'os', 'desejos', 'de', 'sua', 'mae', 'e', 'depois', 'a', 'igreja', 'brasileira', 'tem', 'altos', 'destinos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não esqueçamos que \n",
      "um bispo presidiu a Constituinte, e que o Padre Feijó governou o Império'\n",
      "Tokens gerados: ['nao', 'esquecamos', 'que', 'um', 'bispo', 'presidiu', 'a', 'constituinte', ',', 'e', 'que', 'o', 'padre', 'feijo', 'governou', 'o', 'imperio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '-- Governo como a cara dele! atalhou tio Cosme, cedendo a antigos rancores políticos'\n",
      "Tokens gerados: ['--', 'governo', 'como', 'a', 'cara', 'dele', 'atalhou', 'tio', 'cosme', ',', 'cedendo', 'a', 'antigos', 'rancores', 'politicos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Perdão, doutor, não estou defendendo ninguém, estou citando O que eu quero é dizer que o clero \n",
      "ainda tem grande papel no Brasil'\n",
      "Tokens gerados: ['--', 'perdao', ',', 'doutor', ',', 'nao', 'estou', 'defendendo', 'ninguem', ',', 'estou', 'citando', 'o', 'que', 'eu', 'quero', 'e', 'dizer', 'que', 'o', 'clero', 'ainda', 'tem', 'grande', 'papel', 'no', 'brasil']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Você o que quer é um capote; ande, vá buscar o gamão'\n",
      "Tokens gerados: ['--', 'voce', 'o', 'que', 'quer', 'e', 'um', 'capote', 'ande', ',', 'va', 'buscar', 'o', 'gamao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quanto ao pequeno, se tem de ser padre, \n",
      "realmente é melhor que não comece a dizer missa atrás das portas'\n",
      "Tokens gerados: ['quanto', 'ao', 'pequeno', ',', 'se', 'tem', 'de', 'ser', 'padre', ',', 'realmente', 'e', 'melhor', 'que', 'nao', 'comece', 'a', 'dizer', 'missa', 'atras', 'das', 'portas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, olhe cá, mana Glória, há \n",
      "mesmo necessidade de fazê-lo padre? \n",
      "-- É promessa, há de cumprir-se'\n",
      "Tokens gerados: ['mas', ',', 'olhe', 'ca', ',', 'mana', 'gloria', ',', 'ha', 'mesmo', 'necessidade', 'de', 'faze-lo', 'padre', '?', '--', 'e', 'promessa', ',', 'ha', 'de', 'cumprir-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '--Sei que você fez promessa'\n",
      "Tokens gerados: ['--', 'sei', 'que', 'voce', 'fez', 'promessa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'mas uma promessa assim'\n",
      "Tokens gerados: ['mas', 'uma', 'promessa', 'assim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'não sei'\n",
      "Tokens gerados: ['nao', 'sei']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Creio que, bem pensado'\n",
      "Tokens gerados: ['creio', 'que', ',', 'bem', 'pensado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Você \n",
      "que acha, prima Justina? \n",
      "-- Eu? \n",
      "--Verdade é que cada um sabe melhor de si, continuou tio Cosme- Deus é que sabe de todos'\n",
      "Tokens gerados: ['voce', 'que', 'acha', ',', 'prima', 'justina', '?', '--', 'eu', '?', '--', 'verdade', 'e', 'que', 'cada', 'um', 'sabe', 'melhor', 'de', 'si', ',', 'continuou', 'tio', 'cosme-', 'deus', 'e', 'que', 'sabe', 'de', 'todos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Contudo, uma promessa de tantos anos'\n",
      "Tokens gerados: ['contudo', ',', 'uma', 'promessa', 'de', 'tantos', 'anos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, que é isso, mana Glória? Está chorando? Ora esta \n",
      "pois isto é cousa de lágrimas? \n",
      "Minha mãe assoou-se sem responder'\n",
      "Tokens gerados: ['mas', ',', 'que', 'e', 'isso', ',', 'mana', 'gloria', '?', 'esta', 'chorando', '?', 'ora', 'esta', 'pois', 'isto', 'e', 'cousa', 'de', 'lagrimas', '?', 'minha', 'mae', 'assoou-se', 'sem', 'responder']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prima Justina creio que se levantou e foi ter com ela'\n",
      "Tokens gerados: ['prima', 'justina', 'creio', 'que', 'se', 'levantou', 'e', 'foi', 'ter', 'com', 'ela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seguiu-\n",
      "se um alto silêncio, durante o qual estive a pique de entrar na sala, mas outra força maior, outra \n",
      "emoção'\n",
      "Tokens gerados: ['seguiu-se', 'um', 'alto', 'silencio', ',', 'durante', 'o', 'qual', 'estive', 'a', 'pique', 'de', 'entrar', 'na', 'sala', ',', 'mas', 'outra', 'forca', 'maior', ',', 'outra', 'emocao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não pude ouvir as palavras que tio Cosme entrou a dizer'\n",
      "Tokens gerados: ['nao', 'pude', 'ouvir', 'as', 'palavras', 'que', 'tio', 'cosme', 'entrou', 'a', 'dizer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prima Justina exortava: \"Prima \n",
      "Glória! Prima Glória!\" José Dias desculpava-se: \"Se soubesse, não teria falado, mas falei pela \n",
      "veneração, pela estima, pelo afeto, para cumprir um dever amargo, um dever amaríssimo'\n",
      "Tokens gerados: ['prima', 'justina', 'exortava', '``', 'prima', 'gloria', 'prima', 'gloria', \"''\", 'jose', 'dias', 'desculpava-se', '``', 'se', 'soubesse', ',', 'nao', 'teria', 'falado', ',', 'mas', 'falei', 'pela', 'veneracao', ',', 'pela', 'estima', ',', 'pelo', 'afeto', ',', 'para', 'cumprir', 'um', 'dever', 'amargo', ',', 'um', 'dever', 'amarissimo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '\"'\n",
      "Tokens gerados: ['``']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_dom_casmurro_machado_cap_3.json\n",
      "\n",
      "Processando frase original (após split e strip): 'José Dias amava os superlativos'\n",
      "Tokens gerados: ['jose', 'dias', 'amava', 'os', 'superlativos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era um modo de dar feição monumental às idéias; não as havendo, \n",
      "servia a prolongar as frases'\n",
      "Tokens gerados: ['era', 'um', 'modo', 'de', 'dar', 'feicao', 'monumental', 'as', 'ideias', 'nao', 'as', 'havendo', ',', 'servia', 'a', 'prolongar', 'as', 'frases']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Levantou-se para ir buscar o gamão, que estava no interior da casa'\n",
      "Tokens gerados: ['levantou-se', 'para', 'ir', 'buscar', 'o', 'gamao', ',', 'que', 'estava', 'no', 'interior', 'da', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cosi-me muito à parede, e vi-o passar com as suas calças brancas engomadas, presilhas, rodaque e \n",
      "gravata de mola'\n",
      "Tokens gerados: ['cosi-me', 'muito', 'a', 'parede', ',', 'e', 'vi-o', 'passar', 'com', 'as', 'suas', 'calcas', 'brancas', 'engomadas', ',', 'presilhas', ',', 'rodaque', 'e', 'gravata', 'de', 'mola']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi dos últimos que usaram presilhas no Rio de Janeiro, e talvez neste mundo'\n",
      "Tokens gerados: ['foi', 'dos', 'ultimos', 'que', 'usaram', 'presilhas', 'no', 'rio', 'de', 'janeiro', ',', 'e', 'talvez', 'neste', 'mundo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Trazia as calças curtas para que lhe ficassem bem esticadas'\n",
      "Tokens gerados: ['trazia', 'as', 'calcas', 'curtas', 'para', 'que', 'lhe', 'ficassem', 'bem', 'esticadas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A gravata de cetim preto, com um arco \n",
      "de aço por dentro, imobilizava-lhe o pescoço; era então moda'\n",
      "Tokens gerados: ['a', 'gravata', 'de', 'cetim', 'preto', ',', 'com', 'um', 'arco', 'de', 'aco', 'por', 'dentro', ',', 'imobilizava-lhe', 'o', 'pescoco', 'era', 'entao', 'moda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O rodaque de chita, veste caseira e \n",
      "leve, parecia nele uma casaca de cerimônia'\n",
      "Tokens gerados: ['o', 'rodaque', 'de', 'chita', ',', 'veste', 'caseira', 'e', 'leve', ',', 'parecia', 'nele', 'uma', 'casaca', 'de', 'cerimonia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era magro, chupado, com um princípio de calva; teria \n",
      "os seus cinqüenta e cinco anos'\n",
      "Tokens gerados: ['era', 'magro', ',', 'chupado', ',', 'com', 'um', 'principio', 'de', 'calva', 'teria', 'os', 'seus', 'cinquenta', 'e', 'cinco', 'anos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Levantou-se com o passo vagaroso do costume, não aquele vagar \n",
      "arrastado se era dos preguiçosos, mas um vagar calculado e deduzido, um silogismo completo, a \n",
      "premissa antes da conseqüência, a conseqüência antes da conclusão'\n",
      "Tokens gerados: ['levantou-se', 'com', 'o', 'passo', 'vagaroso', 'do', 'costume', ',', 'nao', 'aquele', 'vagar', 'arrastado', 'se', 'era', 'dos', 'preguicosos', ',', 'mas', 'um', 'vagar', 'calculado', 'e', 'deduzido', ',', 'um', 'silogismo', 'completo', ',', 'a', 'premissa', 'antes', 'da', 'consequencia', ',', 'a', 'consequencia', 'antes', 'da', 'conclusao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um dever amaríssimo!'\n",
      "Tokens gerados: ['um', 'dever', 'amarissimo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_dom_casmurro_machado_cap_4.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Verdes mares bravios de minha terra natal, onde canta a jandaia nas frondes da carnaúba; Verdes  mares  que  brilhais  como  líquida  esmeralda  aos  raios  do  Sol  nascente,  perlongando  as  alvas  praias  \n",
      "ensombradas de coqueiros'\n",
      "Tokens gerados: ['verdes', 'mares', 'bravios', 'de', 'minha', 'terra', 'natal', ',', 'onde', 'canta', 'a', 'jandaia', 'nas', 'frondes', 'da', 'carnauba', 'verdes', 'mares', 'que', 'brilhais', 'como', 'liquida', 'esmeralda', 'aos', 'raios', 'do', 'sol', 'nascente', ',', 'perlongando', 'as', 'alvas', 'praias', 'ensombradas', 'de', 'coqueiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Serenai verdes  mares, e alisai  docemente a  vaga  impetuosa, para que o barco aventureiro manso resvale à flor \n",
      "das águas'\n",
      "Tokens gerados: ['serenai', 'verdes', 'mares', ',', 'e', 'alisai', 'docemente', 'a', 'vaga', 'impetuosa', ',', 'para', 'que', 'o', 'barco', 'aventureiro', 'manso', 'resvale', 'a', 'flor', 'das', 'aguas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Onde vai a afouta jangada, que deixa rápida a costa cearense, aberta ao fresco terral a grande vela? \n",
      "Onde vai como branca alcíone buscando o rochedo pátrio nas solidões do oceano? \n",
      "Três entes respiram sobre o frágil lenho que vai singrando veloce, mar em fora; \n",
      "Um jovem guerreiro cuja tez branca não cora o sangue americano; uma criança e um rafeiro que viram a luz no \n",
      "berço das florestas, e brincam irmãos, filhos ambos da mesma terra selvagem'\n",
      "Tokens gerados: ['onde', 'vai', 'a', 'afouta', 'jangada', ',', 'que', 'deixa', 'rapida', 'a', 'costa', 'cearense', ',', 'aberta', 'ao', 'fresco', 'terral', 'a', 'grande', 'vela', '?', 'onde', 'vai', 'como', 'branca', 'alcione', 'buscando', 'o', 'rochedo', 'patrio', 'nas', 'solidoes', 'do', 'oceano', '?', 'tres', 'entes', 'respiram', 'sobre', 'o', 'fragil', 'lenho', 'que', 'vai', 'singrando', 'veloce', ',', 'mar', 'em', 'fora', 'um', 'jovem', 'guerreiro', 'cuja', 'tez', 'branca', 'nao', 'cora', 'o', 'sangue', 'americano', 'uma', 'crianca', 'e', 'um', 'rafeiro', 'que', 'viram', 'a', 'luz', 'no', 'berco', 'das', 'florestas', ',', 'e', 'brincam', 'irmaos', ',', 'filhos', 'ambos', 'da', 'mesma', 'terra', 'selvagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A lufada intermitente traz da praia um eco vibrante, que ressoa entre o marulho das vagas: \n",
      "— Iracema!'\n",
      "Tokens gerados: ['a', 'lufada', 'intermitente', 'traz', 'da', 'praia', 'um', 'eco', 'vibrante', ',', 'que', 'ressoa', 'entre', 'o', 'marulho', 'das', 'vagas', '—', 'iracema']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O  moço  guerreiro,  encostado  ao  mastro,  leva  os  olhos  presos  na  sombra  fugitiva  da  terra;  a  espaços  o  olhar  \n",
      "empanado  por  tênue  lágrima  cai  sobre  o  jirau,  onde  folgam  as  duas  inocentes  criaturas,  companheiras  de  seu  \n",
      "infortúnio'\n",
      "Tokens gerados: ['o', 'moco', 'guerreiro', ',', 'encostado', 'ao', 'mastro', ',', 'leva', 'os', 'olhos', 'presos', 'na', 'sombra', 'fugitiva', 'da', 'terra', 'a', 'espacos', 'o', 'olhar', 'empanado', 'por', 'tenue', 'lagrima', 'cai', 'sobre', 'o', 'jirau', ',', 'onde', 'folgam', 'as', 'duas', 'inocentes', 'criaturas', ',', 'companheiras', 'de', 'seu', 'infortunio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nesse momento o lábio arranca d’alma um agro sorriso'\n",
      "Tokens gerados: ['nesse', 'momento', 'o', 'labio', 'arranca', 'd', '’', 'alma', 'um', 'agro', 'sorriso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que deixara ele na terra do exílio? \n",
      "Uma história que me contaram nas lindas várzeas onde nasci, à calada da noite, quando a Lua passeava no céu \n",
      "argenteando os campos, e a brisa rugitava nos palmares'\n",
      "Tokens gerados: ['que', 'deixara', 'ele', 'na', 'terra', 'do', 'exilio', '?', 'uma', 'historia', 'que', 'me', 'contaram', 'nas', 'lindas', 'varzeas', 'onde', 'nasci', ',', 'a', 'calada', 'da', 'noite', ',', 'quando', 'a', 'lua', 'passeava', 'no', 'ceu', 'argenteando', 'os', 'campos', ',', 'e', 'a', 'brisa', 'rugitava', 'nos', 'palmares']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Refresca o vento'\n",
      "Tokens gerados: ['refresca', 'o', 'vento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O  rulo  das  vagas  precipita'\n",
      "Tokens gerados: ['o', 'rulo', 'das', 'vagas', 'precipita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O  barco  salta  sobre  as  ondas;  desaparece  no  horizonte'\n",
      "Tokens gerados: ['o', 'barco', 'salta', 'sobre', 'as', 'ondas', 'desaparece', 'no', 'horizonte']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Abre-se  a  imensidade  dos  \n",
      "mares; e a borrasca enverga, como o condor, as foscas asas sobre o abismo'\n",
      "Tokens gerados: ['abre-se', 'a', 'imensidade', 'dos', 'mares', 'e', 'a', 'borrasca', 'enverga', ',', 'como', 'o', 'condor', ',', 'as', 'foscas', 'asas', 'sobre', 'o', 'abismo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deus  te  leve  a  salvo,  brioso  e  altivo  barco,  por  entre    as  vagas  revoltas,  e  te  poje  nalguma  enseada  amiga'\n",
      "Tokens gerados: ['deus', 'te', 'leve', 'a', 'salvo', ',', 'brioso', 'e', 'altivo', 'barco', ',', 'por', 'entre', 'as', 'vagas', 'revoltas', ',', 'e', 'te', 'poje', 'nalguma', 'enseada', 'amiga']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Soprem para ti as brandas  auras; e para ti jaspeie a bonança mares de leite'\n",
      "Tokens gerados: ['soprem', 'para', 'ti', 'as', 'brandas', 'auras', 'e', 'para', 'ti', 'jaspeie', 'a', 'bonanca', 'mares', 'de', 'leite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Enquanto vogas assim à discrição do vento, airoso barco, volva às brancas areias a saudade, que te acompanha, \n",
      "mas não se parte da terra onde revoa'\n",
      "Tokens gerados: ['enquanto', 'vogas', 'assim', 'a', 'discricao', 'do', 'vento', ',', 'airoso', 'barco', ',', 'volva', 'as', 'brancas', 'areias', 'a', 'saudade', ',', 'que', 'te', 'acompanha', ',', 'mas', 'nao', 'se', 'parte', 'da', 'terra', 'onde', 'revoa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_iracema_jose_de_alencar_cap_1.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Além, muito além daquela serra, que ainda azula no horizonte, nasceu Iracema'\n",
      "Tokens gerados: ['alem', ',', 'muito', 'alem', 'daquela', 'serra', ',', 'que', 'ainda', 'azula', 'no', 'horizonte', ',', 'nasceu', 'iracema']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Iracema, a virgem dos lábios de mel, que tinha os cabelos mais negros que a asa da graúna, e mais longos que\n",
      "seu talhe de palmeira'\n",
      "Tokens gerados: ['iracema', ',', 'a', 'virgem', 'dos', 'labios', 'de', 'mel', ',', 'que', 'tinha', 'os', 'cabelos', 'mais', 'negros', 'que', 'a', 'asa', 'da', 'grauna', ',', 'e', 'mais', 'longos', 'queseu', 'talhe', 'de', 'palmeira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O favo da jati não era doce como seu sorriso; nem a baunilha recendia no bosque como seu hálito perfumado'\n",
      "Tokens gerados: ['o', 'favo', 'da', 'jati', 'nao', 'era', 'doce', 'como', 'seu', 'sorriso', 'nem', 'a', 'baunilha', 'recendia', 'no', 'bosque', 'como', 'seu', 'halito', 'perfumado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mais rápida que a corça selvagem, a morena virgem corria o sertão e as matas do Ipu, onde campeava sua\n",
      "guerreira tribo, da grande nação tabajara'\n",
      "Tokens gerados: ['mais', 'rapida', 'que', 'a', 'corca', 'selvagem', ',', 'a', 'morena', 'virgem', 'corria', 'o', 'sertao', 'e', 'as', 'matas', 'do', 'ipu', ',', 'onde', 'campeava', 'suaguerreira', 'tribo', ',', 'da', 'grande', 'nacao', 'tabajara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O pé grácil e nu, mal roçando, alisava apenas a verde pelúcia que vestia a\n",
      "terra com as primeiras águas'\n",
      "Tokens gerados: ['o', 'pe', 'gracil', 'e', 'nu', ',', 'mal', 'rocando', ',', 'alisava', 'apenas', 'a', 'verde', 'pelucia', 'que', 'vestia', 'aterra', 'com', 'as', 'primeiras', 'aguas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um dia, ao pino do Sol, ela repousava em um claro da floresta'\n",
      "Tokens gerados: ['um', 'dia', ',', 'ao', 'pino', 'do', 'sol', ',', 'ela', 'repousava', 'em', 'um', 'claro', 'da', 'floresta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Banhava-lhe o corpo a sombra da oiticica, mais\n",
      "fresca do que o orvalho da noite'\n",
      "Tokens gerados: ['banhava-lhe', 'o', 'corpo', 'a', 'sombra', 'da', 'oiticica', ',', 'maisfresca', 'do', 'que', 'o', 'orvalho', 'da', 'noite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os ramos da acácia silvestre esparziam flores sobre os úmidos cabelos'\n",
      "Tokens gerados: ['os', 'ramos', 'da', 'acacia', 'silvestre', 'esparziam', 'flores', 'sobre', 'os', 'umidos', 'cabelos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Escondidos\n",
      "na folhagem os pássaros ameigavam o canto'\n",
      "Tokens gerados: ['escondidosna', 'folhagem', 'os', 'passaros', 'ameigavam', 'o', 'canto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Iracema saiu do banho: o aljôfar d’água ainda a roreja, como à doce mangaba que corou em manhã de chuva'\n",
      "Tokens gerados: ['iracema', 'saiu', 'do', 'banho', 'o', 'aljofar', 'd', '’', 'agua', 'ainda', 'a', 'roreja', ',', 'como', 'a', 'doce', 'mangaba', 'que', 'corou', 'em', 'manha', 'de', 'chuva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Enquanto repousa, empluma das penas do gará as flechas de seu arco, e concerta com o sabiá da mata, pousado no\n",
      "galho próximo, o canto agreste'\n",
      "Tokens gerados: ['enquanto', 'repousa', ',', 'empluma', 'das', 'penas', 'do', 'gara', 'as', 'flechas', 'de', 'seu', 'arco', ',', 'e', 'concerta', 'com', 'o', 'sabia', 'da', 'mata', ',', 'pousado', 'nogalho', 'proximo', ',', 'o', 'canto', 'agreste']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A graciosa ará, sua companheira e amiga, brinca junto dela'\n",
      "Tokens gerados: ['a', 'graciosa', 'ara', ',', 'sua', 'companheira', 'e', 'amiga', ',', 'brinca', 'junto', 'dela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Às vezes sobe aos ramos da árvore e de lá chama a\n",
      "virgem pelo nome; outras remexe o uru de palha matizada, onde traz a selvagem seus perfumes, os alvos fios do\n",
      "crautá, as agulhas da juçara com que tece a renda, e as tintas de que matiza o algodão'\n",
      "Tokens gerados: ['as', 'vezes', 'sobe', 'aos', 'ramos', 'da', 'arvore', 'e', 'de', 'la', 'chama', 'avirgem', 'pelo', 'nome', 'outras', 'remexe', 'o', 'uru', 'de', 'palha', 'matizada', ',', 'onde', 'traz', 'a', 'selvagem', 'seus', 'perfumes', ',', 'os', 'alvos', 'fios', 'docrauta', ',', 'as', 'agulhas', 'da', 'jucara', 'com', 'que', 'tece', 'a', 'renda', ',', 'e', 'as', 'tintas', 'de', 'que', 'matiza', 'o', 'algodao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Rumor suspeito quebra a doce harmonia da sesta'\n",
      "Tokens gerados: ['rumor', 'suspeito', 'quebra', 'a', 'doce', 'harmonia', 'da', 'sesta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ergue a virgem os olhos, que o sol não deslumbra; sua vista\n",
      "perturba-se'\n",
      "Tokens gerados: ['ergue', 'a', 'virgem', 'os', 'olhos', ',', 'que', 'o', 'sol', 'nao', 'deslumbra', 'sua', 'vistaperturba-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Diante dela e todo a contemplá-la está um guerreiro estranho, se é guerreiro e não algum mau espírito da\n",
      "floresta'\n",
      "Tokens gerados: ['diante', 'dela', 'e', 'todo', 'a', 'contempla-la', 'esta', 'um', 'guerreiro', 'estranho', ',', 'se', 'e', 'guerreiro', 'e', 'nao', 'algum', 'mau', 'espirito', 'dafloresta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tem nas faces o branco das areias que bordam o mar; nos olhos o azul triste das águas profundas'\n",
      "Tokens gerados: ['tem', 'nas', 'faces', 'o', 'branco', 'das', 'areias', 'que', 'bordam', 'o', 'mar', 'nos', 'olhos', 'o', 'azul', 'triste', 'das', 'aguas', 'profundas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ignotas\n",
      "armas e tecidos ignotos cobrem-lhe o corpo'\n",
      "Tokens gerados: ['ignotasarmas', 'e', 'tecidos', 'ignotos', 'cobrem-lhe', 'o', 'corpo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi rápido, como o olhar, o gesto de Iracema'\n",
      "Tokens gerados: ['foi', 'rapido', ',', 'como', 'o', 'olhar', ',', 'o', 'gesto', 'de', 'iracema']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A flecha embebida no arco partiu'\n",
      "Tokens gerados: ['a', 'flecha', 'embebida', 'no', 'arco', 'partiu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Gotas de sangue borbulham na\n",
      "face do desconhecido'\n",
      "Tokens gerados: ['gotas', 'de', 'sangue', 'borbulham', 'naface', 'do', 'desconhecido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De primeiro ímpeto, a mão lesta caiu sobre a cruz da espada; mas logo sorriu'\n",
      "Tokens gerados: ['de', 'primeiro', 'impeto', ',', 'a', 'mao', 'lesta', 'caiu', 'sobre', 'a', 'cruz', 'da', 'espada', 'mas', 'logo', 'sorriu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O moço guerreiro aprendeu na\n",
      "religião de sua mãe, onde a mulher é símbolo de ternura e amor'\n",
      "Tokens gerados: ['o', 'moco', 'guerreiro', 'aprendeu', 'nareligiao', 'de', 'sua', 'mae', ',', 'onde', 'a', 'mulher', 'e', 'simbolo', 'de', 'ternura', 'e', 'amor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sofreu mais d’alma que da ferida'\n",
      "Tokens gerados: ['sofreu', 'mais', 'd', '’', 'alma', 'que', 'da', 'ferida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O sentimento que ele pôs nos olhos e no rosto, não o sei eu'\n",
      "Tokens gerados: ['o', 'sentimento', 'que', 'ele', 'pos', 'nos', 'olhos', 'e', 'no', 'rosto', ',', 'nao', 'o', 'sei', 'eu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Porém a virgem lançou de si o arco e a uiraçaba, e\n",
      "correu para o guerreiro, sentida da mágoa que causara'\n",
      "Tokens gerados: ['porem', 'a', 'virgem', 'lancou', 'de', 'si', 'o', 'arco', 'e', 'a', 'uiracaba', ',', 'ecorreu', 'para', 'o', 'guerreiro', ',', 'sentida', 'da', 'magoa', 'que', 'causara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mão que rápida ferira, estancou mais rápida e compassiva o sangue que gotejava'\n",
      "Tokens gerados: ['a', 'mao', 'que', 'rapida', 'ferira', ',', 'estancou', 'mais', 'rapida', 'e', 'compassiva', 'o', 'sangue', 'que', 'gotejava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois Iracema quebrou a\n",
      "flecha homicida: deu a haste ao desconhecido, guardando consigo a ponta farpada'\n",
      "Tokens gerados: ['depois', 'iracema', 'quebrou', 'aflecha', 'homicida', 'deu', 'a', 'haste', 'ao', 'desconhecido', ',', 'guardando', 'consigo', 'a', 'ponta', 'farpada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O guerreiro falou:\n",
      "— Quebras comigo a flecha da paz?\n",
      "— Quem te ensinou, guerreiro branco, a linguagem de meus irmãos? Donde vieste a estas matas, que nunca\n",
      "viram outro guerreiro como tu?\n",
      "— Venho de bem longe, filha das florestas'\n",
      "Tokens gerados: ['o', 'guerreiro', 'falou—', 'quebras', 'comigo', 'a', 'flecha', 'da', 'paz', '?', '—', 'quem', 'te', 'ensinou', ',', 'guerreiro', 'branco', ',', 'a', 'linguagem', 'de', 'meus', 'irmaos', '?', 'donde', 'vieste', 'a', 'estas', 'matas', ',', 'que', 'nuncaviram', 'outro', 'guerreiro', 'como', 'tu', '?', '—', 'venho', 'de', 'bem', 'longe', ',', 'filha', 'das', 'florestas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Venho das terras que teus irmãos já possuíram, e hoje têm os meus'\n",
      "Tokens gerados: ['venho', 'das', 'terras', 'que', 'teus', 'irmaos', 'ja', 'possuiram', ',', 'e', 'hoje', 'tem', 'os', 'meus']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Bem-vindo seja o estrangeiro aos campos dos tabajaras, senhores das aldeias, e à cabana de Araquém, pai de\n",
      "Iracema'\n",
      "Tokens gerados: ['—', 'bem-vindo', 'seja', 'o', 'estrangeiro', 'aos', 'campos', 'dos', 'tabajaras', ',', 'senhores', 'das', 'aldeias', ',', 'e', 'a', 'cabana', 'de', 'araquem', ',', 'pai', 'deiracema']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_iracema_jose_de_alencar_cap_2.json\n",
      "\n",
      "Processando frase original (após split e strip): 'O estrangeiro seguiu a virgem através da floresta'\n",
      "Tokens gerados: ['o', 'estrangeiro', 'seguiu', 'a', 'virgem', 'atraves', 'da', 'floresta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando o Sol descambava sobre a crista dos montes, e a rola desatava do fundo da mata os primeiros arrulhos,\n",
      "eles descobriram no vale a grande taba; e mais longe, pendurada no rochedo, à sombra dos altos juazeiros, a cabana\n",
      "do pajé'\n",
      "Tokens gerados: ['quando', 'o', 'sol', 'descambava', 'sobre', 'a', 'crista', 'dos', 'montes', ',', 'e', 'a', 'rola', 'desatava', 'do', 'fundo', 'da', 'mata', 'os', 'primeiros', 'arrulhos', ',', 'eles', 'descobriram', 'no', 'vale', 'a', 'grande', 'taba', 'e', 'mais', 'longe', ',', 'pendurada', 'no', 'rochedo', ',', 'a', 'sombra', 'dos', 'altos', 'juazeiros', ',', 'a', 'cabanado', 'paje']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O ancião fumava à porta, sentado na esteira de carnaúba, meditando os sagrados ritos de Tupã'\n",
      "Tokens gerados: ['o', 'anciao', 'fumava', 'a', 'porta', ',', 'sentado', 'na', 'esteira', 'de', 'carnauba', ',', 'meditando', 'os', 'sagrados', 'ritos', 'de', 'tupa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O tênue sopro da\n",
      "brisa carmeava, como frocos de algodão, os compridos e raros cabelos brancos'\n",
      "Tokens gerados: ['o', 'tenue', 'sopro', 'dabrisa', 'carmeava', ',', 'como', 'frocos', 'de', 'algodao', ',', 'os', 'compridos', 'e', 'raros', 'cabelos', 'brancos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De imóvel que estava, sumia a vida\n",
      "nos olhos cavos e nas rugas profundas'\n",
      "Tokens gerados: ['de', 'imovel', 'que', 'estava', ',', 'sumia', 'a', 'vidanos', 'olhos', 'cavos', 'e', 'nas', 'rugas', 'profundas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O pajé lobrigou os dois vultos que avançavam; cuidou ver a sombra de uma árvore solitária que vinha\n",
      "alongando-se pelo vale fora'\n",
      "Tokens gerados: ['o', 'paje', 'lobrigou', 'os', 'dois', 'vultos', 'que', 'avancavam', 'cuidou', 'ver', 'a', 'sombra', 'de', 'uma', 'arvore', 'solitaria', 'que', 'vinhaalongando-se', 'pelo', 'vale', 'fora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando os viajantes entraram na densa penumbra do bosque, então seu olhar como o do tigre, afeito às trevas,\n",
      "conheceu Iracema e viu que a seguia um jovem guerreiro, de estranha raça e longes terras'\n",
      "Tokens gerados: ['quando', 'os', 'viajantes', 'entraram', 'na', 'densa', 'penumbra', 'do', 'bosque', ',', 'entao', 'seu', 'olhar', 'como', 'o', 'do', 'tigre', ',', 'afeito', 'as', 'trevas', ',', 'conheceu', 'iracema', 'e', 'viu', 'que', 'a', 'seguia', 'um', 'jovem', 'guerreiro', ',', 'de', 'estranha', 'raca', 'e', 'longes', 'terras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As tribos tabajaras, d’além Ibiapaba, falavam de uma nova raça de guerreiros, alvos como flores de borrasca, e\n",
      "vindos de remota plaga às margens do Mearim'\n",
      "Tokens gerados: ['as', 'tribos', 'tabajaras', ',', 'd', '’', 'alem', 'ibiapaba', ',', 'falavam', 'de', 'uma', 'nova', 'raca', 'de', 'guerreiros', ',', 'alvos', 'como', 'flores', 'de', 'borrasca', ',', 'evindos', 'de', 'remota', 'plaga', 'as', 'margens', 'do', 'mearim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O ancião pensou que fosse um guerreiro semelhante, aquele que\n",
      "pisava os campos nativos'\n",
      "Tokens gerados: ['o', 'anciao', 'pensou', 'que', 'fosse', 'um', 'guerreiro', 'semelhante', ',', 'aquele', 'quepisava', 'os', 'campos', 'nativos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tranqüilo, esperou'\n",
      "Tokens gerados: ['tranquilo', ',', 'esperou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A virgem aponta para o estrangeiro e diz:\n",
      "— Ele veio, pai'\n",
      "Tokens gerados: ['a', 'virgem', 'aponta', 'para', 'o', 'estrangeiro', 'e', 'diz—', 'ele', 'veio', ',', 'pai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Veio bem'\n",
      "Tokens gerados: ['—', 'veio', 'bem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É Tupã que traz o hóspede à cabana de Araquém'\n",
      "Tokens gerados: ['e', 'tupa', 'que', 'traz', 'o', 'hospede', 'a', 'cabana', 'de', 'araquem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Assim dizendo, o pajé passou o cachimbo ao estrangeiro; e entraram ambos na cabana'\n",
      "Tokens gerados: ['assim', 'dizendo', ',', 'o', 'paje', 'passou', 'o', 'cachimbo', 'ao', 'estrangeiro', 'e', 'entraram', 'ambos', 'na', 'cabana']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O mancebo sentou-se na rede principal, suspensa no centro da habitação'\n",
      "Tokens gerados: ['o', 'mancebo', 'sentou-se', 'na', 'rede', 'principal', ',', 'suspensa', 'no', 'centro', 'da', 'habitacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Iracema acendeu o fogo da hospitalidade; e trouxe o que havia de provisões para satisfazer a fome e a sede:\n",
      "trouxe o resto da caça, a farinha-d’água, os frutos silvestres, os favos de mel e o vinho de caju e ananás'\n",
      "Tokens gerados: ['iracema', 'acendeu', 'o', 'fogo', 'da', 'hospitalidade', 'e', 'trouxe', 'o', 'que', 'havia', 'de', 'provisoes', 'para', 'satisfazer', 'a', 'fome', 'e', 'a', 'sedetrouxe', 'o', 'resto', 'da', 'caca', ',', 'a', 'farinha-d', '’', 'agua', ',', 'os', 'frutos', 'silvestres', ',', 'os', 'favos', 'de', 'mel', 'e', 'o', 'vinho', 'de', 'caju', 'e', 'ananas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois a virgem entrou com a igaçaba, que enchera na fonte próxima de água fresca para lavar o rosto e as mãos\n",
      "do estrangeiro'\n",
      "Tokens gerados: ['depois', 'a', 'virgem', 'entrou', 'com', 'a', 'igacaba', ',', 'que', 'enchera', 'na', 'fonte', 'proxima', 'de', 'agua', 'fresca', 'para', 'lavar', 'o', 'rosto', 'e', 'as', 'maosdo', 'estrangeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando o guerreiro terminou a refeição, o velho pajé apagou o cachimbo e falou:\n",
      "— Vieste?\n",
      "— Vim, respondeu o desconhecido'\n",
      "Tokens gerados: ['quando', 'o', 'guerreiro', 'terminou', 'a', 'refeicao', ',', 'o', 'velho', 'paje', 'apagou', 'o', 'cachimbo', 'e', 'falou—', 'vieste', '?', '—', 'vim', ',', 'respondeu', 'o', 'desconhecido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Bem vieste'\n",
      "Tokens gerados: ['—', 'bem', 'vieste']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O estrangeiro é senhor na cabana de Araquém'\n",
      "Tokens gerados: ['o', 'estrangeiro', 'e', 'senhor', 'na', 'cabana', 'de', 'araquem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os tabajaras têm mil guerreiros para defendê-lo, e\n",
      "mulheres sem conta para servi-lo'\n",
      "Tokens gerados: ['os', 'tabajaras', 'tem', 'mil', 'guerreiros', 'para', 'defende-lo', ',', 'emulheres', 'sem', 'conta', 'para', 'servi-lo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dize, e todos te obedecerão'\n",
      "Tokens gerados: ['dize', ',', 'e', 'todos', 'te', 'obedecerao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pajé, eu te agradeço o agasalho que me deste'\n",
      "Tokens gerados: ['—', 'paje', ',', 'eu', 'te', 'agradeco', 'o', 'agasalho', 'que', 'me', 'deste']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Logo que o Sol nascer, deixarei tua cabana e teus campos\n",
      "aonde vim perdido; mas não devo deixá-los sem dizer-te quem é o guerreiro, que fizeste amigo'\n",
      "Tokens gerados: ['logo', 'que', 'o', 'sol', 'nascer', ',', 'deixarei', 'tua', 'cabana', 'e', 'teus', 'camposaonde', 'vim', 'perdido', 'mas', 'nao', 'devo', 'deixa-los', 'sem', 'dizer-te', 'quem', 'e', 'o', 'guerreiro', ',', 'que', 'fizeste', 'amigo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Foi a Tupã que o pajé serviu: ele te trouxe, ele te levará'\n",
      "Tokens gerados: ['—', 'foi', 'a', 'tupa', 'que', 'o', 'paje', 'serviu', 'ele', 'te', 'trouxe', ',', 'ele', 'te', 'levara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Araquém nada fez pelo hóspede; não pergunta donde\n",
      "vem, e quando vai'\n",
      "Tokens gerados: ['araquem', 'nada', 'fez', 'pelo', 'hospede', 'nao', 'pergunta', 'dondevem', ',', 'e', 'quando', 'vai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se queres dormir, desçam sobre ti os sonhos alegres; se queres falar, teu hóspede escuta'\n",
      "Tokens gerados: ['se', 'queres', 'dormir', ',', 'descam', 'sobre', 'ti', 'os', 'sonhos', 'alegres', 'se', 'queres', 'falar', ',', 'teu', 'hospede', 'escuta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O estrangeiro disse:\n",
      "— Sou dos guerreiros brancos, que levantaram a taba nas margens do Jaguaribe, perto do mar, onde habitam os\n",
      "pitiguaras, inimigos de tua nação'\n",
      "Tokens gerados: ['o', 'estrangeiro', 'disse—', 'sou', 'dos', 'guerreiros', 'brancos', ',', 'que', 'levantaram', 'a', 'taba', 'nas', 'margens', 'do', 'jaguaribe', ',', 'perto', 'do', 'mar', ',', 'onde', 'habitam', 'ospitiguaras', ',', 'inimigos', 'de', 'tua', 'nacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Meu nome é Martim, que na tua língua diz como filho de guerreiro; meu sangue, o\n",
      "do grande povo que primeiro viu as terras de tua pátria'\n",
      "Tokens gerados: ['meu', 'nome', 'e', 'martim', ',', 'que', 'na', 'tua', 'lingua', 'diz', 'como', 'filho', 'de', 'guerreiro', 'meu', 'sangue', ',', 'odo', 'grande', 'povo', 'que', 'primeiro', 'viu', 'as', 'terras', 'de', 'tua', 'patria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Já meus destroçados companheiros voltaram por mar às\n",
      "margens do Paraíba, de onde vieram; e o chefe, desamparado dos seus, atravessa agora os vastos sertões do Apodi'\n",
      "Tokens gerados: ['ja', 'meus', 'destrocados', 'companheiros', 'voltaram', 'por', 'mar', 'asmargens', 'do', 'paraiba', ',', 'de', 'onde', 'vieram', 'e', 'o', 'chefe', ',', 'desamparado', 'dos', 'seus', ',', 'atravessa', 'agora', 'os', 'vastos', 'sertoes', 'do', 'apodi']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Só eu de tantos fiquei, porque estava entre os pitiguaras de Acaraú, na cabana do bravo Poti, irmão de Jacaúna, que\n",
      "plantou comigo a árvore da amizade'\n",
      "Tokens gerados: ['so', 'eu', 'de', 'tantos', 'fiquei', ',', 'porque', 'estava', 'entre', 'os', 'pitiguaras', 'de', 'acarau', ',', 'na', 'cabana', 'do', 'bravo', 'poti', ',', 'irmao', 'de', 'jacauna', ',', 'queplantou', 'comigo', 'a', 'arvore', 'da', 'amizade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Há três sóis partimos para a caça; e perdido dos meus, vim aos campos dos\n",
      "tabajaras'\n",
      "Tokens gerados: ['ha', 'tres', 'sois', 'partimos', 'para', 'a', 'caca', 'e', 'perdido', 'dos', 'meus', ',', 'vim', 'aos', 'campos', 'dostabajaras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Foi algum mau espírito da floresta que cegou o guerreiro branco no escuro da mata, respondeu o ancião'\n",
      "Tokens gerados: ['—', 'foi', 'algum', 'mau', 'espirito', 'da', 'floresta', 'que', 'cegou', 'o', 'guerreiro', 'branco', 'no', 'escuro', 'da', 'mata', ',', 'respondeu', 'o', 'anciao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A cauã piou, além, na extrema do vale'\n",
      "Tokens gerados: ['a', 'caua', 'piou', ',', 'alem', ',', 'na', 'extrema', 'do', 'vale']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Caía a noite'\n",
      "Tokens gerados: ['caia', 'a', 'noite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_iracema_jose_de_alencar_cap_3.json\n",
      "\n",
      "Processando frase original (após split e strip): 'O pajé vibrou o maracá, e saiu da cabana, porém o estrangeiro não ficou só'\n",
      "Tokens gerados: ['o', 'paje', 'vibrou', 'o', 'maraca', ',', 'e', 'saiu', 'da', 'cabana', ',', 'porem', 'o', 'estrangeiro', 'nao', 'ficou', 'so']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Iracema voltara com as mulheres chamadas para servir o hóspede de Araquém, e os guerreiros vindos para\n",
      "obedecer-lhe'\n",
      "Tokens gerados: ['iracema', 'voltara', 'com', 'as', 'mulheres', 'chamadas', 'para', 'servir', 'o', 'hospede', 'de', 'araquem', ',', 'e', 'os', 'guerreiros', 'vindos', 'paraobedecer-lhe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Guerreiro branco, disse a virgem, o prazer embale tua rede durante a noite; e o Sol traga luz a teus olhos,\n",
      "alegria à tua alma'\n",
      "Tokens gerados: ['—', 'guerreiro', 'branco', ',', 'disse', 'a', 'virgem', ',', 'o', 'prazer', 'embale', 'tua', 'rede', 'durante', 'a', 'noite', 'e', 'o', 'sol', 'traga', 'luz', 'a', 'teus', 'olhos', ',', 'alegria', 'a', 'tua', 'alma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E assim dizendo, Iracema tinha o lábio trêmulo, e úmida a pálpebra'\n",
      "Tokens gerados: ['e', 'assim', 'dizendo', ',', 'iracema', 'tinha', 'o', 'labio', 'tremulo', ',', 'e', 'umida', 'a', 'palpebra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Tu me deixas? perguntou Martim'\n",
      "Tokens gerados: ['—', 'tu', 'me', 'deixas', '?', 'perguntou', 'martim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— As mais belas mulheres da grande taba contigo ficam'\n",
      "Tokens gerados: ['—', 'as', 'mais', 'belas', 'mulheres', 'da', 'grande', 'taba', 'contigo', 'ficam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Para elas a filha de Araquém não devia ter conduzido o hóspede à cabana do pajé'\n",
      "Tokens gerados: ['—', 'para', 'elas', 'a', 'filha', 'de', 'araquem', 'nao', 'devia', 'ter', 'conduzido', 'o', 'hospede', 'a', 'cabana', 'do', 'paje']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Estrangeiro, Iracema não pode ser tua serva'\n",
      "Tokens gerados: ['—', 'estrangeiro', ',', 'iracema', 'nao', 'pode', 'ser', 'tua', 'serva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É ela que guarda o segredo da jurema e o mistério do sonho'\n",
      "Tokens gerados: ['e', 'ela', 'que', 'guarda', 'o', 'segredo', 'da', 'jurema', 'e', 'o', 'misterio', 'do', 'sonho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sua\n",
      "mão fabrica para o pajé a bebida de Tupã'\n",
      "Tokens gerados: ['suamao', 'fabrica', 'para', 'o', 'paje', 'a', 'bebida', 'de', 'tupa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O guerreiro cristão atravessou a cabana e sumiu-se na treva'\n",
      "Tokens gerados: ['o', 'guerreiro', 'cristao', 'atravessou', 'a', 'cabana', 'e', 'sumiu-se', 'na', 'treva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A grande taba erguia-se no fundo do vale, iluminada pelos fachos da alegria'\n",
      "Tokens gerados: ['a', 'grande', 'taba', 'erguia-se', 'no', 'fundo', 'do', 'vale', ',', 'iluminada', 'pelos', 'fachos', 'da', 'alegria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Rugia o maracá; ao quebro lento do\n",
      "canto selvagem, batia a dança em trono a rude cadência'\n",
      "Tokens gerados: ['rugia', 'o', 'maraca', 'ao', 'quebro', 'lento', 'docanto', 'selvagem', ',', 'batia', 'a', 'danca', 'em', 'trono', 'a', 'rude', 'cadencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O pajé inspirado conduzia o sagrado tripúdio e dizia ao\n",
      "povo crente os segredos de Tupã'\n",
      "Tokens gerados: ['o', 'paje', 'inspirado', 'conduzia', 'o', 'sagrado', 'tripudio', 'e', 'dizia', 'aopovo', 'crente', 'os', 'segredos', 'de', 'tupa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O maior chefe da nação tabajara, Irapuã, descera do alto da serra Ibiapaba, para levar as tribos do sertão contra o\n",
      "inimigo pitiguara'\n",
      "Tokens gerados: ['o', 'maior', 'chefe', 'da', 'nacao', 'tabajara', ',', 'irapua', ',', 'descera', 'do', 'alto', 'da', 'serra', 'ibiapaba', ',', 'para', 'levar', 'as', 'tribos', 'do', 'sertao', 'contra', 'oinimigo', 'pitiguara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os guerreiros do vale festejam a vinda do chefe, e o próximo combate'\n",
      "Tokens gerados: ['os', 'guerreiros', 'do', 'vale', 'festejam', 'a', 'vinda', 'do', 'chefe', ',', 'e', 'o', 'proximo', 'combate']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O mancebo cristão viu longe o clarão da festa, e passou além, e olhou o céu azul sem nuvens'\n",
      "Tokens gerados: ['o', 'mancebo', 'cristao', 'viu', 'longe', 'o', 'clarao', 'da', 'festa', ',', 'e', 'passou', 'alem', ',', 'e', 'olhou', 'o', 'ceu', 'azul', 'sem', 'nuvens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A estrela morta,\n",
      "que então brilhava sobre a cúpula da floresta, guiou seu passo firme para as frescas margens do Acaraú'\n",
      "Tokens gerados: ['a', 'estrela', 'morta', ',', 'que', 'entao', 'brilhava', 'sobre', 'a', 'cupula', 'da', 'floresta', ',', 'guiou', 'seu', 'passo', 'firme', 'para', 'as', 'frescas', 'margens', 'do', 'acarau']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando ele transmontou o vale e ia penetrar na mata, o vulto de Iracema surgiu'\n",
      "Tokens gerados: ['quando', 'ele', 'transmontou', 'o', 'vale', 'e', 'ia', 'penetrar', 'na', 'mata', ',', 'o', 'vulto', 'de', 'iracema', 'surgiu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A virgem seguira o estrangeiro\n",
      "como a brisa sutil que resvala sem murmurejar por entre a ramagem'\n",
      "Tokens gerados: ['a', 'virgem', 'seguira', 'o', 'estrangeirocomo', 'a', 'brisa', 'sutil', 'que', 'resvala', 'sem', 'murmurejar', 'por', 'entre', 'a', 'ramagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Por que, disse ela, o estrangeiro abandona a cabana hospedeira sem levar o presente da volta? Quem fez mal\n",
      "ao guerreiro branco na terra dos tabajaras?\n",
      "O cristão sentiu quanto era justa a queixa; e achou-se ingrato'\n",
      "Tokens gerados: ['—', 'por', 'que', ',', 'disse', 'ela', ',', 'o', 'estrangeiro', 'abandona', 'a', 'cabana', 'hospedeira', 'sem', 'levar', 'o', 'presente', 'da', 'volta', '?', 'quem', 'fez', 'malao', 'guerreiro', 'branco', 'na', 'terra', 'dos', 'tabajaras', '?', 'o', 'cristao', 'sentiu', 'quanto', 'era', 'justa', 'a', 'queixa', 'e', 'achou-se', 'ingrato']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ninguém fez mal ao teu hóspede, filha de Araquém'\n",
      "Tokens gerados: ['—', 'ninguem', 'fez', 'mal', 'ao', 'teu', 'hospede', ',', 'filha', 'de', 'araquem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era o desejo de ver seus amigos que o afastava dos\n",
      "campos dos tabajaras'\n",
      "Tokens gerados: ['era', 'o', 'desejo', 'de', 'ver', 'seus', 'amigos', 'que', 'o', 'afastava', 'doscampos', 'dos', 'tabajaras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não levava o presente da volta; mas leva em sua alma a lembrança de Iracema'\n",
      "Tokens gerados: ['nao', 'levava', 'o', 'presente', 'da', 'volta', 'mas', 'leva', 'em', 'sua', 'alma', 'a', 'lembranca', 'de', 'iracema']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Se a lembrança de Iracema estivesse n’alma do estrangeiro, ela não o deixaria partir'\n",
      "Tokens gerados: ['—', 'se', 'a', 'lembranca', 'de', 'iracema', 'estivesse', 'n', '’', 'alma', 'do', 'estrangeiro', ',', 'ela', 'nao', 'o', 'deixaria', 'partir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O vento não leva a areia\n",
      "da várzea, quando a areia bebe a água da chuva'\n",
      "Tokens gerados: ['o', 'vento', 'nao', 'leva', 'a', 'areiada', 'varzea', ',', 'quando', 'a', 'areia', 'bebe', 'a', 'agua', 'da', 'chuva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A virgem suspirou:\n",
      "— Guerreiro branco, espera que Caubi volte da caça'\n",
      "Tokens gerados: ['a', 'virgem', 'suspirou—', 'guerreiro', 'branco', ',', 'espera', 'que', 'caubi', 'volte', 'da', 'caca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O irmão de Iracema tem o ouvido sutil que pressente a\n",
      "boicininga entre os rumores da mata; e o olhar do oitibó que vê melhor na treva'\n",
      "Tokens gerados: ['o', 'irmao', 'de', 'iracema', 'tem', 'o', 'ouvido', 'sutil', 'que', 'pressente', 'aboicininga', 'entre', 'os', 'rumores', 'da', 'mata', 'e', 'o', 'olhar', 'do', 'oitibo', 'que', 've', 'melhor', 'na', 'treva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ele te guiará às margens do rio das\n",
      "garças'\n",
      "Tokens gerados: ['ele', 'te', 'guiara', 'as', 'margens', 'do', 'rio', 'dasgarcas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Quanto tempo se passará antes que o irmão de Iracema esteja de volta na cabana de Araquém?\n",
      "— O Sol, que vai nascer, tornará com o guerreiro Caubi aos campos do Ipu'\n",
      "Tokens gerados: ['—', 'quanto', 'tempo', 'se', 'passara', 'antes', 'que', 'o', 'irmao', 'de', 'iracema', 'esteja', 'de', 'volta', 'na', 'cabana', 'de', 'araquem', '?', '—', 'o', 'sol', ',', 'que', 'vai', 'nascer', ',', 'tornara', 'com', 'o', 'guerreiro', 'caubi', 'aos', 'campos', 'do', 'ipu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Teu hóspede espera, filha de Araquém; mas se o Sol tornando não trouxer o irmão de Iracema, ele levará o\n",
      "guerreiro branco à taba dos pitiguaras'\n",
      "Tokens gerados: ['—', 'teu', 'hospede', 'espera', ',', 'filha', 'de', 'araquem', 'mas', 'se', 'o', 'sol', 'tornando', 'nao', 'trouxer', 'o', 'irmao', 'de', 'iracema', ',', 'ele', 'levara', 'oguerreiro', 'branco', 'a', 'taba', 'dos', 'pitiguaras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Martim voltou à cabana do pajé'\n",
      "Tokens gerados: ['martim', 'voltou', 'a', 'cabana', 'do', 'paje']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A alva rede que Iracema perfumara com a resina do benjoim guardava-lhe um sono calmo e doce'\n",
      "Tokens gerados: ['a', 'alva', 'rede', 'que', 'iracema', 'perfumara', 'com', 'a', 'resina', 'do', 'benjoim', 'guardava-lhe', 'um', 'sono', 'calmo', 'e', 'doce']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cristão adormeceu ouvindo suspirar, entre os murmúrios da floresta, o canto mavioso da virgem indiana'\n",
      "Tokens gerados: ['o', 'cristao', 'adormeceu', 'ouvindo', 'suspirar', ',', 'entre', 'os', 'murmurios', 'da', 'floresta', ',', 'o', 'canto', 'mavioso', 'da', 'virgem', 'indiana']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_iracema_jose_de_alencar_cap_4.json\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão foi, dos treze aos vinte e cinco anos, empregado de um vendeiro que enriqueceu entre as quatro \n",
      "paredes de uma suja e obscura taverna nos refolhos do bairro do Botafogo; e tanto economizou do pouco que ganhara \n",
      "nessa dúzia de anos, que, ao retirar-se o patrão para a terra, lhe deixou, em pagamento de ordenados vencidos, nem só a \n",
      "venda com o que estava dentro, como ainda um conto e quinhentos em dinheiro'\n",
      "Tokens gerados: ['joao', 'romao', 'foi', ',', 'dos', 'treze', 'aos', 'vinte', 'e', 'cinco', 'anos', ',', 'empregado', 'de', 'um', 'vendeiro', 'que', 'enriqueceu', 'entre', 'as', 'quatro', 'paredes', 'de', 'uma', 'suja', 'e', 'obscura', 'taverna', 'nos', 'refolhos', 'do', 'bairro', 'do', 'botafogo', 'e', 'tanto', 'economizou', 'do', 'pouco', 'que', 'ganhara', 'nessa', 'duzia', 'de', 'anos', ',', 'que', ',', 'ao', 'retirar-se', 'o', 'patrao', 'para', 'a', 'terra', ',', 'lhe', 'deixou', ',', 'em', 'pagamento', 'de', 'ordenados', 'vencidos', ',', 'nem', 'so', 'a', 'venda', 'com', 'o', 'que', 'estava', 'dentro', ',', 'como', 'ainda', 'um', 'conto', 'e', 'quinhentos', 'em', 'dinheiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Proprietário e estabelecido por sua conta, o rapaz atirou-se à labutação ainda com mais ardor, possuindo-se de tal \n",
      "delírio de enriquecer, que afrontava resignado as mais duras privações'\n",
      "Tokens gerados: ['proprietario', 'e', 'estabelecido', 'por', 'sua', 'conta', ',', 'o', 'rapaz', 'atirou-se', 'a', 'labutacao', 'ainda', 'com', 'mais', 'ardor', ',', 'possuindo-se', 'de', 'tal', 'delirio', 'de', 'enriquecer', ',', 'que', 'afrontava', 'resignado', 'as', 'mais', 'duras', 'privacoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dormia sobre o balcão da própria venda, em \n",
      "cima de uma esteira, fazendo travesseiro de um saco de estopa cheio de palha'\n",
      "Tokens gerados: ['dormia', 'sobre', 'o', 'balcao', 'da', 'propria', 'venda', ',', 'em', 'cima', 'de', 'uma', 'esteira', ',', 'fazendo', 'travesseiro', 'de', 'um', 'saco', 'de', 'estopa', 'cheio', 'de', 'palha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A comida arranjava-lha, mediante \n",
      "quatrocentos réis por dia, uma quitandeira sua vizinha, a Bertoleza, crioula trintona, escrava de um velho cego residente \n",
      "em Juiz de Fora e amigada com um português que tinha uma carroça de mão e fazia fretes na cidade'\n",
      "Tokens gerados: ['a', 'comida', 'arranjava-lha', ',', 'mediante', 'quatrocentos', 'reis', 'por', 'dia', ',', 'uma', 'quitandeira', 'sua', 'vizinha', ',', 'a', 'bertoleza', ',', 'crioula', 'trintona', ',', 'escrava', 'de', 'um', 'velho', 'cego', 'residente', 'em', 'juiz', 'de', 'fora', 'e', 'amigada', 'com', 'um', 'portugues', 'que', 'tinha', 'uma', 'carroca', 'de', 'mao', 'e', 'fazia', 'fretes', 'na', 'cidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bertoleza também trabalhava forte; a sua quitanda era a mais bem afreguesada do bairro'\n",
      "Tokens gerados: ['bertoleza', 'tambem', 'trabalhava', 'forte', 'a', 'sua', 'quitanda', 'era', 'a', 'mais', 'bem', 'afreguesada', 'do', 'bairro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De manhã vendia angu, \n",
      "e à noite peixe frito e iscas de fígado; pagava de jornal a seu dono vinte mil-réis por mês, e, apesar disso, tinha de parte \n",
      "quase que o necessário para a alforria'\n",
      "Tokens gerados: ['de', 'manha', 'vendia', 'angu', ',', 'e', 'a', 'noite', 'peixe', 'frito', 'e', 'iscas', 'de', 'figado', 'pagava', 'de', 'jornal', 'a', 'seu', 'dono', 'vinte', 'mil-reis', 'por', 'mes', ',', 'e', ',', 'apesar', 'disso', ',', 'tinha', 'de', 'parte', 'quase', 'que', 'o', 'necessario', 'para', 'a', 'alforria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um dia, porém, o seu homem, depois de correr meia légua, puxando uma carga \n",
      "superior às suas forças, caiu morto na rua, ao lado da carroça, estrompado como uma besta'\n",
      "Tokens gerados: ['um', 'dia', ',', 'porem', ',', 'o', 'seu', 'homem', ',', 'depois', 'de', 'correr', 'meia', 'legua', ',', 'puxando', 'uma', 'carga', 'superior', 'as', 'suas', 'forcas', ',', 'caiu', 'morto', 'na', 'rua', ',', 'ao', 'lado', 'da', 'carroca', ',', 'estrompado', 'como', 'uma', 'besta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão mostrou grande interesse por esta desgraça, fez-se até participante direto dos sofrimentos da vizinha, \n",
      "e com tamanho empenho a lamentou, que a boa mulher o escolheu para confidente das suas desventuras'\n",
      "Tokens gerados: ['joao', 'romao', 'mostrou', 'grande', 'interesse', 'por', 'esta', 'desgraca', ',', 'fez-se', 'ate', 'participante', 'direto', 'dos', 'sofrimentos', 'da', 'vizinha', ',', 'e', 'com', 'tamanho', 'empenho', 'a', 'lamentou', ',', 'que', 'a', 'boa', 'mulher', 'o', 'escolheu', 'para', 'confidente', 'das', 'suas', 'desventuras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Abriu-se com \n",
      "ele, contou-lhe a sua vida de amofinações e dificuldades'\n",
      "Tokens gerados: ['abriu-se', 'com', 'ele', ',', 'contou-lhe', 'a', 'sua', 'vida', 'de', 'amofinacoes', 'e', 'dificuldades']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Seu senhor comia-lhe a pele do corpo! Não era brinquedo \n",
      "para uma pobre mulher ter de escarrar pr’ali, todos os meses, vinte mil-réis em dinheiro!” E segredou-lhe então o que \n",
      "tinha juntado para a sua liberdade e acabou pedindo ao vendeiro que lhe guardasse as economias, porque já de certa vez \n",
      "fora roubada por gatunos que lhe entraram na quitanda pelos fundos'\n",
      "Tokens gerados: ['“', 'seu', 'senhor', 'comia-lhe', 'a', 'pele', 'do', 'corpo', 'nao', 'era', 'brinquedo', 'para', 'uma', 'pobre', 'mulher', 'ter', 'de', 'escarrar', 'pr', '’', 'ali', ',', 'todos', 'os', 'meses', ',', 'vinte', 'mil-reis', 'em', 'dinheiro', '”', 'e', 'segredou-lhe', 'entao', 'o', 'que', 'tinha', 'juntado', 'para', 'a', 'sua', 'liberdade', 'e', 'acabou', 'pedindo', 'ao', 'vendeiro', 'que', 'lhe', 'guardasse', 'as', 'economias', ',', 'porque', 'ja', 'de', 'certa', 'vez', 'fora', 'roubada', 'por', 'gatunos', 'que', 'lhe', 'entraram', 'na', 'quitanda', 'pelos', 'fundos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Daí em diante, João Romão tornou-se o caixa, o procurador e o conselheiro da crioula'\n",
      "Tokens gerados: ['dai', 'em', 'diante', ',', 'joao', 'romao', 'tornou-se', 'o', 'caixa', ',', 'o', 'procurador', 'e', 'o', 'conselheiro', 'da', 'crioula']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No fim de pouco tempo \n",
      "era ele quem tomava conta de tudo que ela produzia e era também quem punha e dispunha dos seus pecúlios, e quem se \n",
      "encarregava de remeter ao senhor os vinte mil-réis mensais'\n",
      "Tokens gerados: ['no', 'fim', 'de', 'pouco', 'tempo', 'era', 'ele', 'quem', 'tomava', 'conta', 'de', 'tudo', 'que', 'ela', 'produzia', 'e', 'era', 'tambem', 'quem', 'punha', 'e', 'dispunha', 'dos', 'seus', 'peculios', ',', 'e', 'quem', 'se', 'encarregava', 'de', 'remeter', 'ao', 'senhor', 'os', 'vinte', 'mil-reis', 'mensais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Abriu-lhe logo uma conta corrente, e a quitandeira, quando \n",
      "precisava de dinheiro para qualquer coisa, dava um pulo até à venda e recebia-o das mãos do vendeiro, de “Seu João”, \n",
      "como ela dizia'\n",
      "Tokens gerados: ['abriu-lhe', 'logo', 'uma', 'conta', 'corrente', ',', 'e', 'a', 'quitandeira', ',', 'quando', 'precisava', 'de', 'dinheiro', 'para', 'qualquer', 'coisa', ',', 'dava', 'um', 'pulo', 'ate', 'a', 'venda', 'e', 'recebia-o', 'das', 'maos', 'do', 'vendeiro', ',', 'de', '“', 'seu', 'joao', '”', ',', 'como', 'ela', 'dizia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seu João debitava metodicamente essas pequenas quantias num caderninho, em cuja capa de papel \n",
      "pardo lia-se, mal escrito e em letras cortadas de jornal: “Ativo e passivo de Bertoleza”'\n",
      "Tokens gerados: ['seu', 'joao', 'debitava', 'metodicamente', 'essas', 'pequenas', 'quantias', 'num', 'caderninho', ',', 'em', 'cuja', 'capa', 'de', 'papel', 'pardo', 'lia-se', ',', 'mal', 'escrito', 'e', 'em', 'letras', 'cortadas', 'de', 'jornal', '“', 'ativo', 'e', 'passivo', 'de', 'bertoleza', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E por tal forma foi o taverneiro ganhando confiança no espírito da mulher, que esta afinal nada mais resolvia só \n",
      "por si, e aceitava dele, cegamente, todo e qualquer arbítrio'\n",
      "Tokens gerados: ['e', 'por', 'tal', 'forma', 'foi', 'o', 'taverneiro', 'ganhando', 'confianca', 'no', 'espirito', 'da', 'mulher', ',', 'que', 'esta', 'afinal', 'nada', 'mais', 'resolvia', 'so', 'por', 'si', ',', 'e', 'aceitava', 'dele', ',', 'cegamente', ',', 'todo', 'e', 'qualquer', 'arbitrio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por último, se alguém precisava tratar com ela qualquer \n",
      "negócio, nem mais se dava ao trabalho de procurá-la, ia logo direito a João Romão'\n",
      "Tokens gerados: ['por', 'ultimo', ',', 'se', 'alguem', 'precisava', 'tratar', 'com', 'ela', 'qualquer', 'negocio', ',', 'nem', 'mais', 'se', 'dava', 'ao', 'trabalho', 'de', 'procura-la', ',', 'ia', 'logo', 'direito', 'a', 'joao', 'romao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando deram fé estavam amigados'\n",
      "Tokens gerados: ['quando', 'deram', 'fe', 'estavam', 'amigados']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ele propôs-lhe morarem juntos e ela concordou de braços abertos, feliz em meter-se de novo com um português, \n",
      "porque, como toda a cafuza, Bertoleza não queria sujeitar-se a negros e procurava instintivamente o homem numa raça \n",
      "superior à sua'\n",
      "Tokens gerados: ['ele', 'propos-lhe', 'morarem', 'juntos', 'e', 'ela', 'concordou', 'de', 'bracos', 'abertos', ',', 'feliz', 'em', 'meter-se', 'de', 'novo', 'com', 'um', 'portugues', ',', 'porque', ',', 'como', 'toda', 'a', 'cafuza', ',', 'bertoleza', 'nao', 'queria', 'sujeitar-se', 'a', 'negros', 'e', 'procurava', 'instintivamente', 'o', 'homem', 'numa', 'raca', 'superior', 'a', 'sua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão comprou então, com as economias da amiga, alguns palmos de terreno ao lado esquerdo da venda, e \n",
      "levantou uma casinha de duas portas, dividida ao meio paralelamente à rua, sendo a parte da frente destinada à quitanda \n",
      "e a do fundo para um dormitório que se arranjou com os cacarecos de Bertoleza'\n",
      "Tokens gerados: ['joao', 'romao', 'comprou', 'entao', ',', 'com', 'as', 'economias', 'da', 'amiga', ',', 'alguns', 'palmos', 'de', 'terreno', 'ao', 'lado', 'esquerdo', 'da', 'venda', ',', 'e', 'levantou', 'uma', 'casinha', 'de', 'duas', 'portas', ',', 'dividida', 'ao', 'meio', 'paralelamente', 'a', 'rua', ',', 'sendo', 'a', 'parte', 'da', 'frente', 'destinada', 'a', 'quitanda', 'e', 'a', 'do', 'fundo', 'para', 'um', 'dormitorio', 'que', 'se', 'arranjou', 'com', 'os', 'cacarecos', 'de', 'bertoleza']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia, além da cama, uma cômoda de \n",
      "jacarandá muito velha com maçanetas de metal amarelo já mareadas, um oratório cheio de santos e forrado de papel de \n",
      "cor, um baú grande de couro cru tacheado, dois banquinhos de pau feitos de uma só peça e um formidável cabide de \n",
      "pregar na parede, com a sua competente coberta de retalhos de chita'\n",
      "Tokens gerados: ['havia', ',', 'alem', 'da', 'cama', ',', 'uma', 'comoda', 'de', 'jacaranda', 'muito', 'velha', 'com', 'macanetas', 'de', 'metal', 'amarelo', 'ja', 'mareadas', ',', 'um', 'oratorio', 'cheio', 'de', 'santos', 'e', 'forrado', 'de', 'papel', 'de', 'cor', ',', 'um', 'bau', 'grande', 'de', 'couro', 'cru', 'tacheado', ',', 'dois', 'banquinhos', 'de', 'pau', 'feitos', 'de', 'uma', 'so', 'peca', 'e', 'um', 'formidavel', 'cabide', 'de', 'pregar', 'na', 'parede', ',', 'com', 'a', 'sua', 'competente', 'coberta', 'de', 'retalhos', 'de', 'chita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O vendeiro nunca tivera tanta mobília'\n",
      "Tokens gerados: ['o', 'vendeiro', 'nunca', 'tivera', 'tanta', 'mobilia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Agora, disse ele à crioula, as coisas vão correr melhor para você'\n",
      "Tokens gerados: ['—', 'agora', ',', 'disse', 'ele', 'a', 'crioula', ',', 'as', 'coisas', 'vao', 'correr', 'melhor', 'para', 'voce']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Você vai ficar forra; eu entro com o que falta'\n",
      "Tokens gerados: ['voce', 'vai', 'ficar', 'forra', 'eu', 'entro', 'com', 'o', 'que', 'falta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nesse dia ele saiu muito à rua, e uma semana depois apareceu com uma folha de papel toda escrita, que leu em \n",
      "voz alta à companheira'\n",
      "Tokens gerados: ['nesse', 'dia', 'ele', 'saiu', 'muito', 'a', 'rua', ',', 'e', 'uma', 'semana', 'depois', 'apareceu', 'com', 'uma', 'folha', 'de', 'papel', 'toda', 'escrita', ',', 'que', 'leu', 'em', 'voz', 'alta', 'a', 'companheira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Você agora não tem mais senhor! declarou em seguida à leitura, que ela ouviu entre lágrimas agradecidas'\n",
      "Tokens gerados: ['—', 'voce', 'agora', 'nao', 'tem', 'mais', 'senhor', 'declarou', 'em', 'seguida', 'a', 'leitura', ',', 'que', 'ela', 'ouviu', 'entre', 'lagrimas', 'agradecidas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Agora está livre'\n",
      "Tokens gerados: ['agora', 'esta', 'livre']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Doravante o que você fizer é só seu e mais de seus filhos, se os tiver'\n",
      "Tokens gerados: ['doravante', 'o', 'que', 'voce', 'fizer', 'e', 'so', 'seu', 'e', 'mais', 'de', 'seus', 'filhos', ',', 'se', 'os', 'tiver']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Acabou-se o cativeiro de pagar \n",
      "os vinte mil-réis à peste do cego! \n",
      "— Coitado! A gente se queixa é da sorte! Ele, como meu senhor, exigia o jornal, exigia o que era seu! \n",
      "— Seu ou não seu, acabou-se! E vida nova! \n",
      "Contra todo o costume, abriu-se nesse dia uma garrafa de vinho do Porto, e os dois beberam-na em honra ao \n",
      "grande acontecimento'\n",
      "Tokens gerados: ['acabou-se', 'o', 'cativeiro', 'de', 'pagar', 'os', 'vinte', 'mil-reis', 'a', 'peste', 'do', 'cego', '—', 'coitado', 'a', 'gente', 'se', 'queixa', 'e', 'da', 'sorte', 'ele', ',', 'como', 'meu', 'senhor', ',', 'exigia', 'o', 'jornal', ',', 'exigia', 'o', 'que', 'era', 'seu', '—', 'seu', 'ou', 'nao', 'seu', ',', 'acabou-se', 'e', 'vida', 'nova', 'contra', 'todo', 'o', 'costume', ',', 'abriu-se', 'nesse', 'dia', 'uma', 'garrafa', 'de', 'vinho', 'do', 'porto', ',', 'e', 'os', 'dois', 'beberam-na', 'em', 'honra', 'ao', 'grande', 'acontecimento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, a tal carta de liberdade era obra do próprio João Romão, e nem mesmo o selo, que \n",
      "ele entendeu de pespegar-lhe em cima, para dar à burla maior formalidade, representava despesa porque o esperto \n",
      "aproveitara uma estampilha já servida'\n",
      "Tokens gerados: ['entretanto', ',', 'a', 'tal', 'carta', 'de', 'liberdade', 'era', 'obra', 'do', 'proprio', 'joao', 'romao', ',', 'e', 'nem', 'mesmo', 'o', 'selo', ',', 'que', 'ele', 'entendeu', 'de', 'pespegar-lhe', 'em', 'cima', ',', 'para', 'dar', 'a', 'burla', 'maior', 'formalidade', ',', 'representava', 'despesa', 'porque', 'o', 'esperto', 'aproveitara', 'uma', 'estampilha', 'ja', 'servida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O senhor de Bertoleza não teve sequer conhecimento do fato; o que lhe constou, \n",
      "sim, foi que a sua escrava lhe havia fugido para a Bahia depois da morte do amigo'\n",
      "Tokens gerados: ['o', 'senhor', 'de', 'bertoleza', 'nao', 'teve', 'sequer', 'conhecimento', 'do', 'fato', 'o', 'que', 'lhe', 'constou', ',', 'sim', ',', 'foi', 'que', 'a', 'sua', 'escrava', 'lhe', 'havia', 'fugido', 'para', 'a', 'bahia', 'depois', 'da', 'morte', 'do', 'amigo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— O cego que venha buscá-la aqui, se for capaz'\n",
      "Tokens gerados: ['—', 'o', 'cego', 'que', 'venha', 'busca-la', 'aqui', ',', 'se', 'for', 'capaz']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'desafiou o vendeiro de si para si'\n",
      "Tokens gerados: ['desafiou', 'o', 'vendeiro', 'de', 'si', 'para', 'si']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ele que caia nessa e verá se \n",
      "tem ou não pra pêras! \n",
      "Não obstante, só ficou tranqüilo de todo daí a três meses, quando lhe constou a morte do velho'\n",
      "Tokens gerados: ['ele', 'que', 'caia', 'nessa', 'e', 'vera', 'se', 'tem', 'ou', 'nao', 'pra', 'peras', 'nao', 'obstante', ',', 'so', 'ficou', 'tranquilo', 'de', 'todo', 'dai', 'a', 'tres', 'meses', ',', 'quando', 'lhe', 'constou', 'a', 'morte', 'do', 'velho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A escrava passara \n",
      "naturalmente em herança a qualquer dos filhos do morto; mas, por estes, nada havia que recear: dois pândegos de marca \n",
      "maior que, empolgada a legitima, cuidariam de tudo, menos de atirar-se na pista de uma crioula a quem não viam de \n",
      "muitos anos àquela parte'\n",
      "Tokens gerados: ['a', 'escrava', 'passara', 'naturalmente', 'em', 'heranca', 'a', 'qualquer', 'dos', 'filhos', 'do', 'morto', 'mas', ',', 'por', 'estes', ',', 'nada', 'havia', 'que', 'recear', 'dois', 'pandegos', 'de', 'marca', 'maior', 'que', ',', 'empolgada', 'a', 'legitima', ',', 'cuidariam', 'de', 'tudo', ',', 'menos', 'de', 'atirar-se', 'na', 'pista', 'de', 'uma', 'crioula', 'a', 'quem', 'nao', 'viam', 'de', 'muitos', 'anos', 'aquela', 'parte']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Ora! bastava já, e não era pouco, o que lhe tinham sugado durante tanto tempo!” \n",
      "Bertoleza representava agora ao lado de João Romão o papel tríplice de caixeiro, de criada e de amante'\n",
      "Tokens gerados: ['“', 'ora', 'bastava', 'ja', ',', 'e', 'nao', 'era', 'pouco', ',', 'o', 'que', 'lhe', 'tinham', 'sugado', 'durante', 'tanto', 'tempo', '”', 'bertoleza', 'representava', 'agora', 'ao', 'lado', 'de', 'joao', 'romao', 'o', 'papel', 'triplice', 'de', 'caixeiro', ',', 'de', 'criada', 'e', 'de', 'amante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mourejava a valer, mas de cara alegre; às quatro da madrugada estava já na faina de todos os dias, aviando o café para \n",
      "os fregueses e depois preparando o almoço para os trabalhadores de uma pedreira que havia para além de um grande \n",
      "capinzal aos fundos da venda'\n",
      "Tokens gerados: ['mourejava', 'a', 'valer', ',', 'mas', 'de', 'cara', 'alegre', 'as', 'quatro', 'da', 'madrugada', 'estava', 'ja', 'na', 'faina', 'de', 'todos', 'os', 'dias', ',', 'aviando', 'o', 'cafe', 'para', 'os', 'fregueses', 'e', 'depois', 'preparando', 'o', 'almoco', 'para', 'os', 'trabalhadores', 'de', 'uma', 'pedreira', 'que', 'havia', 'para', 'alem', 'de', 'um', 'grande', 'capinzal', 'aos', 'fundos', 'da', 'venda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Varria a casa, cozinhava, vendia ao balcão na taverna, quando o amigo andava ocupado \n",
      "lá por fora; fazia a sua quitanda durante o dia no intervalo de outros serviços, e à noite passava-se para a porta da venda, \n",
      "e, defronte de um fogareiro de barro, fritava fígado e frigia sardinhas, que Romão ia pela manhã, em mangas de camisa, \n",
      "de tamancos e sem meias, comprar à praia do Peixe'\n",
      "Tokens gerados: ['varria', 'a', 'casa', ',', 'cozinhava', ',', 'vendia', 'ao', 'balcao', 'na', 'taverna', ',', 'quando', 'o', 'amigo', 'andava', 'ocupado', 'la', 'por', 'fora', 'fazia', 'a', 'sua', 'quitanda', 'durante', 'o', 'dia', 'no', 'intervalo', 'de', 'outros', 'servicos', ',', 'e', 'a', 'noite', 'passava-se', 'para', 'a', 'porta', 'da', 'venda', ',', 'e', ',', 'defronte', 'de', 'um', 'fogareiro', 'de', 'barro', ',', 'fritava', 'figado', 'e', 'frigia', 'sardinhas', ',', 'que', 'romao', 'ia', 'pela', 'manha', ',', 'em', 'mangas', 'de', 'camisa', ',', 'de', 'tamancos', 'e', 'sem', 'meias', ',', 'comprar', 'a', 'praia', 'do', 'peixe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o demônio da mulher ainda encontrava tempo para lavar e \n",
      "consertar, além da sua, a roupa do seu homem, que esta, valha a verdade, não era tanta e nunca passava em todo o mês \n",
      "de alguns pares de calças de zuarte e outras tantas camisas de riscado'\n",
      "Tokens gerados: ['e', 'o', 'demonio', 'da', 'mulher', 'ainda', 'encontrava', 'tempo', 'para', 'lavar', 'e', 'consertar', ',', 'alem', 'da', 'sua', ',', 'a', 'roupa', 'do', 'seu', 'homem', ',', 'que', 'esta', ',', 'valha', 'a', 'verdade', ',', 'nao', 'era', 'tanta', 'e', 'nunca', 'passava', 'em', 'todo', 'o', 'mes', 'de', 'alguns', 'pares', 'de', 'calcas', 'de', 'zuarte', 'e', 'outras', 'tantas', 'camisas', 'de', 'riscado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão não saia nunca a passeio, nem ia à missa aos domingos; tudo que rendia a sua venda e mais a \n",
      "quitanda seguia direitinho para a caixa econômica e daí então para o banco'\n",
      "Tokens gerados: ['joao', 'romao', 'nao', 'saia', 'nunca', 'a', 'passeio', ',', 'nem', 'ia', 'a', 'missa', 'aos', 'domingos', 'tudo', 'que', 'rendia', 'a', 'sua', 'venda', 'e', 'mais', 'a', 'quitanda', 'seguia', 'direitinho', 'para', 'a', 'caixa', 'economica', 'e', 'dai', 'entao', 'para', 'o', 'banco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tanto assim que, um ano depois da \n",
      "aquisição da crioula, indo em hasta pública algumas braças de terra situadas ao fundo da taverna, arrematou-as logo e \n",
      "tratou, sem perda de tempo, de construir três casinhas de porta e janela'\n",
      "Tokens gerados: ['tanto', 'assim', 'que', ',', 'um', 'ano', 'depois', 'da', 'aquisicao', 'da', 'crioula', ',', 'indo', 'em', 'hasta', 'publica', 'algumas', 'bracas', 'de', 'terra', 'situadas', 'ao', 'fundo', 'da', 'taverna', ',', 'arrematou-as', 'logo', 'e', 'tratou', ',', 'sem', 'perda', 'de', 'tempo', ',', 'de', 'construir', 'tres', 'casinhas', 'de', 'porta', 'e', 'janela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que milagres de esperteza e de economia não realizou ele nessa construção! Servia de pedreiro, amassava e \n",
      "carregava barro, quebrava pedra; pedra, que o velhaco, fora de horas, junto com a amiga, furtavam à pedreira do fundo, \n",
      "da mesma forma que subtraiam o material das casas em obra que havia por ali perto'\n",
      "Tokens gerados: ['que', 'milagres', 'de', 'esperteza', 'e', 'de', 'economia', 'nao', 'realizou', 'ele', 'nessa', 'construcao', 'servia', 'de', 'pedreiro', ',', 'amassava', 'e', 'carregava', 'barro', ',', 'quebrava', 'pedra', 'pedra', ',', 'que', 'o', 'velhaco', ',', 'fora', 'de', 'horas', ',', 'junto', 'com', 'a', 'amiga', ',', 'furtavam', 'a', 'pedreira', 'do', 'fundo', ',', 'da', 'mesma', 'forma', 'que', 'subtraiam', 'o', 'material', 'das', 'casas', 'em', 'obra', 'que', 'havia', 'por', 'ali', 'perto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estes furtos eram feitos com todas as cautelas e sempre coroados do melhor sucesso, graças à circunstância de que \n",
      "nesse tempo a polícia não se mostrava muito por aquelas alturas'\n",
      "Tokens gerados: ['estes', 'furtos', 'eram', 'feitos', 'com', 'todas', 'as', 'cautelas', 'e', 'sempre', 'coroados', 'do', 'melhor', 'sucesso', ',', 'gracas', 'a', 'circunstancia', 'de', 'que', 'nesse', 'tempo', 'a', 'policia', 'nao', 'se', 'mostrava', 'muito', 'por', 'aquelas', 'alturas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão observava durante o dia quais as obras em \n",
      "que ficava material para o dia seguinte, e à noite lá estava ele rente, mais a Bertoleza, a removerem tábuas, tijolos, \n",
      "telhas, sacos de cal, para o meio da rua, com tamanha habilidade que se não ouvia vislumbre de rumor'\n",
      "Tokens gerados: ['joao', 'romao', 'observava', 'durante', 'o', 'dia', 'quais', 'as', 'obras', 'em', 'que', 'ficava', 'material', 'para', 'o', 'dia', 'seguinte', ',', 'e', 'a', 'noite', 'la', 'estava', 'ele', 'rente', ',', 'mais', 'a', 'bertoleza', ',', 'a', 'removerem', 'tabuas', ',', 'tijolos', ',', 'telhas', ',', 'sacos', 'de', 'cal', ',', 'para', 'o', 'meio', 'da', 'rua', ',', 'com', 'tamanha', 'habilidade', 'que', 'se', 'nao', 'ouvia', 'vislumbre', 'de', 'rumor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois, um \n",
      "tomava uma carga e partia para casa, enquanto o outro ficava de alcatéia ao lado do resto, pronto a dar sinal, em caso de \n",
      "perigo; e, quando o que tinha ido voltava, seguia então o companheiro, carregado por sua vez'\n",
      "Tokens gerados: ['depois', ',', 'um', 'tomava', 'uma', 'carga', 'e', 'partia', 'para', 'casa', ',', 'enquanto', 'o', 'outro', 'ficava', 'de', 'alcateia', 'ao', 'lado', 'do', 'resto', ',', 'pronto', 'a', 'dar', 'sinal', ',', 'em', 'caso', 'de', 'perigo', 'e', ',', 'quando', 'o', 'que', 'tinha', 'ido', 'voltava', ',', 'seguia', 'entao', 'o', 'companheiro', ',', 'carregado', 'por', 'sua', 'vez']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nada lhes escapava, nem mesmo as escadas dos pedreiros, os cavalos de pau, o banco ou a ferramenta dos \n",
      "marceneiros'\n",
      "Tokens gerados: ['nada', 'lhes', 'escapava', ',', 'nem', 'mesmo', 'as', 'escadas', 'dos', 'pedreiros', ',', 'os', 'cavalos', 'de', 'pau', ',', 'o', 'banco', 'ou', 'a', 'ferramenta', 'dos', 'marceneiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o fato é que aquelas três casinhas, tão engenhosamente construídas, foram o ponto de partida do grande cortiço \n",
      "de São Romão'\n",
      "Tokens gerados: ['e', 'o', 'fato', 'e', 'que', 'aquelas', 'tres', 'casinhas', ',', 'tao', 'engenhosamente', 'construidas', ',', 'foram', 'o', 'ponto', 'de', 'partida', 'do', 'grande', 'cortico', 'de', 'sao', 'romao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Hoje quatro braças de terra, amanhã seis, depois mais outras, ia o vendeiro conquistando todo o terreno que se \n",
      "estendia pelos fundos da sua bodega; e, à proporção que o conquistava, reproduziam-se os quartos e o número de \n",
      "moradores'\n",
      "Tokens gerados: ['hoje', 'quatro', 'bracas', 'de', 'terra', ',', 'amanha', 'seis', ',', 'depois', 'mais', 'outras', ',', 'ia', 'o', 'vendeiro', 'conquistando', 'todo', 'o', 'terreno', 'que', 'se', 'estendia', 'pelos', 'fundos', 'da', 'sua', 'bodega', 'e', ',', 'a', 'proporcao', 'que', 'o', 'conquistava', ',', 'reproduziam-se', 'os', 'quartos', 'e', 'o', 'numero', 'de', 'moradores']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sempre em mangas de camisa, sem domingo nem dia santo, não perdendo nunca a ocasião de assenhorear-se do \n",
      "alheio, deixando de pagar todas as vezes que podia e nunca deixando de receber, enganando os fregueses, roubando nos \n",
      "pesos e nas medidas, comprando por dez réis de mel coado o que os escravos furtavam da casa dos seus senhores, \n",
      "apertando cada vez mais as próprias despesas, empilhando privações sobre privações, trabalhando e mais a amiga como \n",
      "uma junta de bois, João Romão veio afinal a comprar uma boa parte da bela pedreira, que ele, todos os dias, ao cair da \n",
      "tarde, assentado um instante à porta da venda, contemplava de longe com um resignado olhar de cobiça'\n",
      "Tokens gerados: ['sempre', 'em', 'mangas', 'de', 'camisa', ',', 'sem', 'domingo', 'nem', 'dia', 'santo', ',', 'nao', 'perdendo', 'nunca', 'a', 'ocasiao', 'de', 'assenhorear-se', 'do', 'alheio', ',', 'deixando', 'de', 'pagar', 'todas', 'as', 'vezes', 'que', 'podia', 'e', 'nunca', 'deixando', 'de', 'receber', ',', 'enganando', 'os', 'fregueses', ',', 'roubando', 'nos', 'pesos', 'e', 'nas', 'medidas', ',', 'comprando', 'por', 'dez', 'reis', 'de', 'mel', 'coado', 'o', 'que', 'os', 'escravos', 'furtavam', 'da', 'casa', 'dos', 'seus', 'senhores', ',', 'apertando', 'cada', 'vez', 'mais', 'as', 'proprias', 'despesas', ',', 'empilhando', 'privacoes', 'sobre', 'privacoes', ',', 'trabalhando', 'e', 'mais', 'a', 'amiga', 'como', 'uma', 'junta', 'de', 'bois', ',', 'joao', 'romao', 'veio', 'afinal', 'a', 'comprar', 'uma', 'boa', 'parte', 'da', 'bela', 'pedreira', ',', 'que', 'ele', ',', 'todos', 'os', 'dias', ',', 'ao', 'cair', 'da', 'tarde', ',', 'assentado', 'um', 'instante', 'a', 'porta', 'da', 'venda', ',', 'contemplava', 'de', 'longe', 'com', 'um', 'resignado', 'olhar', 'de', 'cobica']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pôs lá seis homens a quebrarem pedra e outros seis a fazerem lajedos e paralelepípedos, e então principiou a \n",
      "ganhar em grosso, tão em grosso que, dentro de ano e meio, arrematava já todo o espaço compreendido entre as suas \n",
      "casinhas e a pedreira, isto é, umas oitenta braças de fundo sobre vinte de frente em plano enxuto e magnífico para \n",
      "construir'\n",
      "Tokens gerados: ['pos', 'la', 'seis', 'homens', 'a', 'quebrarem', 'pedra', 'e', 'outros', 'seis', 'a', 'fazerem', 'lajedos', 'e', 'paralelepipedos', ',', 'e', 'entao', 'principiou', 'a', 'ganhar', 'em', 'grosso', ',', 'tao', 'em', 'grosso', 'que', ',', 'dentro', 'de', 'ano', 'e', 'meio', ',', 'arrematava', 'ja', 'todo', 'o', 'espaco', 'compreendido', 'entre', 'as', 'suas', 'casinhas', 'e', 'a', 'pedreira', ',', 'isto', 'e', ',', 'umas', 'oitenta', 'bracas', 'de', 'fundo', 'sobre', 'vinte', 'de', 'frente', 'em', 'plano', 'enxuto', 'e', 'magnifico', 'para', 'construir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Justamente por essa ocasião vendeu-se também um sobrado que ficava à direita da venda, separado desta apenas \n",
      "por aquelas vinte braças; de sorte que todo o flanco esquerdo do prédio, coisa de uns vinte e tantos metros, despejava \n",
      "para o terreno do vendeiro as suas nove janelas de peitoril'\n",
      "Tokens gerados: ['justamente', 'por', 'essa', 'ocasiao', 'vendeu-se', 'tambem', 'um', 'sobrado', 'que', 'ficava', 'a', 'direita', 'da', 'venda', ',', 'separado', 'desta', 'apenas', 'por', 'aquelas', 'vinte', 'bracas', 'de', 'sorte', 'que', 'todo', 'o', 'flanco', 'esquerdo', 'do', 'predio', ',', 'coisa', 'de', 'uns', 'vinte', 'e', 'tantos', 'metros', ',', 'despejava', 'para', 'o', 'terreno', 'do', 'vendeiro', 'as', 'suas', 'nove', 'janelas', 'de', 'peitoril']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Comprou-o um tal Miranda, negociante português, \n",
      "estabelecido na Rua do Hospício com uma loja de fazendas por atacado'\n",
      "Tokens gerados: ['comprou-o', 'um', 'tal', 'miranda', ',', 'negociante', 'portugues', ',', 'estabelecido', 'na', 'rua', 'do', 'hospicio', 'com', 'uma', 'loja', 'de', 'fazendas', 'por', 'atacado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Corrida uma limpeza geral no casarão, \n",
      "mudar-se-ia ele para lá com a família, pois que a mulher, Dona Estela, senhora pretensiosa e com fumaças de nobreza, \n",
      "já não podia suportar a residência no centro da cidade, como também sua menina, a Zulmirinha, crescia muito pálida e \n",
      "precisava de largueza para enrijar e tomar corpo'\n",
      "Tokens gerados: ['corrida', 'uma', 'limpeza', 'geral', 'no', 'casarao', ',', 'mudar-se-ia', 'ele', 'para', 'la', 'com', 'a', 'familia', ',', 'pois', 'que', 'a', 'mulher', ',', 'dona', 'estela', ',', 'senhora', 'pretensiosa', 'e', 'com', 'fumacas', 'de', 'nobreza', ',', 'ja', 'nao', 'podia', 'suportar', 'a', 'residencia', 'no', 'centro', 'da', 'cidade', ',', 'como', 'tambem', 'sua', 'menina', ',', 'a', 'zulmirinha', ',', 'crescia', 'muito', 'palida', 'e', 'precisava', 'de', 'largueza', 'para', 'enrijar', 'e', 'tomar', 'corpo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Isto foi o que disse o Miranda aos colegas, porém a verdadeira causa da mudança estava na necessidade, que ele \n",
      "reconhecia urgente, de afastar Dona Estela do alcance dos seus caixeiros'\n",
      "Tokens gerados: ['isto', 'foi', 'o', 'que', 'disse', 'o', 'miranda', 'aos', 'colegas', ',', 'porem', 'a', 'verdadeira', 'causa', 'da', 'mudanca', 'estava', 'na', 'necessidade', ',', 'que', 'ele', 'reconhecia', 'urgente', ',', 'de', 'afastar', 'dona', 'estela', 'do', 'alcance', 'dos', 'seus', 'caixeiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dona Estela era uma mulherzinha levada da \n",
      "breca: achava-se casada havia treze anos e durante esse tempo dera ao marido toda sorte de desgostos'\n",
      "Tokens gerados: ['dona', 'estela', 'era', 'uma', 'mulherzinha', 'levada', 'da', 'breca', 'achava-se', 'casada', 'havia', 'treze', 'anos', 'e', 'durante', 'esse', 'tempo', 'dera', 'ao', 'marido', 'toda', 'sorte', 'de', 'desgostos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ainda antes de \n",
      "terminar o segundo ano de matrimônio, o Miranda pilhou-a em flagrante delito de adultério; ficou furioso e o seu \n",
      "primeiro impulso foi de mandá-la para o diabo junto com o cúmplice; mas a sua casa comercial garantia-se com o dote \n",
      "que ela trouxera, uns oitenta contos em prédios e ações da divida publica, de que se utilizava o desgraçado tanto quanto \n",
      "lhe permitia o regime dotal'\n",
      "Tokens gerados: ['ainda', 'antes', 'de', 'terminar', 'o', 'segundo', 'ano', 'de', 'matrimonio', ',', 'o', 'miranda', 'pilhou-a', 'em', 'flagrante', 'delito', 'de', 'adulterio', 'ficou', 'furioso', 'e', 'o', 'seu', 'primeiro', 'impulso', 'foi', 'de', 'manda-la', 'para', 'o', 'diabo', 'junto', 'com', 'o', 'cumplice', 'mas', 'a', 'sua', 'casa', 'comercial', 'garantia-se', 'com', 'o', 'dote', 'que', 'ela', 'trouxera', ',', 'uns', 'oitenta', 'contos', 'em', 'predios', 'e', 'acoes', 'da', 'divida', 'publica', ',', 'de', 'que', 'se', 'utilizava', 'o', 'desgracado', 'tanto', 'quanto', 'lhe', 'permitia', 'o', 'regime', 'dotal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Além de que, um rompimento brusco seria obra para escândalo, e, segundo a sua opinião, \n",
      "qualquer escândalo doméstico ficava muito mal a um negociante de certa ordem'\n",
      "Tokens gerados: ['alem', 'de', 'que', ',', 'um', 'rompimento', 'brusco', 'seria', 'obra', 'para', 'escandalo', ',', 'e', ',', 'segundo', 'a', 'sua', 'opiniao', ',', 'qualquer', 'escandalo', 'domestico', 'ficava', 'muito', 'mal', 'a', 'um', 'negociante', 'de', 'certa', 'ordem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prezava, acima de tudo, a sua posição \n",
      "social e tremia só com a idéia de ver-se novamente pobre, sem recursos e sem coragem para recomeçar a vida, depois de \n",
      "se haver habituado a umas tantas regalias e afeito à hombridade de português rico que já não tem pátria na Europa'\n",
      "Tokens gerados: ['prezava', ',', 'acima', 'de', 'tudo', ',', 'a', 'sua', 'posicao', 'social', 'e', 'tremia', 'so', 'com', 'a', 'ideia', 'de', 'ver-se', 'novamente', 'pobre', ',', 'sem', 'recursos', 'e', 'sem', 'coragem', 'para', 'recomecar', 'a', 'vida', ',', 'depois', 'de', 'se', 'haver', 'habituado', 'a', 'umas', 'tantas', 'regalias', 'e', 'afeito', 'a', 'hombridade', 'de', 'portugues', 'rico', 'que', 'ja', 'nao', 'tem', 'patria', 'na', 'europa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Acovardado defronte destes raciocínios, contentou-se com uma simples separação de leitos, e os dois passaram a \n",
      "dormir em quartos separados'\n",
      "Tokens gerados: ['acovardado', 'defronte', 'destes', 'raciocinios', ',', 'contentou-se', 'com', 'uma', 'simples', 'separacao', 'de', 'leitos', ',', 'e', 'os', 'dois', 'passaram', 'a', 'dormir', 'em', 'quartos', 'separados']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não comiam juntos, e mal trocavam entre si uma ou outra palavra constrangida, quando \n",
      "qualquer inesperado acaso os reunia a contragosto'\n",
      "Tokens gerados: ['nao', 'comiam', 'juntos', ',', 'e', 'mal', 'trocavam', 'entre', 'si', 'uma', 'ou', 'outra', 'palavra', 'constrangida', ',', 'quando', 'qualquer', 'inesperado', 'acaso', 'os', 'reunia', 'a', 'contragosto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Odiavam-se'\n",
      "Tokens gerados: ['odiavam-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cada qual sentia pelo outro um profundo desprezo, que pouco a pouco se foi transformando em \n",
      "repugnância completa'\n",
      "Tokens gerados: ['cada', 'qual', 'sentia', 'pelo', 'outro', 'um', 'profundo', 'desprezo', ',', 'que', 'pouco', 'a', 'pouco', 'se', 'foi', 'transformando', 'em', 'repugnancia', 'completa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O nascimento de Zulmira veio agravar ainda mais a situação; a pobre criança, em vez de servir \n",
      "de elo aos dois infelizes, foi antes um novo isolador que se estabeleceu entre eles'\n",
      "Tokens gerados: ['o', 'nascimento', 'de', 'zulmira', 'veio', 'agravar', 'ainda', 'mais', 'a', 'situacao', 'a', 'pobre', 'crianca', ',', 'em', 'vez', 'de', 'servir', 'de', 'elo', 'aos', 'dois', 'infelizes', ',', 'foi', 'antes', 'um', 'novo', 'isolador', 'que', 'se', 'estabeleceu', 'entre', 'eles']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estela amava-a menos do que lhe \n",
      "pedia o instinto materno por supô-la filha do marido, e este a detestava porque tinha convicção de não ser seu pai'\n",
      "Tokens gerados: ['estela', 'amava-a', 'menos', 'do', 'que', 'lhe', 'pedia', 'o', 'instinto', 'materno', 'por', 'supo-la', 'filha', 'do', 'marido', ',', 'e', 'este', 'a', 'detestava', 'porque', 'tinha', 'conviccao', 'de', 'nao', 'ser', 'seu', 'pai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma bela noite, porém, o Miranda, que era homem de sangue esperto e orçava então pelos seus trinta e cinco \n",
      "anos, sentiu-se em insuportável estado de lubricidade'\n",
      "Tokens gerados: ['uma', 'bela', 'noite', ',', 'porem', ',', 'o', 'miranda', ',', 'que', 'era', 'homem', 'de', 'sangue', 'esperto', 'e', 'orcava', 'entao', 'pelos', 'seus', 'trinta', 'e', 'cinco', 'anos', ',', 'sentiu-se', 'em', 'insuportavel', 'estado', 'de', 'lubricidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era tarde já e não havia em casa alguma criada que lhe pudesse \n",
      "valer'\n",
      "Tokens gerados: ['era', 'tarde', 'ja', 'e', 'nao', 'havia', 'em', 'casa', 'alguma', 'criada', 'que', 'lhe', 'pudesse', 'valer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Lembrou-se da mulher, mas repeliu logo esta idéia com escrupulosa repugnância'\n",
      "Tokens gerados: ['lembrou-se', 'da', 'mulher', ',', 'mas', 'repeliu', 'logo', 'esta', 'ideia', 'com', 'escrupulosa', 'repugnancia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Continuava a odiá-la'\n",
      "Tokens gerados: ['continuava', 'a', 'odia-la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto este mesmo fato de obrigação em que ele se colocou de não servir-se dela, a responsabilidade de desprezá-la, \n",
      "como que ainda mais lhe assanhava o desejo da carne, fazendo da esposa infiel um fruto proibido'\n",
      "Tokens gerados: ['entretanto', 'este', 'mesmo', 'fato', 'de', 'obrigacao', 'em', 'que', 'ele', 'se', 'colocou', 'de', 'nao', 'servir-se', 'dela', ',', 'a', 'responsabilidade', 'de', 'despreza-la', ',', 'como', 'que', 'ainda', 'mais', 'lhe', 'assanhava', 'o', 'desejo', 'da', 'carne', ',', 'fazendo', 'da', 'esposa', 'infiel', 'um', 'fruto', 'proibido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Afinal, coisa \n",
      "singular, posto que moralmente nada diminuísse a sua repugnância pela perjura, foi ter ao quarto dela'\n",
      "Tokens gerados: ['afinal', ',', 'coisa', 'singular', ',', 'posto', 'que', 'moralmente', 'nada', 'diminuisse', 'a', 'sua', 'repugnancia', 'pela', 'perjura', ',', 'foi', 'ter', 'ao', 'quarto', 'dela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mulher dormia a sono solto'\n",
      "Tokens gerados: ['a', 'mulher', 'dormia', 'a', 'sono', 'solto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Miranda entrou pé ante pé e aproximou-se da cama'\n",
      "Tokens gerados: ['miranda', 'entrou', 'pe', 'ante', 'pe', 'e', 'aproximou-se', 'da', 'cama']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '“Devia voltar!'\n",
      "Tokens gerados: ['“', 'devia', 'voltar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'pensou'\n",
      "Tokens gerados: ['pensou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não \n",
      "lhe ficava bem aquilo!'\n",
      "Tokens gerados: ['nao', 'lhe', 'ficava', 'bem', 'aquilo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” Mas o sangue latejava-lhe, reclamando-a'\n",
      "Tokens gerados: ['”', 'mas', 'o', 'sangue', 'latejava-lhe', ',', 'reclamando-a']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ainda hesitou um instante, imóvel, a contemplá-la \n",
      "no seu desejo'\n",
      "Tokens gerados: ['ainda', 'hesitou', 'um', 'instante', ',', 'imovel', ',', 'a', 'contempla-la', 'no', 'seu', 'desejo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estela, como se o olhar do marido lhe apalpasse o corpo, torceu-se sobre o quadril da esquerda, repuxando com as \n",
      "coxas o lençol para a frente e patenteando uma nesga de nudez estofada e branca'\n",
      "Tokens gerados: ['estela', ',', 'como', 'se', 'o', 'olhar', 'do', 'marido', 'lhe', 'apalpasse', 'o', 'corpo', ',', 'torceu-se', 'sobre', 'o', 'quadril', 'da', 'esquerda', ',', 'repuxando', 'com', 'as', 'coxas', 'o', 'lencol', 'para', 'a', 'frente', 'e', 'patenteando', 'uma', 'nesga', 'de', 'nudez', 'estofada', 'e', 'branca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Miranda não pôde resistir, atirou-se \n",
      "contra ela, que, num pequeno sobressalto, mais de surpresa que de revolta, desviou-se, tornando logo e enfrentando com \n",
      "o marido'\n",
      "Tokens gerados: ['o', 'miranda', 'nao', 'pode', 'resistir', ',', 'atirou-se', 'contra', 'ela', ',', 'que', ',', 'num', 'pequeno', 'sobressalto', ',', 'mais', 'de', 'surpresa', 'que', 'de', 'revolta', ',', 'desviou-se', ',', 'tornando', 'logo', 'e', 'enfrentando', 'com', 'o', 'marido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E deixou-se empolgar pelos rins, de olhos fechados, fingindo que continuava a dormir, sem a menor \n",
      "consciência de tudo aquilo'\n",
      "Tokens gerados: ['e', 'deixou-se', 'empolgar', 'pelos', 'rins', ',', 'de', 'olhos', 'fechados', ',', 'fingindo', 'que', 'continuava', 'a', 'dormir', ',', 'sem', 'a', 'menor', 'consciencia', 'de', 'tudo', 'aquilo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah! ela contava como certo que o esposo, desde que não teve coragem de separar-se de casa, havia, mais cedo ou \n",
      "mais tarde, de procurá-la de novo'\n",
      "Tokens gerados: ['ah', 'ela', 'contava', 'como', 'certo', 'que', 'o', 'esposo', ',', 'desde', 'que', 'nao', 'teve', 'coragem', 'de', 'separar-se', 'de', 'casa', ',', 'havia', ',', 'mais', 'cedo', 'ou', 'mais', 'tarde', ',', 'de', 'procura-la', 'de', 'novo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Conhecia-lhe o temperamento, forte para desejar e fraco para resistir ao desejo'\n",
      "Tokens gerados: ['conhecia-lhe', 'o', 'temperamento', ',', 'forte', 'para', 'desejar', 'e', 'fraco', 'para', 'resistir', 'ao', 'desejo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Consumado o delito, o honrado negociante sentiu-se tolhido de vergonha e arrependimento'\n",
      "Tokens gerados: ['consumado', 'o', 'delito', ',', 'o', 'honrado', 'negociante', 'sentiu-se', 'tolhido', 'de', 'vergonha', 'e', 'arrependimento']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não teve animo de \n",
      "dar palavra, e retirou-se tristonho e murcho para o seu quarto de desquitado'\n",
      "Tokens gerados: ['nao', 'teve', 'animo', 'de', 'dar', 'palavra', ',', 'e', 'retirou-se', 'tristonho', 'e', 'murcho', 'para', 'o', 'seu', 'quarto', 'de', 'desquitado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Oh! como lhe doía agora o que acabava de praticar na cegueira da sua sensualidade'\n",
      "Tokens gerados: ['oh', 'como', 'lhe', 'doia', 'agora', 'o', 'que', 'acabava', 'de', 'praticar', 'na', 'cegueira', 'da', 'sua', 'sensualidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que cabeçada!'\n",
      "Tokens gerados: ['—', 'que', 'cabecada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'dizia ele agitado'\n",
      "Tokens gerados: ['dizia', 'ele', 'agitado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Que formidável cabeçada!'\n",
      "Tokens gerados: ['que', 'formidavel', 'cabecada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No dia seguinte, os dois viram-se e evitaram-se em silêncio, como se nada de extraordinário houvera entre eles \n",
      "acontecido na véspera'\n",
      "Tokens gerados: ['no', 'dia', 'seguinte', ',', 'os', 'dois', 'viram-se', 'e', 'evitaram-se', 'em', 'silencio', ',', 'como', 'se', 'nada', 'de', 'extraordinario', 'houvera', 'entre', 'eles', 'acontecido', 'na', 'vespera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dir-se-ia até que, depois daquela ocorrência, o Miranda sentia crescer o seu ódio contra a \n",
      "esposa'\n",
      "Tokens gerados: ['dir-se-ia', 'ate', 'que', ',', 'depois', 'daquela', 'ocorrencia', ',', 'o', 'miranda', 'sentia', 'crescer', 'o', 'seu', 'odio', 'contra', 'a', 'esposa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, à noite desse mesmo dia, quando se achou sozinho na sua cama estreita, jurou mil vezes aos seus brios nunca \n",
      "mais, nunca mais, praticar semelhante loucura'\n",
      "Tokens gerados: ['e', ',', 'a', 'noite', 'desse', 'mesmo', 'dia', ',', 'quando', 'se', 'achou', 'sozinho', 'na', 'sua', 'cama', 'estreita', ',', 'jurou', 'mil', 'vezes', 'aos', 'seus', 'brios', 'nunca', 'mais', ',', 'nunca', 'mais', ',', 'praticar', 'semelhante', 'loucura']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, daí a um mês, o pobre homem, acometido de um novo acesso de luxúria, voltou ao quarto da mulher'\n",
      "Tokens gerados: ['mas', ',', 'dai', 'a', 'um', 'mes', ',', 'o', 'pobre', 'homem', ',', 'acometido', 'de', 'um', 'novo', 'acesso', 'de', 'luxuria', ',', 'voltou', 'ao', 'quarto', 'da', 'mulher']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estela recebeu-o desta vez como da primeira, fingindo que não acordava; na ocasião, porém, em que ele se \n",
      "apoderava dela febrilmente, a leviana, sem se poder conter, soltou-lhe em cheio contra o rosto uma gargalhada que a \n",
      "custo sopeava'\n",
      "Tokens gerados: ['estela', 'recebeu-o', 'desta', 'vez', 'como', 'da', 'primeira', ',', 'fingindo', 'que', 'nao', 'acordava', 'na', 'ocasiao', ',', 'porem', ',', 'em', 'que', 'ele', 'se', 'apoderava', 'dela', 'febrilmente', ',', 'a', 'leviana', ',', 'sem', 'se', 'poder', 'conter', ',', 'soltou-lhe', 'em', 'cheio', 'contra', 'o', 'rosto', 'uma', 'gargalhada', 'que', 'a', 'custo', 'sopeava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O pobre-diabo desnorteou, deveras escandalizado, soerguendo-se, brusco, num estremunhamento de \n",
      "sonâmbulo acordado com violência'\n",
      "Tokens gerados: ['o', 'pobre-diabo', 'desnorteou', ',', 'deveras', 'escandalizado', ',', 'soerguendo-se', ',', 'brusco', ',', 'num', 'estremunhamento', 'de', 'sonambulo', 'acordado', 'com', 'violencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mulher percebeu a situação e não lhe deu tempo para fugir; passou-lhe rápido as pernas por cima e, \n",
      "grudando-se-lhe ao corpo, cegou-o com uma metralhada de beijos'\n",
      "Tokens gerados: ['a', 'mulher', 'percebeu', 'a', 'situacao', 'e', 'nao', 'lhe', 'deu', 'tempo', 'para', 'fugir', 'passou-lhe', 'rapido', 'as', 'pernas', 'por', 'cima', 'e', ',', 'grudando-se-lhe', 'ao', 'corpo', ',', 'cegou-o', 'com', 'uma', 'metralhada', 'de', 'beijos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não se falaram'\n",
      "Tokens gerados: ['nao', 'se', 'falaram']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Miranda nunca a tivera, nem nunca a vira, assim tão violenta no prazer'\n",
      "Tokens gerados: ['miranda', 'nunca', 'a', 'tivera', ',', 'nem', 'nunca', 'a', 'vira', ',', 'assim', 'tao', 'violenta', 'no', 'prazer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estranhou-a'\n",
      "Tokens gerados: ['estranhou-a']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Afigurou-se-lhe estar nos \n",
      "braços de uma amante apaixonada: descobriu nela o capitoso encanto com que nos embebedam as cortesãs amestradas \n",
      "na ciência do gozo venéreo'\n",
      "Tokens gerados: ['afigurou-se-lhe', 'estar', 'nos', 'bracos', 'de', 'uma', 'amante', 'apaixonada', 'descobriu', 'nela', 'o', 'capitoso', 'encanto', 'com', 'que', 'nos', 'embebedam', 'as', 'cortesas', 'amestradas', 'na', 'ciencia', 'do', 'gozo', 'venereo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Descobriu-lhe no cheiro da pele e no cheiro dos cabelos perfumes que nunca lhe sentira; \n",
      "notou-lhe outro hálito, outro som nos gemidos e nos suspiros'\n",
      "Tokens gerados: ['descobriu-lhe', 'no', 'cheiro', 'da', 'pele', 'e', 'no', 'cheiro', 'dos', 'cabelos', 'perfumes', 'que', 'nunca', 'lhe', 'sentira', 'notou-lhe', 'outro', 'halito', ',', 'outro', 'som', 'nos', 'gemidos', 'e', 'nos', 'suspiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E gozou-a, gozou-a loucamente, com delírio, com \n",
      "verdadeira satisfação de animal no cio'\n",
      "Tokens gerados: ['e', 'gozou-a', ',', 'gozou-a', 'loucamente', ',', 'com', 'delirio', ',', 'com', 'verdadeira', 'satisfacao', 'de', 'animal', 'no', 'cio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E ela também, ela também gozou, estimulada por aquela circunstância picante do ressentimento que os desunia; \n",
      "gozou a desonestidade daquele ato que a ambos acanalhava aos olhos um do outro; estorceu-se toda, rangendo os \n",
      "dentes, grunhindo, debaixo daquele seu inimigo odiado, achando-o também agora, como homem, melhor que nunca, \n",
      "sufocando-o nos seus braços nus, metendo-lhe pela boca a língua úmida e em brasa'\n",
      "Tokens gerados: ['e', 'ela', 'tambem', ',', 'ela', 'tambem', 'gozou', ',', 'estimulada', 'por', 'aquela', 'circunstancia', 'picante', 'do', 'ressentimento', 'que', 'os', 'desunia', 'gozou', 'a', 'desonestidade', 'daquele', 'ato', 'que', 'a', 'ambos', 'acanalhava', 'aos', 'olhos', 'um', 'do', 'outro', 'estorceu-se', 'toda', ',', 'rangendo', 'os', 'dentes', ',', 'grunhindo', ',', 'debaixo', 'daquele', 'seu', 'inimigo', 'odiado', ',', 'achando-o', 'tambem', 'agora', ',', 'como', 'homem', ',', 'melhor', 'que', 'nunca', ',', 'sufocando-o', 'nos', 'seus', 'bracos', 'nus', ',', 'metendo-lhe', 'pela', 'boca', 'a', 'lingua', 'umida', 'e', 'em', 'brasa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois, um arranco de corpo \n",
      "inteiro, com um soluço gutural e estrangulado, arquejante e convulsa, estatelou-se num abandono de pernas e braços \n",
      "abertos, a cabeça para o lado, os olhos moribundos e chorosos, toda ela agonizante, como se a tivessem crucificado na \n",
      "cama'\n",
      "Tokens gerados: ['depois', ',', 'um', 'arranco', 'de', 'corpo', 'inteiro', ',', 'com', 'um', 'soluco', 'gutural', 'e', 'estrangulado', ',', 'arquejante', 'e', 'convulsa', ',', 'estatelou-se', 'num', 'abandono', 'de', 'pernas', 'e', 'bracos', 'abertos', ',', 'a', 'cabeca', 'para', 'o', 'lado', ',', 'os', 'olhos', 'moribundos', 'e', 'chorosos', ',', 'toda', 'ela', 'agonizante', ',', 'como', 'se', 'a', 'tivessem', 'crucificado', 'na', 'cama']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A partir dessa noite, da qual só pela manhã o Miranda se retirou do quarto da mulher, estabeleceu-se entre eles o \n",
      "hábito de uma felicidade sexual, tão completa como ainda não a tinham desfrutado, posto que no intimo de cada um \n",
      "persistisse contra o outro a mesma repugnância moral em nada enfraquecida'\n",
      "Tokens gerados: ['a', 'partir', 'dessa', 'noite', ',', 'da', 'qual', 'so', 'pela', 'manha', 'o', 'miranda', 'se', 'retirou', 'do', 'quarto', 'da', 'mulher', ',', 'estabeleceu-se', 'entre', 'eles', 'o', 'habito', 'de', 'uma', 'felicidade', 'sexual', ',', 'tao', 'completa', 'como', 'ainda', 'nao', 'a', 'tinham', 'desfrutado', ',', 'posto', 'que', 'no', 'intimo', 'de', 'cada', 'um', 'persistisse', 'contra', 'o', 'outro', 'a', 'mesma', 'repugnancia', 'moral', 'em', 'nada', 'enfraquecida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Durante dez anos viveram muito bem casados; agora, porém, tanto tempo depois da primeira infidelidade \n",
      "conjugal, e agora que o negociante já não era acometido tão freqüentemente por aquelas crises que o arrojavam fora de \n",
      "horas ao dormitório de Dona Estela; agora, eis que a leviana parecia disposta a reincidir na culpa, dando corda aos \n",
      "caixeiros do marido, na ocasião em que estes subiam para almoçar ou jantar'\n",
      "Tokens gerados: ['durante', 'dez', 'anos', 'viveram', 'muito', 'bem', 'casados', 'agora', ',', 'porem', ',', 'tanto', 'tempo', 'depois', 'da', 'primeira', 'infidelidade', 'conjugal', ',', 'e', 'agora', 'que', 'o', 'negociante', 'ja', 'nao', 'era', 'acometido', 'tao', 'frequentemente', 'por', 'aquelas', 'crises', 'que', 'o', 'arrojavam', 'fora', 'de', 'horas', 'ao', 'dormitorio', 'de', 'dona', 'estela', 'agora', ',', 'eis', 'que', 'a', 'leviana', 'parecia', 'disposta', 'a', 'reincidir', 'na', 'culpa', ',', 'dando', 'corda', 'aos', 'caixeiros', 'do', 'marido', ',', 'na', 'ocasiao', 'em', 'que', 'estes', 'subiam', 'para', 'almocar', 'ou', 'jantar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Foi por isso que o Miranda comprou o prédio vizinho a João Romão'\n",
      "Tokens gerados: ['foi', 'por', 'isso', 'que', 'o', 'miranda', 'comprou', 'o', 'predio', 'vizinho', 'a', 'joao', 'romao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A casa era boa; seu único defeito estava na escassez do quintal; mas para isso havia remédio: com muito pouco \n",
      "compravam-se umas dez braças daquele terreno do fundo que ia até à pedreira, e mais uns dez ou quinze palmos do lado \n",
      "em que ficava a venda'\n",
      "Tokens gerados: ['a', 'casa', 'era', 'boa', 'seu', 'unico', 'defeito', 'estava', 'na', 'escassez', 'do', 'quintal', 'mas', 'para', 'isso', 'havia', 'remedio', 'com', 'muito', 'pouco', 'compravam-se', 'umas', 'dez', 'bracas', 'daquele', 'terreno', 'do', 'fundo', 'que', 'ia', 'ate', 'a', 'pedreira', ',', 'e', 'mais', 'uns', 'dez', 'ou', 'quinze', 'palmos', 'do', 'lado', 'em', 'que', 'ficava', 'a', 'venda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Miranda foi logo entender-se com o Romão e propôs-lhe negócio'\n",
      "Tokens gerados: ['miranda', 'foi', 'logo', 'entender-se', 'com', 'o', 'romao', 'e', 'propos-lhe', 'negocio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O taverneiro recusou formalmente'\n",
      "Tokens gerados: ['o', 'taverneiro', 'recusou', 'formalmente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Miranda insistiu'\n",
      "Tokens gerados: ['miranda', 'insistiu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— O senhor perde seu tempo e seu latim! retrucou o amigo de Bertoleza'\n",
      "Tokens gerados: ['—', 'o', 'senhor', 'perde', 'seu', 'tempo', 'e', 'seu', 'latim', 'retrucou', 'o', 'amigo', 'de', 'bertoleza']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nem só não cedo uma polegada do meu \n",
      "terreno, como ainda lhe compro, se mo quiser vender, aquele pedaço que lhe fica ao fundo da casa! \n",
      "— O quintal? \n",
      "— É exato'\n",
      "Tokens gerados: ['nem', 'so', 'nao', 'cedo', 'uma', 'polegada', 'do', 'meu', 'terreno', ',', 'como', 'ainda', 'lhe', 'compro', ',', 'se', 'mo', 'quiser', 'vender', ',', 'aquele', 'pedaco', 'que', 'lhe', 'fica', 'ao', 'fundo', 'da', 'casa', '—', 'o', 'quintal', '?', '—', 'e', 'exato']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pois você quer que eu fique sem chácara, sem jardim, sem nada? \n",
      "— Para mim era de vantagem'\n",
      "Tokens gerados: ['—', 'pois', 'voce', 'quer', 'que', 'eu', 'fique', 'sem', 'chacara', ',', 'sem', 'jardim', ',', 'sem', 'nada', '?', '—', 'para', 'mim', 'era', 'de', 'vantagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora, deixe-se disso, homem, e diga lá quanto quer pelo que lhe propus'\n",
      "Tokens gerados: ['—', 'ora', ',', 'deixe-se', 'disso', ',', 'homem', ',', 'e', 'diga', 'la', 'quanto', 'quer', 'pelo', 'que', 'lhe', 'propus']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Já disse o que tinha a dizer'\n",
      "Tokens gerados: ['—', 'ja', 'disse', 'o', 'que', 'tinha', 'a', 'dizer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ceda-me então ao menos as dez braças do fundo'\n",
      "Tokens gerados: ['—', 'ceda-me', 'entao', 'ao', 'menos', 'as', 'dez', 'bracas', 'do', 'fundo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Nem meio palmo! \n",
      "— Isso é maldade de sua parte, sabe? Eu, se faço tamanho empenho, é pela minha pequena, que precisa, coitada, \n",
      "de um pouco de espaço para alargar-se'\n",
      "Tokens gerados: ['—', 'nem', 'meio', 'palmo', '—', 'isso', 'e', 'maldade', 'de', 'sua', 'parte', ',', 'sabe', '?', 'eu', ',', 'se', 'faco', 'tamanho', 'empenho', ',', 'e', 'pela', 'minha', 'pequena', ',', 'que', 'precisa', ',', 'coitada', ',', 'de', 'um', 'pouco', 'de', 'espaco', 'para', 'alargar-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— E eu não cedo, porque preciso do meu terreno! \n",
      "— Ora qual! Que diabo pode lá você fazer ali? Uma porcaria de um pedaço de terreno quase grudado ao morro e \n",
      "aos fundos de minha casa! quando você, aliás, dispõe de tanto espaço ainda! \n",
      "— Hei de lhe mostrar se tenho ou não o que fazer ali! \n",
      "— É que você é teimoso! Olhe, se me cedesse as dez braças do fundo, a sua parte ficaria cortada em linha reta até \n",
      "à pedreira, e escusava eu de ficar com uma aba de terreno alheio a meter-se pelo meu'\n",
      "Tokens gerados: ['—', 'e', 'eu', 'nao', 'cedo', ',', 'porque', 'preciso', 'do', 'meu', 'terreno', '—', 'ora', 'qual', 'que', 'diabo', 'pode', 'la', 'voce', 'fazer', 'ali', '?', 'uma', 'porcaria', 'de', 'um', 'pedaco', 'de', 'terreno', 'quase', 'grudado', 'ao', 'morro', 'e', 'aos', 'fundos', 'de', 'minha', 'casa', 'quando', 'voce', ',', 'alias', ',', 'dispoe', 'de', 'tanto', 'espaco', 'ainda', '—', 'hei', 'de', 'lhe', 'mostrar', 'se', 'tenho', 'ou', 'nao', 'o', 'que', 'fazer', 'ali', '—', 'e', 'que', 'voce', 'e', 'teimoso', 'olhe', ',', 'se', 'me', 'cedesse', 'as', 'dez', 'bracas', 'do', 'fundo', ',', 'a', 'sua', 'parte', 'ficaria', 'cortada', 'em', 'linha', 'reta', 'ate', 'a', 'pedreira', ',', 'e', 'escusava', 'eu', 'de', 'ficar', 'com', 'uma', 'aba', 'de', 'terreno', 'alheio', 'a', 'meter-se', 'pelo', 'meu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quer saber? não amuro o quintal \n",
      "sem você decidir-se! \n",
      "— Então ficará com o quintal para sempre sem muro, porque o que tinha a dizer já disse! \n",
      "— Mas, homem de Deus, que diabo! pense um pouco! Você ali não pode construir nada! Ou pensará que lhe \n",
      "deixarei abrir janelas sobre o meu quintal!'\n",
      "Tokens gerados: ['quer', 'saber', '?', 'nao', 'amuro', 'o', 'quintal', 'sem', 'voce', 'decidir-se', '—', 'entao', 'ficara', 'com', 'o', 'quintal', 'para', 'sempre', 'sem', 'muro', ',', 'porque', 'o', 'que', 'tinha', 'a', 'dizer', 'ja', 'disse', '—', 'mas', ',', 'homem', 'de', 'deus', ',', 'que', 'diabo', 'pense', 'um', 'pouco', 'voce', 'ali', 'nao', 'pode', 'construir', 'nada', 'ou', 'pensara', 'que', 'lhe', 'deixarei', 'abrir', 'janelas', 'sobre', 'o', 'meu', 'quintal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não preciso abrir janelas sobre o quintal de ninguém! \n",
      "— Nem tampouco lhe deixarei levantar parede, tapando-me as janelas da esquerda! \n",
      "— Não preciso levantar parede desse lado'\n",
      "Tokens gerados: ['—', 'nao', 'preciso', 'abrir', 'janelas', 'sobre', 'o', 'quintal', 'de', 'ninguem', '—', 'nem', 'tampouco', 'lhe', 'deixarei', 'levantar', 'parede', ',', 'tapando-me', 'as', 'janelas', 'da', 'esquerda', '—', 'nao', 'preciso', 'levantar', 'parede', 'desse', 'lado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então que diabo vai você fazer de todo este terreno?'\n",
      "Tokens gerados: ['—', 'entao', 'que', 'diabo', 'vai', 'voce', 'fazer', 'de', 'todo', 'este', 'terreno', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah! isso agora é cá comigo!'\n",
      "Tokens gerados: ['—', 'ah', 'isso', 'agora', 'e', 'ca', 'comigo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O que for soará! \n",
      "— Pois creia que se arrepende de não me ceder o terreno!'\n",
      "Tokens gerados: ['o', 'que', 'for', 'soara', '—', 'pois', 'creia', 'que', 'se', 'arrepende', 'de', 'nao', 'me', 'ceder', 'o', 'terreno']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Se me arrepender, paciência! Só lhe digo é que muito mal se sairá quem quiser meter-se cá com a minha vida! \n",
      "— Passe bem! \n",
      "— Adeus! \n",
      "Travou-se então uma lata renhida e surda entre o português negociante de fazendas por atacado e o português \n",
      "negociante de secos e molhados'\n",
      "Tokens gerados: ['—', 'se', 'me', 'arrepender', ',', 'paciencia', 'so', 'lhe', 'digo', 'e', 'que', 'muito', 'mal', 'se', 'saira', 'quem', 'quiser', 'meter-se', 'ca', 'com', 'a', 'minha', 'vida', '—', 'passe', 'bem', '—', 'adeus', 'travou-se', 'entao', 'uma', 'lata', 'renhida', 'e', 'surda', 'entre', 'o', 'portugues', 'negociante', 'de', 'fazendas', 'por', 'atacado', 'e', 'o', 'portugues', 'negociante', 'de', 'secos', 'e', 'molhados']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aquele não se resolvia a fazer o muro do quintal, sem ter alcançado o pedaço de \n",
      "terreno que o separava do morro; e o outro, por seu lado, não perdia a esperança de apanhar-lhe ainda, pelo menos, duas \n",
      "ou três braças aos fundos da casa; parte esta que, conforme os seus cálculos, valeria ouro, uma vez realizado o grande \n",
      "projeto que ultimamente o trazia preocupado — a criação de uma estalagem em ponto enorme, uma estalagem monstro, \n",
      "sem exemplo, destinada a matar toda aquela miuçalha de cortiços que alastravam por Botafogo'\n",
      "Tokens gerados: ['aquele', 'nao', 'se', 'resolvia', 'a', 'fazer', 'o', 'muro', 'do', 'quintal', ',', 'sem', 'ter', 'alcancado', 'o', 'pedaco', 'de', 'terreno', 'que', 'o', 'separava', 'do', 'morro', 'e', 'o', 'outro', ',', 'por', 'seu', 'lado', ',', 'nao', 'perdia', 'a', 'esperanca', 'de', 'apanhar-lhe', 'ainda', ',', 'pelo', 'menos', ',', 'duas', 'ou', 'tres', 'bracas', 'aos', 'fundos', 'da', 'casa', 'parte', 'esta', 'que', ',', 'conforme', 'os', 'seus', 'calculos', ',', 'valeria', 'ouro', ',', 'uma', 'vez', 'realizado', 'o', 'grande', 'projeto', 'que', 'ultimamente', 'o', 'trazia', 'preocupado', '—', 'a', 'criacao', 'de', 'uma', 'estalagem', 'em', 'ponto', 'enorme', ',', 'uma', 'estalagem', 'monstro', ',', 'sem', 'exemplo', ',', 'destinada', 'a', 'matar', 'toda', 'aquela', 'miucalha', 'de', 'corticos', 'que', 'alastravam', 'por', 'botafogo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era este o seu ideal'\n",
      "Tokens gerados: ['era', 'este', 'o', 'seu', 'ideal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia muito que João Romão vivia exclusivamente para essa idéia; sonhava com ela todas as \n",
      "noites; comparecia a todos os leilões de materiais de construção; arrematava madeiramentos já servidos; comprava telha \n",
      "em segunda mão; fazia pechinchas de cal e tijolos; o que era tudo depositado no seu extenso chão vazio, cujo aspecto \n",
      "tomava em breve o caráter estranho de uma enorme barricada, tal era a variedade dos objetos que ali se apinhavam \n",
      "acumulados: tábuas e sarrafos, troncos de árvore, mastros de navio, caibros, restos de carroças, chaminés de barro e de \n",
      "ferro, fogões desmantelados, pilhas e pilhas de tijolos de todos os feitios, barricas de cimento, montes de areia e terra \n",
      "vermelha, aglomerações de telhas velhas, escadas partidas, depósitos de cal, o diabo enfim; ao que ele, que sabia \n",
      "perfeitamente como essas coisas se furtavam, resguardava, soltando à noite um formidável cão de fila'\n",
      "Tokens gerados: ['havia', 'muito', 'que', 'joao', 'romao', 'vivia', 'exclusivamente', 'para', 'essa', 'ideia', 'sonhava', 'com', 'ela', 'todas', 'as', 'noites', 'comparecia', 'a', 'todos', 'os', 'leiloes', 'de', 'materiais', 'de', 'construcao', 'arrematava', 'madeiramentos', 'ja', 'servidos', 'comprava', 'telha', 'em', 'segunda', 'mao', 'fazia', 'pechinchas', 'de', 'cal', 'e', 'tijolos', 'o', 'que', 'era', 'tudo', 'depositado', 'no', 'seu', 'extenso', 'chao', 'vazio', ',', 'cujo', 'aspecto', 'tomava', 'em', 'breve', 'o', 'carater', 'estranho', 'de', 'uma', 'enorme', 'barricada', ',', 'tal', 'era', 'a', 'variedade', 'dos', 'objetos', 'que', 'ali', 'se', 'apinhavam', 'acumulados', 'tabuas', 'e', 'sarrafos', ',', 'troncos', 'de', 'arvore', ',', 'mastros', 'de', 'navio', ',', 'caibros', ',', 'restos', 'de', 'carrocas', ',', 'chamines', 'de', 'barro', 'e', 'de', 'ferro', ',', 'fogoes', 'desmantelados', ',', 'pilhas', 'e', 'pilhas', 'de', 'tijolos', 'de', 'todos', 'os', 'feitios', ',', 'barricas', 'de', 'cimento', ',', 'montes', 'de', 'areia', 'e', 'terra', 'vermelha', ',', 'aglomeracoes', 'de', 'telhas', 'velhas', ',', 'escadas', 'partidas', ',', 'depositos', 'de', 'cal', ',', 'o', 'diabo', 'enfim', 'ao', 'que', 'ele', ',', 'que', 'sabia', 'perfeitamente', 'como', 'essas', 'coisas', 'se', 'furtavam', ',', 'resguardava', ',', 'soltando', 'a', 'noite', 'um', 'formidavel', 'cao', 'de', 'fila']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Este cão era pretexto de eternas resingas com a gente do Miranda, a cujo quintal ninguém de casa podia descer, \n",
      "depois das dez horas da noite, sem correr o risco de ser assaltado pela fera'\n",
      "Tokens gerados: ['este', 'cao', 'era', 'pretexto', 'de', 'eternas', 'resingas', 'com', 'a', 'gente', 'do', 'miranda', ',', 'a', 'cujo', 'quintal', 'ninguem', 'de', 'casa', 'podia', 'descer', ',', 'depois', 'das', 'dez', 'horas', 'da', 'noite', ',', 'sem', 'correr', 'o', 'risco', 'de', 'ser', 'assaltado', 'pela', 'fera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— É fazer o muro! dizia o João Romão, sacudindo os ombros'\n",
      "Tokens gerados: ['—', 'e', 'fazer', 'o', 'muro', 'dizia', 'o', 'joao', 'romao', ',', 'sacudindo', 'os', 'ombros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não faço! replicava o outro'\n",
      "Tokens gerados: ['—', 'nao', 'faco', 'replicava', 'o', 'outro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se ele é questão de capricho eu também tenho capricho! \n",
      "Em compensação, não caia no quintal do Miranda galinha ou frango, fugidos do cercado do vendeiro, que não \n",
      "levasse imediato sumiço'\n",
      "Tokens gerados: ['se', 'ele', 'e', 'questao', 'de', 'capricho', 'eu', 'tambem', 'tenho', 'capricho', 'em', 'compensacao', ',', 'nao', 'caia', 'no', 'quintal', 'do', 'miranda', 'galinha', 'ou', 'frango', ',', 'fugidos', 'do', 'cercado', 'do', 'vendeiro', ',', 'que', 'nao', 'levasse', 'imediato', 'sumico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão protestava contra o roubo em termos violentos, jurando vinganças terríveis, \n",
      "falando em dar tiros'\n",
      "Tokens gerados: ['joao', 'romao', 'protestava', 'contra', 'o', 'roubo', 'em', 'termos', 'violentos', ',', 'jurando', 'vingancas', 'terriveis', ',', 'falando', 'em', 'dar', 'tiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pois é fazer um muro no galinheiro! repontava o marido de Estela'\n",
      "Tokens gerados: ['—', 'pois', 'e', 'fazer', 'um', 'muro', 'no', 'galinheiro', 'repontava', 'o', 'marido', 'de', 'estela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Daí a alguns meses, João Romão, depois de tentar um derradeiro esforço para conseguir algumas braças do quintal \n",
      "do vizinho, resolveu principiar as obras da estalagem'\n",
      "Tokens gerados: ['dai', 'a', 'alguns', 'meses', ',', 'joao', 'romao', ',', 'depois', 'de', 'tentar', 'um', 'derradeiro', 'esforco', 'para', 'conseguir', 'algumas', 'bracas', 'do', 'quintal', 'do', 'vizinho', ',', 'resolveu', 'principiar', 'as', 'obras', 'da', 'estalagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Deixa estar, conversava ele na cama com a Bertoleza; deixa estar que ainda lhe hei de entrar pelos fundos da \n",
      "casa, se é que não lhe entre pela frente! Mais cedo ou mais tarde como-lhe, não duas braças, mas seis, oito, todo o \n",
      "quintal e até o próprio sobrado talvez! \n",
      "E dizia isto com uma convicção de quem tudo pode e tudo espera da sua perseverança, do seu esforço \n",
      "inquebrantável e da fecundidade prodigiosa do seu dinheiro, dinheiro que só lhe saia das unhas para voltar multiplicado'\n",
      "Tokens gerados: ['—', 'deixa', 'estar', ',', 'conversava', 'ele', 'na', 'cama', 'com', 'a', 'bertoleza', 'deixa', 'estar', 'que', 'ainda', 'lhe', 'hei', 'de', 'entrar', 'pelos', 'fundos', 'da', 'casa', ',', 'se', 'e', 'que', 'nao', 'lhe', 'entre', 'pela', 'frente', 'mais', 'cedo', 'ou', 'mais', 'tarde', 'como-lhe', ',', 'nao', 'duas', 'bracas', ',', 'mas', 'seis', ',', 'oito', ',', 'todo', 'o', 'quintal', 'e', 'ate', 'o', 'proprio', 'sobrado', 'talvez', 'e', 'dizia', 'isto', 'com', 'uma', 'conviccao', 'de', 'quem', 'tudo', 'pode', 'e', 'tudo', 'espera', 'da', 'sua', 'perseveranca', ',', 'do', 'seu', 'esforco', 'inquebrantavel', 'e', 'da', 'fecundidade', 'prodigiosa', 'do', 'seu', 'dinheiro', ',', 'dinheiro', 'que', 'so', 'lhe', 'saia', 'das', 'unhas', 'para', 'voltar', 'multiplicado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desde que a febre de possuir se apoderou dele totalmente, todos os seus atos, todos, fosse o mais simples, visavam \n",
      "um interesse pecuniário'\n",
      "Tokens gerados: ['desde', 'que', 'a', 'febre', 'de', 'possuir', 'se', 'apoderou', 'dele', 'totalmente', ',', 'todos', 'os', 'seus', 'atos', ',', 'todos', ',', 'fosse', 'o', 'mais', 'simples', ',', 'visavam', 'um', 'interesse', 'pecuniario']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Só tinha uma preocupação: aumentar os bens'\n",
      "Tokens gerados: ['so', 'tinha', 'uma', 'preocupacao', 'aumentar', 'os', 'bens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Das suas hortas recolhia para si e para a \n",
      "companheira os piores legumes, aqueles que, por maus, ninguém compraria; as suas galinhas produziam muito e ele não \n",
      "comia um ovo, do que no entanto gostava imenso; vendia-os todos e contentava-se com os restos da comida dos \n",
      "trabalhadores'\n",
      "Tokens gerados: ['das', 'suas', 'hortas', 'recolhia', 'para', 'si', 'e', 'para', 'a', 'companheira', 'os', 'piores', 'legumes', ',', 'aqueles', 'que', ',', 'por', 'maus', ',', 'ninguem', 'compraria', 'as', 'suas', 'galinhas', 'produziam', 'muito', 'e', 'ele', 'nao', 'comia', 'um', 'ovo', ',', 'do', 'que', 'no', 'entanto', 'gostava', 'imenso', 'vendia-os', 'todos', 'e', 'contentava-se', 'com', 'os', 'restos', 'da', 'comida', 'dos', 'trabalhadores']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aquilo já não era ambição, era uma moléstia nervosa, uma loucura, um desespero de acumular; de \n",
      "reduzir tudo a moeda'\n",
      "Tokens gerados: ['aquilo', 'ja', 'nao', 'era', 'ambicao', ',', 'era', 'uma', 'molestia', 'nervosa', ',', 'uma', 'loucura', ',', 'um', 'desespero', 'de', 'acumular', 'de', 'reduzir', 'tudo', 'a', 'moeda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E seu tipo baixote, socado, de cabelos à escovinha, a barba sempre por fazer, ia e vinha da \n",
      "pedreira para a venda, da venda às hortas e ao capinzal, sempre em mangas de camisa, de tamancos, sem meias, olhando \n",
      "para todos os lados, com o seu eterno ar de cobiça, apoderando-se, com os olhos, de tudo aquilo de que ele não podia \n",
      "apoderar-se logo com as unhas'\n",
      "Tokens gerados: ['e', 'seu', 'tipo', 'baixote', ',', 'socado', ',', 'de', 'cabelos', 'a', 'escovinha', ',', 'a', 'barba', 'sempre', 'por', 'fazer', ',', 'ia', 'e', 'vinha', 'da', 'pedreira', 'para', 'a', 'venda', ',', 'da', 'venda', 'as', 'hortas', 'e', 'ao', 'capinzal', ',', 'sempre', 'em', 'mangas', 'de', 'camisa', ',', 'de', 'tamancos', ',', 'sem', 'meias', ',', 'olhando', 'para', 'todos', 'os', 'lados', ',', 'com', 'o', 'seu', 'eterno', 'ar', 'de', 'cobica', ',', 'apoderando-se', ',', 'com', 'os', 'olhos', ',', 'de', 'tudo', 'aquilo', 'de', 'que', 'ele', 'nao', 'podia', 'apoderar-se', 'logo', 'com', 'as', 'unhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, a rua lá fora povoava-se de um modo admirável'\n",
      "Tokens gerados: ['entretanto', ',', 'a', 'rua', 'la', 'fora', 'povoava-se', 'de', 'um', 'modo', 'admiravel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Construía-se mal, porém muito; surgiam chalés e \n",
      "casinhas da noite para o dia; subiam os aluguéis; as propriedades dobravam de valor'\n",
      "Tokens gerados: ['construia-se', 'mal', ',', 'porem', 'muito', 'surgiam', 'chales', 'e', 'casinhas', 'da', 'noite', 'para', 'o', 'dia', 'subiam', 'os', 'alugueis', 'as', 'propriedades', 'dobravam', 'de', 'valor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Montara-se uma fábrica de massas \n",
      "italianas e outra de velas, e os trabalhadores passavam de manhã e às Ave-Marias, e a maior parte deles ia comer à casa \n",
      "de pasto que João Romão arranjara aos fundos da sua varanda'\n",
      "Tokens gerados: ['montara-se', 'uma', 'fabrica', 'de', 'massas', 'italianas', 'e', 'outra', 'de', 'velas', ',', 'e', 'os', 'trabalhadores', 'passavam', 'de', 'manha', 'e', 'as', 'ave-marias', ',', 'e', 'a', 'maior', 'parte', 'deles', 'ia', 'comer', 'a', 'casa', 'de', 'pasto', 'que', 'joao', 'romao', 'arranjara', 'aos', 'fundos', 'da', 'sua', 'varanda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Abriram-se novas tavernas; nenhuma, porém, conseguia \n",
      "ser tão afreguesada como a dele'\n",
      "Tokens gerados: ['abriram-se', 'novas', 'tavernas', 'nenhuma', ',', 'porem', ',', 'conseguia', 'ser', 'tao', 'afreguesada', 'como', 'a', 'dele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nunca o seu negocio fora tão bem, nunca o finório vendera tanto; vendia mais agora, \n",
      "muito mais, que nos anos anteriores'\n",
      "Tokens gerados: ['nunca', 'o', 'seu', 'negocio', 'fora', 'tao', 'bem', ',', 'nunca', 'o', 'finorio', 'vendera', 'tanto', 'vendia', 'mais', 'agora', ',', 'muito', 'mais', ',', 'que', 'nos', 'anos', 'anteriores']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Teve até de admitir caixeiros'\n",
      "Tokens gerados: ['teve', 'ate', 'de', 'admitir', 'caixeiros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As mercadorias não lhe paravam nas prateleiras; o \n",
      "balcão estava cada vez mais lustroso, mais gasto'\n",
      "Tokens gerados: ['as', 'mercadorias', 'nao', 'lhe', 'paravam', 'nas', 'prateleiras', 'o', 'balcao', 'estava', 'cada', 'vez', 'mais', 'lustroso', ',', 'mais', 'gasto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o dinheiro a pingar, vintém por vintém, dentro da gaveta, e a \n",
      "escorrer da gaveta para a barra, aos cinqüenta e aos cem mil-réis, e da burra para o banco, aos contos e aos contos'\n",
      "Tokens gerados: ['e', 'o', 'dinheiro', 'a', 'pingar', ',', 'vintem', 'por', 'vintem', ',', 'dentro', 'da', 'gaveta', ',', 'e', 'a', 'escorrer', 'da', 'gaveta', 'para', 'a', 'barra', ',', 'aos', 'cinquenta', 'e', 'aos', 'cem', 'mil-reis', ',', 'e', 'da', 'burra', 'para', 'o', 'banco', ',', 'aos', 'contos', 'e', 'aos', 'contos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Afinal, já lhe não bastava sortir o seu estabelecimento nos armazéns fornecedores; começou a receber alguns \n",
      "gêneros diretamente da Europa: o vinho, por exemplo, que ele dantes comprava aos quintos nas casas de atacado, \n",
      "vinha-lhe agora de Portugal às pipas, e de cada uma fazia três com água e cachaça; e despachava faturas de barris de \n",
      "manteiga, de caixas de conserva, caixões de fósforos, azeite, queijos, louça e muitas outras mercadorias'\n",
      "Tokens gerados: ['afinal', ',', 'ja', 'lhe', 'nao', 'bastava', 'sortir', 'o', 'seu', 'estabelecimento', 'nos', 'armazens', 'fornecedores', 'comecou', 'a', 'receber', 'alguns', 'generos', 'diretamente', 'da', 'europa', 'o', 'vinho', ',', 'por', 'exemplo', ',', 'que', 'ele', 'dantes', 'comprava', 'aos', 'quintos', 'nas', 'casas', 'de', 'atacado', ',', 'vinha-lhe', 'agora', 'de', 'portugal', 'as', 'pipas', ',', 'e', 'de', 'cada', 'uma', 'fazia', 'tres', 'com', 'agua', 'e', 'cachaca', 'e', 'despachava', 'faturas', 'de', 'barris', 'de', 'manteiga', ',', 'de', 'caixas', 'de', 'conserva', ',', 'caixoes', 'de', 'fosforos', ',', 'azeite', ',', 'queijos', ',', 'louca', 'e', 'muitas', 'outras', 'mercadorias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Criou armazéns para depósito, aboliu a quitanda e transferiu o dormitório, aproveitando o espaço para ampliar a \n",
      "venda, que dobrou de tamanho e ganhou mais duas portas'\n",
      "Tokens gerados: ['criou', 'armazens', 'para', 'deposito', ',', 'aboliu', 'a', 'quitanda', 'e', 'transferiu', 'o', 'dormitorio', ',', 'aproveitando', 'o', 'espaco', 'para', 'ampliar', 'a', 'venda', ',', 'que', 'dobrou', 'de', 'tamanho', 'e', 'ganhou', 'mais', 'duas', 'portas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Já não era uma simples taverna, era um bazar em que se encontrava de tudo, objetos de armarinho, ferragens, \n",
      "porcelanas, utensílios de escritório, roupa de riscado para os trabalhadores, fazenda para roupa de mulher, chapéus de \n",
      "palha próprios para o serviço ao sol, perfumarias baratas, pentes de chifre, lenços com versos de amor, e anéis e brincos \n",
      "de metal ordinário'\n",
      "Tokens gerados: ['ja', 'nao', 'era', 'uma', 'simples', 'taverna', ',', 'era', 'um', 'bazar', 'em', 'que', 'se', 'encontrava', 'de', 'tudo', ',', 'objetos', 'de', 'armarinho', ',', 'ferragens', ',', 'porcelanas', ',', 'utensilios', 'de', 'escritorio', ',', 'roupa', 'de', 'riscado', 'para', 'os', 'trabalhadores', ',', 'fazenda', 'para', 'roupa', 'de', 'mulher', ',', 'chapeus', 'de', 'palha', 'proprios', 'para', 'o', 'servico', 'ao', 'sol', ',', 'perfumarias', 'baratas', ',', 'pentes', 'de', 'chifre', ',', 'lencos', 'com', 'versos', 'de', 'amor', ',', 'e', 'aneis', 'e', 'brincos', 'de', 'metal', 'ordinario']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E toda a gentalha daquelas redondezas ia cair lá, ou então ali ao lado, na casa de pasto, onde os operários das \n",
      "fábricas e os trabalhadores da pedreira se reuniam depois do serviço, e ficavam bebendo e conversando até as dez horas \n",
      "da noite, entre o espesso fumo dos cachimbos, do peixe frito em azeite e dos lampiões de querosene'\n",
      "Tokens gerados: ['e', 'toda', 'a', 'gentalha', 'daquelas', 'redondezas', 'ia', 'cair', 'la', ',', 'ou', 'entao', 'ali', 'ao', 'lado', ',', 'na', 'casa', 'de', 'pasto', ',', 'onde', 'os', 'operarios', 'das', 'fabricas', 'e', 'os', 'trabalhadores', 'da', 'pedreira', 'se', 'reuniam', 'depois', 'do', 'servico', ',', 'e', 'ficavam', 'bebendo', 'e', 'conversando', 'ate', 'as', 'dez', 'horas', 'da', 'noite', ',', 'entre', 'o', 'espesso', 'fumo', 'dos', 'cachimbos', ',', 'do', 'peixe', 'frito', 'em', 'azeite', 'e', 'dos', 'lampioes', 'de', 'querosene']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era João Romão quem lhes fornecia tudo, tudo, até dinheiro adiantado, quando algum precisava'\n",
      "Tokens gerados: ['era', 'joao', 'romao', 'quem', 'lhes', 'fornecia', 'tudo', ',', 'tudo', ',', 'ate', 'dinheiro', 'adiantado', ',', 'quando', 'algum', 'precisava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por ali não se \n",
      "encontrava jornaleiro, cujo ordenado não fosse inteirinho parar às mãos do velhaco'\n",
      "Tokens gerados: ['por', 'ali', 'nao', 'se', 'encontrava', 'jornaleiro', ',', 'cujo', 'ordenado', 'nao', 'fosse', 'inteirinho', 'parar', 'as', 'maos', 'do', 'velhaco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E sobre este cobre, quase sempre \n",
      "emprestado aos tostões, cobrava juros de oito por cento ao mês, um pouco mais do que levava aos que garantiam a \n",
      "divida com penhores de ouro ou prata'\n",
      "Tokens gerados: ['e', 'sobre', 'este', 'cobre', ',', 'quase', 'sempre', 'emprestado', 'aos', 'tostoes', ',', 'cobrava', 'juros', 'de', 'oito', 'por', 'cento', 'ao', 'mes', ',', 'um', 'pouco', 'mais', 'do', 'que', 'levava', 'aos', 'que', 'garantiam', 'a', 'divida', 'com', 'penhores', 'de', 'ouro', 'ou', 'prata']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não obstante, as casinhas do cortiço, à proporção que se atamancavam, enchiam-se logo, sem mesmo dar tempo a \n",
      "que as tintas secassem'\n",
      "Tokens gerados: ['nao', 'obstante', ',', 'as', 'casinhas', 'do', 'cortico', ',', 'a', 'proporcao', 'que', 'se', 'atamancavam', ',', 'enchiam-se', 'logo', ',', 'sem', 'mesmo', 'dar', 'tempo', 'a', 'que', 'as', 'tintas', 'secassem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia grande avidez em alugá-las; aquele era o melhor ponto do bairro para a gente do trabalho'\n",
      "Tokens gerados: ['havia', 'grande', 'avidez', 'em', 'aluga-las', 'aquele', 'era', 'o', 'melhor', 'ponto', 'do', 'bairro', 'para', 'a', 'gente', 'do', 'trabalho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Os empregados da pedreira preferiam todos morar lá, porque ficavam a dois passos da obrigação'\n",
      "Tokens gerados: ['os', 'empregados', 'da', 'pedreira', 'preferiam', 'todos', 'morar', 'la', ',', 'porque', 'ficavam', 'a', 'dois', 'passos', 'da', 'obrigacao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Miranda rebentava de raiva'\n",
      "Tokens gerados: ['o', 'miranda', 'rebentava', 'de', 'raiva']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Um cortiço! exclamava ele, possesso'\n",
      "Tokens gerados: ['—', 'um', 'cortico', 'exclamava', 'ele', ',', 'possesso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um cortiço! Maldito seja aquele vendeiro de todos os diabos! Fazer-me \n",
      "um cortiço debaixo das janelas!'\n",
      "Tokens gerados: ['um', 'cortico', 'maldito', 'seja', 'aquele', 'vendeiro', 'de', 'todos', 'os', 'diabos', 'fazer-me', 'um', 'cortico', 'debaixo', 'das', 'janelas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estragou-me a casa, o malvado! \n",
      "E vomitava pragas, jurando que havia de vingar-se, e protestando aos berros contra o pó que lhe invadia em ondas \n",
      "as salas, e contra o infernal baralho dos pedreiros e carpinteiros que levavam a martelar de sol a sol'\n",
      "Tokens gerados: ['estragou-me', 'a', 'casa', ',', 'o', 'malvado', 'e', 'vomitava', 'pragas', ',', 'jurando', 'que', 'havia', 'de', 'vingar-se', ',', 'e', 'protestando', 'aos', 'berros', 'contra', 'o', 'po', 'que', 'lhe', 'invadia', 'em', 'ondas', 'as', 'salas', ',', 'e', 'contra', 'o', 'infernal', 'baralho', 'dos', 'pedreiros', 'e', 'carpinteiros', 'que', 'levavam', 'a', 'martelar', 'de', 'sol', 'a', 'sol']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O que aliás não impediu que as casinhas continuassem a surgir, uma após outra, e fossem logo se enchendo, a \n",
      "estenderem-se unidas por ali a fora, desde a venda até quase ao morro, e depois dobrassem para o lado do Miranda e \n",
      "avançassem sobre o quintal deste, que parecia ameaçado por aquela serpente de pedra e cal'\n",
      "Tokens gerados: ['o', 'que', 'alias', 'nao', 'impediu', 'que', 'as', 'casinhas', 'continuassem', 'a', 'surgir', ',', 'uma', 'apos', 'outra', ',', 'e', 'fossem', 'logo', 'se', 'enchendo', ',', 'a', 'estenderem-se', 'unidas', 'por', 'ali', 'a', 'fora', ',', 'desde', 'a', 'venda', 'ate', 'quase', 'ao', 'morro', ',', 'e', 'depois', 'dobrassem', 'para', 'o', 'lado', 'do', 'miranda', 'e', 'avancassem', 'sobre', 'o', 'quintal', 'deste', ',', 'que', 'parecia', 'ameacado', 'por', 'aquela', 'serpente', 'de', 'pedra', 'e', 'cal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Miranda mandou logo levantar o muro'\n",
      "Tokens gerados: ['o', 'miranda', 'mandou', 'logo', 'levantar', 'o', 'muro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nada! aquele demônio era capaz de invadir-lhe a casa até a sala de visitas! \n",
      "E os quartos do cortiço pararam enfim de encontro ao muro do negociante, formando com a continuação da casa \n",
      "deste um grande quadrilongo, espécie de pátio de quartel, onde podia formar um batalhão'\n",
      "Tokens gerados: ['nada', 'aquele', 'demonio', 'era', 'capaz', 'de', 'invadir-lhe', 'a', 'casa', 'ate', 'a', 'sala', 'de', 'visitas', 'e', 'os', 'quartos', 'do', 'cortico', 'pararam', 'enfim', 'de', 'encontro', 'ao', 'muro', 'do', 'negociante', ',', 'formando', 'com', 'a', 'continuacao', 'da', 'casa', 'deste', 'um', 'grande', 'quadrilongo', ',', 'especie', 'de', 'patio', 'de', 'quartel', ',', 'onde', 'podia', 'formar', 'um', 'batalhao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Noventa e cinco casinhas comportou a imensa estalagem'\n",
      "Tokens gerados: ['noventa', 'e', 'cinco', 'casinhas', 'comportou', 'a', 'imensa', 'estalagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prontas, João Romão mandou levantar na frente, nas vinte braças que separavam a venda do sobrado do Miranda, \n",
      "um grosso muro de dez palmos de altura, coroado de cacos de vidro e fundos de garrafa, e com um grande portão no \n",
      "centro, onde se dependurou uma lanterna de vidraças vermelhas, por cima de uma tabuleta amarela, em que se lia o \n",
      "seguinte, escrito a tinta encarnada e sem ortografia: \n",
      "“Estalagem de São Romão'\n",
      "Tokens gerados: ['prontas', ',', 'joao', 'romao', 'mandou', 'levantar', 'na', 'frente', ',', 'nas', 'vinte', 'bracas', 'que', 'separavam', 'a', 'venda', 'do', 'sobrado', 'do', 'miranda', ',', 'um', 'grosso', 'muro', 'de', 'dez', 'palmos', 'de', 'altura', ',', 'coroado', 'de', 'cacos', 'de', 'vidro', 'e', 'fundos', 'de', 'garrafa', ',', 'e', 'com', 'um', 'grande', 'portao', 'no', 'centro', ',', 'onde', 'se', 'dependurou', 'uma', 'lanterna', 'de', 'vidracas', 'vermelhas', ',', 'por', 'cima', 'de', 'uma', 'tabuleta', 'amarela', ',', 'em', 'que', 'se', 'lia', 'o', 'seguinte', ',', 'escrito', 'a', 'tinta', 'encarnada', 'e', 'sem', 'ortografia', '“', 'estalagem', 'de', 'sao', 'romao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Alugam-se casinhas e tinas para lavadeiras”'\n",
      "Tokens gerados: ['alugam-se', 'casinhas', 'e', 'tinas', 'para', 'lavadeiras', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As casinhas eram alugadas por mês e as tinas por dia; tudo pago adiantado'\n",
      "Tokens gerados: ['as', 'casinhas', 'eram', 'alugadas', 'por', 'mes', 'e', 'as', 'tinas', 'por', 'dia', 'tudo', 'pago', 'adiantado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O preço de cada tina, metendo a água, \n",
      "quinhentos réis; sabão à parte'\n",
      "Tokens gerados: ['o', 'preco', 'de', 'cada', 'tina', ',', 'metendo', 'a', 'agua', ',', 'quinhentos', 'reis', 'sabao', 'a', 'parte']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As moradoras do cortiço tinham preferência e não pagavam nada para lavar'\n",
      "Tokens gerados: ['as', 'moradoras', 'do', 'cortico', 'tinham', 'preferencia', 'e', 'nao', 'pagavam', 'nada', 'para', 'lavar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Graças à abundância da água que lá havia, como em nenhuma outra parte, e graças ao muito espaço de que se \n",
      "dispunha no cortiço para estender a roupa, a concorrência às tinas não se fez esperar; acudiram lavadeiras de todos os \n",
      "pontos da cidade, entre elas algumas vindas de bem longe'\n",
      "Tokens gerados: ['gracas', 'a', 'abundancia', 'da', 'agua', 'que', 'la', 'havia', ',', 'como', 'em', 'nenhuma', 'outra', 'parte', ',', 'e', 'gracas', 'ao', 'muito', 'espaco', 'de', 'que', 'se', 'dispunha', 'no', 'cortico', 'para', 'estender', 'a', 'roupa', ',', 'a', 'concorrencia', 'as', 'tinas', 'nao', 'se', 'fez', 'esperar', 'acudiram', 'lavadeiras', 'de', 'todos', 'os', 'pontos', 'da', 'cidade', ',', 'entre', 'elas', 'algumas', 'vindas', 'de', 'bem', 'longe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, mal vagava uma das casinhas, ou um quarto, um canto \n",
      "onde coubesse um colchão, surgia uma nuvem de pretendentes a disputá-los'\n",
      "Tokens gerados: ['e', ',', 'mal', 'vagava', 'uma', 'das', 'casinhas', ',', 'ou', 'um', 'quarto', ',', 'um', 'canto', 'onde', 'coubesse', 'um', 'colchao', ',', 'surgia', 'uma', 'nuvem', 'de', 'pretendentes', 'a', 'disputa-los']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E aquilo se foi constituindo numa grande lavanderia, agitada e barulhenta, com as suas cercas de varas, as suas \n",
      "hortaliças verdejantes e os seus jardinzinhos de três e quatro palmos, que apareciam como manchas alegres por entre a \n",
      "negrura das limosas tinas transbordantes e o revérbero das claras barracas de algodão cru, armadas sobre os lustrosos \n",
      "bancos de lavar'\n",
      "Tokens gerados: ['e', 'aquilo', 'se', 'foi', 'constituindo', 'numa', 'grande', 'lavanderia', ',', 'agitada', 'e', 'barulhenta', ',', 'com', 'as', 'suas', 'cercas', 'de', 'varas', ',', 'as', 'suas', 'hortalicas', 'verdejantes', 'e', 'os', 'seus', 'jardinzinhos', 'de', 'tres', 'e', 'quatro', 'palmos', ',', 'que', 'apareciam', 'como', 'manchas', 'alegres', 'por', 'entre', 'a', 'negrura', 'das', 'limosas', 'tinas', 'transbordantes', 'e', 'o', 'reverbero', 'das', 'claras', 'barracas', 'de', 'algodao', 'cru', ',', 'armadas', 'sobre', 'os', 'lustrosos', 'bancos', 'de', 'lavar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E os gotejantes jiraus, cobertos de roupa molhada, cintilavam ao sol, que nem lagos de metal branco'\n",
      "Tokens gerados: ['e', 'os', 'gotejantes', 'jiraus', ',', 'cobertos', 'de', 'roupa', 'molhada', ',', 'cintilavam', 'ao', 'sol', ',', 'que', 'nem', 'lagos', 'de', 'metal', 'branco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E naquela terra encharcada e fumegante, naquela umidade quente e lodosa, começou a minhocar, a esfervilhar, a \n",
      "crescer, um mundo, uma coisa viva, uma geração, que parecia brotar espontânea, ali mesmo, daquele lameiro, e \n",
      "multiplicar-se como larvas no esterco'\n",
      "Tokens gerados: ['e', 'naquela', 'terra', 'encharcada', 'e', 'fumegante', ',', 'naquela', 'umidade', 'quente', 'e', 'lodosa', ',', 'comecou', 'a', 'minhocar', ',', 'a', 'esfervilhar', ',', 'a', 'crescer', ',', 'um', 'mundo', ',', 'uma', 'coisa', 'viva', ',', 'uma', 'geracao', ',', 'que', 'parecia', 'brotar', 'espontanea', ',', 'ali', 'mesmo', ',', 'daquele', 'lameiro', ',', 'e', 'multiplicar-se', 'como', 'larvas', 'no', 'esterco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_o_cortico_aluisio_azevedo_cap_1.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Durante dois anos o cortiço prosperou de dia para dia, ganhando forças, socando-se de gente'\n",
      "Tokens gerados: ['durante', 'dois', 'anos', 'o', 'cortico', 'prosperou', 'de', 'dia', 'para', 'dia', ',', 'ganhando', 'forcas', ',', 'socando-se', 'de', 'gente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E ao lado o Miranda \n",
      "assustava-se, inquieto com aquela exuberância brutal de vida, aterrado defronte daquela floresta implacável que lhe \n",
      "crescia junto da casa, por debaixo das janelas, e cujas raízes, piores e mais grossas do que serpentes, minavam por toda \n",
      "a parte, ameaçando rebentar o chão em torno dela, rachando o solo e abalando tudo'\n",
      "Tokens gerados: ['e', 'ao', 'lado', 'o', 'miranda', 'assustava-se', ',', 'inquieto', 'com', 'aquela', 'exuberancia', 'brutal', 'de', 'vida', ',', 'aterrado', 'defronte', 'daquela', 'floresta', 'implacavel', 'que', 'lhe', 'crescia', 'junto', 'da', 'casa', ',', 'por', 'debaixo', 'das', 'janelas', ',', 'e', 'cujas', 'raizes', ',', 'piores', 'e', 'mais', 'grossas', 'do', 'que', 'serpentes', ',', 'minavam', 'por', 'toda', 'a', 'parte', ',', 'ameacando', 'rebentar', 'o', 'chao', 'em', 'torno', 'dela', ',', 'rachando', 'o', 'solo', 'e', 'abalando', 'tudo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Posto que lá na Rua do Hospício os seus negócios não corressem mal, custava-lhe a sofrer a escandalosa fortuna \n",
      "do vendeiro “aquele tipo! um miserável, um sujo, que não pusera nunca um paletó, e que vivia de cama e mesa com \n",
      "uma negra!” \n",
      "À noite e aos domingos ainda mais recrudescia o seu azedume, quando ele, recolhendo-se fatigado do serviço, \n",
      "deixava-se ficar estendido numa preguiçosa, junto à mesa da sala de jantar, e ouvia, a contragosto, o grosseiro rumor \n",
      "que vinha da estalagem numa exalação forte de animais cansados'\n",
      "Tokens gerados: ['posto', 'que', 'la', 'na', 'rua', 'do', 'hospicio', 'os', 'seus', 'negocios', 'nao', 'corressem', 'mal', ',', 'custava-lhe', 'a', 'sofrer', 'a', 'escandalosa', 'fortuna', 'do', 'vendeiro', '“', 'aquele', 'tipo', 'um', 'miseravel', ',', 'um', 'sujo', ',', 'que', 'nao', 'pusera', 'nunca', 'um', 'paleto', ',', 'e', 'que', 'vivia', 'de', 'cama', 'e', 'mesa', 'com', 'uma', 'negra', '”', 'a', 'noite', 'e', 'aos', 'domingos', 'ainda', 'mais', 'recrudescia', 'o', 'seu', 'azedume', ',', 'quando', 'ele', ',', 'recolhendo-se', 'fatigado', 'do', 'servico', ',', 'deixava-se', 'ficar', 'estendido', 'numa', 'preguicosa', ',', 'junto', 'a', 'mesa', 'da', 'sala', 'de', 'jantar', ',', 'e', 'ouvia', ',', 'a', 'contragosto', ',', 'o', 'grosseiro', 'rumor', 'que', 'vinha', 'da', 'estalagem', 'numa', 'exalacao', 'forte', 'de', 'animais', 'cansados']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não podia chegar à janela sem receber no rosto \n",
      "aquele bafo, quente e sensual, que o embebedava com o seu fartum de bestas no coito'\n",
      "Tokens gerados: ['nao', 'podia', 'chegar', 'a', 'janela', 'sem', 'receber', 'no', 'rosto', 'aquele', 'bafo', ',', 'quente', 'e', 'sensual', ',', 'que', 'o', 'embebedava', 'com', 'o', 'seu', 'fartum', 'de', 'bestas', 'no', 'coito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E depois, fechado no quarto de dormir, indiferente e habituado às torpezas carnais da mulher, isento já dos \n",
      "primitivos sobressaltos que lhe faziam, a ele, ferver o sangue e perder a tramontana, era ainda a prosperidade do vizinho \n",
      "o que lhe obsedava o espírito, enegrecendo-lhe a alma com um feio ressentimento de despeito'\n",
      "Tokens gerados: ['e', 'depois', ',', 'fechado', 'no', 'quarto', 'de', 'dormir', ',', 'indiferente', 'e', 'habituado', 'as', 'torpezas', 'carnais', 'da', 'mulher', ',', 'isento', 'ja', 'dos', 'primitivos', 'sobressaltos', 'que', 'lhe', 'faziam', ',', 'a', 'ele', ',', 'ferver', 'o', 'sangue', 'e', 'perder', 'a', 'tramontana', ',', 'era', 'ainda', 'a', 'prosperidade', 'do', 'vizinho', 'o', 'que', 'lhe', 'obsedava', 'o', 'espirito', ',', 'enegrecendo-lhe', 'a', 'alma', 'com', 'um', 'feio', 'ressentimento', 'de', 'despeito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha inveja do outro, daquele outro português que fizera fortuna, sem precisar roer nenhum chifre; daquele outro \n",
      "que, para ser mais rico três vezes do que ele, não teve de casar com a filha do patrão ou com a bastarda de algum \n",
      "fazendeiro freguês da casa! \n",
      "Mas então, ele Miranda, que se supunha a última expressão da ladinagem e da esperteza; ele, que, logo depois do \n",
      "seu casamento, respondendo para Portugal a um ex-colega que o felicitava, dissera que o Brasil era uma cavalgadura \n",
      "carregada de dinheiro, cujas rédeas um homem fino empolgava facilmente; ele, que se tinha na conta de invencível \n",
      "matreiro, não passava afinal de um pedaço de asno comparado com o seu vizinho! Pensara fazer-se senhor do Brasil e \n",
      "fizera-se escravo de uma brasileira mal-educada e sem escrúpulos de virtude! Imaginara-se talhado para grandes \n",
      "conquistas, e não passava de uma vitima ridícula e sofredora!'\n",
      "Tokens gerados: ['tinha', 'inveja', 'do', 'outro', ',', 'daquele', 'outro', 'portugues', 'que', 'fizera', 'fortuna', ',', 'sem', 'precisar', 'roer', 'nenhum', 'chifre', 'daquele', 'outro', 'que', ',', 'para', 'ser', 'mais', 'rico', 'tres', 'vezes', 'do', 'que', 'ele', ',', 'nao', 'teve', 'de', 'casar', 'com', 'a', 'filha', 'do', 'patrao', 'ou', 'com', 'a', 'bastarda', 'de', 'algum', 'fazendeiro', 'fregues', 'da', 'casa', 'mas', 'entao', ',', 'ele', 'miranda', ',', 'que', 'se', 'supunha', 'a', 'ultima', 'expressao', 'da', 'ladinagem', 'e', 'da', 'esperteza', 'ele', ',', 'que', ',', 'logo', 'depois', 'do', 'seu', 'casamento', ',', 'respondendo', 'para', 'portugal', 'a', 'um', 'ex-colega', 'que', 'o', 'felicitava', ',', 'dissera', 'que', 'o', 'brasil', 'era', 'uma', 'cavalgadura', 'carregada', 'de', 'dinheiro', ',', 'cujas', 'redeas', 'um', 'homem', 'fino', 'empolgava', 'facilmente', 'ele', ',', 'que', 'se', 'tinha', 'na', 'conta', 'de', 'invencivel', 'matreiro', ',', 'nao', 'passava', 'afinal', 'de', 'um', 'pedaco', 'de', 'asno', 'comparado', 'com', 'o', 'seu', 'vizinho', 'pensara', 'fazer-se', 'senhor', 'do', 'brasil', 'e', 'fizera-se', 'escravo', 'de', 'uma', 'brasileira', 'mal-educada', 'e', 'sem', 'escrupulos', 'de', 'virtude', 'imaginara-se', 'talhado', 'para', 'grandes', 'conquistas', ',', 'e', 'nao', 'passava', 'de', 'uma', 'vitima', 'ridicula', 'e', 'sofredora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sim! no fim de contas qual fora a sua África?'\n",
      "Tokens gerados: ['sim', 'no', 'fim', 'de', 'contas', 'qual', 'fora', 'a', 'sua', 'africa', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Enriquecera um pouco, é verdade, mas como? a que preço? hipotecando-se a um diabo, que lhe trouxera oitenta contos \n",
      "de réis, mas incalculáveis milhões de desgostos e vergonhas! Arranjara a vida, sim, mas teve de aturar eternamente uma \n",
      "mulher que ele odiava! E do que afinal lhe aproveitar tudo isso? Qual era afinal a sua grande existência? Do inferno da \n",
      "casa para o purgatório do trabalho e vice-versa! Invejável sorte, não havia dúvida! \n",
      "Na dolorosa incerteza de que Zulmira fosse sua filha, o desgraçado nem sequer gozava o prazer de ser pai'\n",
      "Tokens gerados: ['enriquecera', 'um', 'pouco', ',', 'e', 'verdade', ',', 'mas', 'como', '?', 'a', 'que', 'preco', '?', 'hipotecando-se', 'a', 'um', 'diabo', ',', 'que', 'lhe', 'trouxera', 'oitenta', 'contos', 'de', 'reis', ',', 'mas', 'incalculaveis', 'milhoes', 'de', 'desgostos', 'e', 'vergonhas', 'arranjara', 'a', 'vida', ',', 'sim', ',', 'mas', 'teve', 'de', 'aturar', 'eternamente', 'uma', 'mulher', 'que', 'ele', 'odiava', 'e', 'do', 'que', 'afinal', 'lhe', 'aproveitar', 'tudo', 'isso', '?', 'qual', 'era', 'afinal', 'a', 'sua', 'grande', 'existencia', '?', 'do', 'inferno', 'da', 'casa', 'para', 'o', 'purgatorio', 'do', 'trabalho', 'e', 'vice-versa', 'invejavel', 'sorte', ',', 'nao', 'havia', 'duvida', 'na', 'dolorosa', 'incerteza', 'de', 'que', 'zulmira', 'fosse', 'sua', 'filha', ',', 'o', 'desgracado', 'nem', 'sequer', 'gozava', 'o', 'prazer', 'de', 'ser', 'pai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se ela, \n",
      "em vez de nascer de Estela, fora uma enjeitadinha recolhida por ele, é natural que a amasse e então a vida lhe correria \n",
      "de outro modo; mas naquelas condições, a pobre criança nada mais representava que o documento vivo do ludibrio \n",
      "materno, e o Miranda estendia até à inocentezinha d'África o ódio que sustentava contra a esposa'\n",
      "Tokens gerados: ['se', 'ela', ',', 'em', 'vez', 'de', 'nascer', 'de', 'estela', ',', 'fora', 'uma', 'enjeitadinha', 'recolhida', 'por', 'ele', ',', 'e', 'natural', 'que', 'a', 'amasse', 'e', 'entao', 'a', 'vida', 'lhe', 'correria', 'de', 'outro', 'modo', 'mas', 'naquelas', 'condicoes', ',', 'a', 'pobre', 'crianca', 'nada', 'mais', 'representava', 'que', 'o', 'documento', 'vivo', 'do', 'ludibrio', 'materno', ',', 'e', 'o', 'miranda', 'estendia', 'ate', 'a', 'inocentezinha', \"d'africa\", 'o', 'odio', 'que', 'sustentava', 'contra', 'a', 'esposa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma espiga a tal da sua vida! \n",
      "— Fui uma besta! resumiu ele, em voz alta, apeando-se da cama, onde se havia recolhido inutilmente'\n",
      "Tokens gerados: ['uma', 'espiga', 'a', 'tal', 'da', 'sua', 'vida', '—', 'fui', 'uma', 'besta', 'resumiu', 'ele', ',', 'em', 'voz', 'alta', ',', 'apeando-se', 'da', 'cama', ',', 'onde', 'se', 'havia', 'recolhido', 'inutilmente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E pôs-se a passear no quarto sem vontade de dormir, sentindo que a febre daquela inveja lhe estorricava os \n",
      "miolos'\n",
      "Tokens gerados: ['e', 'pos-se', 'a', 'passear', 'no', 'quarto', 'sem', 'vontade', 'de', 'dormir', ',', 'sentindo', 'que', 'a', 'febre', 'daquela', 'inveja', 'lhe', 'estorricava', 'os', 'miolos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Feliz e esperto era o João Romão! esse, sim, senhor! Para esse é que havia de ser a vida!'\n",
      "Tokens gerados: ['feliz', 'e', 'esperto', 'era', 'o', 'joao', 'romao', 'esse', ',', 'sim', ',', 'senhor', 'para', 'esse', 'e', 'que', 'havia', 'de', 'ser', 'a', 'vida']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Filho da mãe, que \n",
      "estava hoje tão livre e desembaraçado como no dia em que chegou da terra sem um vintém de seu! esse, sim, que era \n",
      "moço e podia ainda gozar muito, porque quando mesmo viesse a casar e a mulher lhe saísse uma outra Estela era só \n",
      "mandá-la para o diabo com um pontapé! Podia fazê-lo! Para esse é que era o Brasil! \n",
      "— Fui uma besta! repisava ele sem conseguir conformar-se com a felicidade do vendeiro'\n",
      "Tokens gerados: ['filho', 'da', 'mae', ',', 'que', 'estava', 'hoje', 'tao', 'livre', 'e', 'desembaracado', 'como', 'no', 'dia', 'em', 'que', 'chegou', 'da', 'terra', 'sem', 'um', 'vintem', 'de', 'seu', 'esse', ',', 'sim', ',', 'que', 'era', 'moco', 'e', 'podia', 'ainda', 'gozar', 'muito', ',', 'porque', 'quando', 'mesmo', 'viesse', 'a', 'casar', 'e', 'a', 'mulher', 'lhe', 'saisse', 'uma', 'outra', 'estela', 'era', 'so', 'manda-la', 'para', 'o', 'diabo', 'com', 'um', 'pontape', 'podia', 'faze-lo', 'para', 'esse', 'e', 'que', 'era', 'o', 'brasil', '—', 'fui', 'uma', 'besta', 'repisava', 'ele', 'sem', 'conseguir', 'conformar-se', 'com', 'a', 'felicidade', 'do', 'vendeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma grandíssima! No \n",
      "fim de contas que diabo possuo eu?'\n",
      "Tokens gerados: ['uma', 'grandissima', 'no', 'fim', 'de', 'contas', 'que', 'diabo', 'possuo', 'eu', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma casa de negócio, da qual não posso separar-me sem comprometer o que lá \n",
      "está enterrado! um capital metido numa rede de transações que não se liquidam nunca, e cada vez mais se complicam e \n",
      "mais me grudam ao estupor desta terra, onde deixarei a casca! Que tenho de meu, se a alma do meu crédito é o dote, que \n",
      "me trouxe aquela sem-vergonha e que a ela me prende como a peste da casa comercial me prende a esta Costa d’África? \n",
      "Foi da supuração fétida destas idéias que se formou no coração vazio do Miranda um novo ideal — o título'\n",
      "Tokens gerados: ['uma', 'casa', 'de', 'negocio', ',', 'da', 'qual', 'nao', 'posso', 'separar-me', 'sem', 'comprometer', 'o', 'que', 'la', 'esta', 'enterrado', 'um', 'capital', 'metido', 'numa', 'rede', 'de', 'transacoes', 'que', 'nao', 'se', 'liquidam', 'nunca', ',', 'e', 'cada', 'vez', 'mais', 'se', 'complicam', 'e', 'mais', 'me', 'grudam', 'ao', 'estupor', 'desta', 'terra', ',', 'onde', 'deixarei', 'a', 'casca', 'que', 'tenho', 'de', 'meu', ',', 'se', 'a', 'alma', 'do', 'meu', 'credito', 'e', 'o', 'dote', ',', 'que', 'me', 'trouxe', 'aquela', 'sem-vergonha', 'e', 'que', 'a', 'ela', 'me', 'prende', 'como', 'a', 'peste', 'da', 'casa', 'comercial', 'me', 'prende', 'a', 'esta', 'costa', 'd', '’', 'africa', '?', 'foi', 'da', 'supuracao', 'fetida', 'destas', 'ideias', 'que', 'se', 'formou', 'no', 'coracao', 'vazio', 'do', 'miranda', 'um', 'novo', 'ideal', '—', 'o', 'titulo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Faltando-lhe temperamento próprio para os vícios fortes que enchem a vida de um homem; sem família, a quem amar e \n",
      "sem imaginação para poder gozar com as prostitutas, o náufrago agarrou-se àquela tábua, como um agonizante, \n",
      "consciente da morte, que se apega à esperança de uma vida futura'\n",
      "Tokens gerados: ['faltando-lhe', 'temperamento', 'proprio', 'para', 'os', 'vicios', 'fortes', 'que', 'enchem', 'a', 'vida', 'de', 'um', 'homem', 'sem', 'familia', ',', 'a', 'quem', 'amar', 'e', 'sem', 'imaginacao', 'para', 'poder', 'gozar', 'com', 'as', 'prostitutas', ',', 'o', 'naufrago', 'agarrou-se', 'aquela', 'tabua', ',', 'como', 'um', 'agonizante', ',', 'consciente', 'da', 'morte', ',', 'que', 'se', 'apega', 'a', 'esperanca', 'de', 'uma', 'vida', 'futura']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A vaidade de Estela, que a principio lhe tirava dos \n",
      "lábios incrédulos sorrisos de mofa, agora lhe comprazia à farta'\n",
      "Tokens gerados: ['a', 'vaidade', 'de', 'estela', ',', 'que', 'a', 'principio', 'lhe', 'tirava', 'dos', 'labios', 'incredulos', 'sorrisos', 'de', 'mofa', ',', 'agora', 'lhe', 'comprazia', 'a', 'farta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Procurou capacitar-se de que ela com efeito herdara \n",
      "sangue nobre, que ele, por sua vez, se não o tinha herdado, trouxera-o por natureza própria, o que devia valer mais \n",
      "ainda; e desde então principiou a sonhar com um baronato, fazendo disso o objeto querido da sua existência, muito \n",
      "satisfeito no intimo por ter afinal descoberto uma coisa em que podia empregar dinheiro, sem ter, nunca mais, de \n",
      "restituí-lo à mulher, nem ter de deixá-lo a pessoa alguma'\n",
      "Tokens gerados: ['procurou', 'capacitar-se', 'de', 'que', 'ela', 'com', 'efeito', 'herdara', 'sangue', 'nobre', ',', 'que', 'ele', ',', 'por', 'sua', 'vez', ',', 'se', 'nao', 'o', 'tinha', 'herdado', ',', 'trouxera-o', 'por', 'natureza', 'propria', ',', 'o', 'que', 'devia', 'valer', 'mais', 'ainda', 'e', 'desde', 'entao', 'principiou', 'a', 'sonhar', 'com', 'um', 'baronato', ',', 'fazendo', 'disso', 'o', 'objeto', 'querido', 'da', 'sua', 'existencia', ',', 'muito', 'satisfeito', 'no', 'intimo', 'por', 'ter', 'afinal', 'descoberto', 'uma', 'coisa', 'em', 'que', 'podia', 'empregar', 'dinheiro', ',', 'sem', 'ter', ',', 'nunca', 'mais', ',', 'de', 'restitui-lo', 'a', 'mulher', ',', 'nem', 'ter', 'de', 'deixa-lo', 'a', 'pessoa', 'alguma']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Semelhante preocupação modificou-o em extremo'\n",
      "Tokens gerados: ['semelhante', 'preocupacao', 'modificou-o', 'em', 'extremo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deu logo para fingir-se escravo das conveniências, afetando \n",
      "escrúpulos sociais, empertigando-se quanto podia e disfarçando a sua inveja pelo vizinho com um desdenhoso ar de \n",
      "superioridade condescendente'\n",
      "Tokens gerados: ['deu', 'logo', 'para', 'fingir-se', 'escravo', 'das', 'conveniencias', ',', 'afetando', 'escrupulos', 'sociais', ',', 'empertigando-se', 'quanto', 'podia', 'e', 'disfarcando', 'a', 'sua', 'inveja', 'pelo', 'vizinho', 'com', 'um', 'desdenhoso', 'ar', 'de', 'superioridade', 'condescendente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao passar-lhe todos os dias pela venda, cumprimentava-o com proteção, sorrindo sem rir \n",
      "e fechando logo a cara em seguida, muito sério'\n",
      "Tokens gerados: ['ao', 'passar-lhe', 'todos', 'os', 'dias', 'pela', 'venda', ',', 'cumprimentava-o', 'com', 'protecao', ',', 'sorrindo', 'sem', 'rir', 'e', 'fechando', 'logo', 'a', 'cara', 'em', 'seguida', ',', 'muito', 'serio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dados os primeiros passos para a compra do titulo abriu a casa e deu festas'\n",
      "Tokens gerados: ['dados', 'os', 'primeiros', 'passos', 'para', 'a', 'compra', 'do', 'titulo', 'abriu', 'a', 'casa', 'e', 'deu', 'festas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mulher, posto que lhe apontassem \n",
      "já os cabelos brancos, rejubilou com isso'\n",
      "Tokens gerados: ['a', 'mulher', ',', 'posto', 'que', 'lhe', 'apontassem', 'ja', 'os', 'cabelos', 'brancos', ',', 'rejubilou', 'com', 'isso']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Zulmira tinha então doze para treze anos e era o tipo acabado da fluminense; pálida, magrinha, com pequeninas \n",
      "manchas roxas nas mucosas do nariz, das pálpebras e dos lábios, faces levemente pintalgadas de sardas'\n",
      "Tokens gerados: ['zulmira', 'tinha', 'entao', 'doze', 'para', 'treze', 'anos', 'e', 'era', 'o', 'tipo', 'acabado', 'da', 'fluminense', 'palida', ',', 'magrinha', ',', 'com', 'pequeninas', 'manchas', 'roxas', 'nas', 'mucosas', 'do', 'nariz', ',', 'das', 'palpebras', 'e', 'dos', 'labios', ',', 'faces', 'levemente', 'pintalgadas', 'de', 'sardas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Respirava o \n",
      "tom úmido das flores noturnas, uma brancura fria de magnólia; cabelos castanho-claros, mãos quase transparentes, \n",
      "unhas moles e curtas, como as da mãe, dentes pouco mais claros do que a cútis do rosto, pés pequeninos, quadril estreito \n",
      "mas os olhos grandes, negros, vivos e maliciosos'\n",
      "Tokens gerados: ['respirava', 'o', 'tom', 'umido', 'das', 'flores', 'noturnas', ',', 'uma', 'brancura', 'fria', 'de', 'magnolia', 'cabelos', 'castanho-claros', ',', 'maos', 'quase', 'transparentes', ',', 'unhas', 'moles', 'e', 'curtas', ',', 'como', 'as', 'da', 'mae', ',', 'dentes', 'pouco', 'mais', 'claros', 'do', 'que', 'a', 'cutis', 'do', 'rosto', ',', 'pes', 'pequeninos', ',', 'quadril', 'estreito', 'mas', 'os', 'olhos', 'grandes', ',', 'negros', ',', 'vivos', 'e', 'maliciosos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por essa época, justamente, chegava de Minas, recomendado ao pai dela, o filho de um fazendeiro \n",
      "importantíssimo que dava belos lucros à casa comercial de Miranda e que era talvez o melhor freguês que este possuía \n",
      "no interior'\n",
      "Tokens gerados: ['por', 'essa', 'epoca', ',', 'justamente', ',', 'chegava', 'de', 'minas', ',', 'recomendado', 'ao', 'pai', 'dela', ',', 'o', 'filho', 'de', 'um', 'fazendeiro', 'importantissimo', 'que', 'dava', 'belos', 'lucros', 'a', 'casa', 'comercial', 'de', 'miranda', 'e', 'que', 'era', 'talvez', 'o', 'melhor', 'fregues', 'que', 'este', 'possuia', 'no', 'interior']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O rapaz chamava-se Henrique, tinha quinze anos e vinha terminar na corte alguns preparatórios que lhe faltavam \n",
      "para entrar na Academia de Medicina'\n",
      "Tokens gerados: ['o', 'rapaz', 'chamava-se', 'henrique', ',', 'tinha', 'quinze', 'anos', 'e', 'vinha', 'terminar', 'na', 'corte', 'alguns', 'preparatorios', 'que', 'lhe', 'faltavam', 'para', 'entrar', 'na', 'academia', 'de', 'medicina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Miranda hospedou-o no seu sobrado da Rua do Hospício mas o estudante \n",
      "queixou-se, no fim de alguns dias, de que ai ficava mal acomodado, e o negociante, a quem não convinha \n",
      "desagradar-lhe, carregou com ele para a sua residência particular de Botafogo'\n",
      "Tokens gerados: ['miranda', 'hospedou-o', 'no', 'seu', 'sobrado', 'da', 'rua', 'do', 'hospicio', 'mas', 'o', 'estudante', 'queixou-se', ',', 'no', 'fim', 'de', 'alguns', 'dias', ',', 'de', 'que', 'ai', 'ficava', 'mal', 'acomodado', ',', 'e', 'o', 'negociante', ',', 'a', 'quem', 'nao', 'convinha', 'desagradar-lhe', ',', 'carregou', 'com', 'ele', 'para', 'a', 'sua', 'residencia', 'particular', 'de', 'botafogo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Henrique era bonitinho, cheio de acanhamentos, com umas delicadezas de menina'\n",
      "Tokens gerados: ['henrique', 'era', 'bonitinho', ',', 'cheio', 'de', 'acanhamentos', ',', 'com', 'umas', 'delicadezas', 'de', 'menina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Parecia muito cuidadoso dos \n",
      "seus estudos e tão pouco extravagante e gastador, que não despendia um vintém fora das necessidade de primeira \n",
      "urgência'\n",
      "Tokens gerados: ['parecia', 'muito', 'cuidadoso', 'dos', 'seus', 'estudos', 'e', 'tao', 'pouco', 'extravagante', 'e', 'gastador', ',', 'que', 'nao', 'despendia', 'um', 'vintem', 'fora', 'das', 'necessidade', 'de', 'primeira', 'urgencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De resto, a não ser de manhã para as aulas, que ia sempre com o Miranda, não arredava pé de casa senão em \n",
      "companhia da família, deste'\n",
      "Tokens gerados: ['de', 'resto', ',', 'a', 'nao', 'ser', 'de', 'manha', 'para', 'as', 'aulas', ',', 'que', 'ia', 'sempre', 'com', 'o', 'miranda', ',', 'nao', 'arredava', 'pe', 'de', 'casa', 'senao', 'em', 'companhia', 'da', 'familia', ',', 'deste']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dona Estela, no cabo de pouco tempo, mostrou por ele estima quase maternal e \n",
      "encarregou-se de tomar conta da sua mesada, mesada posta pelo negociante, visto que o Henriquinho tinha ordem \n",
      "franca do pai'\n",
      "Tokens gerados: ['dona', 'estela', ',', 'no', 'cabo', 'de', 'pouco', 'tempo', ',', 'mostrou', 'por', 'ele', 'estima', 'quase', 'maternal', 'e', 'encarregou-se', 'de', 'tomar', 'conta', 'da', 'sua', 'mesada', ',', 'mesada', 'posta', 'pelo', 'negociante', ',', 'visto', 'que', 'o', 'henriquinho', 'tinha', 'ordem', 'franca', 'do', 'pai']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nunca pedia dinheiro; quando precisava de qualquer coisa, reclamava-a de Dona Estela, que por sua vez \n",
      "encarregava o marido de comprá-la, sendo o objeto lançado na conta do fazendeiro com uma comissão de usurário'\n",
      "Tokens gerados: ['nunca', 'pedia', 'dinheiro', 'quando', 'precisava', 'de', 'qualquer', 'coisa', ',', 'reclamava-a', 'de', 'dona', 'estela', ',', 'que', 'por', 'sua', 'vez', 'encarregava', 'o', 'marido', 'de', 'compra-la', ',', 'sendo', 'o', 'objeto', 'lancado', 'na', 'conta', 'do', 'fazendeiro', 'com', 'uma', 'comissao', 'de', 'usurario']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sua \n",
      "hospedagem custava duzentos e cinqüenta mil-réis por mês, do que ele todavia não tinha conhecimento, nem queria ter'\n",
      "Tokens gerados: ['sua', 'hospedagem', 'custava', 'duzentos', 'e', 'cinquenta', 'mil-reis', 'por', 'mes', ',', 'do', 'que', 'ele', 'todavia', 'nao', 'tinha', 'conhecimento', ',', 'nem', 'queria', 'ter']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nada lhe faltava, e os criados da casa o respeitavam como a um filho do próprio senhor'\n",
      "Tokens gerados: ['nada', 'lhe', 'faltava', ',', 'e', 'os', 'criados', 'da', 'casa', 'o', 'respeitavam', 'como', 'a', 'um', 'filho', 'do', 'proprio', 'senhor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'À noite, às vezes, quando o tempo estava bom, Dona Estela saia com ele, a filha e um moleque, o Valentim, a \n",
      "darem uma volta ate à praia e, em tendo convite para qualquer festa em casa das amigas, levava-o em sua companhia'\n",
      "Tokens gerados: ['a', 'noite', ',', 'as', 'vezes', ',', 'quando', 'o', 'tempo', 'estava', 'bom', ',', 'dona', 'estela', 'saia', 'com', 'ele', ',', 'a', 'filha', 'e', 'um', 'moleque', ',', 'o', 'valentim', ',', 'a', 'darem', 'uma', 'volta', 'ate', 'a', 'praia', 'e', ',', 'em', 'tendo', 'convite', 'para', 'qualquer', 'festa', 'em', 'casa', 'das', 'amigas', ',', 'levava-o', 'em', 'sua', 'companhia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A criadagem da família, do Miranda compunha-se de Isaura, mulata ainda moça, moleirona e tola, que gastava \n",
      "todo o vintenzinho que pilhava em comprar capilé na venda de João Romão; uma negrinha virgem, chamada Leonor, \n",
      "muito ligeira e viva, lisa e seca como um moleque, conhecendo de orelha, sem lhe faltar um termo, a vasta tecnologia da \n",
      "obscenidade, e dizendo, sempre que os caixeiros ou os fregueses da taverna, só para mexer com ela, lhe davam \n",
      "atracações: “Óia, que eu me queixo ao juiz de orfe!”, e finalmente o tal Valentim, filho de uma escrava que foi de Dona \n",
      "Estela e a quem esta havia alforriado'\n",
      "Tokens gerados: ['a', 'criadagem', 'da', 'familia', ',', 'do', 'miranda', 'compunha-se', 'de', 'isaura', ',', 'mulata', 'ainda', 'moca', ',', 'moleirona', 'e', 'tola', ',', 'que', 'gastava', 'todo', 'o', 'vintenzinho', 'que', 'pilhava', 'em', 'comprar', 'capile', 'na', 'venda', 'de', 'joao', 'romao', 'uma', 'negrinha', 'virgem', ',', 'chamada', 'leonor', ',', 'muito', 'ligeira', 'e', 'viva', ',', 'lisa', 'e', 'seca', 'como', 'um', 'moleque', ',', 'conhecendo', 'de', 'orelha', ',', 'sem', 'lhe', 'faltar', 'um', 'termo', ',', 'a', 'vasta', 'tecnologia', 'da', 'obscenidade', ',', 'e', 'dizendo', ',', 'sempre', 'que', 'os', 'caixeiros', 'ou', 'os', 'fregueses', 'da', 'taverna', ',', 'so', 'para', 'mexer', 'com', 'ela', ',', 'lhe', 'davam', 'atracacoes', '“', 'oia', ',', 'que', 'eu', 'me', 'queixo', 'ao', 'juiz', 'de', 'orfe', '”', ',', 'e', 'finalmente', 'o', 'tal', 'valentim', ',', 'filho', 'de', 'uma', 'escrava', 'que', 'foi', 'de', 'dona', 'estela', 'e', 'a', 'quem', 'esta', 'havia', 'alforriado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mulher do Miranda tinha por este moleque uma afeição sem limites: dava-lhe toda a liberdade, dinheiro, \n",
      "presentes, levava-o consigo a passeio, trazia-o bem vestido e muita vez chegou a fazer ciúmes à filha, de tão solicita que \n",
      "se mostrava com ele'\n",
      "Tokens gerados: ['a', 'mulher', 'do', 'miranda', 'tinha', 'por', 'este', 'moleque', 'uma', 'afeicao', 'sem', 'limites', 'dava-lhe', 'toda', 'a', 'liberdade', ',', 'dinheiro', ',', 'presentes', ',', 'levava-o', 'consigo', 'a', 'passeio', ',', 'trazia-o', 'bem', 'vestido', 'e', 'muita', 'vez', 'chegou', 'a', 'fazer', 'ciumes', 'a', 'filha', ',', 'de', 'tao', 'solicita', 'que', 'se', 'mostrava', 'com', 'ele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois se a caprichosa senhora ralhava com Zulmira por causa do negrinho! Pois, se quando se \n",
      "queixavam os dois, um contra o outro, ela nunca dava razão à filha! Pois se o que havia de melhor na casa era para o \n",
      "Valentim! Pois, se quando foi este atacado de bexigas e o Miranda, apesar das súplicas e dos protestos da esposa, \n",
      "mandou-o para um hospital, Dona Estela chorava todos os dias e durante a ausência dele não tocou piano, nem cantou, \n",
      "nem mostrou os dentes a ninguém? E o pobre Miranda, se não queria sofrer impertinências da mulher e ouvir \n",
      "sensaborias defronte dos criados, tinha de dar ao moleque toda a consideração e fazer-lhe humildemente todas as \n",
      "vontades'\n",
      "Tokens gerados: ['pois', 'se', 'a', 'caprichosa', 'senhora', 'ralhava', 'com', 'zulmira', 'por', 'causa', 'do', 'negrinho', 'pois', ',', 'se', 'quando', 'se', 'queixavam', 'os', 'dois', ',', 'um', 'contra', 'o', 'outro', ',', 'ela', 'nunca', 'dava', 'razao', 'a', 'filha', 'pois', 'se', 'o', 'que', 'havia', 'de', 'melhor', 'na', 'casa', 'era', 'para', 'o', 'valentim', 'pois', ',', 'se', 'quando', 'foi', 'este', 'atacado', 'de', 'bexigas', 'e', 'o', 'miranda', ',', 'apesar', 'das', 'suplicas', 'e', 'dos', 'protestos', 'da', 'esposa', ',', 'mandou-o', 'para', 'um', 'hospital', ',', 'dona', 'estela', 'chorava', 'todos', 'os', 'dias', 'e', 'durante', 'a', 'ausencia', 'dele', 'nao', 'tocou', 'piano', ',', 'nem', 'cantou', ',', 'nem', 'mostrou', 'os', 'dentes', 'a', 'ninguem', '?', 'e', 'o', 'pobre', 'miranda', ',', 'se', 'nao', 'queria', 'sofrer', 'impertinencias', 'da', 'mulher', 'e', 'ouvir', 'sensaborias', 'defronte', 'dos', 'criados', ',', 'tinha', 'de', 'dar', 'ao', 'moleque', 'toda', 'a', 'consideracao', 'e', 'fazer-lhe', 'humildemente', 'todas', 'as', 'vontades']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia ainda, sob as telhas do negociante, um outro hóspede além do Henrique, o velho Botelho'\n",
      "Tokens gerados: ['havia', 'ainda', ',', 'sob', 'as', 'telhas', 'do', 'negociante', ',', 'um', 'outro', 'hospede', 'alem', 'do', 'henrique', ',', 'o', 'velho', 'botelho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Este, porém, na \n",
      "qualidade de parasita'\n",
      "Tokens gerados: ['este', ',', 'porem', ',', 'na', 'qualidade', 'de', 'parasita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era um pobre-diabo caminhando para os setenta anos, antipático, cabelo branco, curto e duro, como escova, barba \n",
      "e bigode do mesmo teor; muito macilento, com uns óculos redondos que lhe aumentavam o tamanho da pupila e \n",
      "davam-lhe à cara uma expressão de abutre, perfeitamente de acordo com o seu nariz adunco e com a sua boca sem \n",
      "lábios: viam-se-lhe ainda todos os dentes, mas, tão gastos, que pareciam limados até ao meio'\n",
      "Tokens gerados: ['era', 'um', 'pobre-diabo', 'caminhando', 'para', 'os', 'setenta', 'anos', ',', 'antipatico', ',', 'cabelo', 'branco', ',', 'curto', 'e', 'duro', ',', 'como', 'escova', ',', 'barba', 'e', 'bigode', 'do', 'mesmo', 'teor', 'muito', 'macilento', ',', 'com', 'uns', 'oculos', 'redondos', 'que', 'lhe', 'aumentavam', 'o', 'tamanho', 'da', 'pupila', 'e', 'davam-lhe', 'a', 'cara', 'uma', 'expressao', 'de', 'abutre', ',', 'perfeitamente', 'de', 'acordo', 'com', 'o', 'seu', 'nariz', 'adunco', 'e', 'com', 'a', 'sua', 'boca', 'sem', 'labios', 'viam-se-lhe', 'ainda', 'todos', 'os', 'dentes', ',', 'mas', ',', 'tao', 'gastos', ',', 'que', 'pareciam', 'limados', 'ate', 'ao', 'meio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Andava sempre de preto, \n",
      "com um guarda-chuva debaixo do braço e um chapéu de Braga enterrado nas orelhas'\n",
      "Tokens gerados: ['andava', 'sempre', 'de', 'preto', ',', 'com', 'um', 'guarda-chuva', 'debaixo', 'do', 'braco', 'e', 'um', 'chapeu', 'de', 'braga', 'enterrado', 'nas', 'orelhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fora em seu tempo empregado do \n",
      "comércio, depois corretor de escravos; contava mesmo que estivera mais de uma vez na África negociando negros por \n",
      "sua conta'\n",
      "Tokens gerados: ['fora', 'em', 'seu', 'tempo', 'empregado', 'do', 'comercio', ',', 'depois', 'corretor', 'de', 'escravos', 'contava', 'mesmo', 'que', 'estivera', 'mais', 'de', 'uma', 'vez', 'na', 'africa', 'negociando', 'negros', 'por', 'sua', 'conta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Atirou-se muito às especulações; durante a guerra do Paraguai ainda ganhara forte, chegando a ser bem rico; \n",
      "mas a roda desandou e, de malogro em malogro, foi-lhe escapando tudo por entre as suas garras de ave de rapina'\n",
      "Tokens gerados: ['atirou-se', 'muito', 'as', 'especulacoes', 'durante', 'a', 'guerra', 'do', 'paraguai', 'ainda', 'ganhara', 'forte', ',', 'chegando', 'a', 'ser', 'bem', 'rico', 'mas', 'a', 'roda', 'desandou', 'e', ',', 'de', 'malogro', 'em', 'malogro', ',', 'foi-lhe', 'escapando', 'tudo', 'por', 'entre', 'as', 'suas', 'garras', 'de', 'ave', 'de', 'rapina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E \n",
      "agora, coitado, já velho, comido de desilusões, cheio de hemorróidas, via-se totalmente sem recursos e vegetava à \n",
      "sombra do Mirada, com quem por muitos anos trabalhou em rapaz, sob as ordens do mesmo patrão, e de quem se \n",
      "conservara amigo, a principio por acaso e mais tarde por necessidade'\n",
      "Tokens gerados: ['e', 'agora', ',', 'coitado', ',', 'ja', 'velho', ',', 'comido', 'de', 'desilusoes', ',', 'cheio', 'de', 'hemorroidas', ',', 'via-se', 'totalmente', 'sem', 'recursos', 'e', 'vegetava', 'a', 'sombra', 'do', 'mirada', ',', 'com', 'quem', 'por', 'muitos', 'anos', 'trabalhou', 'em', 'rapaz', ',', 'sob', 'as', 'ordens', 'do', 'mesmo', 'patrao', ',', 'e', 'de', 'quem', 'se', 'conservara', 'amigo', ',', 'a', 'principio', 'por', 'acaso', 'e', 'mais', 'tarde', 'por', 'necessidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Devorava-o, noite e dia, uma implacável amargura, uma surda tristeza de vencido, um desespero impotente, \n",
      "contra tudo e contra todos, por não lhe ter sido possível empolgar o mundo com as suas mãos hoje inúteis e trêmulas'\n",
      "Tokens gerados: ['devorava-o', ',', 'noite', 'e', 'dia', ',', 'uma', 'implacavel', 'amargura', ',', 'uma', 'surda', 'tristeza', 'de', 'vencido', ',', 'um', 'desespero', 'impotente', ',', 'contra', 'tudo', 'e', 'contra', 'todos', ',', 'por', 'nao', 'lhe', 'ter', 'sido', 'possivel', 'empolgar', 'o', 'mundo', 'com', 'as', 'suas', 'maos', 'hoje', 'inuteis', 'e', 'tremulas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, \n",
      "como o seu atual estado de miséria não lhe permitia abrir contra ninguém o bico, desabafava vituperando as idéias da \n",
      "época'\n",
      "Tokens gerados: ['e', ',', 'como', 'o', 'seu', 'atual', 'estado', 'de', 'miseria', 'nao', 'lhe', 'permitia', 'abrir', 'contra', 'ninguem', 'o', 'bico', ',', 'desabafava', 'vituperando', 'as', 'ideias', 'da', 'epoca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Assim, eram às vezes muito quentes as sobremesas do Miranda, quando, entre outros assuntos palpitantes, vinha à \n",
      "discussão o movimento abolicionista que principiava a formar-se em torno da lei Rio Branco'\n",
      "Tokens gerados: ['assim', ',', 'eram', 'as', 'vezes', 'muito', 'quentes', 'as', 'sobremesas', 'do', 'miranda', ',', 'quando', ',', 'entre', 'outros', 'assuntos', 'palpitantes', ',', 'vinha', 'a', 'discussao', 'o', 'movimento', 'abolicionista', 'que', 'principiava', 'a', 'formar-se', 'em', 'torno', 'da', 'lei', 'rio', 'branco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Então o Botelho ficava \n",
      "possesso e vomitava frases terríveis, para a direita e para a esquerda, como quem dispara tiros sem fazer alvo, e \n",
      "vociferava imprecações, aproveitando aquela válvula para desafogar o velho ódio acumulado dentro dele'\n",
      "Tokens gerados: ['entao', 'o', 'botelho', 'ficava', 'possesso', 'e', 'vomitava', 'frases', 'terriveis', ',', 'para', 'a', 'direita', 'e', 'para', 'a', 'esquerda', ',', 'como', 'quem', 'dispara', 'tiros', 'sem', 'fazer', 'alvo', ',', 'e', 'vociferava', 'imprecacoes', ',', 'aproveitando', 'aquela', 'valvula', 'para', 'desafogar', 'o', 'velho', 'odio', 'acumulado', 'dentro', 'dele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Bandidos! berrava apoplético'\n",
      "Tokens gerados: ['—', 'bandidos', 'berrava', 'apopletico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cáfila de salteadores! \n",
      "E o seu rancor irradiava-lhe dos olhos em setas envenenadas, procurando cravar-se em todas as brancuras e em \n",
      "todas as claridades'\n",
      "Tokens gerados: ['cafila', 'de', 'salteadores', 'e', 'o', 'seu', 'rancor', 'irradiava-lhe', 'dos', 'olhos', 'em', 'setas', 'envenenadas', ',', 'procurando', 'cravar-se', 'em', 'todas', 'as', 'brancuras', 'e', 'em', 'todas', 'as', 'claridades']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A virtude, a beleza, o talento, a mocidade, a força, a saúde, e principalmente a fortuna, eis o que ele \n",
      "não perdoava a ninguém, amaldiçoando todo aquele que conseguia o que ele não obtivera; que gozava o que ele não \n",
      "desfrutara; que sabia o que ele não aprendera'\n",
      "Tokens gerados: ['a', 'virtude', ',', 'a', 'beleza', ',', 'o', 'talento', ',', 'a', 'mocidade', ',', 'a', 'forca', ',', 'a', 'saude', ',', 'e', 'principalmente', 'a', 'fortuna', ',', 'eis', 'o', 'que', 'ele', 'nao', 'perdoava', 'a', 'ninguem', ',', 'amaldicoando', 'todo', 'aquele', 'que', 'conseguia', 'o', 'que', 'ele', 'nao', 'obtivera', 'que', 'gozava', 'o', 'que', 'ele', 'nao', 'desfrutara', 'que', 'sabia', 'o', 'que', 'ele', 'nao', 'aprendera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, para individualizar o objeto do seu ódio, voltava-se contra o Brasil, \n",
      "essa terra que, na sua opinião, só tinha uma serventia: enriquecer os portugueses, e que, no entanto, o deixara, a ele, na \n",
      "penúria'\n",
      "Tokens gerados: ['e', ',', 'para', 'individualizar', 'o', 'objeto', 'do', 'seu', 'odio', ',', 'voltava-se', 'contra', 'o', 'brasil', ',', 'essa', 'terra', 'que', ',', 'na', 'sua', 'opiniao', ',', 'so', 'tinha', 'uma', 'serventia', 'enriquecer', 'os', 'portugueses', ',', 'e', 'que', ',', 'no', 'entanto', ',', 'o', 'deixara', ',', 'a', 'ele', ',', 'na', 'penuria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seus dias eram consumidos do seguinte modo: acordava às oito da manhã, lavava-se mesmo no quarto com uma \n",
      "toalha molhada em espírito de vinho; depois ia ler os jornais para a sala de jantar, à espera do almoço; almoçava e sala, \n",
      "tomava o bonde e ia direitinho para uma charutaria da Rua do Ouvidor, onde costumava ficar assentado até às horas do \n",
      "jantar, entretido a dizer mal das pessoas que passavam lá fora, defronte dele'\n",
      "Tokens gerados: ['seus', 'dias', 'eram', 'consumidos', 'do', 'seguinte', 'modo', 'acordava', 'as', 'oito', 'da', 'manha', ',', 'lavava-se', 'mesmo', 'no', 'quarto', 'com', 'uma', 'toalha', 'molhada', 'em', 'espirito', 'de', 'vinho', 'depois', 'ia', 'ler', 'os', 'jornais', 'para', 'a', 'sala', 'de', 'jantar', ',', 'a', 'espera', 'do', 'almoco', 'almocava', 'e', 'sala', ',', 'tomava', 'o', 'bonde', 'e', 'ia', 'direitinho', 'para', 'uma', 'charutaria', 'da', 'rua', 'do', 'ouvidor', ',', 'onde', 'costumava', 'ficar', 'assentado', 'ate', 'as', 'horas', 'do', 'jantar', ',', 'entretido', 'a', 'dizer', 'mal', 'das', 'pessoas', 'que', 'passavam', 'la', 'fora', ',', 'defronte', 'dele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha a pretensão de conhecer todo o Rio \n",
      "de Janeiro e os podres de cada um em particular'\n",
      "Tokens gerados: ['tinha', 'a', 'pretensao', 'de', 'conhecer', 'todo', 'o', 'rio', 'de', 'janeiro', 'e', 'os', 'podres', 'de', 'cada', 'um', 'em', 'particular']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Às vezes, poucas, Dona Estela encarregava-o de fazer pequenas \n",
      "compras de armarinho, o que o Botelho desempenhava melhor que ninguém? Mas a sua grande paixão, o seu fraco, era \n",
      "a farda, adorava tudo que dissesse respeito a militarismo, posto que tivera sempre invencível medo às armas de qualquer \n",
      "espécie, mormente às de fogo'\n",
      "Tokens gerados: ['as', 'vezes', ',', 'poucas', ',', 'dona', 'estela', 'encarregava-o', 'de', 'fazer', 'pequenas', 'compras', 'de', 'armarinho', ',', 'o', 'que', 'o', 'botelho', 'desempenhava', 'melhor', 'que', 'ninguem', '?', 'mas', 'a', 'sua', 'grande', 'paixao', ',', 'o', 'seu', 'fraco', ',', 'era', 'a', 'farda', ',', 'adorava', 'tudo', 'que', 'dissesse', 'respeito', 'a', 'militarismo', ',', 'posto', 'que', 'tivera', 'sempre', 'invencivel', 'medo', 'as', 'armas', 'de', 'qualquer', 'especie', ',', 'mormente', 'as', 'de', 'fogo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não podia ouvir disparar perto de si uma espingarda, entusiasmava-se porém com tudo \n",
      "que cheirasse a guerra; a presença de um oficial em grande uniforme tirava-lhe lágrimas de comoção; conhecia na ponta \n",
      "da língua o que se referia à vida de quartel; distinguia ao primeiro lance de olhos o posto e o corpo a que pertencia \n",
      "qualquer soldado e, apesar dos seus achaques, era ouvir tocar na rua a corneta ou o tambor conduzindo o batalhão, \n",
      "ficava logo no ar, e, muita vez, quando dava por si, fazia parte dos que acompanhavam a tropa'\n",
      "Tokens gerados: ['nao', 'podia', 'ouvir', 'disparar', 'perto', 'de', 'si', 'uma', 'espingarda', ',', 'entusiasmava-se', 'porem', 'com', 'tudo', 'que', 'cheirasse', 'a', 'guerra', 'a', 'presenca', 'de', 'um', 'oficial', 'em', 'grande', 'uniforme', 'tirava-lhe', 'lagrimas', 'de', 'comocao', 'conhecia', 'na', 'ponta', 'da', 'lingua', 'o', 'que', 'se', 'referia', 'a', 'vida', 'de', 'quartel', 'distinguia', 'ao', 'primeiro', 'lance', 'de', 'olhos', 'o', 'posto', 'e', 'o', 'corpo', 'a', 'que', 'pertencia', 'qualquer', 'soldado', 'e', ',', 'apesar', 'dos', 'seus', 'achaques', ',', 'era', 'ouvir', 'tocar', 'na', 'rua', 'a', 'corneta', 'ou', 'o', 'tambor', 'conduzindo', 'o', 'batalhao', ',', 'ficava', 'logo', 'no', 'ar', ',', 'e', ',', 'muita', 'vez', ',', 'quando', 'dava', 'por', 'si', ',', 'fazia', 'parte', 'dos', 'que', 'acompanhavam', 'a', 'tropa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Então, não tornava para \n",
      "casa enquanto os militares neo se recolhessem'\n",
      "Tokens gerados: ['entao', ',', 'nao', 'tornava', 'para', 'casa', 'enquanto', 'os', 'militares', 'neo', 'se', 'recolhessem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quase sempre voltava dessa loucura às seis da tarde, moído a fazer dó, \n",
      "sem poder ter-se nas pernas, estrompado de marchar horas e horas ao som da música de pancadaria'\n",
      "Tokens gerados: ['quase', 'sempre', 'voltava', 'dessa', 'loucura', 'as', 'seis', 'da', 'tarde', ',', 'moido', 'a', 'fazer', 'do', ',', 'sem', 'poder', 'ter-se', 'nas', 'pernas', ',', 'estrompado', 'de', 'marchar', 'horas', 'e', 'horas', 'ao', 'som', 'da', 'musica', 'de', 'pancadaria']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o mais \n",
      "interessante é que ele, ao vir-lhe a reação, revoltava-se furioso contra o maldito comandante que o obrigava àquela \n",
      "estopada, levando o batalhão por uma infinidade de ruas e fazendo de propósito o caminho mais longo'\n",
      "Tokens gerados: ['e', 'o', 'mais', 'interessante', 'e', 'que', 'ele', ',', 'ao', 'vir-lhe', 'a', 'reacao', ',', 'revoltava-se', 'furioso', 'contra', 'o', 'maldito', 'comandante', 'que', 'o', 'obrigava', 'aquela', 'estopada', ',', 'levando', 'o', 'batalhao', 'por', 'uma', 'infinidade', 'de', 'ruas', 'e', 'fazendo', 'de', 'proposito', 'o', 'caminho', 'mais', 'longo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Só parece, lamentava-se ele, que a intenção daquele malvado era dar-me cabo da pele! Ora vejam! Três horas \n",
      "de marche-marche por uma soalheira de todos os diabos! \n",
      "Uma das birras mais cômicas do Botelho era o seu ódio pelo Valentim'\n",
      "Tokens gerados: ['—', 'so', 'parece', ',', 'lamentava-se', 'ele', ',', 'que', 'a', 'intencao', 'daquele', 'malvado', 'era', 'dar-me', 'cabo', 'da', 'pele', 'ora', 'vejam', 'tres', 'horas', 'de', 'marche-marche', 'por', 'uma', 'soalheira', 'de', 'todos', 'os', 'diabos', 'uma', 'das', 'birras', 'mais', 'comicas', 'do', 'botelho', 'era', 'o', 'seu', 'odio', 'pelo', 'valentim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O moleque causava-lhe febre com as suas \n",
      "petulâncias de mimalho, e, velhaco, percebendo quanto elas o irritavam, ainda mais abusava, seguro na proteção de \n",
      "Dona Estela'\n",
      "Tokens gerados: ['o', 'moleque', 'causava-lhe', 'febre', 'com', 'as', 'suas', 'petulancias', 'de', 'mimalho', ',', 'e', ',', 'velhaco', ',', 'percebendo', 'quanto', 'elas', 'o', 'irritavam', ',', 'ainda', 'mais', 'abusava', ',', 'seguro', 'na', 'protecao', 'de', 'dona', 'estela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O parasita de muito que o teria estrangulado, se não fora a necessidade de agradar à dona da casa'\n",
      "Tokens gerados: ['o', 'parasita', 'de', 'muito', 'que', 'o', 'teria', 'estrangulado', ',', 'se', 'nao', 'fora', 'a', 'necessidade', 'de', 'agradar', 'a', 'dona', 'da', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Botelho conhecia as faltas de Estela como as palmas da própria mão'\n",
      "Tokens gerados: ['botelho', 'conhecia', 'as', 'faltas', 'de', 'estela', 'como', 'as', 'palmas', 'da', 'propria', 'mao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Miranda mesmo, que o via em conta de \n",
      "amigo fiel, muitas e muitas vezes lhas confiara em ocasiões desesperadas de desabafo, declarando francamente o quanto \n",
      "no intimo a desprezava e a razão por que não a punha na rua aos pontapés'\n",
      "Tokens gerados: ['o', 'miranda', 'mesmo', ',', 'que', 'o', 'via', 'em', 'conta', 'de', 'amigo', 'fiel', ',', 'muitas', 'e', 'muitas', 'vezes', 'lhas', 'confiara', 'em', 'ocasioes', 'desesperadas', 'de', 'desabafo', ',', 'declarando', 'francamente', 'o', 'quanto', 'no', 'intimo', 'a', 'desprezava', 'e', 'a', 'razao', 'por', 'que', 'nao', 'a', 'punha', 'na', 'rua', 'aos', 'pontapes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o Botelho dava-lhe toda a razão; entendia \n",
      "também que os sérios interesses comerciais estavam acima de tudo'\n",
      "Tokens gerados: ['e', 'o', 'botelho', 'dava-lhe', 'toda', 'a', 'razao', 'entendia', 'tambem', 'que', 'os', 'serios', 'interesses', 'comerciais', 'estavam', 'acima', 'de', 'tudo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Uma mulher naquelas condições, dizia ele convicto, representa nada menos que o capital, e um capital em caso \n",
      "nenhum a gente despreza! Agora, você o que devia era nunca chegar-se para ela'\n",
      "Tokens gerados: ['—', 'uma', 'mulher', 'naquelas', 'condicoes', ',', 'dizia', 'ele', 'convicto', ',', 'representa', 'nada', 'menos', 'que', 'o', 'capital', ',', 'e', 'um', 'capital', 'em', 'caso', 'nenhum', 'a', 'gente', 'despreza', 'agora', ',', 'voce', 'o', 'que', 'devia', 'era', 'nunca', 'chegar-se', 'para', 'ela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora! explicava o marido'\n",
      "Tokens gerados: ['—', 'ora', 'explicava', 'o', 'marido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Eu me sirvo dela como quem se serve de uma escarradeira! \n",
      "O parasita, feliz por ver quanto o amigo aviltava a mulher, concordava em tudo plenamente, dando-lhe um \n",
      "carinhoso abraço de admiração'\n",
      "Tokens gerados: ['eu', 'me', 'sirvo', 'dela', 'como', 'quem', 'se', 'serve', 'de', 'uma', 'escarradeira', 'o', 'parasita', ',', 'feliz', 'por', 'ver', 'quanto', 'o', 'amigo', 'aviltava', 'a', 'mulher', ',', 'concordava', 'em', 'tudo', 'plenamente', ',', 'dando-lhe', 'um', 'carinhoso', 'abraco', 'de', 'admiracao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas por outro lado, quando ouvia Estela falar do marido, com infinito desdém e até \n",
      "com asco, ainda mais resplandecia de contente'\n",
      "Tokens gerados: ['mas', 'por', 'outro', 'lado', ',', 'quando', 'ouvia', 'estela', 'falar', 'do', 'marido', ',', 'com', 'infinito', 'desdem', 'e', 'ate', 'com', 'asco', ',', 'ainda', 'mais', 'resplandecia', 'de', 'contente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Você quer saber? afirmava ela, eu bem percebo quanto aquele traste do senhor meu marido me detesta, mas \n",
      "isso tanto se me dá como a primeira camisa que vesti! Desgraçadamente para nós, mulheres de sociedade, não podemos \n",
      "viver sem esposo, quando somos casadas; de forma que tenho de aturar o que me caiu em sorte, quer goste dele quer \n",
      "não goste! Juro-lhe, porém, que, se consinto que o Miranda se chegue às vezes para mim, é porque entendo que paga \n",
      "mais à pena ceder do que puxar discussão com uma besta daquela ordem! \n",
      "O Botelho, com a sua encanecida experiência do mundo, nunca transmitia a nenhum dos dois o que cada qual lhe \n",
      "dizia contra o outro; tanto assim que, certa ocasião, recolhendo-se à casa incomodado, em hora que não era do seu \n",
      "costume, ouviu, ao passar pelo quintal, sussurros de vozes abafadas que pareciam vir de um canto afogado de verdura, \n",
      "onde em geral não ia ninguém'\n",
      "Tokens gerados: ['—', 'voce', 'quer', 'saber', '?', 'afirmava', 'ela', ',', 'eu', 'bem', 'percebo', 'quanto', 'aquele', 'traste', 'do', 'senhor', 'meu', 'marido', 'me', 'detesta', ',', 'mas', 'isso', 'tanto', 'se', 'me', 'da', 'como', 'a', 'primeira', 'camisa', 'que', 'vesti', 'desgracadamente', 'para', 'nos', ',', 'mulheres', 'de', 'sociedade', ',', 'nao', 'podemos', 'viver', 'sem', 'esposo', ',', 'quando', 'somos', 'casadas', 'de', 'forma', 'que', 'tenho', 'de', 'aturar', 'o', 'que', 'me', 'caiu', 'em', 'sorte', ',', 'quer', 'goste', 'dele', 'quer', 'nao', 'goste', 'juro-lhe', ',', 'porem', ',', 'que', ',', 'se', 'consinto', 'que', 'o', 'miranda', 'se', 'chegue', 'as', 'vezes', 'para', 'mim', ',', 'e', 'porque', 'entendo', 'que', 'paga', 'mais', 'a', 'pena', 'ceder', 'do', 'que', 'puxar', 'discussao', 'com', 'uma', 'besta', 'daquela', 'ordem', 'o', 'botelho', ',', 'com', 'a', 'sua', 'encanecida', 'experiencia', 'do', 'mundo', ',', 'nunca', 'transmitia', 'a', 'nenhum', 'dos', 'dois', 'o', 'que', 'cada', 'qual', 'lhe', 'dizia', 'contra', 'o', 'outro', 'tanto', 'assim', 'que', ',', 'certa', 'ocasiao', ',', 'recolhendo-se', 'a', 'casa', 'incomodado', ',', 'em', 'hora', 'que', 'nao', 'era', 'do', 'seu', 'costume', ',', 'ouviu', ',', 'ao', 'passar', 'pelo', 'quintal', ',', 'sussurros', 'de', 'vozes', 'abafadas', 'que', 'pareciam', 'vir', 'de', 'um', 'canto', 'afogado', 'de', 'verdura', ',', 'onde', 'em', 'geral', 'nao', 'ia', 'ninguem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Encaminhou-se para lá em bicos de pés e, sem ser percebido, descobriu Estela entalada entre o muro e o \n",
      "Henrique'\n",
      "Tokens gerados: ['encaminhou-se', 'para', 'la', 'em', 'bicos', 'de', 'pes', 'e', ',', 'sem', 'ser', 'percebido', ',', 'descobriu', 'estela', 'entalada', 'entre', 'o', 'muro', 'e', 'o', 'henrique']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deixou-se ficar espiando, sem tugir nem mugir, e, só quando os dois se separaram, foi que ele se mostrou'\n",
      "Tokens gerados: ['deixou-se', 'ficar', 'espiando', ',', 'sem', 'tugir', 'nem', 'mugir', ',', 'e', ',', 'so', 'quando', 'os', 'dois', 'se', 'separaram', ',', 'foi', 'que', 'ele', 'se', 'mostrou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A senhora soltou um pequeno grito, e o rapaz, de vermelho que estava, fez-se cor de cera; mas o Botelho procurou \n",
      "tranqüilizá-los, dizendo em voz amiga e misteriosa: \n",
      "— Isso é uma imprudência o que vocês estão fazendo!'\n",
      "Tokens gerados: ['a', 'senhora', 'soltou', 'um', 'pequeno', 'grito', ',', 'e', 'o', 'rapaz', ',', 'de', 'vermelho', 'que', 'estava', ',', 'fez-se', 'cor', 'de', 'cera', 'mas', 'o', 'botelho', 'procurou', 'tranquiliza-los', ',', 'dizendo', 'em', 'voz', 'amiga', 'e', 'misteriosa', '—', 'isso', 'e', 'uma', 'imprudencia', 'o', 'que', 'voces', 'estao', 'fazendo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estas coisas não é deste modo que se arranjam! Assim \n",
      "como fui eu, podia ser outra pessoa'\n",
      "Tokens gerados: ['estas', 'coisas', 'nao', 'e', 'deste', 'modo', 'que', 'se', 'arranjam', 'assim', 'como', 'fui', 'eu', ',', 'podia', 'ser', 'outra', 'pessoa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois numa casa em que há tantos quartos, é lá preciso vir meterem-se neste canto \n",
      "do quintal?'\n",
      "Tokens gerados: ['pois', 'numa', 'casa', 'em', 'que', 'ha', 'tantos', 'quartos', ',', 'e', 'la', 'preciso', 'vir', 'meterem-se', 'neste', 'canto', 'do', 'quintal', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Nós não estávamos fazendo nada! disse Estela, recuperando o sangue-frio'\n",
      "Tokens gerados: ['—', 'nos', 'nao', 'estavamos', 'fazendo', 'nada', 'disse', 'estela', ',', 'recuperando', 'o', 'sangue-frio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah! tornou o velho, aparentando sumo respeito: então desculpe, pensei que estivessem'\n",
      "Tokens gerados: ['—', 'ah', 'tornou', 'o', 'velho', ',', 'aparentando', 'sumo', 'respeito', 'entao', 'desculpe', ',', 'pensei', 'que', 'estivessem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E olhe que, se assim \n",
      "fosse, para mim seria o mesmo, porque acho isso a coisa mais natural do mundo e entendo que desta vida a gente só \n",
      "leva o que come!'\n",
      "Tokens gerados: ['e', 'olhe', 'que', ',', 'se', 'assim', 'fosse', ',', 'para', 'mim', 'seria', 'o', 'mesmo', ',', 'porque', 'acho', 'isso', 'a', 'coisa', 'mais', 'natural', 'do', 'mundo', 'e', 'entendo', 'que', 'desta', 'vida', 'a', 'gente', 'so', 'leva', 'o', 'que', 'come']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Se vi, creia, foi como se nada visse, porque nada tenho a cheirar com a vida de cada um!'\n",
      "Tokens gerados: ['se', 'vi', ',', 'creia', ',', 'foi', 'como', 'se', 'nada', 'visse', ',', 'porque', 'nada', 'tenho', 'a', 'cheirar', 'com', 'a', 'vida', 'de', 'cada', 'um']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A \n",
      "senhora está moça, está na força dos anos; seu marido não a satisfaz, é justo que o substitua por outro! Ah! isto é o \n",
      "mundo, e, se é torto, não fomos nós que o fizemos torto!'\n",
      "Tokens gerados: ['a', 'senhora', 'esta', 'moca', ',', 'esta', 'na', 'forca', 'dos', 'anos', 'seu', 'marido', 'nao', 'a', 'satisfaz', ',', 'e', 'justo', 'que', 'o', 'substitua', 'por', 'outro', 'ah', 'isto', 'e', 'o', 'mundo', ',', 'e', ',', 'se', 'e', 'torto', ',', 'nao', 'fomos', 'nos', 'que', 'o', 'fizemos', 'torto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Até certa idade todos temos dentro um bichinho-carpinteiro, \n",
      "que é preciso matar, antes que ele nos mate! Não lhes doam as mãos!'\n",
      "Tokens gerados: ['ate', 'certa', 'idade', 'todos', 'temos', 'dentro', 'um', 'bichinho-carpinteiro', ',', 'que', 'e', 'preciso', 'matar', ',', 'antes', 'que', 'ele', 'nos', 'mate', 'nao', 'lhes', 'doam', 'as', 'maos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'apenas acho que, para outra vez, devem ter um \n",
      "pouquinho mais de cuidado e'\n",
      "Tokens gerados: ['apenas', 'acho', 'que', ',', 'para', 'outra', 'vez', ',', 'devem', 'ter', 'um', 'pouquinho', 'mais', 'de', 'cuidado', 'e']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Está bom! basta! ordenou Estela'\n",
      "Tokens gerados: ['—', 'esta', 'bom', 'basta', 'ordenou', 'estela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Perdão! eu, se digo isto, é para deixá-los bem tranqüilos a meu respeito'\n",
      "Tokens gerados: ['—', 'perdao', 'eu', ',', 'se', 'digo', 'isto', ',', 'e', 'para', 'deixa-los', 'bem', 'tranquilos', 'a', 'meu', 'respeito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não quero, nem por sombra, que se \n",
      "persuadam de que'\n",
      "Tokens gerados: ['nao', 'quero', ',', 'nem', 'por', 'sombra', ',', 'que', 'se', 'persuadam', 'de', 'que']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Henrique atalhou, com a voz ainda comovida: \n",
      "— Mas, acredite, seu Botelho, que'\n",
      "Tokens gerados: ['o', 'henrique', 'atalhou', ',', 'com', 'a', 'voz', 'ainda', 'comovida', '—', 'mas', ',', 'acredite', ',', 'seu', 'botelho', ',', 'que']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O velho interrompeu-o também por sua vez, passando-lhe a mão no ombro e afastando-o consigo: \n",
      "— Não tenha receio, que não o comprometerei, menino! \n",
      "E, como já estivessem distantes de Estela, segredou-lhe em tom protetor: \n",
      "— Não torne a fazer isto assim, que você se estraga'\n",
      "Tokens gerados: ['o', 'velho', 'interrompeu-o', 'tambem', 'por', 'sua', 'vez', ',', 'passando-lhe', 'a', 'mao', 'no', 'ombro', 'e', 'afastando-o', 'consigo', '—', 'nao', 'tenha', 'receio', ',', 'que', 'nao', 'o', 'comprometerei', ',', 'menino', 'e', ',', 'como', 'ja', 'estivessem', 'distantes', 'de', 'estela', ',', 'segredou-lhe', 'em', 'tom', 'protetor', '—', 'nao', 'torne', 'a', 'fazer', 'isto', 'assim', ',', 'que', 'voce', 'se', 'estraga']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Olhe como lhe tremem as pernas! \n",
      "Dona Estela acompanhou-os a distancia, vagarosamente, afetando preocupação em compor um ramalhete, cujas \n",
      "flores ela ia colhendo com muita graça, ora toda vergada sobre as plantas rasteiras, ora pondo-se na pontinha dos pés \n",
      "para alcançar os heliotrópios e os manacás'\n",
      "Tokens gerados: ['olhe', 'como', 'lhe', 'tremem', 'as', 'pernas', 'dona', 'estela', 'acompanhou-os', 'a', 'distancia', ',', 'vagarosamente', ',', 'afetando', 'preocupacao', 'em', 'compor', 'um', 'ramalhete', ',', 'cujas', 'flores', 'ela', 'ia', 'colhendo', 'com', 'muita', 'graca', ',', 'ora', 'toda', 'vergada', 'sobre', 'as', 'plantas', 'rasteiras', ',', 'ora', 'pondo-se', 'na', 'pontinha', 'dos', 'pes', 'para', 'alcancar', 'os', 'heliotropios', 'e', 'os', 'manacas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Henrique seguiu o Botelho até ao quarto deste, conversando sem mudar de assunto'\n",
      "Tokens gerados: ['henrique', 'seguiu', 'o', 'botelho', 'ate', 'ao', 'quarto', 'deste', ',', 'conversando', 'sem', 'mudar', 'de', 'assunto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Você então não fala nisto, hein? Jura? perguntou-lhe'\n",
      "Tokens gerados: ['—', 'voce', 'entao', 'nao', 'fala', 'nisto', ',', 'hein', '?', 'jura', '?', 'perguntou-lhe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O velho tinha já declarado, a rir, que os pilhara em flagrante e que ficara bom tempo à espreita'\n",
      "Tokens gerados: ['o', 'velho', 'tinha', 'ja', 'declarado', ',', 'a', 'rir', ',', 'que', 'os', 'pilhara', 'em', 'flagrante', 'e', 'que', 'ficara', 'bom', 'tempo', 'a', 'espreita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Falar o quê, seu tolo?'\n",
      "Tokens gerados: ['—', 'falar', 'o', 'que', ',', 'seu', 'tolo', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pois então quem pensa você que eu sou?'\n",
      "Tokens gerados: ['pois', 'entao', 'quem', 'pensa', 'voce', 'que', 'eu', 'sou', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Só abrirei o bico se você me der motivo \n",
      "para isso, mas estou convencido que não dará'\n",
      "Tokens gerados: ['so', 'abrirei', 'o', 'bico', 'se', 'voce', 'me', 'der', 'motivo', 'para', 'isso', ',', 'mas', 'estou', 'convencido', 'que', 'nao', 'dara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quer saber? eu até simpatizo muito com você, Henrique! Acho que \n",
      "você é um excelente menino, uma flor! E digo-lhe mais: hei de proteger os seus negócios com Dona Estela'\n",
      "Tokens gerados: ['quer', 'saber', '?', 'eu', 'ate', 'simpatizo', 'muito', 'com', 'voce', ',', 'henrique', 'acho', 'que', 'voce', 'e', 'um', 'excelente', 'menino', ',', 'uma', 'flor', 'e', 'digo-lhe', 'mais', 'hei', 'de', 'proteger', 'os', 'seus', 'negocios', 'com', 'dona', 'estela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Falando assim, tinha-lhe tomado as mãos e afagava-as'\n",
      "Tokens gerados: ['falando', 'assim', ',', 'tinha-lhe', 'tomado', 'as', 'maos', 'e', 'afagava-as']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Olhe, continuou, acariciando-o sempre; não se meta com donzelas, entende?'\n",
      "Tokens gerados: ['—', 'olhe', ',', 'continuou', ',', 'acariciando-o', 'sempre', 'nao', 'se', 'meta', 'com', 'donzelas', ',', 'entende', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'São o diabo! Por dá cá aquela \n",
      "palha fica um homem em apuros! agora quanto às outras, papo com elas! Não mande nenhuma ao vigário, nem lhe doa \n",
      "a cabeça, porque, no fim de contas, nas circunstâncias de Dona Estela, é até um grande serviço que você lhe faz! Meu \n",
      "rico amiguinho, quando uma mulher já passou dos trinta e pilha a jeito um rapazito da sua idade, é como se descobrisse \n",
      "ouro em pó! sabe-lhe a gaitas! Fique então sabendo de que não é só a ela que você faz o obséquio, mas também ao \n",
      "marido: quanto mais escovar-lhe você a mulher, melhor ela ficará de gênio, e por conseguinte melhor será para o pobre \n",
      "homem, coitado! que tem já bastante com que se aborrecer lá por baixo, com os seus negócios, e precisa de um pouco \n",
      "de descanso quando volta do serviço e mete-se em casa! Escove-a, escove-a! que a porá macia que nem veludo! O que é \n",
      "preciso é muito juizinho, percebe? Não faça outra criançada como a de hoje e continue para diante, não só com ela, mas \n",
      "com todas as que lhe caírem debaixo da asa! Vá passando! menos as de casa aberta, que isso é perigoso por causa das \n",
      "moléstias; nem tampouco donzelas! Não se meta com a Zulmira! E creia que lhe falo assim, porque sou seu amigo, \n",
      "porque o acho simpático, porque o acho bonito! \n",
      "E acarinhou-o tão vivamente dessa vez, que o estudante, fugindo-lhe das mãos, afastou-se com um gesto de \n",
      "repugnância e desprezo, enquanto o velho lhe dizia em voz comprimida: \n",
      "— Olha! Espera! Vem cá! Você é desconfiado!'\n",
      "Tokens gerados: ['sao', 'o', 'diabo', 'por', 'da', 'ca', 'aquela', 'palha', 'fica', 'um', 'homem', 'em', 'apuros', 'agora', 'quanto', 'as', 'outras', ',', 'papo', 'com', 'elas', 'nao', 'mande', 'nenhuma', 'ao', 'vigario', ',', 'nem', 'lhe', 'doa', 'a', 'cabeca', ',', 'porque', ',', 'no', 'fim', 'de', 'contas', ',', 'nas', 'circunstancias', 'de', 'dona', 'estela', ',', 'e', 'ate', 'um', 'grande', 'servico', 'que', 'voce', 'lhe', 'faz', 'meu', 'rico', 'amiguinho', ',', 'quando', 'uma', 'mulher', 'ja', 'passou', 'dos', 'trinta', 'e', 'pilha', 'a', 'jeito', 'um', 'rapazito', 'da', 'sua', 'idade', ',', 'e', 'como', 'se', 'descobrisse', 'ouro', 'em', 'po', 'sabe-lhe', 'a', 'gaitas', 'fique', 'entao', 'sabendo', 'de', 'que', 'nao', 'e', 'so', 'a', 'ela', 'que', 'voce', 'faz', 'o', 'obsequio', ',', 'mas', 'tambem', 'ao', 'marido', 'quanto', 'mais', 'escovar-lhe', 'voce', 'a', 'mulher', ',', 'melhor', 'ela', 'ficara', 'de', 'genio', ',', 'e', 'por', 'conseguinte', 'melhor', 'sera', 'para', 'o', 'pobre', 'homem', ',', 'coitado', 'que', 'tem', 'ja', 'bastante', 'com', 'que', 'se', 'aborrecer', 'la', 'por', 'baixo', ',', 'com', 'os', 'seus', 'negocios', ',', 'e', 'precisa', 'de', 'um', 'pouco', 'de', 'descanso', 'quando', 'volta', 'do', 'servico', 'e', 'mete-se', 'em', 'casa', 'escove-a', ',', 'escove-a', 'que', 'a', 'pora', 'macia', 'que', 'nem', 'veludo', 'o', 'que', 'e', 'preciso', 'e', 'muito', 'juizinho', ',', 'percebe', '?', 'nao', 'faca', 'outra', 'criancada', 'como', 'a', 'de', 'hoje', 'e', 'continue', 'para', 'diante', ',', 'nao', 'so', 'com', 'ela', ',', 'mas', 'com', 'todas', 'as', 'que', 'lhe', 'cairem', 'debaixo', 'da', 'asa', 'va', 'passando', 'menos', 'as', 'de', 'casa', 'aberta', ',', 'que', 'isso', 'e', 'perigoso', 'por', 'causa', 'das', 'molestias', 'nem', 'tampouco', 'donzelas', 'nao', 'se', 'meta', 'com', 'a', 'zulmira', 'e', 'creia', 'que', 'lhe', 'falo', 'assim', ',', 'porque', 'sou', 'seu', 'amigo', ',', 'porque', 'o', 'acho', 'simpatico', ',', 'porque', 'o', 'acho', 'bonito', 'e', 'acarinhou-o', 'tao', 'vivamente', 'dessa', 'vez', ',', 'que', 'o', 'estudante', ',', 'fugindo-lhe', 'das', 'maos', ',', 'afastou-se', 'com', 'um', 'gesto', 'de', 'repugnancia', 'e', 'desprezo', ',', 'enquanto', 'o', 'velho', 'lhe', 'dizia', 'em', 'voz', 'comprimida', '—', 'olha', 'espera', 'vem', 'ca', 'voce', 'e', 'desconfiado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_o_cortico_aluisio_azevedo_cap_2.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Eram cinco horas da manhã e o cortiço acordava, abrindo, não os olhos, mas a sua infinidade de portas e janelas \n",
      "alinhadas'\n",
      "Tokens gerados: ['eram', 'cinco', 'horas', 'da', 'manha', 'e', 'o', 'cortico', 'acordava', ',', 'abrindo', ',', 'nao', 'os', 'olhos', ',', 'mas', 'a', 'sua', 'infinidade', 'de', 'portas', 'e', 'janelas', 'alinhadas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um acordar alegre e farto de quem dormiu de uma assentada sete horas de chumbo'\n",
      "Tokens gerados: ['um', 'acordar', 'alegre', 'e', 'farto', 'de', 'quem', 'dormiu', 'de', 'uma', 'assentada', 'sete', 'horas', 'de', 'chumbo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Como que se sentiam ainda \n",
      "na indolência de neblina as derradeiras notas da ultima guitarra da noite antecedente, dissolvendo-se à luz loura e tenra \n",
      "da aurora, que nem um suspiro de saudade perdido em terra alheia'\n",
      "Tokens gerados: ['como', 'que', 'se', 'sentiam', 'ainda', 'na', 'indolencia', 'de', 'neblina', 'as', 'derradeiras', 'notas', 'da', 'ultima', 'guitarra', 'da', 'noite', 'antecedente', ',', 'dissolvendo-se', 'a', 'luz', 'loura', 'e', 'tenra', 'da', 'aurora', ',', 'que', 'nem', 'um', 'suspiro', 'de', 'saudade', 'perdido', 'em', 'terra', 'alheia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A roupa lavada, que ficara de véspera nos coradouros, umedecia o ar e punha-lhe um farto acre de sabão \n",
      "ordinário'\n",
      "Tokens gerados: ['a', 'roupa', 'lavada', ',', 'que', 'ficara', 'de', 'vespera', 'nos', 'coradouros', ',', 'umedecia', 'o', 'ar', 'e', 'punha-lhe', 'um', 'farto', 'acre', 'de', 'sabao', 'ordinario']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As pedras do chão, esbranquiçadas no lugar da lavagem e em alguns pontos azuladas pelo anil, mostravam \n",
      "uma palidez grisalha e triste, feita de acumulações de espumas secas'\n",
      "Tokens gerados: ['as', 'pedras', 'do', 'chao', ',', 'esbranquicadas', 'no', 'lugar', 'da', 'lavagem', 'e', 'em', 'alguns', 'pontos', 'azuladas', 'pelo', 'anil', ',', 'mostravam', 'uma', 'palidez', 'grisalha', 'e', 'triste', ',', 'feita', 'de', 'acumulacoes', 'de', 'espumas', 'secas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, das portas surgiam cabeças congestionadas de sono; ouviam-se amplos bocejos, fortes como o \n",
      "marulhar das ondas; pigarreava-se grosso por toda a parte; começavam as xícaras a tilintar; o cheiro quente do café \n",
      "aquecia, suplantando todos os outros; trocavam-se de janela para janela as primeiras palavras, os bons-dias; reatavam-se \n",
      "conversas interrompidas à noite; a pequenada cá fora traquinava já, e lá dentro das casas vinham choros abafados de \n",
      "crianças que ainda não andam'\n",
      "Tokens gerados: ['entretanto', ',', 'das', 'portas', 'surgiam', 'cabecas', 'congestionadas', 'de', 'sono', 'ouviam-se', 'amplos', 'bocejos', ',', 'fortes', 'como', 'o', 'marulhar', 'das', 'ondas', 'pigarreava-se', 'grosso', 'por', 'toda', 'a', 'parte', 'comecavam', 'as', 'xicaras', 'a', 'tilintar', 'o', 'cheiro', 'quente', 'do', 'cafe', 'aquecia', ',', 'suplantando', 'todos', 'os', 'outros', 'trocavam-se', 'de', 'janela', 'para', 'janela', 'as', 'primeiras', 'palavras', ',', 'os', 'bons-dias', 'reatavam-se', 'conversas', 'interrompidas', 'a', 'noite', 'a', 'pequenada', 'ca', 'fora', 'traquinava', 'ja', ',', 'e', 'la', 'dentro', 'das', 'casas', 'vinham', 'choros', 'abafados', 'de', 'criancas', 'que', 'ainda', 'nao', 'andam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No confuso rumor que se formava, destacavam-se risos, sons de vozes que altercavam, \n",
      "sem se saber onde, grasnar de marrecos, cantar de galos, cacarejar de galinhas'\n",
      "Tokens gerados: ['no', 'confuso', 'rumor', 'que', 'se', 'formava', ',', 'destacavam-se', 'risos', ',', 'sons', 'de', 'vozes', 'que', 'altercavam', ',', 'sem', 'se', 'saber', 'onde', ',', 'grasnar', 'de', 'marrecos', ',', 'cantar', 'de', 'galos', ',', 'cacarejar', 'de', 'galinhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De alguns quartos saiam mulheres que \n",
      "vinham pendurar cá fora, na parede, a gaiola do papagaio, e os louros, à semelhança dos donos, cumprimentavam-se \n",
      "ruidosamente, espanejando-se à luz nova do dia'\n",
      "Tokens gerados: ['de', 'alguns', 'quartos', 'saiam', 'mulheres', 'que', 'vinham', 'pendurar', 'ca', 'fora', ',', 'na', 'parede', ',', 'a', 'gaiola', 'do', 'papagaio', ',', 'e', 'os', 'louros', ',', 'a', 'semelhanca', 'dos', 'donos', ',', 'cumprimentavam-se', 'ruidosamente', ',', 'espanejando-se', 'a', 'luz', 'nova', 'do', 'dia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Daí a pouco, em volta das bicas era um zunzum crescente; uma aglomeração tumultuosa de machos e fêmeas'\n",
      "Tokens gerados: ['dai', 'a', 'pouco', ',', 'em', 'volta', 'das', 'bicas', 'era', 'um', 'zunzum', 'crescente', 'uma', 'aglomeracao', 'tumultuosa', 'de', 'machos', 'e', 'femeas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uns, após outros, lavavam a cara, incomodamente, debaixo do fio de água que escorria da altura de uns cinco palmos'\n",
      "Tokens gerados: ['uns', ',', 'apos', 'outros', ',', 'lavavam', 'a', 'cara', ',', 'incomodamente', ',', 'debaixo', 'do', 'fio', 'de', 'agua', 'que', 'escorria', 'da', 'altura', 'de', 'uns', 'cinco', 'palmos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O \n",
      "chão inundava-se'\n",
      "Tokens gerados: ['o', 'chao', 'inundava-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As mulheres precisavam já prender as saias entre as coxas para não as molhar; via-se-lhes a tostada \n",
      "nudez dos braços e do pescoço, que elas despiam, suspendendo o cabelo todo para o alto do casco; os homens, esses não \n",
      "se preocupavam em não molhar o pêlo, ao contrário metiam a cabeça bem debaixo da água e esfregavam com força as \n",
      "ventas e as barbas, fossando e fungando contra as palmas da mão'\n",
      "Tokens gerados: ['as', 'mulheres', 'precisavam', 'ja', 'prender', 'as', 'saias', 'entre', 'as', 'coxas', 'para', 'nao', 'as', 'molhar', 'via-se-lhes', 'a', 'tostada', 'nudez', 'dos', 'bracos', 'e', 'do', 'pescoco', ',', 'que', 'elas', 'despiam', ',', 'suspendendo', 'o', 'cabelo', 'todo', 'para', 'o', 'alto', 'do', 'casco', 'os', 'homens', ',', 'esses', 'nao', 'se', 'preocupavam', 'em', 'nao', 'molhar', 'o', 'pelo', ',', 'ao', 'contrario', 'metiam', 'a', 'cabeca', 'bem', 'debaixo', 'da', 'agua', 'e', 'esfregavam', 'com', 'forca', 'as', 'ventas', 'e', 'as', 'barbas', ',', 'fossando', 'e', 'fungando', 'contra', 'as', 'palmas', 'da', 'mao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As portas das latrinas não descansavam, era um abrir \n",
      "e fechar de cada instante, um entrar e sair sem tréguas'\n",
      "Tokens gerados: ['as', 'portas', 'das', 'latrinas', 'nao', 'descansavam', ',', 'era', 'um', 'abrir', 'e', 'fechar', 'de', 'cada', 'instante', ',', 'um', 'entrar', 'e', 'sair', 'sem', 'treguas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não se demoravam lá dentro e vinham ainda amarrando as \n",
      "calças ou as saias; as crianças não se davam ao trabalho de lá ir, despachavam-se ali mesmo, no capinzal dos fundos, \n",
      "por detrás da estalagem ou no recanto das hortas'\n",
      "Tokens gerados: ['nao', 'se', 'demoravam', 'la', 'dentro', 'e', 'vinham', 'ainda', 'amarrando', 'as', 'calcas', 'ou', 'as', 'saias', 'as', 'criancas', 'nao', 'se', 'davam', 'ao', 'trabalho', 'de', 'la', 'ir', ',', 'despachavam-se', 'ali', 'mesmo', ',', 'no', 'capinzal', 'dos', 'fundos', ',', 'por', 'detras', 'da', 'estalagem', 'ou', 'no', 'recanto', 'das', 'hortas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O rumor crescia, condensando-se; o zunzum de todos os dias acentuava-se; já se não destacavam vozes dispersas, \n",
      "mas um só ruído compacto que enchia todo o cortiço'\n",
      "Tokens gerados: ['o', 'rumor', 'crescia', ',', 'condensando-se', 'o', 'zunzum', 'de', 'todos', 'os', 'dias', 'acentuava-se', 'ja', 'se', 'nao', 'destacavam', 'vozes', 'dispersas', ',', 'mas', 'um', 'so', 'ruido', 'compacto', 'que', 'enchia', 'todo', 'o', 'cortico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Começavam a fazer compras na venda; ensarilhavam-se \n",
      "discussões e resingas; ouviam-se gargalhadas e pragas; já se não falava, gritava-se'\n",
      "Tokens gerados: ['comecavam', 'a', 'fazer', 'compras', 'na', 'venda', 'ensarilhavam-se', 'discussoes', 'e', 'resingas', 'ouviam-se', 'gargalhadas', 'e', 'pragas', 'ja', 'se', 'nao', 'falava', ',', 'gritava-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sentia-se naquela fermentação \n",
      "sangüínea, naquela gula viçosa de plantas rasteiras que mergulham os pés vigorosos na lama preta e nutriente da vida, o \n",
      "prazer animal de existir, a triunfante satisfação de respirar sobre a terra'\n",
      "Tokens gerados: ['sentia-se', 'naquela', 'fermentacao', 'sanguinea', ',', 'naquela', 'gula', 'vicosa', 'de', 'plantas', 'rasteiras', 'que', 'mergulham', 'os', 'pes', 'vigorosos', 'na', 'lama', 'preta', 'e', 'nutriente', 'da', 'vida', ',', 'o', 'prazer', 'animal', 'de', 'existir', ',', 'a', 'triunfante', 'satisfacao', 'de', 'respirar', 'sobre', 'a', 'terra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Da porta da venda que dava para o cortiço iam e vinham como formigas; fazendo compras'\n",
      "Tokens gerados: ['da', 'porta', 'da', 'venda', 'que', 'dava', 'para', 'o', 'cortico', 'iam', 'e', 'vinham', 'como', 'formigas', 'fazendo', 'compras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Duas janelas do Miranda abriram-se'\n",
      "Tokens gerados: ['duas', 'janelas', 'do', 'miranda', 'abriram-se']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Apareceu numa a Isaura, que se dispunha a começar a limpeza da casa'\n",
      "Tokens gerados: ['apareceu', 'numa', 'a', 'isaura', ',', 'que', 'se', 'dispunha', 'a', 'comecar', 'a', 'limpeza', 'da', 'casa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Nhá Dunga! gritou ela para baixo, a sacudir um pano de mesa; se você tem cuscuz de milho hoje, bata na \n",
      "porta, ouviu? \n",
      "A Leonor surgiu logo também, enfiando curiosa a carapinha por entre o pescoço e o ombro da mulata'\n",
      "Tokens gerados: ['—', 'nha', 'dunga', 'gritou', 'ela', 'para', 'baixo', ',', 'a', 'sacudir', 'um', 'pano', 'de', 'mesa', 'se', 'voce', 'tem', 'cuscuz', 'de', 'milho', 'hoje', ',', 'bata', 'na', 'porta', ',', 'ouviu', '?', 'a', 'leonor', 'surgiu', 'logo', 'tambem', ',', 'enfiando', 'curiosa', 'a', 'carapinha', 'por', 'entre', 'o', 'pescoco', 'e', 'o', 'ombro', 'da', 'mulata']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O padeiro entrou na estalagem, com a sua grande cesta à cabeça e o seu banco de pau fechado debaixo do braço, e \n",
      "foi estacionar em meio do pátio, à espera dos fregueses, pousando a canastra sobre o cavalete que ele armou \n",
      "prontamente'\n",
      "Tokens gerados: ['o', 'padeiro', 'entrou', 'na', 'estalagem', ',', 'com', 'a', 'sua', 'grande', 'cesta', 'a', 'cabeca', 'e', 'o', 'seu', 'banco', 'de', 'pau', 'fechado', 'debaixo', 'do', 'braco', ',', 'e', 'foi', 'estacionar', 'em', 'meio', 'do', 'patio', ',', 'a', 'espera', 'dos', 'fregueses', ',', 'pousando', 'a', 'canastra', 'sobre', 'o', 'cavalete', 'que', 'ele', 'armou', 'prontamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em breve estava cercado por uma nuvem de gente'\n",
      "Tokens gerados: ['em', 'breve', 'estava', 'cercado', 'por', 'uma', 'nuvem', 'de', 'gente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As crianças adulavam-no, e, à proporção que cada \n",
      "mulher ou cada homem recebia o pão, disparava para casa com este abraçado contra o peito'\n",
      "Tokens gerados: ['as', 'criancas', 'adulavam-no', ',', 'e', ',', 'a', 'proporcao', 'que', 'cada', 'mulher', 'ou', 'cada', 'homem', 'recebia', 'o', 'pao', ',', 'disparava', 'para', 'casa', 'com', 'este', 'abracado', 'contra', 'o', 'peito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma vaca, seguida por um \n",
      "bezerro amordaçado, ia, tilintando tristemente o seu chocalho, de porta em porta, guiada por um homem carregado de \n",
      "vasilhame de folha'\n",
      "Tokens gerados: ['uma', 'vaca', ',', 'seguida', 'por', 'um', 'bezerro', 'amordacado', ',', 'ia', ',', 'tilintando', 'tristemente', 'o', 'seu', 'chocalho', ',', 'de', 'porta', 'em', 'porta', ',', 'guiada', 'por', 'um', 'homem', 'carregado', 'de', 'vasilhame', 'de', 'folha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O zunzum chegava ao seu apogeu'\n",
      "Tokens gerados: ['o', 'zunzum', 'chegava', 'ao', 'seu', 'apogeu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A fábrica de massas italianas, ali mesmo da vizinhança, começou a trabalhar, \n",
      "engrossando o barulho com o seu arfar monótono de máquina a vapor'\n",
      "Tokens gerados: ['a', 'fabrica', 'de', 'massas', 'italianas', ',', 'ali', 'mesmo', 'da', 'vizinhanca', ',', 'comecou', 'a', 'trabalhar', ',', 'engrossando', 'o', 'barulho', 'com', 'o', 'seu', 'arfar', 'monotono', 'de', 'maquina', 'a', 'vapor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As corridas até à venda reproduziam-se, \n",
      "transformando-se num verminar constante de formigueiro assanhado'\n",
      "Tokens gerados: ['as', 'corridas', 'ate', 'a', 'venda', 'reproduziam-se', ',', 'transformando-se', 'num', 'verminar', 'constante', 'de', 'formigueiro', 'assanhado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Agora, no lugar das bicas apinhavam-se latas de \n",
      "todos os feitios, sobressaindo as de querosene com um braço de madeira em cima; sentia-se o trapejar da água caindo na \n",
      "folha'\n",
      "Tokens gerados: ['agora', ',', 'no', 'lugar', 'das', 'bicas', 'apinhavam-se', 'latas', 'de', 'todos', 'os', 'feitios', ',', 'sobressaindo', 'as', 'de', 'querosene', 'com', 'um', 'braco', 'de', 'madeira', 'em', 'cima', 'sentia-se', 'o', 'trapejar', 'da', 'agua', 'caindo', 'na', 'folha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Algumas lavadeiras enchiam já as suas tinas; outras estendiam nos coradouros a roupa que ficara de molho'\n",
      "Tokens gerados: ['algumas', 'lavadeiras', 'enchiam', 'ja', 'as', 'suas', 'tinas', 'outras', 'estendiam', 'nos', 'coradouros', 'a', 'roupa', 'que', 'ficara', 'de', 'molho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Principiava o trabalho'\n",
      "Tokens gerados: ['principiava', 'o', 'trabalho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Rompiam das gargantas os fados portugueses e as modinhas brasileiras'\n",
      "Tokens gerados: ['rompiam', 'das', 'gargantas', 'os', 'fados', 'portugueses', 'e', 'as', 'modinhas', 'brasileiras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um carroção de lixo \n",
      "entrou com grande barulho de rodas na pedra, seguido de uma algazarra medonha algaraviada pelo carroceiro contra o \n",
      "burro'\n",
      "Tokens gerados: ['um', 'carrocao', 'de', 'lixo', 'entrou', 'com', 'grande', 'barulho', 'de', 'rodas', 'na', 'pedra', ',', 'seguido', 'de', 'uma', 'algazarra', 'medonha', 'algaraviada', 'pelo', 'carroceiro', 'contra', 'o', 'burro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, durante muito tempo, fez-se um vaivém de mercadores'\n",
      "Tokens gerados: ['e', ',', 'durante', 'muito', 'tempo', ',', 'fez-se', 'um', 'vaivem', 'de', 'mercadores']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Apareceram os tabuleiros de carne fresca e outros de \n",
      "tripas e fatos de boi; só não vinham hortaliças, porque havia muitas hortas no cortiço'\n",
      "Tokens gerados: ['apareceram', 'os', 'tabuleiros', 'de', 'carne', 'fresca', 'e', 'outros', 'de', 'tripas', 'e', 'fatos', 'de', 'boi', 'so', 'nao', 'vinham', 'hortalicas', ',', 'porque', 'havia', 'muitas', 'hortas', 'no', 'cortico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vieram os ruidosos mascates, \n",
      "com as suas latas de quinquilharia, com as suas caixas de candeeiros e objetos de vidro e com o seu fornecimento de \n",
      "caçarolas e chocolateiras, de folha-de-flandres'\n",
      "Tokens gerados: ['vieram', 'os', 'ruidosos', 'mascates', ',', 'com', 'as', 'suas', 'latas', 'de', 'quinquilharia', ',', 'com', 'as', 'suas', 'caixas', 'de', 'candeeiros', 'e', 'objetos', 'de', 'vidro', 'e', 'com', 'o', 'seu', 'fornecimento', 'de', 'cacarolas', 'e', 'chocolateiras', ',', 'de', 'folha-de-flandres']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cada vendedor tinha o seu modo especial de apregoar, destacando-se o \n",
      "homem das sardinhas, com as cestas do peixe dependuradas, à moda de balança, de um pau que ele trazia ao ombro'\n",
      "Tokens gerados: ['cada', 'vendedor', 'tinha', 'o', 'seu', 'modo', 'especial', 'de', 'apregoar', ',', 'destacando-se', 'o', 'homem', 'das', 'sardinhas', ',', 'com', 'as', 'cestas', 'do', 'peixe', 'dependuradas', ',', 'a', 'moda', 'de', 'balanca', ',', 'de', 'um', 'pau', 'que', 'ele', 'trazia', 'ao', 'ombro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nada mais foi preciso do que o seu primeiro guincho estridente e gutural para surgirem logo, como por encanto, uma \n",
      "enorme variedade de gatos, que vieram correndo acercar-se dele com grande familiaridade, roçando-se-lhe nas pernas \n",
      "arregaçadas e miando suplicantemente'\n",
      "Tokens gerados: ['nada', 'mais', 'foi', 'preciso', 'do', 'que', 'o', 'seu', 'primeiro', 'guincho', 'estridente', 'e', 'gutural', 'para', 'surgirem', 'logo', ',', 'como', 'por', 'encanto', ',', 'uma', 'enorme', 'variedade', 'de', 'gatos', ',', 'que', 'vieram', 'correndo', 'acercar-se', 'dele', 'com', 'grande', 'familiaridade', ',', 'rocando-se-lhe', 'nas', 'pernas', 'arregacadas', 'e', 'miando', 'suplicantemente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O sardinheiro os afastava com o pé, enquanto vendia o seu peixe à porta das \n",
      "casinhas, mas os bichanos não desistiam e continuavam a implorar, arranhando os cestos que o homem cuidadosamente \n",
      "tapava mal servia ao freguês'\n",
      "Tokens gerados: ['o', 'sardinheiro', 'os', 'afastava', 'com', 'o', 'pe', ',', 'enquanto', 'vendia', 'o', 'seu', 'peixe', 'a', 'porta', 'das', 'casinhas', ',', 'mas', 'os', 'bichanos', 'nao', 'desistiam', 'e', 'continuavam', 'a', 'implorar', ',', 'arranhando', 'os', 'cestos', 'que', 'o', 'homem', 'cuidadosamente', 'tapava', 'mal', 'servia', 'ao', 'fregues']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Para ver-se livre por um instante dos importunos era necessário atirar para bem longe um \n",
      "punhado de sardinhas, sobre o qual se precipitava logo, aos pulos, o grupo dos pedinchões'\n",
      "Tokens gerados: ['para', 'ver-se', 'livre', 'por', 'um', 'instante', 'dos', 'importunos', 'era', 'necessario', 'atirar', 'para', 'bem', 'longe', 'um', 'punhado', 'de', 'sardinhas', ',', 'sobre', 'o', 'qual', 'se', 'precipitava', 'logo', ',', 'aos', 'pulos', ',', 'o', 'grupo', 'dos', 'pedinchoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A primeira que se pôs a lavar foi a Leandra, por alcunha a “Machona”, portuguesa feroz, berradora, pulsos \n",
      "cabeludos e grossos, anca de animal do campo'\n",
      "Tokens gerados: ['a', 'primeira', 'que', 'se', 'pos', 'a', 'lavar', 'foi', 'a', 'leandra', ',', 'por', 'alcunha', 'a', '“', 'machona', '”', ',', 'portuguesa', 'feroz', ',', 'berradora', ',', 'pulsos', 'cabeludos', 'e', 'grossos', ',', 'anca', 'de', 'animal', 'do', 'campo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha duas filhas, uma casada e separada do marido, Ana das Dores, a \n",
      "quem só chamavam a “das Dores” e outra donzela ainda, a Nenen, e mais um filho, o Agostinho, menino levado dos \n",
      "diabos, que gritava tanto ou melhor que a mãe'\n",
      "Tokens gerados: ['tinha', 'duas', 'filhas', ',', 'uma', 'casada', 'e', 'separada', 'do', 'marido', ',', 'ana', 'das', 'dores', ',', 'a', 'quem', 'so', 'chamavam', 'a', '“', 'das', 'dores', '”', 'e', 'outra', 'donzela', 'ainda', ',', 'a', 'nenen', ',', 'e', 'mais', 'um', 'filho', ',', 'o', 'agostinho', ',', 'menino', 'levado', 'dos', 'diabos', ',', 'que', 'gritava', 'tanto', 'ou', 'melhor', 'que', 'a', 'mae']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A das Dores morava em sua casinha à parte, mas toda a família habitava \n",
      "no cortiço'\n",
      "Tokens gerados: ['a', 'das', 'dores', 'morava', 'em', 'sua', 'casinha', 'a', 'parte', ',', 'mas', 'toda', 'a', 'familia', 'habitava', 'no', 'cortico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ninguém ali sabia ao certo se a Machona era viúva ou desquitada; os filhos não se pareciam uns com os outros'\n",
      "Tokens gerados: ['ninguem', 'ali', 'sabia', 'ao', 'certo', 'se', 'a', 'machona', 'era', 'viuva', 'ou', 'desquitada', 'os', 'filhos', 'nao', 'se', 'pareciam', 'uns', 'com', 'os', 'outros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A \n",
      "das Dores, sim, afirmavam que fora casada e que largara o marido para meter-se com um homem do comércio; e que \n",
      "este, retirando-se para a terra e não querendo soltá-la ao desamparo, deixara o sócio em seu lugar'\n",
      "Tokens gerados: ['a', 'das', 'dores', ',', 'sim', ',', 'afirmavam', 'que', 'fora', 'casada', 'e', 'que', 'largara', 'o', 'marido', 'para', 'meter-se', 'com', 'um', 'homem', 'do', 'comercio', 'e', 'que', 'este', ',', 'retirando-se', 'para', 'a', 'terra', 'e', 'nao', 'querendo', 'solta-la', 'ao', 'desamparo', ',', 'deixara', 'o', 'socio', 'em', 'seu', 'lugar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Teria vinte e cinco \n",
      "anos'\n",
      "Tokens gerados: ['teria', 'vinte', 'e', 'cinco', 'anos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Nenen dezessete'\n",
      "Tokens gerados: ['nenen', 'dezessete']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Espigada, franzina e forte, com uma proazinha de orgulho da sua virgindade, escapando como \n",
      "enguia por entre os dedos dos rapazes que a queriam sem ser para casar'\n",
      "Tokens gerados: ['espigada', ',', 'franzina', 'e', 'forte', ',', 'com', 'uma', 'proazinha', 'de', 'orgulho', 'da', 'sua', 'virgindade', ',', 'escapando', 'como', 'enguia', 'por', 'entre', 'os', 'dedos', 'dos', 'rapazes', 'que', 'a', 'queriam', 'sem', 'ser', 'para', 'casar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Engomava bem e sabia fazer roupa branca de \n",
      "homem com muita perfeição'\n",
      "Tokens gerados: ['engomava', 'bem', 'e', 'sabia', 'fazer', 'roupa', 'branca', 'de', 'homem', 'com', 'muita', 'perfeicao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao lado da Leandra foi colocar-se à sua tina a Augusta Carne-Mole, brasileira, branca, mulher de Alexandre, um \n",
      "mulato de quarenta anos, soldado de policia, pernóstico, de grande bigode preto, queixo sempre escanhoado e um luxo \n",
      "de calças brancas engomadas e botões limpos na farda, quando estava de serviço'\n",
      "Tokens gerados: ['ao', 'lado', 'da', 'leandra', 'foi', 'colocar-se', 'a', 'sua', 'tina', 'a', 'augusta', 'carne-mole', ',', 'brasileira', ',', 'branca', ',', 'mulher', 'de', 'alexandre', ',', 'um', 'mulato', 'de', 'quarenta', 'anos', ',', 'soldado', 'de', 'policia', ',', 'pernostico', ',', 'de', 'grande', 'bigode', 'preto', ',', 'queixo', 'sempre', 'escanhoado', 'e', 'um', 'luxo', 'de', 'calcas', 'brancas', 'engomadas', 'e', 'botoes', 'limpos', 'na', 'farda', ',', 'quando', 'estava', 'de', 'servico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Também tinham filhos, mas ainda \n",
      "pequenos, um dos quais, a Juju, vivia na cidade com a madrinha que se encarregava dela'\n",
      "Tokens gerados: ['tambem', 'tinham', 'filhos', ',', 'mas', 'ainda', 'pequenos', ',', 'um', 'dos', 'quais', ',', 'a', 'juju', ',', 'vivia', 'na', 'cidade', 'com', 'a', 'madrinha', 'que', 'se', 'encarregava', 'dela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Esta madrinha era uma cocote \n",
      "de trinta mil-réis para cima, a Léonie, com sobrado na cidade'\n",
      "Tokens gerados: ['esta', 'madrinha', 'era', 'uma', 'cocote', 'de', 'trinta', 'mil-reis', 'para', 'cima', ',', 'a', 'leonie', ',', 'com', 'sobrado', 'na', 'cidade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Procedência francesa'\n",
      "Tokens gerados: ['procedencia', 'francesa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Alexandre, em casa, à hora de descanso, nos seus chinelos e na sua camisa desabotoada, era muito chão com os \n",
      "companheiros de estalagem, conversava, ria e brincava, mas envergando o uniforme, encerando o bigode e empunhando \n",
      "a sua chibata, com que tinha o costume de fustigar as calças de brim, ninguém mais lhe via os dentes e então a todos \n",
      "falava teso e por cima do ombro'\n",
      "Tokens gerados: ['alexandre', ',', 'em', 'casa', ',', 'a', 'hora', 'de', 'descanso', ',', 'nos', 'seus', 'chinelos', 'e', 'na', 'sua', 'camisa', 'desabotoada', ',', 'era', 'muito', 'chao', 'com', 'os', 'companheiros', 'de', 'estalagem', ',', 'conversava', ',', 'ria', 'e', 'brincava', ',', 'mas', 'envergando', 'o', 'uniforme', ',', 'encerando', 'o', 'bigode', 'e', 'empunhando', 'a', 'sua', 'chibata', ',', 'com', 'que', 'tinha', 'o', 'costume', 'de', 'fustigar', 'as', 'calcas', 'de', 'brim', ',', 'ninguem', 'mais', 'lhe', 'via', 'os', 'dentes', 'e', 'entao', 'a', 'todos', 'falava', 'teso', 'e', 'por', 'cima', 'do', 'ombro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mulher, a quem ele só dava tu quando não estava fardado, era de uma honestidade \n",
      "proverbial no cortiço, honestidade sem mérito, porque vinha da indolência do seu temperamento e não do arbítrio do \n",
      "seu caráter'\n",
      "Tokens gerados: ['a', 'mulher', ',', 'a', 'quem', 'ele', 'so', 'dava', 'tu', 'quando', 'nao', 'estava', 'fardado', ',', 'era', 'de', 'uma', 'honestidade', 'proverbial', 'no', 'cortico', ',', 'honestidade', 'sem', 'merito', ',', 'porque', 'vinha', 'da', 'indolencia', 'do', 'seu', 'temperamento', 'e', 'nao', 'do', 'arbitrio', 'do', 'seu', 'carater']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Junto dela pôs-se a trabalhar a Leocádia, mulher de um ferreiro chamado Bruno, portuguesa pequena e socada, de \n",
      "carnes duras, com uma fama terrível de leviana entre as suas vizinhas'\n",
      "Tokens gerados: ['junto', 'dela', 'pos-se', 'a', 'trabalhar', 'a', 'leocadia', ',', 'mulher', 'de', 'um', 'ferreiro', 'chamado', 'bruno', ',', 'portuguesa', 'pequena', 'e', 'socada', ',', 'de', 'carnes', 'duras', ',', 'com', 'uma', 'fama', 'terrivel', 'de', 'leviana', 'entre', 'as', 'suas', 'vizinhas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Seguia-se a Paula, uma cabocla velha, meio idiota, a quem respeitavam todos pelas virtudes de que só ela \n",
      "dispunha para benzer erisipelas e cortar febres por meio de rezas e feitiçarias'\n",
      "Tokens gerados: ['seguia-se', 'a', 'paula', ',', 'uma', 'cabocla', 'velha', ',', 'meio', 'idiota', ',', 'a', 'quem', 'respeitavam', 'todos', 'pelas', 'virtudes', 'de', 'que', 'so', 'ela', 'dispunha', 'para', 'benzer', 'erisipelas', 'e', 'cortar', 'febres', 'por', 'meio', 'de', 'rezas', 'e', 'feiticarias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era extremamente feia, grossa, triste, com \n",
      "olhos desvairados, dentes cortados à navalha, formando ponta, como dentes de cão, cabelos lisos, escorridos e ainda \n",
      "retintos apesar da idade'\n",
      "Tokens gerados: ['era', 'extremamente', 'feia', ',', 'grossa', ',', 'triste', ',', 'com', 'olhos', 'desvairados', ',', 'dentes', 'cortados', 'a', 'navalha', ',', 'formando', 'ponta', ',', 'como', 'dentes', 'de', 'cao', ',', 'cabelos', 'lisos', ',', 'escorridos', 'e', 'ainda', 'retintos', 'apesar', 'da', 'idade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Chamavam-lhe “Bruxa”'\n",
      "Tokens gerados: ['chamavam-lhe', '“', 'bruxa', '”']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois seguiam-se a Marciana e mais a sua filha Florinda'\n",
      "Tokens gerados: ['depois', 'seguiam-se', 'a', 'marciana', 'e', 'mais', 'a', 'sua', 'filha', 'florinda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A primeira, mulata antiga, muito seria e asseada em \n",
      "exagero: a sua casa estava sempre úmida das consecutivas lavagens'\n",
      "Tokens gerados: ['a', 'primeira', ',', 'mulata', 'antiga', ',', 'muito', 'seria', 'e', 'asseada', 'em', 'exagero', 'a', 'sua', 'casa', 'estava', 'sempre', 'umida', 'das', 'consecutivas', 'lavagens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em lhe apanhando o mau humor punha-se logo a \n",
      "espanar, a varrer febrilmente, e, quando a raiva era grande, corria a buscar um balde de água e descarregava-o com fúria \n",
      "pelo chão da sala'\n",
      "Tokens gerados: ['em', 'lhe', 'apanhando', 'o', 'mau', 'humor', 'punha-se', 'logo', 'a', 'espanar', ',', 'a', 'varrer', 'febrilmente', ',', 'e', ',', 'quando', 'a', 'raiva', 'era', 'grande', ',', 'corria', 'a', 'buscar', 'um', 'balde', 'de', 'agua', 'e', 'descarregava-o', 'com', 'furia', 'pelo', 'chao', 'da', 'sala']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A filha tinha quinze anos, a pele de um moreno quente, beiços sensuais, bonitos dentes, olhos \n",
      "luxuriosos de macaca'\n",
      "Tokens gerados: ['a', 'filha', 'tinha', 'quinze', 'anos', ',', 'a', 'pele', 'de', 'um', 'moreno', 'quente', ',', 'beicos', 'sensuais', ',', 'bonitos', 'dentes', ',', 'olhos', 'luxuriosos', 'de', 'macaca']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Toda ela estava a pedir homem, mas sustentava ainda a sua virgindade e não cedia, nem à mão de \n",
      "Deus Padre, aos rogos de João Romão, que a desejava apanhar a troco de pequenas concessões na medida e no peso das \n",
      "compras que Florinda fazia diariamente à venda'\n",
      "Tokens gerados: ['toda', 'ela', 'estava', 'a', 'pedir', 'homem', ',', 'mas', 'sustentava', 'ainda', 'a', 'sua', 'virgindade', 'e', 'nao', 'cedia', ',', 'nem', 'a', 'mao', 'de', 'deus', 'padre', ',', 'aos', 'rogos', 'de', 'joao', 'romao', ',', 'que', 'a', 'desejava', 'apanhar', 'a', 'troco', 'de', 'pequenas', 'concessoes', 'na', 'medida', 'e', 'no', 'peso', 'das', 'compras', 'que', 'florinda', 'fazia', 'diariamente', 'a', 'venda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Depois via-se a velha Isabel, isto é, Dona Isabel, porque ali na estalagem lhes dispensavam todos certa \n",
      "consideração, privilegiada pelas suas maneiras graves de pessoa que já teve tratamento: uma pobre mulher comida de \n",
      "desgostos'\n",
      "Tokens gerados: ['depois', 'via-se', 'a', 'velha', 'isabel', ',', 'isto', 'e', ',', 'dona', 'isabel', ',', 'porque', 'ali', 'na', 'estalagem', 'lhes', 'dispensavam', 'todos', 'certa', 'consideracao', ',', 'privilegiada', 'pelas', 'suas', 'maneiras', 'graves', 'de', 'pessoa', 'que', 'ja', 'teve', 'tratamento', 'uma', 'pobre', 'mulher', 'comida', 'de', 'desgostos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fora casada com o dono de uma casa de chapéus, que quebrou e suicidou-se, deixando-lhe uma filha muito \n",
      "doentinha e fraca, a quem Isabel sacrificou tudo para educar, dando-lhe mestre até de francês'\n",
      "Tokens gerados: ['fora', 'casada', 'com', 'o', 'dono', 'de', 'uma', 'casa', 'de', 'chapeus', ',', 'que', 'quebrou', 'e', 'suicidou-se', ',', 'deixando-lhe', 'uma', 'filha', 'muito', 'doentinha', 'e', 'fraca', ',', 'a', 'quem', 'isabel', 'sacrificou', 'tudo', 'para', 'educar', ',', 'dando-lhe', 'mestre', 'ate', 'de', 'frances']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha uma cara macilenta \n",
      "de velha portuguesa devota, que já foi gorda, bochechas moles de pelancas rechupadas, que lhe pendiam dos cantos da \n",
      "boca como saquinhos vazios; fios negros no queixo, olhos castanhos, sempre chorosos engolidos pelas pálpebras'\n",
      "Tokens gerados: ['tinha', 'uma', 'cara', 'macilenta', 'de', 'velha', 'portuguesa', 'devota', ',', 'que', 'ja', 'foi', 'gorda', ',', 'bochechas', 'moles', 'de', 'pelancas', 'rechupadas', ',', 'que', 'lhe', 'pendiam', 'dos', 'cantos', 'da', 'boca', 'como', 'saquinhos', 'vazios', 'fios', 'negros', 'no', 'queixo', ',', 'olhos', 'castanhos', ',', 'sempre', 'chorosos', 'engolidos', 'pelas', 'palpebras']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Puxava em bandos sobre as fontes o escasso cabelo grisalho untado de óleo de amêndoas doces'\n",
      "Tokens gerados: ['puxava', 'em', 'bandos', 'sobre', 'as', 'fontes', 'o', 'escasso', 'cabelo', 'grisalho', 'untado', 'de', 'oleo', 'de', 'amendoas', 'doces']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando saia à rua \n",
      "punha um eterno vestido de seda preta, achamalotada, cuja saia não fazia rugas, e um xale encarnado que lhe dava a \n",
      "todo o corpo um feitio piramidal'\n",
      "Tokens gerados: ['quando', 'saia', 'a', 'rua', 'punha', 'um', 'eterno', 'vestido', 'de', 'seda', 'preta', ',', 'achamalotada', ',', 'cuja', 'saia', 'nao', 'fazia', 'rugas', ',', 'e', 'um', 'xale', 'encarnado', 'que', 'lhe', 'dava', 'a', 'todo', 'o', 'corpo', 'um', 'feitio', 'piramidal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Da sua passada grandeza só lhe ficara uma caixa de rapé de ouro, na qual a \n",
      "inconsolável senhora pitadeava  agora, suspirando a cada pitada'\n",
      "Tokens gerados: ['da', 'sua', 'passada', 'grandeza', 'so', 'lhe', 'ficara', 'uma', 'caixa', 'de', 'rape', 'de', 'ouro', ',', 'na', 'qual', 'a', 'inconsolavel', 'senhora', 'pitadeava', 'agora', ',', 'suspirando', 'a', 'cada', 'pitada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A filha era a flor do cortiço'\n",
      "Tokens gerados: ['a', 'filha', 'era', 'a', 'flor', 'do', 'cortico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Chamavam-lhe Pombinha'\n",
      "Tokens gerados: ['chamavam-lhe', 'pombinha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Bonita, posto que enfermiça e nervosa ao último ponto; \n",
      "loura, muito pálida, com uns modos de menina de boa família'\n",
      "Tokens gerados: ['bonita', ',', 'posto', 'que', 'enfermica', 'e', 'nervosa', 'ao', 'ultimo', 'ponto', 'loura', ',', 'muito', 'palida', ',', 'com', 'uns', 'modos', 'de', 'menina', 'de', 'boa', 'familia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A mãe não lhe permitia lavar, nem engomar, mesmo \n",
      "porque o médico a proibira expressamente'\n",
      "Tokens gerados: ['a', 'mae', 'nao', 'lhe', 'permitia', 'lavar', ',', 'nem', 'engomar', ',', 'mesmo', 'porque', 'o', 'medico', 'a', 'proibira', 'expressamente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha o seu noivo, o João da Costa, moço do comércio, estimado do patrão e dos colegas, com muito futuro, e que \n",
      "a adorava e conhecia desde pequenita; mas Dona Isabel não queria que o casamento se fizesse já'\n",
      "Tokens gerados: ['tinha', 'o', 'seu', 'noivo', ',', 'o', 'joao', 'da', 'costa', ',', 'moco', 'do', 'comercio', ',', 'estimado', 'do', 'patrao', 'e', 'dos', 'colegas', ',', 'com', 'muito', 'futuro', ',', 'e', 'que', 'a', 'adorava', 'e', 'conhecia', 'desde', 'pequenita', 'mas', 'dona', 'isabel', 'nao', 'queria', 'que', 'o', 'casamento', 'se', 'fizesse', 'ja']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É que Pombinha, \n",
      "orçando aliás pelos dezoito anos, não tinha ainda pago à natureza o cruento tributo da puberdade, apesar do zelo da \n",
      "velha e dos sacrifícios que esta fazia para cumprir à risca as prescrições do médico e não faltar à filha o menor desvelo'\n",
      "Tokens gerados: ['e', 'que', 'pombinha', ',', 'orcando', 'alias', 'pelos', 'dezoito', 'anos', ',', 'nao', 'tinha', 'ainda', 'pago', 'a', 'natureza', 'o', 'cruento', 'tributo', 'da', 'puberdade', ',', 'apesar', 'do', 'zelo', 'da', 'velha', 'e', 'dos', 'sacrificios', 'que', 'esta', 'fazia', 'para', 'cumprir', 'a', 'risca', 'as', 'prescricoes', 'do', 'medico', 'e', 'nao', 'faltar', 'a', 'filha', 'o', 'menor', 'desvelo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No entanto, coitadas! daquele casamento dependia a felicidade de ambas, porque o Costa, bem empregado como se \n",
      "achava em casa de um tio seu, de quem mais tarde havia de ser sócio, tencionava, logo que mudasse de estado, \n",
      "restituí-las ao seu primitivo circulo social'\n",
      "Tokens gerados: ['no', 'entanto', ',', 'coitadas', 'daquele', 'casamento', 'dependia', 'a', 'felicidade', 'de', 'ambas', ',', 'porque', 'o', 'costa', ',', 'bem', 'empregado', 'como', 'se', 'achava', 'em', 'casa', 'de', 'um', 'tio', 'seu', ',', 'de', 'quem', 'mais', 'tarde', 'havia', 'de', 'ser', 'socio', ',', 'tencionava', ',', 'logo', 'que', 'mudasse', 'de', 'estado', ',', 'restitui-las', 'ao', 'seu', 'primitivo', 'circulo', 'social']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A pobre velha desesperava-se com o fato e pedia a Deus, todas as noites, \n",
      "antes de dormir, que as protegesse e conferisse à filha uma graça tão simples que ele fazia, sem distinção de \n",
      "merecimento, a quantas raparigas havia pelo mundo; mas, a despeito de tamanho empenho, por coisa nenhuma desta \n",
      "vida consentiria que a sua pequena casasse antes de “ser mulher”, como dizia ela'\n",
      "Tokens gerados: ['a', 'pobre', 'velha', 'desesperava-se', 'com', 'o', 'fato', 'e', 'pedia', 'a', 'deus', ',', 'todas', 'as', 'noites', ',', 'antes', 'de', 'dormir', ',', 'que', 'as', 'protegesse', 'e', 'conferisse', 'a', 'filha', 'uma', 'graca', 'tao', 'simples', 'que', 'ele', 'fazia', ',', 'sem', 'distincao', 'de', 'merecimento', ',', 'a', 'quantas', 'raparigas', 'havia', 'pelo', 'mundo', 'mas', ',', 'a', 'despeito', 'de', 'tamanho', 'empenho', ',', 'por', 'coisa', 'nenhuma', 'desta', 'vida', 'consentiria', 'que', 'a', 'sua', 'pequena', 'casasse', 'antes', 'de', '“', 'ser', 'mulher', '”', ',', 'como', 'dizia', 'ela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E “que deixassem lá falar o doutor, \n",
      "entendia que não era decente, nem tinha jeito, dar homem a uma moça que ainda não fora visitada pelas regras! Não! \n",
      "Antes vê-la solteira toda a vida e ficarem ambas curtindo para sempre aquele inferno da estalagem!” \n",
      "Lá no cortiço estavam todos a par desta história; não era segredo para ninguém'\n",
      "Tokens gerados: ['e', '“', 'que', 'deixassem', 'la', 'falar', 'o', 'doutor', ',', 'entendia', 'que', 'nao', 'era', 'decente', ',', 'nem', 'tinha', 'jeito', ',', 'dar', 'homem', 'a', 'uma', 'moca', 'que', 'ainda', 'nao', 'fora', 'visitada', 'pelas', 'regras', 'nao', 'antes', 've-la', 'solteira', 'toda', 'a', 'vida', 'e', 'ficarem', 'ambas', 'curtindo', 'para', 'sempre', 'aquele', 'inferno', 'da', 'estalagem', '”', 'la', 'no', 'cortico', 'estavam', 'todos', 'a', 'par', 'desta', 'historia', 'nao', 'era', 'segredo', 'para', 'ninguem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E não se passava um dia que não \n",
      "interrogassem duas e três vezes a velha com estas frases: \n",
      "— Então? Já veio? \n",
      "— Por que não tenta os banhos de mar? \n",
      "— Por que não chama outro médico? \n",
      "— Eu, se fosse a senhora, casava-os assim mesmo! \n",
      "A velha respondia dizendo que a felicidade não se fizera para ela'\n",
      "Tokens gerados: ['e', 'nao', 'se', 'passava', 'um', 'dia', 'que', 'nao', 'interrogassem', 'duas', 'e', 'tres', 'vezes', 'a', 'velha', 'com', 'estas', 'frases', '—', 'entao', '?', 'ja', 'veio', '?', '—', 'por', 'que', 'nao', 'tenta', 'os', 'banhos', 'de', 'mar', '?', '—', 'por', 'que', 'nao', 'chama', 'outro', 'medico', '?', '—', 'eu', ',', 'se', 'fosse', 'a', 'senhora', ',', 'casava-os', 'assim', 'mesmo', 'a', 'velha', 'respondia', 'dizendo', 'que', 'a', 'felicidade', 'nao', 'se', 'fizera', 'para', 'ela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E suspirava resignada'\n",
      "Tokens gerados: ['e', 'suspirava', 'resignada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando o Costa aparecia depois da sua obrigação para visitar a noiva, os moradores da estalagem \n",
      "cumprimentavam-no em silêncio com um respeitoso ar de lástima e piedade, empenhados tacitamente por aquele \n",
      "caiporismo, contra o qual não valiam nem mesmo as virtudes da Bruxa'\n",
      "Tokens gerados: ['quando', 'o', 'costa', 'aparecia', 'depois', 'da', 'sua', 'obrigacao', 'para', 'visitar', 'a', 'noiva', ',', 'os', 'moradores', 'da', 'estalagem', 'cumprimentavam-no', 'em', 'silencio', 'com', 'um', 'respeitoso', 'ar', 'de', 'lastima', 'e', 'piedade', ',', 'empenhados', 'tacitamente', 'por', 'aquele', 'caiporismo', ',', 'contra', 'o', 'qual', 'nao', 'valiam', 'nem', 'mesmo', 'as', 'virtudes', 'da', 'bruxa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Pombinha era muito querida por toda aquela gente'\n",
      "Tokens gerados: ['pombinha', 'era', 'muito', 'querida', 'por', 'toda', 'aquela', 'gente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era quem lhe escrevia as cartas; quem em geral fazia o rol \n",
      "para as lavadeiras; quem tirava as contas; quem lia o jornal para os que quisessem ouvir'\n",
      "Tokens gerados: ['era', 'quem', 'lhe', 'escrevia', 'as', 'cartas', 'quem', 'em', 'geral', 'fazia', 'o', 'rol', 'para', 'as', 'lavadeiras', 'quem', 'tirava', 'as', 'contas', 'quem', 'lia', 'o', 'jornal', 'para', 'os', 'que', 'quisessem', 'ouvir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Prezavam-na com muito \n",
      "respeito e davam-lhe presentes, o que lhe permitia certo luxo relativo'\n",
      "Tokens gerados: ['prezavam-na', 'com', 'muito', 'respeito', 'e', 'davam-lhe', 'presentes', ',', 'o', 'que', 'lhe', 'permitia', 'certo', 'luxo', 'relativo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Andava sempre de botinhas ou sapatinhos com \n",
      "meias de cor, seu vestido de chita engomado; tinha as suas joiazinhas para sair à rua, e, aos domingos, quem a \n",
      "encontrasse à missa na igreja de São João Batista, não seria capaz de desconfiar que ela morava em cortiço'\n",
      "Tokens gerados: ['andava', 'sempre', 'de', 'botinhas', 'ou', 'sapatinhos', 'com', 'meias', 'de', 'cor', ',', 'seu', 'vestido', 'de', 'chita', 'engomado', 'tinha', 'as', 'suas', 'joiazinhas', 'para', 'sair', 'a', 'rua', ',', 'e', ',', 'aos', 'domingos', ',', 'quem', 'a', 'encontrasse', 'a', 'missa', 'na', 'igreja', 'de', 'sao', 'joao', 'batista', ',', 'nao', 'seria', 'capaz', 'de', 'desconfiar', 'que', 'ela', 'morava', 'em', 'cortico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fechava a fila das primeiras lavadeiras, o Albino, um sujeito afeminado, fraco, cor de espargo cozido e com um \n",
      "cabelinho castanho, deslavado e pobre, que lhe caia, numa só linha, até ao pescocinho mole e fino'\n",
      "Tokens gerados: ['fechava', 'a', 'fila', 'das', 'primeiras', 'lavadeiras', ',', 'o', 'albino', ',', 'um', 'sujeito', 'afeminado', ',', 'fraco', ',', 'cor', 'de', 'espargo', 'cozido', 'e', 'com', 'um', 'cabelinho', 'castanho', ',', 'deslavado', 'e', 'pobre', ',', 'que', 'lhe', 'caia', ',', 'numa', 'so', 'linha', ',', 'ate', 'ao', 'pescocinho', 'mole', 'e', 'fino']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era lavadeiro e vivia \n",
      "sempre entre as mulheres, com quem já estava tão familiarizado que elas o tratavam como a uma pessoa do mesmo \n",
      "sexo; em presença dele falavam de coisas que não exporiam em presença de outro homem; faziam-no até confidente dos \n",
      "seus amores e das suas infidelidades, com uma franqueza que o não revoltava, nem comovia'\n",
      "Tokens gerados: ['era', 'lavadeiro', 'e', 'vivia', 'sempre', 'entre', 'as', 'mulheres', ',', 'com', 'quem', 'ja', 'estava', 'tao', 'familiarizado', 'que', 'elas', 'o', 'tratavam', 'como', 'a', 'uma', 'pessoa', 'do', 'mesmo', 'sexo', 'em', 'presenca', 'dele', 'falavam', 'de', 'coisas', 'que', 'nao', 'exporiam', 'em', 'presenca', 'de', 'outro', 'homem', 'faziam-no', 'ate', 'confidente', 'dos', 'seus', 'amores', 'e', 'das', 'suas', 'infidelidades', ',', 'com', 'uma', 'franqueza', 'que', 'o', 'nao', 'revoltava', ',', 'nem', 'comovia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Quando um casal brigava \n",
      "ou duas amigas se disputavam, era sempre Albino quem tratava de reconciliá-los, exortando as mulheres à concórdia'\n",
      "Tokens gerados: ['quando', 'um', 'casal', 'brigava', 'ou', 'duas', 'amigas', 'se', 'disputavam', ',', 'era', 'sempre', 'albino', 'quem', 'tratava', 'de', 'reconcilia-los', ',', 'exortando', 'as', 'mulheres', 'a', 'concordia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Dantes encarregava-se de cobrar o rol das colegas, por amabilidade; mas uma vez, indo a uma república de estudantes, \n",
      "deram-lhe lá, ninguém sabia por quê, uma dúzia de bolos, e o pobre-diabo jurou então, entre lágrimas e soluços, que \n",
      "nunca mais se incumbiria de receber os róis'\n",
      "Tokens gerados: ['dantes', 'encarregava-se', 'de', 'cobrar', 'o', 'rol', 'das', 'colegas', ',', 'por', 'amabilidade', 'mas', 'uma', 'vez', ',', 'indo', 'a', 'uma', 'republica', 'de', 'estudantes', ',', 'deram-lhe', 'la', ',', 'ninguem', 'sabia', 'por', 'que', ',', 'uma', 'duzia', 'de', 'bolos', ',', 'e', 'o', 'pobre-diabo', 'jurou', 'entao', ',', 'entre', 'lagrimas', 'e', 'solucos', ',', 'que', 'nunca', 'mais', 'se', 'incumbiria', 'de', 'receber', 'os', 'rois']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E daí em diante, com efeito, não arredava os pezinhos do cortiço, a não ser nos dias de carnaval, em que ia, \n",
      "vestido de dançarina, passear à tarde pelas ruas e à noite dançar nos bailes dos teatros'\n",
      "Tokens gerados: ['e', 'dai', 'em', 'diante', ',', 'com', 'efeito', ',', 'nao', 'arredava', 'os', 'pezinhos', 'do', 'cortico', ',', 'a', 'nao', 'ser', 'nos', 'dias', 'de', 'carnaval', ',', 'em', 'que', 'ia', ',', 'vestido', 'de', 'dancarina', ',', 'passear', 'a', 'tarde', 'pelas', 'ruas', 'e', 'a', 'noite', 'dancar', 'nos', 'bailes', 'dos', 'teatros']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tinha verdadeira paixão por esse \n",
      "divertimento; ajuntava dinheiro durante o ano para gastar todo com a mascarada'\n",
      "Tokens gerados: ['tinha', 'verdadeira', 'paixao', 'por', 'esse', 'divertimento', 'ajuntava', 'dinheiro', 'durante', 'o', 'ano', 'para', 'gastar', 'todo', 'com', 'a', 'mascarada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E ninguém o encontrava, domingo ou \n",
      "dia de semana, lavando ou descansando, que não estivesse com a sua calça branca engomada, a sua camisa limpa, um \n",
      "lenço ao pescoço, e, amarrado à cinta, um avental que lhe caia sobre as pernas como uma saia'\n",
      "Tokens gerados: ['e', 'ninguem', 'o', 'encontrava', ',', 'domingo', 'ou', 'dia', 'de', 'semana', ',', 'lavando', 'ou', 'descansando', ',', 'que', 'nao', 'estivesse', 'com', 'a', 'sua', 'calca', 'branca', 'engomada', ',', 'a', 'sua', 'camisa', 'limpa', ',', 'um', 'lenco', 'ao', 'pescoco', ',', 'e', ',', 'amarrado', 'a', 'cinta', ',', 'um', 'avental', 'que', 'lhe', 'caia', 'sobre', 'as', 'pernas', 'como', 'uma', 'saia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não fumava, não bebia \n",
      "espíritos e trazia sempre as mãos geladas e úmidas'\n",
      "Tokens gerados: ['nao', 'fumava', ',', 'nao', 'bebia', 'espiritos', 'e', 'trazia', 'sempre', 'as', 'maos', 'geladas', 'e', 'umidas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Naquela manhã levantara-se ainda um pouco mais lânguido que do costume, porque passara mal a noite'\n",
      "Tokens gerados: ['naquela', 'manha', 'levantara-se', 'ainda', 'um', 'pouco', 'mais', 'languido', 'que', 'do', 'costume', ',', 'porque', 'passara', 'mal', 'a', 'noite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A velha \n",
      "Isabel, que lhe ficava ao lado esquerdo, ouvindo-o suspirar com insistência, perguntou-lhe o que tinha'\n",
      "Tokens gerados: ['a', 'velha', 'isabel', ',', 'que', 'lhe', 'ficava', 'ao', 'lado', 'esquerdo', ',', 'ouvindo-o', 'suspirar', 'com', 'insistencia', ',', 'perguntou-lhe', 'o', 'que', 'tinha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ah! muita moleza de corpo e uma pontada do vazio que o não deixava! \n",
      "A velha receitou diversos remédios, e ficaram os dois, no meio de toda aquela vida, a falar tristemente sobre \n",
      "moléstias'\n",
      "Tokens gerados: ['ah', 'muita', 'moleza', 'de', 'corpo', 'e', 'uma', 'pontada', 'do', 'vazio', 'que', 'o', 'nao', 'deixava', 'a', 'velha', 'receitou', 'diversos', 'remedios', ',', 'e', 'ficaram', 'os', 'dois', ',', 'no', 'meio', 'de', 'toda', 'aquela', 'vida', ',', 'a', 'falar', 'tristemente', 'sobre', 'molestias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, enquanto, no resto da fileira, a Machona, a Augusta, a Leocádia, a Bruxa, a Marciana e sua filha conversavam \n",
      "de tina a tina, berrando e quase sem se ouvirem, a voz um tanto cansada já pelo serviço, defronte delas, separado pelos \n",
      "jiraus, formava-se um novo renque de lavadeiras, que acudiam de fora, carregadas de trouxas, e iam ruidosamente \n",
      "tomando lagar ao lado umas das outras, entre uma agitação sem tréguas, onde se não distinguia o que era galhofa e o \n",
      "que era briga'\n",
      "Tokens gerados: ['e', ',', 'enquanto', ',', 'no', 'resto', 'da', 'fileira', ',', 'a', 'machona', ',', 'a', 'augusta', ',', 'a', 'leocadia', ',', 'a', 'bruxa', ',', 'a', 'marciana', 'e', 'sua', 'filha', 'conversavam', 'de', 'tina', 'a', 'tina', ',', 'berrando', 'e', 'quase', 'sem', 'se', 'ouvirem', ',', 'a', 'voz', 'um', 'tanto', 'cansada', 'ja', 'pelo', 'servico', ',', 'defronte', 'delas', ',', 'separado', 'pelos', 'jiraus', ',', 'formava-se', 'um', 'novo', 'renque', 'de', 'lavadeiras', ',', 'que', 'acudiam', 'de', 'fora', ',', 'carregadas', 'de', 'trouxas', ',', 'e', 'iam', 'ruidosamente', 'tomando', 'lagar', 'ao', 'lado', 'umas', 'das', 'outras', ',', 'entre', 'uma', 'agitacao', 'sem', 'treguas', ',', 'onde', 'se', 'nao', 'distinguia', 'o', 'que', 'era', 'galhofa', 'e', 'o', 'que', 'era', 'briga']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma a uma ocupavam-se todas as tinas'\n",
      "Tokens gerados: ['uma', 'a', 'uma', 'ocupavam-se', 'todas', 'as', 'tinas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E de todos os casulos do cortiço saiam homens para as suas \n",
      "obrigações'\n",
      "Tokens gerados: ['e', 'de', 'todos', 'os', 'casulos', 'do', 'cortico', 'saiam', 'homens', 'para', 'as', 'suas', 'obrigacoes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Por uma porta que havia ao fundo da estalagem desapareciam os trabalhadores da pedreira, donde vinha \n",
      "agora o retinir dos alviões e das picaretas'\n",
      "Tokens gerados: ['por', 'uma', 'porta', 'que', 'havia', 'ao', 'fundo', 'da', 'estalagem', 'desapareciam', 'os', 'trabalhadores', 'da', 'pedreira', ',', 'donde', 'vinha', 'agora', 'o', 'retinir', 'dos', 'alvioes', 'e', 'das', 'picaretas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Miranda, de calças de brim, chapéu alto e sobrecasaca preta, passou lá fora, \n",
      "em caminho para o armazém, acompanhado pelo Henrique que ia para as aulas'\n",
      "Tokens gerados: ['o', 'miranda', ',', 'de', 'calcas', 'de', 'brim', ',', 'chapeu', 'alto', 'e', 'sobrecasaca', 'preta', ',', 'passou', 'la', 'fora', ',', 'em', 'caminho', 'para', 'o', 'armazem', ',', 'acompanhado', 'pelo', 'henrique', 'que', 'ia', 'para', 'as', 'aulas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O Alexandre, que estivera de serviço \n",
      "essa madrugada, entrou solene, atravessou o pátio, sem falar a ninguém, nem mesmo à mulher, e recolheu-se à casa, \n",
      "para dormir'\n",
      "Tokens gerados: ['o', 'alexandre', ',', 'que', 'estivera', 'de', 'servico', 'essa', 'madrugada', ',', 'entrou', 'solene', ',', 'atravessou', 'o', 'patio', ',', 'sem', 'falar', 'a', 'ninguem', ',', 'nem', 'mesmo', 'a', 'mulher', ',', 'e', 'recolheu-se', 'a', 'casa', ',', 'para', 'dormir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um grupo de mascates, o Delporto, o Pompeo, o Francesco e o Andréa, armado cada qual com a sua \n",
      "grande caixa de bugigangas, saiu para a peregrinação de todos os dias, altercando e praguejando em italiano'\n",
      "Tokens gerados: ['um', 'grupo', 'de', 'mascates', ',', 'o', 'delporto', ',', 'o', 'pompeo', ',', 'o', 'francesco', 'e', 'o', 'andrea', ',', 'armado', 'cada', 'qual', 'com', 'a', 'sua', 'grande', 'caixa', 'de', 'bugigangas', ',', 'saiu', 'para', 'a', 'peregrinacao', 'de', 'todos', 'os', 'dias', ',', 'altercando', 'e', 'praguejando', 'em', 'italiano']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um rapazito de paletó entrou da rua e foi perguntar à Machona pela Nhá Rita'\n",
      "Tokens gerados: ['um', 'rapazito', 'de', 'paleto', 'entrou', 'da', 'rua', 'e', 'foi', 'perguntar', 'a', 'machona', 'pela', 'nha', 'rita']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— A Rita Baiana? Sei cá! Faz amanhã oito dias que ela arribou! \n",
      "A Leocádia explicou logo que a mulata estava com certeza de pândega com o Firmo'\n",
      "Tokens gerados: ['—', 'a', 'rita', 'baiana', '?', 'sei', 'ca', 'faz', 'amanha', 'oito', 'dias', 'que', 'ela', 'arribou', 'a', 'leocadia', 'explicou', 'logo', 'que', 'a', 'mulata', 'estava', 'com', 'certeza', 'de', 'pandega', 'com', 'o', 'firmo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que Firmo? interrogou Augusta'\n",
      "Tokens gerados: ['—', 'que', 'firmo', '?', 'interrogou', 'augusta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Aquele cabravasco que se metia às vezes ai com ela'\n",
      "Tokens gerados: ['—', 'aquele', 'cabravasco', 'que', 'se', 'metia', 'as', 'vezes', 'ai', 'com', 'ela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Diz que é torneiro'\n",
      "Tokens gerados: ['diz', 'que', 'e', 'torneiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ela mudou-se? perguntou o pequeno'\n",
      "Tokens gerados: ['—', 'ela', 'mudou-se', '?', 'perguntou', 'o', 'pequeno']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não, disse a Machona; o quarto está fechado, mas a mulata tem coisas lá'\n",
      "Tokens gerados: ['—', 'nao', ',', 'disse', 'a', 'machona', 'o', 'quarto', 'esta', 'fechado', ',', 'mas', 'a', 'mulata', 'tem', 'coisas', 'la']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Você o que queria? \n",
      "— Vinha buscar uma roupa que está com ela'\n",
      "Tokens gerados: ['voce', 'o', 'que', 'queria', '?', '—', 'vinha', 'buscar', 'uma', 'roupa', 'que', 'esta', 'com', 'ela']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não sei, filho, pergunta na venda ao João Romão, que talvez te possa dizer alguma coisa'\n",
      "Tokens gerados: ['—', 'nao', 'sei', ',', 'filho', ',', 'pergunta', 'na', 'venda', 'ao', 'joao', 'romao', ',', 'que', 'talvez', 'te', 'possa', 'dizer', 'alguma', 'coisa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ali? \n",
      "— Sim, pequeno, naquela porta, onde a preta do tabuleiro está vendendo! Ó diabo! olha que pisas a boneca de \n",
      "anil! Já se viu que sorte? Parece que não vê onde pisa este raio de criança! \n",
      "E, notando que o filho, o Agostinho, se aproximava para tomar o lugar do outro que já se ia: \n",
      "— Sai daí, tu também, peste! Já principias na reinação de todos os dias? Vem para cá, que levas! Mas, é verdade, \n",
      "que fazes tu que não vais regar a horta do Comendador? \n",
      "— Ele disse ontem que eu agora fosse à tarde, que era melhor'\n",
      "Tokens gerados: ['—', 'ali', '?', '—', 'sim', ',', 'pequeno', ',', 'naquela', 'porta', ',', 'onde', 'a', 'preta', 'do', 'tabuleiro', 'esta', 'vendendo', 'o', 'diabo', 'olha', 'que', 'pisas', 'a', 'boneca', 'de', 'anil', 'ja', 'se', 'viu', 'que', 'sorte', '?', 'parece', 'que', 'nao', 've', 'onde', 'pisa', 'este', 'raio', 'de', 'crianca', 'e', ',', 'notando', 'que', 'o', 'filho', ',', 'o', 'agostinho', ',', 'se', 'aproximava', 'para', 'tomar', 'o', 'lugar', 'do', 'outro', 'que', 'ja', 'se', 'ia', '—', 'sai', 'dai', ',', 'tu', 'tambem', ',', 'peste', 'ja', 'principias', 'na', 'reinacao', 'de', 'todos', 'os', 'dias', '?', 'vem', 'para', 'ca', ',', 'que', 'levas', 'mas', ',', 'e', 'verdade', ',', 'que', 'fazes', 'tu', 'que', 'nao', 'vais', 'regar', 'a', 'horta', 'do', 'comendador', '?', '—', 'ele', 'disse', 'ontem', 'que', 'eu', 'agora', 'fosse', 'a', 'tarde', ',', 'que', 'era', 'melhor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah! E amanhã, não te esqueças, recebe os dois mil-réis, que é fim do mês'\n",
      "Tokens gerados: ['—', 'ah', 'e', 'amanha', ',', 'nao', 'te', 'esquecas', ',', 'recebe', 'os', 'dois', 'mil-reis', ',', 'que', 'e', 'fim', 'do', 'mes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Olha! Vai lá dentro e diz a Nenen \n",
      "que te entregue a roupa que veio ontem à noite'\n",
      "Tokens gerados: ['olha', 'vai', 'la', 'dentro', 'e', 'diz', 'a', 'nenen', 'que', 'te', 'entregue', 'a', 'roupa', 'que', 'veio', 'ontem', 'a', 'noite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O pequeno afastou-se de carreira, e ela lhe gritou na pista: \n",
      "— E que não ponha o refogado no fogo sem eu ter lá ido! \n",
      "Uma conversa cerrada travara-se no resto da fila de lavadeiras a respeito da Rita Baiana'\n",
      "Tokens gerados: ['o', 'pequeno', 'afastou-se', 'de', 'carreira', ',', 'e', 'ela', 'lhe', 'gritou', 'na', 'pista', '—', 'e', 'que', 'nao', 'ponha', 'o', 'refogado', 'no', 'fogo', 'sem', 'eu', 'ter', 'la', 'ido', 'uma', 'conversa', 'cerrada', 'travara-se', 'no', 'resto', 'da', 'fila', 'de', 'lavadeiras', 'a', 'respeito', 'da', 'rita', 'baiana']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— É doida mesmo!'\n",
      "Tokens gerados: ['—', 'e', 'doida', 'mesmo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'censurava Augusta'\n",
      "Tokens gerados: ['censurava', 'augusta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Meter-se na pândega sem dar conta da roupa que lhe entregaram'\n",
      "Tokens gerados: ['meter-se', 'na', 'pandega', 'sem', 'dar', 'conta', 'da', 'roupa', 'que', 'lhe', 'entregaram']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Assim há de ficar sem um freguês'\n",
      "Tokens gerados: ['assim', 'ha', 'de', 'ficar', 'sem', 'um', 'fregues']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Aquela não endireita mais!'\n",
      "Tokens gerados: ['—', 'aquela', 'nao', 'endireita', 'mais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Cada vez fica até mais assanhada!'\n",
      "Tokens gerados: ['cada', 'vez', 'fica', 'ate', 'mais', 'assanhada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Parece que tem fogo no rabo! Pode haver o \n",
      "serviço que houver, aparecendo pagode, vai tudo pro lado! Olha o que saiu o ano passado com a festa da Penha!'\n",
      "Tokens gerados: ['parece', 'que', 'tem', 'fogo', 'no', 'rabo', 'pode', 'haver', 'o', 'servico', 'que', 'houver', ',', 'aparecendo', 'pagode', ',', 'vai', 'tudo', 'pro', 'lado', 'olha', 'o', 'que', 'saiu', 'o', 'ano', 'passado', 'com', 'a', 'festa', 'da', 'penha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então agora, com este mulato, o Firmo, é uma pouca-vergonha! Est’ro dia, pois você não viu? levaram ai \n",
      "numa bebedeira, a dançar e cantar à viola, que nem sei o que parecia! Deus te livre! \n",
      "— Para tudo há horas e há dias!'\n",
      "Tokens gerados: ['—', 'entao', 'agora', ',', 'com', 'este', 'mulato', ',', 'o', 'firmo', ',', 'e', 'uma', 'pouca-vergonha', 'est', '’', 'ro', 'dia', ',', 'pois', 'voce', 'nao', 'viu', '?', 'levaram', 'ai', 'numa', 'bebedeira', ',', 'a', 'dancar', 'e', 'cantar', 'a', 'viola', ',', 'que', 'nem', 'sei', 'o', 'que', 'parecia', 'deus', 'te', 'livre', '—', 'para', 'tudo', 'ha', 'horas', 'e', 'ha', 'dias']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Para a Rita todos os dias são dias santos! A questão é aparecer quem puxe por ela! \n",
      "— Ainda assim não e má criatura'\n",
      "Tokens gerados: ['—', 'para', 'a', 'rita', 'todos', 'os', 'dias', 'sao', 'dias', 'santos', 'a', 'questao', 'e', 'aparecer', 'quem', 'puxe', 'por', 'ela', '—', 'ainda', 'assim', 'nao', 'e', 'ma', 'criatura']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tirante o defeito da vadiagem'\n",
      "Tokens gerados: ['tirante', 'o', 'defeito', 'da', 'vadiagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Bom coração tem ela, até demais, que não guarda um vintém pro dia de amanhã'\n",
      "Tokens gerados: ['—', 'bom', 'coracao', 'tem', 'ela', ',', 'ate', 'demais', ',', 'que', 'nao', 'guarda', 'um', 'vintem', 'pro', 'dia', 'de', 'amanha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Parece que o dinheiro lhe faz \n",
      "comichão no corpo! \n",
      "— Depois é que são elas!'\n",
      "Tokens gerados: ['parece', 'que', 'o', 'dinheiro', 'lhe', 'faz', 'comichao', 'no', 'corpo', '—', 'depois', 'e', 'que', 'sao', 'elas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O João Romão já lhe não fia! \n",
      "— Pois olhe que a Rita lhe tem enchido bem as mãos; quando ela tem dinheiro é porque o gasta mesmo! \n",
      "E as lavadeiras não se calavam, sempre a esfregar, e a bater, e a torcer camisas e ceroulas, esfogueadas já pelo \n",
      "exercício'\n",
      "Tokens gerados: ['o', 'joao', 'romao', 'ja', 'lhe', 'nao', 'fia', '—', 'pois', 'olhe', 'que', 'a', 'rita', 'lhe', 'tem', 'enchido', 'bem', 'as', 'maos', 'quando', 'ela', 'tem', 'dinheiro', 'e', 'porque', 'o', 'gasta', 'mesmo', 'e', 'as', 'lavadeiras', 'nao', 'se', 'calavam', ',', 'sempre', 'a', 'esfregar', ',', 'e', 'a', 'bater', ',', 'e', 'a', 'torcer', 'camisas', 'e', 'ceroulas', ',', 'esfogueadas', 'ja', 'pelo', 'exercicio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao passo que, em torno da sua tagarelice, o cortiço se embandeirava todo de roupa molhada, de onde o sol \n",
      "tirava cintilações de prata'\n",
      "Tokens gerados: ['ao', 'passo', 'que', ',', 'em', 'torno', 'da', 'sua', 'tagarelice', ',', 'o', 'cortico', 'se', 'embandeirava', 'todo', 'de', 'roupa', 'molhada', ',', 'de', 'onde', 'o', 'sol', 'tirava', 'cintilacoes', 'de', 'prata']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estavam em dezembro e o dia era ardente'\n",
      "Tokens gerados: ['estavam', 'em', 'dezembro', 'e', 'o', 'dia', 'era', 'ardente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A grama dos coradouros tinha reflexos esmeraldinos; as paredes que \n",
      "davam frente ao Nascente, caiadinhas de novo, reverberavam iluminadas, ofuscando a vista'\n",
      "Tokens gerados: ['a', 'grama', 'dos', 'coradouros', 'tinha', 'reflexos', 'esmeraldinos', 'as', 'paredes', 'que', 'davam', 'frente', 'ao', 'nascente', ',', 'caiadinhas', 'de', 'novo', ',', 'reverberavam', 'iluminadas', ',', 'ofuscando', 'a', 'vista']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em uma das janelas da sala \n",
      "de jantar do Miranda, Dona Estela e Zulmira, ambas vestidas de claro e ambas a limarem as unhas, conversavam em \n",
      "voz surda, indiferentes à agitação que ia lá embaixo, muito esquecidas na sua tranqüilidade de entes felizes'\n",
      "Tokens gerados: ['em', 'uma', 'das', 'janelas', 'da', 'sala', 'de', 'jantar', 'do', 'miranda', ',', 'dona', 'estela', 'e', 'zulmira', ',', 'ambas', 'vestidas', 'de', 'claro', 'e', 'ambas', 'a', 'limarem', 'as', 'unhas', ',', 'conversavam', 'em', 'voz', 'surda', ',', 'indiferentes', 'a', 'agitacao', 'que', 'ia', 'la', 'embaixo', ',', 'muito', 'esquecidas', 'na', 'sua', 'tranquilidade', 'de', 'entes', 'felizes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, agora o maior movimento era na venda à entrada da estalagem'\n",
      "Tokens gerados: ['entretanto', ',', 'agora', 'o', 'maior', 'movimento', 'era', 'na', 'venda', 'a', 'entrada', 'da', 'estalagem']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Davam nove horas e os operários das \n",
      "fábricas chegavam-se para o almoço'\n",
      "Tokens gerados: ['davam', 'nove', 'horas', 'e', 'os', 'operarios', 'das', 'fabricas', 'chegavam-se', 'para', 'o', 'almoco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ao balcão o Domingos e o Manuel não tinham mãos a medir com a criadagem da \n",
      "vizinhança; os embrulhos de papel amarelo sucediam-se, e o dinheiro pingava sem intermitência dentro da gaveta'\n",
      "Tokens gerados: ['ao', 'balcao', 'o', 'domingos', 'e', 'o', 'manuel', 'nao', 'tinham', 'maos', 'a', 'medir', 'com', 'a', 'criadagem', 'da', 'vizinhanca', 'os', 'embrulhos', 'de', 'papel', 'amarelo', 'sucediam-se', ',', 'e', 'o', 'dinheiro', 'pingava', 'sem', 'intermitencia', 'dentro', 'da', 'gaveta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Meio quilo de arroz! \n",
      "— Um tostão de açúcar! \n",
      "— Uma garrafa de vinagre! \n",
      "— Dois martelos de vinho! \n",
      "— Dois vinténs de fumo! \n",
      "— Quatro de sabão! \n",
      "E os gritos confundiam-se numa mistura de vozes de todos os tons'\n",
      "Tokens gerados: ['—', 'meio', 'quilo', 'de', 'arroz', '—', 'um', 'tostao', 'de', 'acucar', '—', 'uma', 'garrafa', 'de', 'vinagre', '—', 'dois', 'martelos', 'de', 'vinho', '—', 'dois', 'vintens', 'de', 'fumo', '—', 'quatro', 'de', 'sabao', 'e', 'os', 'gritos', 'confundiam-se', 'numa', 'mistura', 'de', 'vozes', 'de', 'todos', 'os', 'tons']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ouviam-se protestos entre os compradores: \n",
      "— Me avie, seu Domingos! Eu deixei a comida no fogo! \n",
      "— Ó peste! dá cá as batatas, que eu tenho mais o que fazer! \n",
      "— Seu Manuel, não me demore essa manteiga! \n",
      "Ao lado, na casinha de pasto, a Bertoleza, de saias arrepanhadas no quadril, o cachaço grosso e negro, reluzindo \n",
      "de suor, ia e vinha de uma panela à outra, fazendo pratos, que João Romão levava de carreira aos trabalhadores \n",
      "assentados num compartimento junto'\n",
      "Tokens gerados: ['ouviam-se', 'protestos', 'entre', 'os', 'compradores', '—', 'me', 'avie', ',', 'seu', 'domingos', 'eu', 'deixei', 'a', 'comida', 'no', 'fogo', '—', 'o', 'peste', 'da', 'ca', 'as', 'batatas', ',', 'que', 'eu', 'tenho', 'mais', 'o', 'que', 'fazer', '—', 'seu', 'manuel', ',', 'nao', 'me', 'demore', 'essa', 'manteiga', 'ao', 'lado', ',', 'na', 'casinha', 'de', 'pasto', ',', 'a', 'bertoleza', ',', 'de', 'saias', 'arrepanhadas', 'no', 'quadril', ',', 'o', 'cachaco', 'grosso', 'e', 'negro', ',', 'reluzindo', 'de', 'suor', ',', 'ia', 'e', 'vinha', 'de', 'uma', 'panela', 'a', 'outra', ',', 'fazendo', 'pratos', ',', 'que', 'joao', 'romao', 'levava', 'de', 'carreira', 'aos', 'trabalhadores', 'assentados', 'num', 'compartimento', 'junto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Admitira-se um novo caixeiro, só para o frege, e o rapaz, a cada comensal que ia \n",
      "chegando, recitava, em tom cantado e estridente, a sua interminável lista das comidas que havia'\n",
      "Tokens gerados: ['admitira-se', 'um', 'novo', 'caixeiro', ',', 'so', 'para', 'o', 'frege', ',', 'e', 'o', 'rapaz', ',', 'a', 'cada', 'comensal', 'que', 'ia', 'chegando', ',', 'recitava', ',', 'em', 'tom', 'cantado', 'e', 'estridente', ',', 'a', 'sua', 'interminavel', 'lista', 'das', 'comidas', 'que', 'havia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um cheiro forte de \n",
      "azeite frito predominava'\n",
      "Tokens gerados: ['um', 'cheiro', 'forte', 'de', 'azeite', 'frito', 'predominava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O parati circulava por todas as mesas, e cada caneca de café, de louça espessa, erguia um \n",
      "vulcão de fumo tresandando a milho queimado'\n",
      "Tokens gerados: ['o', 'parati', 'circulava', 'por', 'todas', 'as', 'mesas', ',', 'e', 'cada', 'caneca', 'de', 'cafe', ',', 'de', 'louca', 'espessa', ',', 'erguia', 'um', 'vulcao', 'de', 'fumo', 'tresandando', 'a', 'milho', 'queimado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Uma algazarra medonha, em que ninguém se entendia! Cruzavam-se \n",
      "conversas em todas as direções, discutia-se a berros, com valentes punhadas sobre as mesas'\n",
      "Tokens gerados: ['uma', 'algazarra', 'medonha', ',', 'em', 'que', 'ninguem', 'se', 'entendia', 'cruzavam-se', 'conversas', 'em', 'todas', 'as', 'direcoes', ',', 'discutia-se', 'a', 'berros', ',', 'com', 'valentes', 'punhadas', 'sobre', 'as', 'mesas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E sempre a sair, e sempre a \n",
      "entrar gente, e os que saiam, depois daquela comezaina grossa, iam radiantes de contentamento, com a barriga bem \n",
      "cheia, a arrotar'\n",
      "Tokens gerados: ['e', 'sempre', 'a', 'sair', ',', 'e', 'sempre', 'a', 'entrar', 'gente', ',', 'e', 'os', 'que', 'saiam', ',', 'depois', 'daquela', 'comezaina', 'grossa', ',', 'iam', 'radiantes', 'de', 'contentamento', ',', 'com', 'a', 'barriga', 'bem', 'cheia', ',', 'a', 'arrotar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Num banco de pau tosco, que existia do lado de fora, junto à parede e perto da venda, um homem, de calça e \n",
      "camisa de zuarte, chinelos de couro cru, esperava, havia já uma boa hora, para falar com o vendeiro'\n",
      "Tokens gerados: ['num', 'banco', 'de', 'pau', 'tosco', ',', 'que', 'existia', 'do', 'lado', 'de', 'fora', ',', 'junto', 'a', 'parede', 'e', 'perto', 'da', 'venda', ',', 'um', 'homem', ',', 'de', 'calca', 'e', 'camisa', 'de', 'zuarte', ',', 'chinelos', 'de', 'couro', 'cru', ',', 'esperava', ',', 'havia', 'ja', 'uma', 'boa', 'hora', ',', 'para', 'falar', 'com', 'o', 'vendeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era um português de seus trinta e cinco a quarenta anos, alto, espadaúdo, barbas ásperas, cabelos pretos e \n",
      "maltratados caindo-lhe sobre a testa, por debaixo de um chapéu de feltro ordinário: pescoço de touro e cara de Hércules, \n",
      "na qual os olhos todavia, humildes como os olhos de um boi de canga, exprimiam tranqüila bondade'\n",
      "Tokens gerados: ['era', 'um', 'portugues', 'de', 'seus', 'trinta', 'e', 'cinco', 'a', 'quarenta', 'anos', ',', 'alto', ',', 'espadaudo', ',', 'barbas', 'asperas', ',', 'cabelos', 'pretos', 'e', 'maltratados', 'caindo-lhe', 'sobre', 'a', 'testa', ',', 'por', 'debaixo', 'de', 'um', 'chapeu', 'de', 'feltro', 'ordinario', 'pescoco', 'de', 'touro', 'e', 'cara', 'de', 'hercules', ',', 'na', 'qual', 'os', 'olhos', 'todavia', ',', 'humildes', 'como', 'os', 'olhos', 'de', 'um', 'boi', 'de', 'canga', ',', 'exprimiam', 'tranquila', 'bondade']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Então ainda não se pode falar ao homem? perguntou ele, indo ao balcão entender-se com o Domingos'\n",
      "Tokens gerados: ['—', 'entao', 'ainda', 'nao', 'se', 'pode', 'falar', 'ao', 'homem', '?', 'perguntou', 'ele', ',', 'indo', 'ao', 'balcao', 'entender-se', 'com', 'o', 'domingos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— O patrão está agora muito ocupado'\n",
      "Tokens gerados: ['—', 'o', 'patrao', 'esta', 'agora', 'muito', 'ocupado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Espere! \n",
      "— Mas são quase dez horas e estou com um gole de café no estômago! \n",
      "— Volte logo! \n",
      "— Moro na cidade nova'\n",
      "Tokens gerados: ['espere', '—', 'mas', 'sao', 'quase', 'dez', 'horas', 'e', 'estou', 'com', 'um', 'gole', 'de', 'cafe', 'no', 'estomago', '—', 'volte', 'logo', '—', 'moro', 'na', 'cidade', 'nova']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É um estirão daqui! \n",
      "O caixeiro gritou então para a cozinha, sem interromper o que fazia: \n",
      "— O homem que ai está, seu João, diz que se vai embora! \n",
      "— Ele que espere um pouco, que já lhe falo! respondeu o vendeiro no meio de uma carreira'\n",
      "Tokens gerados: ['e', 'um', 'estirao', 'daqui', 'o', 'caixeiro', 'gritou', 'entao', 'para', 'a', 'cozinha', ',', 'sem', 'interromper', 'o', 'que', 'fazia', '—', 'o', 'homem', 'que', 'ai', 'esta', ',', 'seu', 'joao', ',', 'diz', 'que', 'se', 'vai', 'embora', '—', 'ele', 'que', 'espere', 'um', 'pouco', ',', 'que', 'ja', 'lhe', 'falo', 'respondeu', 'o', 'vendeiro', 'no', 'meio', 'de', 'uma', 'carreira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Diga-lhe que não vá! \n",
      "— Mas é que ainda não almocei e estou aqui a tinir!'\n",
      "Tokens gerados: ['diga-lhe', 'que', 'nao', 'va', '—', 'mas', 'e', 'que', 'ainda', 'nao', 'almocei', 'e', 'estou', 'aqui', 'a', 'tinir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'observou o Hércules com a sua voz grossa e sonora'\n",
      "Tokens gerados: ['observou', 'o', 'hercules', 'com', 'a', 'sua', 'voz', 'grossa', 'e', 'sonora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ó filho, almoce ai mesmo! Aqui o que não falta é de comer'\n",
      "Tokens gerados: ['—', 'o', 'filho', ',', 'almoce', 'ai', 'mesmo', 'aqui', 'o', 'que', 'nao', 'falta', 'e', 'de', 'comer']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Já podia estar aviado! \n",
      "— Pois vá lá! resolveu o homenzarrão, saindo da venda para entrar na casa de pasto, onde os que lá se achavam o \n",
      "receberam com ar curioso, medindo-o da cabeça aos pés, como faziam sempre com todos os que ai se apresentavam \n",
      "pela primeira vez'\n",
      "Tokens gerados: ['ja', 'podia', 'estar', 'aviado', '—', 'pois', 'va', 'la', 'resolveu', 'o', 'homenzarrao', ',', 'saindo', 'da', 'venda', 'para', 'entrar', 'na', 'casa', 'de', 'pasto', ',', 'onde', 'os', 'que', 'la', 'se', 'achavam', 'o', 'receberam', 'com', 'ar', 'curioso', ',', 'medindo-o', 'da', 'cabeca', 'aos', 'pes', ',', 'como', 'faziam', 'sempre', 'com', 'todos', 'os', 'que', 'ai', 'se', 'apresentavam', 'pela', 'primeira', 'vez']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E assentou-se a uma das mesinhas, vindo logo o caixeiro cantar-lhe a lista dos pratos'\n",
      "Tokens gerados: ['e', 'assentou-se', 'a', 'uma', 'das', 'mesinhas', ',', 'vindo', 'logo', 'o', 'caixeiro', 'cantar-lhe', 'a', 'lista', 'dos', 'pratos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Traga lá o pescado com batatas e veja um martelo de vinho'\n",
      "Tokens gerados: ['—', 'traga', 'la', 'o', 'pescado', 'com', 'batatas', 'e', 'veja', 'um', 'martelo', 'de', 'vinho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Quer verde ou virgem? \n",
      "— Venha o verde; mas anda com isso, filho, que já não vem sem tempo!'\n",
      "Tokens gerados: ['—', 'quer', 'verde', 'ou', 'virgem', '?', '—', 'venha', 'o', 'verde', 'mas', 'anda', 'com', 'isso', ',', 'filho', ',', 'que', 'ja', 'nao', 'vem', 'sem', 'tempo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_o_cortico_aluisio_azevedo_cap_3.json\n",
      "\n",
      "Processando frase original (após split e strip): 'Meia hora depois, quando João Romão se viu menos ocupado, foi ter com o sujeito que o procurava e assentou-se \n",
      "defronte dele, caindo de fadiga, mas sem se queixar, nem se lhe trair a fisionomia o menor sintoma de cansaço'\n",
      "Tokens gerados: ['meia', 'hora', 'depois', ',', 'quando', 'joao', 'romao', 'se', 'viu', 'menos', 'ocupado', ',', 'foi', 'ter', 'com', 'o', 'sujeito', 'que', 'o', 'procurava', 'e', 'assentou-se', 'defronte', 'dele', ',', 'caindo', 'de', 'fadiga', ',', 'mas', 'sem', 'se', 'queixar', ',', 'nem', 'se', 'lhe', 'trair', 'a', 'fisionomia', 'o', 'menor', 'sintoma', 'de', 'cansaco']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Você vem da parte do Machucas? perguntou-lhe'\n",
      "Tokens gerados: ['—', 'voce', 'vem', 'da', 'parte', 'do', 'machucas', '?', 'perguntou-lhe']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ele falou-me de um homem que sabe calçar pedra, lascar \n",
      "fogo e fazer lajedo'\n",
      "Tokens gerados: ['ele', 'falou-me', 'de', 'um', 'homem', 'que', 'sabe', 'calcar', 'pedra', ',', 'lascar', 'fogo', 'e', 'fazer', 'lajedo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Sou eu'\n",
      "Tokens gerados: ['—', 'sou', 'eu']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Estava empregado em outra pedreira? \n",
      "— Estava e estou'\n",
      "Tokens gerados: ['—', 'estava', 'empregado', 'em', 'outra', 'pedreira', '?', '—', 'estava', 'e', 'estou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Na de São Diogo, mas desgostei-me dela e quero passar adiante'\n",
      "Tokens gerados: ['na', 'de', 'sao', 'diogo', ',', 'mas', 'desgostei-me', 'dela', 'e', 'quero', 'passar', 'adiante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Quanto lhe dão lá? \n",
      "— Setenta mil-réis'\n",
      "Tokens gerados: ['—', 'quanto', 'lhe', 'dao', 'la', '?', '—', 'setenta', 'mil-reis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Oh! Isso é um disparate! \n",
      "— Não trabalho por menos'\n",
      "Tokens gerados: ['—', 'oh', 'isso', 'e', 'um', 'disparate', '—', 'nao', 'trabalho', 'por', 'menos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Eu, o maior ordenado que faço é de cinqüenta'\n",
      "Tokens gerados: ['—', 'eu', ',', 'o', 'maior', 'ordenado', 'que', 'faco', 'e', 'de', 'cinquenta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Cinqüenta ganha um macaqueiro'\n",
      "Tokens gerados: ['—', 'cinquenta', 'ganha', 'um', 'macaqueiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ora! tenho aí muitos trabalhadores de lajedo por esse preço! \n",
      "— Duvido que prestem! Aposto a mão direita em como o senhor não encontra por cinqüenta mil-réis quem dirija \n",
      "a broca, pese a pólvora e lasque fogo, sem lhe estragar a pedra e sem fazer desastres! \n",
      "— Sim, mas setenta mil-réis é um ordenado impossível! \n",
      "— Nesse caso vou como vim'\n",
      "Tokens gerados: ['—', 'ora', 'tenho', 'ai', 'muitos', 'trabalhadores', 'de', 'lajedo', 'por', 'esse', 'preco', '—', 'duvido', 'que', 'prestem', 'aposto', 'a', 'mao', 'direita', 'em', 'como', 'o', 'senhor', 'nao', 'encontra', 'por', 'cinquenta', 'mil-reis', 'quem', 'dirija', 'a', 'broca', ',', 'pese', 'a', 'polvora', 'e', 'lasque', 'fogo', ',', 'sem', 'lhe', 'estragar', 'a', 'pedra', 'e', 'sem', 'fazer', 'desastres', '—', 'sim', ',', 'mas', 'setenta', 'mil-reis', 'e', 'um', 'ordenado', 'impossivel', '—', 'nesse', 'caso', 'vou', 'como', 'vim']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Fica o dito por não dito! \n",
      "— Setenta mil-réis é muito dinheiro!'\n",
      "Tokens gerados: ['fica', 'o', 'dito', 'por', 'nao', 'dito', '—', 'setenta', 'mil-reis', 'e', 'muito', 'dinheiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Cá por mim, entendo que vale a pena pagar mais um pouco a um trabalhador bom, do que estar a sofrer \n",
      "desastres, como o que sofreu sua pedreira a semana passada! Não falando na vida do pobre de Cristo que ficou debaixo \n",
      "da pedra! \n",
      "— Ah! O Machucas falou-lhe no desastre? \n",
      "— Contou-mo, sim senhor, e o desastre não aconteceria se o homem soubesse fazer o serviço! \n",
      "— Mas setenta mil-réis é impossível'\n",
      "Tokens gerados: ['—', 'ca', 'por', 'mim', ',', 'entendo', 'que', 'vale', 'a', 'pena', 'pagar', 'mais', 'um', 'pouco', 'a', 'um', 'trabalhador', 'bom', ',', 'do', 'que', 'estar', 'a', 'sofrer', 'desastres', ',', 'como', 'o', 'que', 'sofreu', 'sua', 'pedreira', 'a', 'semana', 'passada', 'nao', 'falando', 'na', 'vida', 'do', 'pobre', 'de', 'cristo', 'que', 'ficou', 'debaixo', 'da', 'pedra', '—', 'ah', 'o', 'machucas', 'falou-lhe', 'no', 'desastre', '?', '—', 'contou-mo', ',', 'sim', 'senhor', ',', 'e', 'o', 'desastre', 'nao', 'aconteceria', 'se', 'o', 'homem', 'soubesse', 'fazer', 'o', 'servico', '—', 'mas', 'setenta', 'mil-reis', 'e', 'impossivel']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Desça um pouco! \n",
      "— Por menos não me serve'\n",
      "Tokens gerados: ['desca', 'um', 'pouco', '—', 'por', 'menos', 'nao', 'me', 'serve']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E escusamos de gastar palavras! \n",
      "— Você conhece a pedreira? \n",
      "— Nunca a vi de perto, mas quis me parecer que é boa'\n",
      "Tokens gerados: ['e', 'escusamos', 'de', 'gastar', 'palavras', '—', 'voce', 'conhece', 'a', 'pedreira', '?', '—', 'nunca', 'a', 'vi', 'de', 'perto', ',', 'mas', 'quis', 'me', 'parecer', 'que', 'e', 'boa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De longe cheirou-me a granito'\n",
      "Tokens gerados: ['de', 'longe', 'cheirou-me', 'a', 'granito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Espere um instante'\n",
      "Tokens gerados: ['—', 'espere', 'um', 'instante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão deu um pulo à venda, deixou algumas ordens, enterrou um chapéu na cabeça e voltou a ter com o \n",
      "outro'\n",
      "Tokens gerados: ['joao', 'romao', 'deu', 'um', 'pulo', 'a', 'venda', ',', 'deixou', 'algumas', 'ordens', ',', 'enterrou', 'um', 'chapeu', 'na', 'cabeca', 'e', 'voltou', 'a', 'ter', 'com', 'o', 'outro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ande a ver! gritou-lhe da porta do frege, que a pouco e pouco se esvaziara de todo'\n",
      "Tokens gerados: ['—', 'ande', 'a', 'ver', 'gritou-lhe', 'da', 'porta', 'do', 'frege', ',', 'que', 'a', 'pouco', 'e', 'pouco', 'se', 'esvaziara', 'de', 'todo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cavouqueiro pagou doze vinténs pelo seu almoço e acompanhou-o em silêncio'\n",
      "Tokens gerados: ['o', 'cavouqueiro', 'pagou', 'doze', 'vintens', 'pelo', 'seu', 'almoco', 'e', 'acompanhou-o', 'em', 'silencio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Atravessaram o cortiço'\n",
      "Tokens gerados: ['atravessaram', 'o', 'cortico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A labutação continuava'\n",
      "Tokens gerados: ['a', 'labutacao', 'continuava']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'As lavadeiras tinham já ido almoçar e tinham voltado de novo para o trabalho'\n",
      "Tokens gerados: ['as', 'lavadeiras', 'tinham', 'ja', 'ido', 'almocar', 'e', 'tinham', 'voltado', 'de', 'novo', 'para', 'o', 'trabalho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Agora \n",
      "estavam todas de chapéu de palha, apesar das toldas que se armaram'\n",
      "Tokens gerados: ['agora', 'estavam', 'todas', 'de', 'chapeu', 'de', 'palha', ',', 'apesar', 'das', 'toldas', 'que', 'se', 'armaram']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um calor de cáustico mordia-lhes os toutiços em \n",
      "brasa e cintilantes de suor'\n",
      "Tokens gerados: ['um', 'calor', 'de', 'caustico', 'mordia-lhes', 'os', 'touticos', 'em', 'brasa', 'e', 'cintilantes', 'de', 'suor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Um estado febril apoderava-se delas naquele rescaldo; aquela digestão feita ao sol \n",
      "fermentava-lhes o sangue'\n",
      "Tokens gerados: ['um', 'estado', 'febril', 'apoderava-se', 'delas', 'naquele', 'rescaldo', 'aquela', 'digestao', 'feita', 'ao', 'sol', 'fermentava-lhes', 'o', 'sangue']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A Machona altercava com uma preta que fora reclamar um par de meias e destrocar uma \n",
      "camisa; a Augusta, muito mole sobre a sua tábua de lavar, parecia derreter-se como sebo; a Leocádia largava de vez em \n",
      "quando a roupa e o sabão para coçar as comichões do quadril e das virilhas, assanhadas pelo mormaço; a Bruxa \n",
      "monologava, resmungando numa insistência de idiota, ao lado da Marciana que, com o seu tipo de mulata velha, um \n",
      "cachimbo ao canto da boca, cantava toadas monótonas do sertão: \n",
      " \n",
      "“Maricas tá marimbando, \n",
      "Maricas tá marimbando, \n",
      "Na passage do riacho \n",
      "Maricas tá marimbando'\n",
      "Tokens gerados: ['a', 'machona', 'altercava', 'com', 'uma', 'preta', 'que', 'fora', 'reclamar', 'um', 'par', 'de', 'meias', 'e', 'destrocar', 'uma', 'camisa', 'a', 'augusta', ',', 'muito', 'mole', 'sobre', 'a', 'sua', 'tabua', 'de', 'lavar', ',', 'parecia', 'derreter-se', 'como', 'sebo', 'a', 'leocadia', 'largava', 'de', 'vez', 'em', 'quando', 'a', 'roupa', 'e', 'o', 'sabao', 'para', 'cocar', 'as', 'comichoes', 'do', 'quadril', 'e', 'das', 'virilhas', ',', 'assanhadas', 'pelo', 'mormaco', 'a', 'bruxa', 'monologava', ',', 'resmungando', 'numa', 'insistencia', 'de', 'idiota', ',', 'ao', 'lado', 'da', 'marciana', 'que', ',', 'com', 'o', 'seu', 'tipo', 'de', 'mulata', 'velha', ',', 'um', 'cachimbo', 'ao', 'canto', 'da', 'boca', ',', 'cantava', 'toadas', 'monotonas', 'do', 'sertao', '“', 'maricas', 'ta', 'marimbando', ',', 'maricas', 'ta', 'marimbando', ',', 'na', 'passage', 'do', 'riacho', 'maricas', 'ta', 'marimbando']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '” \n",
      " \n",
      "A Florinda, alegre, perfeitamente bem com o rigor do sol, a rebolar sem fadigas, assoviava os chorados e lundus \n",
      "que se tocavam na estalagem, e junto dela, a melancólica senhora Dona Isabel suspirava, esfregando a sua roupa dentro \n",
      "da tina, automaticamente, como um condenado a trabalhar no presídio; ao passo que o Albino, saracoteando os seus \n",
      "quadris pobres de homem linfático, batia na tábua um par de calças, no ritmo cadenciado e miúdo de um cozinheiro a \n",
      "bater bifes'\n",
      "Tokens gerados: ['”', 'a', 'florinda', ',', 'alegre', ',', 'perfeitamente', 'bem', 'com', 'o', 'rigor', 'do', 'sol', ',', 'a', 'rebolar', 'sem', 'fadigas', ',', 'assoviava', 'os', 'chorados', 'e', 'lundus', 'que', 'se', 'tocavam', 'na', 'estalagem', ',', 'e', 'junto', 'dela', ',', 'a', 'melancolica', 'senhora', 'dona', 'isabel', 'suspirava', ',', 'esfregando', 'a', 'sua', 'roupa', 'dentro', 'da', 'tina', ',', 'automaticamente', ',', 'como', 'um', 'condenado', 'a', 'trabalhar', 'no', 'presidio', 'ao', 'passo', 'que', 'o', 'albino', ',', 'saracoteando', 'os', 'seus', 'quadris', 'pobres', 'de', 'homem', 'linfatico', ',', 'batia', 'na', 'tabua', 'um', 'par', 'de', 'calcas', ',', 'no', 'ritmo', 'cadenciado', 'e', 'miudo', 'de', 'um', 'cozinheiro', 'a', 'bater', 'bifes']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O corpo tremia-lhe todo, e ele, de vez em quando, suspendia o lenço do pescoço para enxugar a fronte, e \n",
      "então um gemido suspirado subia-lhe aos lábios'\n",
      "Tokens gerados: ['o', 'corpo', 'tremia-lhe', 'todo', ',', 'e', 'ele', ',', 'de', 'vez', 'em', 'quando', ',', 'suspendia', 'o', 'lenco', 'do', 'pescoco', 'para', 'enxugar', 'a', 'fronte', ',', 'e', 'entao', 'um', 'gemido', 'suspirado', 'subia-lhe', 'aos', 'labios']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Da casinha número 8 vinha um falsete agudo, mas afinado'\n",
      "Tokens gerados: ['da', 'casinha', 'numero', '8', 'vinha', 'um', 'falsete', 'agudo', ',', 'mas', 'afinado']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era a das Dores que principiava o seu serviço; não \n",
      "sabia engomar sem cantar'\n",
      "Tokens gerados: ['era', 'a', 'das', 'dores', 'que', 'principiava', 'o', 'seu', 'servico', 'nao', 'sabia', 'engomar', 'sem', 'cantar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'No número 7 Nenen cantarolava em tom muito mais baixo; e de um dos quartos do fundo da \n",
      "estalagem saia de espaço a espaço uma nota áspera de trombone'\n",
      "Tokens gerados: ['no', 'numero', '7', 'nenen', 'cantarolava', 'em', 'tom', 'muito', 'mais', 'baixo', 'e', 'de', 'um', 'dos', 'quartos', 'do', 'fundo', 'da', 'estalagem', 'saia', 'de', 'espaco', 'a', 'espaco', 'uma', 'nota', 'aspera', 'de', 'trombone']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O vendeiro, ao passar por detrás de Florinda, que no momento apanhava roupa do chão, ferrou-lhe uma palmada \n",
      "na parte do corpo então mais em evidência'\n",
      "Tokens gerados: ['o', 'vendeiro', ',', 'ao', 'passar', 'por', 'detras', 'de', 'florinda', ',', 'que', 'no', 'momento', 'apanhava', 'roupa', 'do', 'chao', ',', 'ferrou-lhe', 'uma', 'palmada', 'na', 'parte', 'do', 'corpo', 'entao', 'mais', 'em', 'evidencia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Não bula, hein?!'\n",
      "Tokens gerados: ['—', 'nao', 'bula', ',', 'hein', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'gritou ela, rápido, erguendo-se tesa'\n",
      "Tokens gerados: ['gritou', 'ela', ',', 'rapido', ',', 'erguendo-se', 'tesa']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E, dando com João Romão: \n",
      "— Eu logo vi'\n",
      "Tokens gerados: ['e', ',', 'dando', 'com', 'joao', 'romao', '—', 'eu', 'logo', 'vi']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Leva implicando aqui com a gente e depois, vai-se comprar na venda, o safado rouba no peso! \n",
      "Diabo do galego  Eu não te quero, sabe? \n",
      "O vendeiro soltou-lhe nova palmada com mais força e fugiu, porque ela se armara com um regador cheio de água'\n",
      "Tokens gerados: ['leva', 'implicando', 'aqui', 'com', 'a', 'gente', 'e', 'depois', ',', 'vai-se', 'comprar', 'na', 'venda', ',', 'o', 'safado', 'rouba', 'no', 'peso', 'diabo', 'do', 'galego', 'eu', 'nao', 'te', 'quero', ',', 'sabe', '?', 'o', 'vendeiro', 'soltou-lhe', 'nova', 'palmada', 'com', 'mais', 'forca', 'e', 'fugiu', ',', 'porque', 'ela', 'se', 'armara', 'com', 'um', 'regador', 'cheio', 'de', 'agua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Vem pra cá, se és capaz! Diabo da peste! \n",
      "João Romão já se havia afastado com o cavouqueiro'\n",
      "Tokens gerados: ['—', 'vem', 'pra', 'ca', ',', 'se', 'es', 'capaz', 'diabo', 'da', 'peste', 'joao', 'romao', 'ja', 'se', 'havia', 'afastado', 'com', 'o', 'cavouqueiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— O senhor tem aqui muita gente!'\n",
      "Tokens gerados: ['—', 'o', 'senhor', 'tem', 'aqui', 'muita', 'gente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'observou-lhe este'\n",
      "Tokens gerados: ['observou-lhe', 'este']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Oh! fez o outro, sacudindo os ombros, e disse depois com empáfia: — Houvesse mais cem quartos que \n",
      "estariam cheios! Mas é tudo gente séria! Não há chinfrins nesta estalagem; se aparece uma rusga, eu chego, e tudo \n",
      "acaba logo! Nunca nos entrou cá a policia, nem nunca a deixaremos entrar! E olhe que se divertem bem com as suas \n",
      "violas! Tudo gente muita boa! \n",
      "Tinham chegado ao fim do pátio do cortiço e, depois de transporem uma porta que se fechava com um peso \n",
      "amarrado a uma corda, acharam-se no capinzal que havia antes da pedreira'\n",
      "Tokens gerados: ['—', 'oh', 'fez', 'o', 'outro', ',', 'sacudindo', 'os', 'ombros', ',', 'e', 'disse', 'depois', 'com', 'empafia', '—', 'houvesse', 'mais', 'cem', 'quartos', 'que', 'estariam', 'cheios', 'mas', 'e', 'tudo', 'gente', 'seria', 'nao', 'ha', 'chinfrins', 'nesta', 'estalagem', 'se', 'aparece', 'uma', 'rusga', ',', 'eu', 'chego', ',', 'e', 'tudo', 'acaba', 'logo', 'nunca', 'nos', 'entrou', 'ca', 'a', 'policia', ',', 'nem', 'nunca', 'a', 'deixaremos', 'entrar', 'e', 'olhe', 'que', 'se', 'divertem', 'bem', 'com', 'as', 'suas', 'violas', 'tudo', 'gente', 'muita', 'boa', 'tinham', 'chegado', 'ao', 'fim', 'do', 'patio', 'do', 'cortico', 'e', ',', 'depois', 'de', 'transporem', 'uma', 'porta', 'que', 'se', 'fechava', 'com', 'um', 'peso', 'amarrado', 'a', 'uma', 'corda', ',', 'acharam-se', 'no', 'capinzal', 'que', 'havia', 'antes', 'da', 'pedreira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Vamos por aqui mesmo que é mais perto, aconselhou o vendeiro'\n",
      "Tokens gerados: ['—', 'vamos', 'por', 'aqui', 'mesmo', 'que', 'e', 'mais', 'perto', ',', 'aconselhou', 'o', 'vendeiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E os dois, em vez de procurarem a estrada, atravessaram o capim quente e trescalante'\n",
      "Tokens gerados: ['e', 'os', 'dois', ',', 'em', 'vez', 'de', 'procurarem', 'a', 'estrada', ',', 'atravessaram', 'o', 'capim', 'quente', 'e', 'trescalante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Meio-dia em ponto'\n",
      "Tokens gerados: ['meio-dia', 'em', 'ponto']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O sol estava a pino; tudo reverberava a luz irreconciliável de dezembro, num dia sem nuvens'\n",
      "Tokens gerados: ['o', 'sol', 'estava', 'a', 'pino', 'tudo', 'reverberava', 'a', 'luz', 'irreconciliavel', 'de', 'dezembro', ',', 'num', 'dia', 'sem', 'nuvens']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A pedreira, em que ela batia de chapa em cima, cegava olhada de frente'\n",
      "Tokens gerados: ['a', 'pedreira', ',', 'em', 'que', 'ela', 'batia', 'de', 'chapa', 'em', 'cima', ',', 'cegava', 'olhada', 'de', 'frente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Era preciso martirizar a vista para descobrir as \n",
      "nuanças da pedra; nada mais que uma grande mancha branca e luminosa, terminando pela parte de baixo no chão \n",
      "coberto de cascalho miúdo, que ao longe produzia o efeito de um betume cinzento, e pela parte de cima na espessura \n",
      "compacta do arvoredo, onde se não distinguiam outros tons mais do que nódoas negras, bem negras, sobre o \n",
      "verde-escuro'\n",
      "Tokens gerados: ['era', 'preciso', 'martirizar', 'a', 'vista', 'para', 'descobrir', 'as', 'nuancas', 'da', 'pedra', 'nada', 'mais', 'que', 'uma', 'grande', 'mancha', 'branca', 'e', 'luminosa', ',', 'terminando', 'pela', 'parte', 'de', 'baixo', 'no', 'chao', 'coberto', 'de', 'cascalho', 'miudo', ',', 'que', 'ao', 'longe', 'produzia', 'o', 'efeito', 'de', 'um', 'betume', 'cinzento', ',', 'e', 'pela', 'parte', 'de', 'cima', 'na', 'espessura', 'compacta', 'do', 'arvoredo', ',', 'onde', 'se', 'nao', 'distinguiam', 'outros', 'tons', 'mais', 'do', 'que', 'nodoas', 'negras', ',', 'bem', 'negras', ',', 'sobre', 'o', 'verde-escuro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'À proporção que os dois se aproximavam da imponente pedreira, o terreno ia-se tornando mais e mais cascalhudo; \n",
      "os sapatos enfarinhavam-se de uma poeira clara'\n",
      "Tokens gerados: ['a', 'proporcao', 'que', 'os', 'dois', 'se', 'aproximavam', 'da', 'imponente', 'pedreira', ',', 'o', 'terreno', 'ia-se', 'tornando', 'mais', 'e', 'mais', 'cascalhudo', 'os', 'sapatos', 'enfarinhavam-se', 'de', 'uma', 'poeira', 'clara']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mais adiante, por aqui e por ali, havia muitas carroças, algumas em \n",
      "movimento, puxadas a burro e cheias de calhaus partidos; outras já prontas para seguir, à espera do animal, e outras \n",
      "enfim com os braços para o ar, como se acabassem de ser despejadas naquele instante'\n",
      "Tokens gerados: ['mais', 'adiante', ',', 'por', 'aqui', 'e', 'por', 'ali', ',', 'havia', 'muitas', 'carrocas', ',', 'algumas', 'em', 'movimento', ',', 'puxadas', 'a', 'burro', 'e', 'cheias', 'de', 'calhaus', 'partidos', 'outras', 'ja', 'prontas', 'para', 'seguir', ',', 'a', 'espera', 'do', 'animal', ',', 'e', 'outras', 'enfim', 'com', 'os', 'bracos', 'para', 'o', 'ar', ',', 'como', 'se', 'acabassem', 'de', 'ser', 'despejadas', 'naquele', 'instante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Homens labutavam'\n",
      "Tokens gerados: ['homens', 'labutavam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'À esquerda, por cima de um vestígio de rio, que parecia ter sido bebido de um trago por aquele sol sedento, havia \n",
      "uma ponte de tábuas, onde três pequenos, quase nus, conversavam assentados, sem fazer sombra, iluminados a prumo \n",
      "pelo sol do meio-dia'\n",
      "Tokens gerados: ['a', 'esquerda', ',', 'por', 'cima', 'de', 'um', 'vestigio', 'de', 'rio', ',', 'que', 'parecia', 'ter', 'sido', 'bebido', 'de', 'um', 'trago', 'por', 'aquele', 'sol', 'sedento', ',', 'havia', 'uma', 'ponte', 'de', 'tabuas', ',', 'onde', 'tres', 'pequenos', ',', 'quase', 'nus', ',', 'conversavam', 'assentados', ',', 'sem', 'fazer', 'sombra', ',', 'iluminados', 'a', 'prumo', 'pelo', 'sol', 'do', 'meio-dia']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Para adiante, na mesma direção, corria um vasto telheiro, velho e sujo, firmado sobre colunas de \n",
      "pedra tosca; ai muitos portugueses trabalhavam de canteiro, ao barulho metálico do picão que feria o granito'\n",
      "Tokens gerados: ['para', 'adiante', ',', 'na', 'mesma', 'direcao', ',', 'corria', 'um', 'vasto', 'telheiro', ',', 'velho', 'e', 'sujo', ',', 'firmado', 'sobre', 'colunas', 'de', 'pedra', 'tosca', 'ai', 'muitos', 'portugueses', 'trabalhavam', 'de', 'canteiro', ',', 'ao', 'barulho', 'metalico', 'do', 'picao', 'que', 'feria', 'o', 'granito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Logo em \n",
      "seguida, surgia uma oficina de ferreiro, toda atravancada de destroços e objetos quebrados, entre os quais avultavam \n",
      "rodas de carro; em volta da bigorna dois homens, de corpo nu, banhados de suor e alumiados de vermelho como dois \n",
      "diabos, martelavam cadenciosamente sobre um pedaço de ferro em brasa; e ali mesmo, perto deles, a forja escancarava \n",
      "uma goela infernal, de onde saiam pequenas línguas de fogo, irrequietas e gulosas'\n",
      "Tokens gerados: ['logo', 'em', 'seguida', ',', 'surgia', 'uma', 'oficina', 'de', 'ferreiro', ',', 'toda', 'atravancada', 'de', 'destrocos', 'e', 'objetos', 'quebrados', ',', 'entre', 'os', 'quais', 'avultavam', 'rodas', 'de', 'carro', 'em', 'volta', 'da', 'bigorna', 'dois', 'homens', ',', 'de', 'corpo', 'nu', ',', 'banhados', 'de', 'suor', 'e', 'alumiados', 'de', 'vermelho', 'como', 'dois', 'diabos', ',', 'martelavam', 'cadenciosamente', 'sobre', 'um', 'pedaco', 'de', 'ferro', 'em', 'brasa', 'e', 'ali', 'mesmo', ',', 'perto', 'deles', ',', 'a', 'forja', 'escancarava', 'uma', 'goela', 'infernal', ',', 'de', 'onde', 'saiam', 'pequenas', 'linguas', 'de', 'fogo', ',', 'irrequietas', 'e', 'gulosas']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'João Romão parou à entrada da oficina e gritou para um dos ferreiros: \n",
      "— O Bruno! Não se esqueça do varal da lanterna do portão! \n",
      "Os dois homens suspenderam por um instante o trabalho'\n",
      "Tokens gerados: ['joao', 'romao', 'parou', 'a', 'entrada', 'da', 'oficina', 'e', 'gritou', 'para', 'um', 'dos', 'ferreiros', '—', 'o', 'bruno', 'nao', 'se', 'esqueca', 'do', 'varal', 'da', 'lanterna', 'do', 'portao', 'os', 'dois', 'homens', 'suspenderam', 'por', 'um', 'instante', 'o', 'trabalho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Já lá fui ver, respondeu o Bruno'\n",
      "Tokens gerados: ['—', 'ja', 'la', 'fui', 'ver', ',', 'respondeu', 'o', 'bruno']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Não vale a pena consertá-lo; está todo comido de ferragem! Faz-se-lhe um \n",
      "novo, que é melhor! \n",
      "— Pois veja lá isso, que a lanterna está a cair! \n",
      "E o vendeiro seguiu adiante com o outro, enquanto atrás recomeçava o martelar sobre a bigorna'\n",
      "Tokens gerados: ['nao', 'vale', 'a', 'pena', 'conserta-lo', 'esta', 'todo', 'comido', 'de', 'ferragem', 'faz-se-lhe', 'um', 'novo', ',', 'que', 'e', 'melhor', '—', 'pois', 'veja', 'la', 'isso', ',', 'que', 'a', 'lanterna', 'esta', 'a', 'cair', 'e', 'o', 'vendeiro', 'seguiu', 'adiante', 'com', 'o', 'outro', ',', 'enquanto', 'atras', 'recomecava', 'o', 'martelar', 'sobre', 'a', 'bigorna']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em seguida via-se uma miserável estrebaria, cheia de capim seco e excremento de bestas, com lugar para meia \n",
      "dúzia de animais'\n",
      "Tokens gerados: ['em', 'seguida', 'via-se', 'uma', 'miseravel', 'estrebaria', ',', 'cheia', 'de', 'capim', 'seco', 'e', 'excremento', 'de', 'bestas', ',', 'com', 'lugar', 'para', 'meia', 'duzia', 'de', 'animais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Estava deserta, mas, no vivo fartum exalado de lá, sentia-se que fora habitada ainda aquela noite'\n",
      "Tokens gerados: ['estava', 'deserta', ',', 'mas', ',', 'no', 'vivo', 'fartum', 'exalado', 'de', 'la', ',', 'sentia-se', 'que', 'fora', 'habitada', 'ainda', 'aquela', 'noite']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Havia depois um depósito de madeiras, servindo ao mesmo tempo de oficina de carpinteiro, tendo à porta troncos de \n",
      "arvore, alguns já serrados, muitas tábuas empilhadas, restos de cavernas e mastros de navio'\n",
      "Tokens gerados: ['havia', 'depois', 'um', 'deposito', 'de', 'madeiras', ',', 'servindo', 'ao', 'mesmo', 'tempo', 'de', 'oficina', 'de', 'carpinteiro', ',', 'tendo', 'a', 'porta', 'troncos', 'de', 'arvore', ',', 'alguns', 'ja', 'serrados', ',', 'muitas', 'tabuas', 'empilhadas', ',', 'restos', 'de', 'cavernas', 'e', 'mastros', 'de', 'navio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Daí à pedreira restavam apenas uns cinqüenta passos e o chão era já todo coberto por uma farinha de pedra moída \n",
      "que sujava como a cal'\n",
      "Tokens gerados: ['dai', 'a', 'pedreira', 'restavam', 'apenas', 'uns', 'cinquenta', 'passos', 'e', 'o', 'chao', 'era', 'ja', 'todo', 'coberto', 'por', 'uma', 'farinha', 'de', 'pedra', 'moida', 'que', 'sujava', 'como', 'a', 'cal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aqui, ali, por toda a parte, encontravam-se trabalhadores, uns ao sol, outros debaixo de pequenas barracas feitas \n",
      "de lona ou de folhas de palmeira'\n",
      "Tokens gerados: ['aqui', ',', 'ali', ',', 'por', 'toda', 'a', 'parte', ',', 'encontravam-se', 'trabalhadores', ',', 'uns', 'ao', 'sol', ',', 'outros', 'debaixo', 'de', 'pequenas', 'barracas', 'feitas', 'de', 'lona', 'ou', 'de', 'folhas', 'de', 'palmeira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De um lado cunhavam pedra cantando; de outro a quebravam a picareta; de outro \n",
      "afeiçoavam lajedos a ponta de picão; mais adiante faziam paralelepípedos a escopro e macete'\n",
      "Tokens gerados: ['de', 'um', 'lado', 'cunhavam', 'pedra', 'cantando', 'de', 'outro', 'a', 'quebravam', 'a', 'picareta', 'de', 'outro', 'afeicoavam', 'lajedos', 'a', 'ponta', 'de', 'picao', 'mais', 'adiante', 'faziam', 'paralelepipedos', 'a', 'escopro', 'e', 'macete']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E todo aquele retintim de \n",
      "ferramentas, e o martelar da forja, e o coro dos que lá em cima brocavam a rocha para lançar-lhe fogo, e a surda zoada \n",
      "ao longe, que vinha do cortiço, como de uma aldeia alarmada; tudo dava a idéia de uma atividade feroz, de uma luta de \n",
      "vingança e de ódio'\n",
      "Tokens gerados: ['e', 'todo', 'aquele', 'retintim', 'de', 'ferramentas', ',', 'e', 'o', 'martelar', 'da', 'forja', ',', 'e', 'o', 'coro', 'dos', 'que', 'la', 'em', 'cima', 'brocavam', 'a', 'rocha', 'para', 'lancar-lhe', 'fogo', ',', 'e', 'a', 'surda', 'zoada', 'ao', 'longe', ',', 'que', 'vinha', 'do', 'cortico', ',', 'como', 'de', 'uma', 'aldeia', 'alarmada', 'tudo', 'dava', 'a', 'ideia', 'de', 'uma', 'atividade', 'feroz', ',', 'de', 'uma', 'luta', 'de', 'vinganca', 'e', 'de', 'odio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Aqueles homens gotejantes de suor, bêbados de calor, desvairados de insolação, a quebrarem, a \n",
      "espicaçarem, a torturarem a pedra, pareciam um punhado de demônios revoltados na sua impotência contra o impassível \n",
      "gigante que os contemplava com desprezo, imperturbável a todos os golpes e a todos os tiros que lhe desfechavam no \n",
      "dorso, deixando sem um gemido que lhe abrissem as entranhas de granito'\n",
      "Tokens gerados: ['aqueles', 'homens', 'gotejantes', 'de', 'suor', ',', 'bebados', 'de', 'calor', ',', 'desvairados', 'de', 'insolacao', ',', 'a', 'quebrarem', ',', 'a', 'espicacarem', ',', 'a', 'torturarem', 'a', 'pedra', ',', 'pareciam', 'um', 'punhado', 'de', 'demonios', 'revoltados', 'na', 'sua', 'impotencia', 'contra', 'o', 'impassivel', 'gigante', 'que', 'os', 'contemplava', 'com', 'desprezo', ',', 'imperturbavel', 'a', 'todos', 'os', 'golpes', 'e', 'a', 'todos', 'os', 'tiros', 'que', 'lhe', 'desfechavam', 'no', 'dorso', ',', 'deixando', 'sem', 'um', 'gemido', 'que', 'lhe', 'abrissem', 'as', 'entranhas', 'de', 'granito']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O membrudo cavouqueiro havia chegado a \n",
      "fralda do orgulhoso monstro de pedra; tinha-o cara a cara, mediu-o de alto a baixo, arrogante, num desafio surdo'\n",
      "Tokens gerados: ['o', 'membrudo', 'cavouqueiro', 'havia', 'chegado', 'a', 'fralda', 'do', 'orgulhoso', 'monstro', 'de', 'pedra', 'tinha-o', 'cara', 'a', 'cara', ',', 'mediu-o', 'de', 'alto', 'a', 'baixo', ',', 'arrogante', ',', 'num', 'desafio', 'surdo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'A pedreira mostrava nesse ponto de vista o seu lado mais imponente'\n",
      "Tokens gerados: ['a', 'pedreira', 'mostrava', 'nesse', 'ponto', 'de', 'vista', 'o', 'seu', 'lado', 'mais', 'imponente']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Descomposta, com o escalavrado flanco \n",
      "exposto ao sol, erguia-se altaneira e desassombrada, afrontando o céu, muito íngreme, lisa, escaldante e cheia de cordas \n",
      "que mesquinhamente lhe escorriam pela ciclópica nudez com um efeito de teias de aranha'\n",
      "Tokens gerados: ['descomposta', ',', 'com', 'o', 'escalavrado', 'flanco', 'exposto', 'ao', 'sol', ',', 'erguia-se', 'altaneira', 'e', 'desassombrada', ',', 'afrontando', 'o', 'ceu', ',', 'muito', 'ingreme', ',', 'lisa', ',', 'escaldante', 'e', 'cheia', 'de', 'cordas', 'que', 'mesquinhamente', 'lhe', 'escorriam', 'pela', 'ciclopica', 'nudez', 'com', 'um', 'efeito', 'de', 'teias', 'de', 'aranha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Em certos lugares, muito \n",
      "alto do chão, lhe haviam espetado alfinetes de ferro, amparando, sobre um precipício, miseráveis tábuas que, vistas cá \n",
      "de baixo, pareciam palitos, mas em cima das quais uns atrevidos pigmeus de forma humana equilibravam-se, \n",
      "desfechando golpes de picareta contra o gigante'\n",
      "Tokens gerados: ['em', 'certos', 'lugares', ',', 'muito', 'alto', 'do', 'chao', ',', 'lhe', 'haviam', 'espetado', 'alfinetes', 'de', 'ferro', ',', 'amparando', ',', 'sobre', 'um', 'precipicio', ',', 'miseraveis', 'tabuas', 'que', ',', 'vistas', 'ca', 'de', 'baixo', ',', 'pareciam', 'palitos', ',', 'mas', 'em', 'cima', 'das', 'quais', 'uns', 'atrevidos', 'pigmeus', 'de', 'forma', 'humana', 'equilibravam-se', ',', 'desfechando', 'golpes', 'de', 'picareta', 'contra', 'o', 'gigante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O cavouqueiro meneou a cabeça com ar de lástima'\n",
      "Tokens gerados: ['o', 'cavouqueiro', 'meneou', 'a', 'cabeca', 'com', 'ar', 'de', 'lastima']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'O seu gesto desaprovava todo aquele serviço'\n",
      "Tokens gerados: ['o', 'seu', 'gesto', 'desaprovava', 'todo', 'aquele', 'servico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Veja lá! disse ele, apontando para certo ponto da rocha'\n",
      "Tokens gerados: ['—', 'veja', 'la', 'disse', 'ele', ',', 'apontando', 'para', 'certo', 'ponto', 'da', 'rocha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Olhe para aquilo! Sua gente tem ido às cegas no \n",
      "trabalho desta pedreira'\n",
      "Tokens gerados: ['olhe', 'para', 'aquilo', 'sua', 'gente', 'tem', 'ido', 'as', 'cegas', 'no', 'trabalho', 'desta', 'pedreira']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deviam atacá-la justamente por aquele outro lado, para não contrariar os veios da pedra'\n",
      "Tokens gerados: ['deviam', 'ataca-la', 'justamente', 'por', 'aquele', 'outro', 'lado', ',', 'para', 'nao', 'contrariar', 'os', 'veios', 'da', 'pedra']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Esta \n",
      "parte aqui é toda granito, é a melhor! Pois olhe só o que eles têm tirado de lá — umas lascas, uns calhaus que não \n",
      "servem para nada! É uma dor de coração ver estragar assim uma peça tão boa! Agora o que hão de fazer dessa \n",
      "cascalhada que ai está senão macacos? E brada aos céus, creia! ter pedra desta ordem para empregá-la em macacos! \n",
      "O vendeiro escutava-o em silêncio, apertando os beiços, aborrecido com a idéia daquele prejuízo'\n",
      "Tokens gerados: ['esta', 'parte', 'aqui', 'e', 'toda', 'granito', ',', 'e', 'a', 'melhor', 'pois', 'olhe', 'so', 'o', 'que', 'eles', 'tem', 'tirado', 'de', 'la', '—', 'umas', 'lascas', ',', 'uns', 'calhaus', 'que', 'nao', 'servem', 'para', 'nada', 'e', 'uma', 'dor', 'de', 'coracao', 'ver', 'estragar', 'assim', 'uma', 'peca', 'tao', 'boa', 'agora', 'o', 'que', 'hao', 'de', 'fazer', 'dessa', 'cascalhada', 'que', 'ai', 'esta', 'senao', 'macacos', '?', 'e', 'brada', 'aos', 'ceus', ',', 'creia', 'ter', 'pedra', 'desta', 'ordem', 'para', 'emprega-la', 'em', 'macacos', 'o', 'vendeiro', 'escutava-o', 'em', 'silencio', ',', 'apertando', 'os', 'beicos', ',', 'aborrecido', 'com', 'a', 'ideia', 'daquele', 'prejuizo']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Uma porcaria de serviço! continuou o outro'\n",
      "Tokens gerados: ['—', 'uma', 'porcaria', 'de', 'servico', 'continuou', 'o', 'outro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Ali onde está aquele homem é que deviam ter feito a broca, \n",
      "porque a explosão punha abaixo toda esta aba que é separada por um veio'\n",
      "Tokens gerados: ['ali', 'onde', 'esta', 'aquele', 'homem', 'e', 'que', 'deviam', 'ter', 'feito', 'a', 'broca', ',', 'porque', 'a', 'explosao', 'punha', 'abaixo', 'toda', 'esta', 'aba', 'que', 'e', 'separada', 'por', 'um', 'veio']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas quem tem ai o senhor capaz de fazer \n",
      "isso? Ninguém; porque é preciso um empregado que saiba o que faz; que, se a pólvora não for muito bem medida, nem \n",
      "só não se abre o veio, como ainda sucede ao trabalhador o mesmo que sucedeu ao outro! É preciso conhecer muito bem \n",
      "o trabalho para se poder tirar partido vantajoso desta pedreira! Boa é ela, mas não nas mãos em que está! É muito \n",
      "perigosa nas explosões; é muito em pé! Quem lhe lascar fogo não pode fugir senão para cima pela corda, e se o sujeito \n",
      "não for fino leva-o o demo! Sou eu quem o diz! \n",
      "E depois de uma pausa, acrescentou, tomando na sua mão, grossa como o próprio cascalho, um paralelepípedo \n",
      "que estava no chão: \n",
      "— Que digo eu?! Cá está! Macacos de granito! Isto até é uma coisa que estes burros deviam esconder por \n",
      "vergonha! \n",
      "Acompanhando a pedreira pelo lado direito e seguindo-a na volta que ela dava depois, formando um ângulo \n",
      "obtuso, é que se via quanto era grande'\n",
      "Tokens gerados: ['mas', 'quem', 'tem', 'ai', 'o', 'senhor', 'capaz', 'de', 'fazer', 'isso', '?', 'ninguem', 'porque', 'e', 'preciso', 'um', 'empregado', 'que', 'saiba', 'o', 'que', 'faz', 'que', ',', 'se', 'a', 'polvora', 'nao', 'for', 'muito', 'bem', 'medida', ',', 'nem', 'so', 'nao', 'se', 'abre', 'o', 'veio', ',', 'como', 'ainda', 'sucede', 'ao', 'trabalhador', 'o', 'mesmo', 'que', 'sucedeu', 'ao', 'outro', 'e', 'preciso', 'conhecer', 'muito', 'bem', 'o', 'trabalho', 'para', 'se', 'poder', 'tirar', 'partido', 'vantajoso', 'desta', 'pedreira', 'boa', 'e', 'ela', ',', 'mas', 'nao', 'nas', 'maos', 'em', 'que', 'esta', 'e', 'muito', 'perigosa', 'nas', 'explosoes', 'e', 'muito', 'em', 'pe', 'quem', 'lhe', 'lascar', 'fogo', 'nao', 'pode', 'fugir', 'senao', 'para', 'cima', 'pela', 'corda', ',', 'e', 'se', 'o', 'sujeito', 'nao', 'for', 'fino', 'leva-o', 'o', 'demo', 'sou', 'eu', 'quem', 'o', 'diz', 'e', 'depois', 'de', 'uma', 'pausa', ',', 'acrescentou', ',', 'tomando', 'na', 'sua', 'mao', ',', 'grossa', 'como', 'o', 'proprio', 'cascalho', ',', 'um', 'paralelepipedo', 'que', 'estava', 'no', 'chao', '—', 'que', 'digo', 'eu', '?', 'ca', 'esta', 'macacos', 'de', 'granito', 'isto', 'ate', 'e', 'uma', 'coisa', 'que', 'estes', 'burros', 'deviam', 'esconder', 'por', 'vergonha', 'acompanhando', 'a', 'pedreira', 'pelo', 'lado', 'direito', 'e', 'seguindo-a', 'na', 'volta', 'que', 'ela', 'dava', 'depois', ',', 'formando', 'um', 'angulo', 'obtuso', ',', 'e', 'que', 'se', 'via', 'quanto', 'era', 'grande']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Suava-se bem antes de chegar ao seu limite com a mata'\n",
      "Tokens gerados: ['suava-se', 'bem', 'antes', 'de', 'chegar', 'ao', 'seu', 'limite', 'com', 'a', 'mata']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que mina de dinheiro!'\n",
      "Tokens gerados: ['—', 'que', 'mina', 'de', 'dinheiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'dizia o homenzarrão, parando entusiasmado defronte do novo pano de rocha viva que \n",
      "se desdobrava na presença dele'\n",
      "Tokens gerados: ['dizia', 'o', 'homenzarrao', ',', 'parando', 'entusiasmado', 'defronte', 'do', 'novo', 'pano', 'de', 'rocha', 'viva', 'que', 'se', 'desdobrava', 'na', 'presenca', 'dele']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Toda esta parte que se segue agora, declarou João Romão, ainda não é minha'\n",
      "Tokens gerados: ['—', 'toda', 'esta', 'parte', 'que', 'se', 'segue', 'agora', ',', 'declarou', 'joao', 'romao', ',', 'ainda', 'nao', 'e', 'minha']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E continuaram a andar para diante'\n",
      "Tokens gerados: ['e', 'continuaram', 'a', 'andar', 'para', 'diante']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Deste lado multiplicavam-se as barraquinhas; os macaqueiros trabalhavam à sombra delas, indiferentes àqueles \n",
      "dois'\n",
      "Tokens gerados: ['deste', 'lado', 'multiplicavam-se', 'as', 'barraquinhas', 'os', 'macaqueiros', 'trabalhavam', 'a', 'sombra', 'delas', ',', 'indiferentes', 'aqueles', 'dois']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Viam-se panelas ao fogo, sobre quatro pedras, ao ar livre, e rapazitos tratando do jantar dos pais'\n",
      "Tokens gerados: ['viam-se', 'panelas', 'ao', 'fogo', ',', 'sobre', 'quatro', 'pedras', ',', 'ao', 'ar', 'livre', ',', 'e', 'rapazitos', 'tratando', 'do', 'jantar', 'dos', 'pais']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De mulher nem \n",
      "sinal'\n",
      "Tokens gerados: ['de', 'mulher', 'nem', 'sinal']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'De vez em quando, na penumbra de um ensombro de lona, dava-se com um grupo de homens, comendo de \n",
      "cócoras defronte uns dos outros, uma sardinha na mão esquerda, um pão na direita, ao lado de uma garrafa de água'\n",
      "Tokens gerados: ['de', 'vez', 'em', 'quando', ',', 'na', 'penumbra', 'de', 'um', 'ensombro', 'de', 'lona', ',', 'dava-se', 'com', 'um', 'grupo', 'de', 'homens', ',', 'comendo', 'de', 'cocoras', 'defronte', 'uns', 'dos', 'outros', ',', 'uma', 'sardinha', 'na', 'mao', 'esquerda', ',', 'um', 'pao', 'na', 'direita', ',', 'ao', 'lado', 'de', 'uma', 'garrafa', 'de', 'agua']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Sempre o mesmo serviço malfeito e mal dirigido!'\n",
      "Tokens gerados: ['—', 'sempre', 'o', 'mesmo', 'servico', 'malfeito', 'e', 'mal', 'dirigido']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'resmungou o cavouqueiro'\n",
      "Tokens gerados: ['resmungou', 'o', 'cavouqueiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Entretanto, a mesma atividade parecia reinar por toda a parte'\n",
      "Tokens gerados: ['entretanto', ',', 'a', 'mesma', 'atividade', 'parecia', 'reinar', 'por', 'toda', 'a', 'parte']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas, lá no fim, debaixo dos bambus que marcavam \n",
      "o limite da pedreira, alguns trabalhadores dormiam à sombra, de papo para o ar, a barba espetando para o alto, o \n",
      "pescoço intumescido de cordoveias grossas como enxárcias de navio, a boca aberta, a respiração forte e tranqüila de \n",
      "animal sadio, num feliz e pletórico resfolgar de besta cansada'\n",
      "Tokens gerados: ['mas', ',', 'la', 'no', 'fim', ',', 'debaixo', 'dos', 'bambus', 'que', 'marcavam', 'o', 'limite', 'da', 'pedreira', ',', 'alguns', 'trabalhadores', 'dormiam', 'a', 'sombra', ',', 'de', 'papo', 'para', 'o', 'ar', ',', 'a', 'barba', 'espetando', 'para', 'o', 'alto', ',', 'o', 'pescoco', 'intumescido', 'de', 'cordoveias', 'grossas', 'como', 'enxarcias', 'de', 'navio', ',', 'a', 'boca', 'aberta', ',', 'a', 'respiracao', 'forte', 'e', 'tranquila', 'de', 'animal', 'sadio', ',', 'num', 'feliz', 'e', 'pletorico', 'resfolgar', 'de', 'besta', 'cansada']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Que relaxamento! resmungou de novo o cavouqueiro'\n",
      "Tokens gerados: ['—', 'que', 'relaxamento', 'resmungou', 'de', 'novo', 'o', 'cavouqueiro']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tudo isto está a reclamar um homem teso que olhe a \n",
      "sério para o serviço! \n",
      "— Eu nada tenho que ver com este lado! observou Romão'\n",
      "Tokens gerados: ['tudo', 'isto', 'esta', 'a', 'reclamar', 'um', 'homem', 'teso', 'que', 'olhe', 'a', 'serio', 'para', 'o', 'servico', '—', 'eu', 'nada', 'tenho', 'que', 'ver', 'com', 'este', 'lado', 'observou', 'romao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Mas lá da sua banda hão de fazer o mesmo! Olará! \n",
      "— Abusam, porque tenho de olhar pelo negócio lá fora'\n",
      "Tokens gerados: ['—', 'mas', 'la', 'da', 'sua', 'banda', 'hao', 'de', 'fazer', 'o', 'mesmo', 'olara', '—', 'abusam', ',', 'porque', 'tenho', 'de', 'olhar', 'pelo', 'negocio', 'la', 'fora']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Comigo aqui é que eles não fariam cera'\n",
      "Tokens gerados: ['—', 'comigo', 'aqui', 'e', 'que', 'eles', 'nao', 'fariam', 'cera']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'isso juro eu! Entendo que o empregado deve ser bem pago, ter para a \n",
      "sua comida à farta, o seu gole de vinho, mas que deve fazer serviço que se veja, ou, então, rua! Rua, que não falta por ai \n",
      "quem queira ganhar dinheiro! Autorize-me a olhar por eles e verá! \n",
      "— O diabo é que você quer setenta mil-réis'\n",
      "Tokens gerados: ['isso', 'juro', 'eu', 'entendo', 'que', 'o', 'empregado', 'deve', 'ser', 'bem', 'pago', ',', 'ter', 'para', 'a', 'sua', 'comida', 'a', 'farta', ',', 'o', 'seu', 'gole', 'de', 'vinho', ',', 'mas', 'que', 'deve', 'fazer', 'servico', 'que', 'se', 'veja', ',', 'ou', ',', 'entao', ',', 'rua', 'rua', ',', 'que', 'nao', 'falta', 'por', 'ai', 'quem', 'queira', 'ganhar', 'dinheiro', 'autorize-me', 'a', 'olhar', 'por', 'eles', 'e', 'vera', '—', 'o', 'diabo', 'e', 'que', 'voce', 'quer', 'setenta', 'mil-reis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'suspirou João Romão'\n",
      "Tokens gerados: ['suspirou', 'joao', 'romao']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah! nem menos um real!'\n",
      "Tokens gerados: ['—', 'ah', 'nem', 'menos', 'um', 'real']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Mas comigo aqui há de ver o que lhe faço entrar para algibeira! Temos cá muita \n",
      "gente que não precisa estar'\n",
      "Tokens gerados: ['mas', 'comigo', 'aqui', 'ha', 'de', 'ver', 'o', 'que', 'lhe', 'faco', 'entrar', 'para', 'algibeira', 'temos', 'ca', 'muita', 'gente', 'que', 'nao', 'precisa', 'estar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Para que tanto macaqueiro, por exemplo? Aquilo é serviço para descanso; é serviço de \n",
      "criança! Em vez de todas aquelas lesmas, pagas talvez a trinta mil-réis'\n",
      "Tokens gerados: ['para', 'que', 'tanto', 'macaqueiro', ',', 'por', 'exemplo', '?', 'aquilo', 'e', 'servico', 'para', 'descanso', 'e', 'servico', 'de', 'crianca', 'em', 'vez', 'de', 'todas', 'aquelas', 'lesmas', ',', 'pagas', 'talvez', 'a', 'trinta', 'mil-reis']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— É justamente quanto lhes dou'\n",
      "Tokens gerados: ['—', 'e', 'justamente', 'quanto', 'lhes', 'dou']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '—'\n",
      "Tokens gerados: ['—']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'melhor seria tomar dois bons trabalhadores de cinqüenta, que fazem o dobro do que fazem aqueles monos e \n",
      "que podem servir para outras coisas! Parece que nunca trabalharam! Olhe, é já a terceira vez que aquele que ali está \n",
      "deixa cair o escopro! Com efeito! \n",
      " \n",
      "João Romão ficou calado, a cismar, enquanto voltavam'\n",
      "Tokens gerados: ['melhor', 'seria', 'tomar', 'dois', 'bons', 'trabalhadores', 'de', 'cinquenta', ',', 'que', 'fazem', 'o', 'dobro', 'do', 'que', 'fazem', 'aqueles', 'monos', 'e', 'que', 'podem', 'servir', 'para', 'outras', 'coisas', 'parece', 'que', 'nunca', 'trabalharam', 'olhe', ',', 'e', 'ja', 'a', 'terceira', 'vez', 'que', 'aquele', 'que', 'ali', 'esta', 'deixa', 'cair', 'o', 'escopro', 'com', 'efeito', 'joao', 'romao', 'ficou', 'calado', ',', 'a', 'cismar', ',', 'enquanto', 'voltavam']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vinham ambos pensativos'\n",
      "Tokens gerados: ['vinham', 'ambos', 'pensativos']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— E você, se eu o tomar, disse depois o vendeiro, muda-se cá para a estalagem?'\n",
      "Tokens gerados: ['—', 'e', 'voce', ',', 'se', 'eu', 'o', 'tomar', ',', 'disse', 'depois', 'o', 'vendeiro', ',', 'muda-se', 'ca', 'para', 'a', 'estalagem', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Naturalmente! não hei de ficar lá na cidade nova, tendo o serviço aqui!'\n",
      "Tokens gerados: ['—', 'naturalmente', 'nao', 'hei', 'de', 'ficar', 'la', 'na', 'cidade', 'nova', ',', 'tendo', 'o', 'servico', 'aqui']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— E a comida, forneço-a eu?'\n",
      "Tokens gerados: ['—', 'e', 'a', 'comida', ',', 'forneco-a', 'eu', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Isso é que a mulher é quem a faz; mas as compras saem-lhe da venda'\n",
      "Tokens gerados: ['—', 'isso', 'e', 'que', 'a', 'mulher', 'e', 'quem', 'a', 'faz', 'mas', 'as', 'compras', 'saem-lhe', 'da', 'venda']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Pois está fechado o negócio! deliberou João Romão, convencido de que não podia, por economia, dispensar \n",
      "um homem daqueles'\n",
      "Tokens gerados: ['—', 'pois', 'esta', 'fechado', 'o', 'negocio', 'deliberou', 'joao', 'romao', ',', 'convencido', 'de', 'que', 'nao', 'podia', ',', 'por', 'economia', ',', 'dispensar', 'um', 'homem', 'daqueles']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E pensou lá de si para si: “Os meus setenta mil-réis voltar-me-ão à gaveta'\n",
      "Tokens gerados: ['e', 'pensou', 'la', 'de', 'si', 'para', 'si', '“', 'os', 'meus', 'setenta', 'mil-reis', 'voltar-me-ao', 'a', 'gaveta']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Tudo me fica em \n",
      "casa!” \n",
      "— Então estamos entendidos?'\n",
      "Tokens gerados: ['tudo', 'me', 'fica', 'em', 'casa', '”', '—', 'entao', 'estamos', 'entendidos', '?']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Estamos entendidos! \n",
      "— Posso amanhã fazer a mudança? \n",
      "— Hoje mesmo, se quiser; tenho um cômodo que lhe há de calhar'\n",
      "Tokens gerados: ['—', 'estamos', 'entendidos', '—', 'posso', 'amanha', 'fazer', 'a', 'mudanca', '?', '—', 'hoje', 'mesmo', ',', 'se', 'quiser', 'tenho', 'um', 'comodo', 'que', 'lhe', 'ha', 'de', 'calhar']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'É o número 35'\n",
      "Tokens gerados: ['e', 'o', 'numero', '35']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Vou mostrar-lho'\n",
      "Tokens gerados: ['vou', 'mostrar-lho']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E aligeirando o passo, penetraram na estrada do capinzal com direção ao fundo do cortiço'\n",
      "Tokens gerados: ['e', 'aligeirando', 'o', 'passo', ',', 'penetraram', 'na', 'estrada', 'do', 'capinzal', 'com', 'direcao', 'ao', 'fundo', 'do', 'cortico']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Ah! é verdade! como você se chama? \n",
      "— Jerônimo, para o servir'\n",
      "Tokens gerados: ['—', 'ah', 'e', 'verdade', 'como', 'voce', 'se', 'chama', '?', '—', 'jeronimo', ',', 'para', 'o', 'servir']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Servir a Deus'\n",
      "Tokens gerados: ['—', 'servir', 'a', 'deus']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'Sua mulher lava? \n",
      "— É lavadeira, sim senhor'\n",
      "Tokens gerados: ['sua', 'mulher', 'lava', '?', '—', 'e', 'lavadeira', ',', 'sim', 'senhor']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): '— Bem, precisamos ver-lhe uma tina'\n",
      "Tokens gerados: ['—', 'bem', ',', 'precisamos', 'ver-lhe', 'uma', 'tina']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Processando frase original (após split e strip): 'E o vendeiro empurrou a porta do fundo da estalagem, de onde escapou, como de uma panela fervendo que se \n",
      "destapa, uma baforada quente, vozeria tresandante à fermentação de suores e roupa ensaboada secando ao sol'\n",
      "Tokens gerados: ['e', 'o', 'vendeiro', 'empurrou', 'a', 'porta', 'do', 'fundo', 'da', 'estalagem', ',', 'de', 'onde', 'escapou', ',', 'como', 'de', 'uma', 'panela', 'fervendo', 'que', 'se', 'destapa', ',', 'uma', 'baforada', 'quente', ',', 'vozeria', 'tresandante', 'a', 'fermentacao', 'de', 'suores', 'e', 'roupa', 'ensaboada', 'secando', 'ao', 'sol']\n",
      "Tipo dos tokens gerados: <class 'list'>\n",
      "\n",
      "Tokens estruturados salvos em: data/caps_processados\\preprocessado_o_cortico_aluisio_azevedo_cap_4.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Caminhos\n",
    "caminho_txts = \"data/caps\"\n",
    "caminho_saida_pasta = \"data/caps_processados\"\n",
    "os.makedirs(caminho_saida_pasta, exist_ok=True)\n",
    "\n",
    "for arquivo_nome in os.listdir(caminho_txts):\n",
    "    dumped_dict = {}\n",
    "    dumped_dict['titulo'], dumped_dict['autor'] = extrair_titulo_autor(arquivo_nome)\n",
    "    dumped_dict['tokens'] = [] # Esta será uma lista de listas de tokens\n",
    "    caminho_arquivo = os.path.join(caminho_txts, arquivo_nome)\n",
    "    if os.path.isfile(caminho_arquivo):\n",
    "        with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo:\n",
    "            texto_pre = arquivo.read()\n",
    "            frases = texto_pre.split('.') # Divide o texto em segmentos baseados no ponto final\n",
    "\n",
    "            for frase_segmento in frases: # Renomeei para clareza\n",
    "                frase_limpa_para_tokenizar = frase_segmento.strip()\n",
    "                if frase_limpa_para_tokenizar: # Pula segmentos vazios\n",
    "                    print(f\"\\nProcessando frase original (após split e strip): '{frase_limpa_para_tokenizar}'\") # DEBUG\n",
    "\n",
    "                    # A função limpar_e_tokenizar_texto deve retornar uma lista plana de tokens\n",
    "                    tokens_da_frase = limpar_e_tokenizar_texto(frase_limpa_para_tokenizar)\n",
    "\n",
    "                    print(f\"Tokens gerados: {tokens_da_frase}\") # DEBUG\n",
    "                    print(f\"Tipo dos tokens gerados: {type(tokens_da_frase)}\") # DEBUG\n",
    "                    if tokens_da_frase and isinstance(tokens_da_frase[0], list):\n",
    "                        print(f\"ALERTA: Parece que os tokens estão aninhados desnecessariamente: {tokens_da_frase}\") # DEBUG\n",
    "\n",
    "                    # Adiciona a lista de tokens (que deve ser plana) à lista principal\n",
    "                    dumped_dict['tokens'].append(tokens_da_frase)\n",
    "\n",
    "            caminho_saida = os.path.join(caminho_saida_pasta, \"preprocessado_\" + arquivo_nome.replace('.txt', '.json'))\n",
    "            with open(caminho_saida, 'w', encoding='utf-8') as arquivo_saida:\n",
    "                json.dump(dumped_dict, arquivo_saida, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"\\nTokens estruturados salvos em: {caminho_saida}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964421f0",
   "metadata": {},
   "source": [
    "## Extração de features\n",
    "\n",
    "Após a etapa de pré processamento, iniciaremos a extração de features. Definimos alguns grupos de features que acreditamos serem úteis para o nosso projeto, sendo elas: \n",
    "#### Linguísticas (POS, gramática, estilo):\n",
    "- Frequência relativa de substantivos, verbos, adjetivos, advérbios\n",
    "- Frequência de tempos verbais (passado, presente, futuro)\n",
    "- Frequência de pronomes pessoais (\"eu\", \"nós\", etc.)\n",
    "- Frequência de artigos definidos (\"o\", \"a\") e indefinidos (\"um\", \"uma\")\n",
    "- Frequência de conjunções (\"e\", \"mas\", \"porque\")\n",
    "- Número médio de palavras por frase\n",
    "- Comprimento médio das palavras\n",
    "- Variabilidade de comprimento de palavras (desvio padrão)\n",
    "- Número de frases curtas vs. frases longas\n",
    "- Uso de voz passiva (frases com \"foi feito\", \"era conhecido\", etc.)\n",
    "- Proporção de substantivos abstratos vs. concretos (se você quiser ser avançado)\n",
    "\n",
    "#### Lexicais (vocabulário):\n",
    "- Número total de palavras (tokens)\n",
    "- Número de palavras únicas (tipos)\n",
    "- Índice de riqueza lexical: tipos / tokens\n",
    "- Frequência de palavras raras (pouco frequentes em um corpus comum)\n",
    "- Frequência de palavras comuns (ex: palavras do top-1000 do português)\n",
    "- Uso de palavras sofisticadas (frequência de palavras acima de certo número de sílabas)\n",
    "\n",
    "#### Semânticas (significado):\n",
    "- Similaridade semântica média entre frases (usando embeddings como Word2Vec, BERT, etc.)\n",
    "- Distância semântica entre parágrafos\n",
    "- Frequência de negação (\"não\", \"nunca\", \"jamais\")\n",
    "- Sentimento médio do texto (positivo/negativo/neutro)\n",
    "- Frequência de emoções específicas (raiva, alegria, tristeza, surpresa)\n",
    "\n",
    "#### Estilísticas:\n",
    "- Uso de metáforas, hipérboles (difícil, mas dá pra tentar detectar por padrões)\n",
    "- Uso de citações diretas (\"...\")\n",
    "- Frequência de perguntas feitas (\"?\")\n",
    "- Uso de primeira pessoa (\"eu\", \"meu\") vs. terceira pessoa (\"ele\", \"ela\")\n",
    "\n",
    "#### Estatísticas avançadas:\n",
    "- TF-IDF de palavras ou n-grams (unigrams, bigrams, trigrams)\n",
    "- Topic Modeling (LDA) — para ver quais temas o autor tende a abordar\n",
    "- Frequência de erros ortográficos (se tiver corpus sujo)\n",
    "- Medidas de entropia do texto (quanto o texto é previsível)\n",
    "Abaixo vamos buscar extrair features lexicais e semânticas dos textos. A partir disso, podemos comparar cada texto buscando entender melhor o que define os padrões na escrita de um autor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8c088c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho para os textos processados\n",
    "caminho_textos_processados = \"data/caps_processados\"\n",
    "\n",
    "def normalizar(valor, minimo, maximo, escala_min=1, escala_max=5):\n",
    "    valor_normalizado = (valor - minimo) / (maximo - minimo)\n",
    "    valor_normalizado = max(0, min(1, valor_normalizado)) \n",
    "    return escala_min + valor_normalizado * (escala_max - escala_min)\n",
    "\n",
    "def diversidade_lexical(tokens):\n",
    "    tokens_alfabeticos = [t.lower() for t in tokens if re.match(r'[a-zA-ZáàâãéèêíìóòôõúùûçÁÀÂÃÉÈÊÍÌÓÒÔÕÚÙÛÇ]+$', t)]\n",
    "    num_tokens = len(tokens_alfabeticos)\n",
    "    num_types = len(set(tokens_alfabeticos))\n",
    "\n",
    "    # Inicializa todas as variáveis com valor padrão\n",
    "    ttr = 0\n",
    "    hapax_legomena = 0\n",
    "    proporcao_hapax = 0\n",
    "    score_ttr = 0\n",
    "    score_hapax = 0\n",
    "    score_diversidade = 1\n",
    "    comprimento_medio_sentencas = 0\n",
    "    comprimento_medio_palavras = 0\n",
    "    media_stopwords = 0\n",
    "\n",
    "    if num_tokens > 0:\n",
    "        ttr = num_types / num_tokens\n",
    "        contador = Counter(tokens_alfabeticos)\n",
    "        hapax_legomena = sum(1 for palavra, freq in contador.items() if freq == 1)\n",
    "        proporcao_hapax = hapax_legomena / num_tokens\n",
    "        score_ttr = normalizar(ttr, minimo=0.0, maximo=1.0)\n",
    "        score_hapax = normalizar(proporcao_hapax, minimo=0.0, maximo=1.0)\n",
    "        score_diversidade = (score_ttr + score_hapax) / 2\n",
    "        comprimento_medio_sentencas = sum(len(token) for token in tokens_alfabeticos) / num_tokens\n",
    "        comprimento_medio_palavras = sum(len(token) for token in tokens_alfabeticos) / num_tokens\n",
    "        stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "        media_stopwords = sum(1 for token in tokens_alfabeticos if token in stopwords) / num_tokens\n",
    "\n",
    "    return {\n",
    "        'ttr': ttr,\n",
    "        'score_ttr': score_ttr,\n",
    "        'comprimento_medio_sentencas': comprimento_medio_sentencas,\n",
    "        'comprimento_medio_palavras': comprimento_medio_palavras,\n",
    "        'media_stopwords': media_stopwords,\n",
    "        'hapax_legomena': hapax_legomena,\n",
    "        'proporcao_hapax': proporcao_hapax,\n",
    "        'score_diversidade': score_diversidade,\n",
    "        'score_palavras_unicas': score_hapax\n",
    "    }\n",
    "\n",
    "# Iterar sobre os textos processados e calcular a diversidade lexical\n",
    "resultados_por_texto = []\n",
    "for arquivo_nome in os.listdir(caminho_textos_processados):\n",
    "    caminho_arquivo = os.path.join(caminho_textos_processados, arquivo_nome)\n",
    "    if os.path.isfile(caminho_arquivo) and arquivo_nome.endswith('.json'):\n",
    "        with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo:\n",
    "            dados = json.load(arquivo)\n",
    "            tokens = [token for sublist in dados['tokens'] for token in sublist]\n",
    "            resultado = diversidade_lexical(tokens)\n",
    "            resultado['titulo'] = dados.get('titulo', 'Desconhecido')\n",
    "            resultados_por_texto.append(resultado)\n",
    "\n",
    "# Plotar os resultados\n",
    "titulos = [r['titulo'] for r in resultados_por_texto]\n",
    "scores_diversidade = [r['score_diversidade'] for r in resultados_por_texto]\n",
    "ttrs = [r['ttr'] for r in resultados_por_texto]\n",
    "proporcoes_hapax = [r['proporcao_hapax'] for r in resultados_por_texto]\n",
    "\n",
    "# Criar um DataFrame com os resultados\n",
    "df_resultados = pd.DataFrame(resultados_por_texto)\n",
    "\n",
    "# Reordenar as colunas para que o título seja o primeiro\n",
    "colunas_reordenadas = ['titulo'] + [coluna for coluna in df_resultados.columns if coluna != 'titulo']\n",
    "df_resultados = df_resultados[colunas_reordenadas]\n",
    "\n",
    "# Salvar o DataFrame em um arquivo CSV\n",
    "caminho_csv = os.path.join(caminho_textos_processados, 'resultados.csv')\n",
    "df_resultados.to_csv(caminho_csv, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6f2990f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Tokenized sentence 0: tensor([[  101,   230,  2954,  5809,   117,  6661,   180,   651,   221,   146,\n",
      "          3815, 22280,  1160,   117,   778,  8393,  1362,  8574,   180,  2692,\n",
      "           222, 13254,  5863,   171,  2907,   117,   179,  2779,   818, 22280,\n",
      "           125,  3122,   122,   125,  1690,  7817,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101, 15322, 22288,   118,   311,   117,  1636,   203,   118,   176,\n",
      "           320,   766,   125,  9726,   117, 11234,   180, 13943,   122,   298,\n",
      "         12058,   117,   122,  2467,  8825,  1552,   118,   311, 11454,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   123,  4036,   495,  6003,   117,   122,   259, 11454,   706,\n",
      "           333,   179,   229, 22280,  5327,  9192, 14697,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101, 12519,   117,   240,   210,   117,   179,   117,   271,  2779,\n",
      "          1011,   822, 13550,   117,  3030,  9193,   259,  5708,  1510, 22281,\n",
      "           291,  1256,  1176,  1971,  1587,   654,   221,   179,   368,  8082,\n",
      "          1249,   123,  8092,   122,  3285,  1249,   259, 11454,   202,  4102,\n",
      "           293,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   118,   118,  1236, 22279,   117,  1996,  2779, 19994,   214,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   118,   118,  1941,  1642,   244,   117,   362, 22282,  4643,\n",
      "           748,   368,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   118,   118,   629, 22280,   785, 22003, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,  1976,   118,  2036,  1434,   222, 22000,   221,  4551,   118,\n",
      "         15887,  1858,   576,   171,  4102,   293,   117,   449,   229, 22280,\n",
      "          1367,   171, 22000,  1011,  1052,  3611,   243,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   202,   644,  1457,  3033,   123,  4640,   125,  9726,  3360,\n",
      "          2996,   128,   117,   122,  2467, 20025,   214,   118,   311,  2350,\n",
      "           504,  4643, 22282,   157,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   259,  9000,   117,   179,   229, 22280,  5971, 22287,   298,\n",
      "         17080, 19877,   128,   570,  1435,   128,   122,  1945,   308,   117,\n",
      "          7320,  3418,   123, 20025,   117,   179,   870,   509, 16436,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,  2798,   240,  1257,   311, 11510,   833,  3897, 22283,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,  1519, 22283,   123, 15631,   243,   154,   712,  3667,   180,\n",
      "           651,   117,   122,  1061,   117,   240,   416,   304,   117, 17221,\n",
      "           118,   311,  1016,   117,  1089,   173,  7645,   555, 22443, 22443,\n",
      "          2350,   504,  4643, 22282,   157,   117, 11388, 17891, 14890,   170,\n",
      "          2354, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101, 22443, 22443,   118,   118,   112,   112, 17891,   221, 17249,\n",
      "         10213,   117,  2350,   504,  4643, 22282,   157,   123,  1105,   122,\n",
      "           123,  1589,   180,  5302,  6156,   873,   176,  5308, 22281,  1921,\n",
      "         18147,   171,  3815, 22280,  1160,   117,   122,  2541,  1084,  3852,\n",
      "          7226,  8184,  1564,   170,  1039,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101, 22443, 22443,   118,   118,   112,   112,  7343, 15045,  2350,\n",
      "           504,  4643, 22282,   157,   117,   229, 22280,  4450, 22279,   179,\n",
      "           146, 12119, 22280,   171,  3822, 22032,   252,  1214,   252,   122,\n",
      "         18165, 22278,  5863,   229,   651,  3687,   118,  2036,  9461,   157,\n",
      "           185,   117,  3687,   118,  2036,  1690,   117,  3687,   118,  2036,\n",
      "          9461,   331,   229, 22280,  2036,  3687,   390,   304,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101, 22443, 22443,   229, 22280,  5305,   555,   434,  9223,   501,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   504,  4643, 22282,   157,   229, 22280,   418,  5863,   202,\n",
      "          3288,   179,  1061,  2036,   180, 22280,   117,   449,   202,   179,\n",
      "          2036,   429,   146,  5184,   339,   125,  2397,  1945,   201,   122,\n",
      "          3285,   286, 11631,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,  2350,  3429,   240, 18447,   151,   117,   221, 16266, 22282,\n",
      "           118,   311,   572,   793,   125, 10692, 11437,   339,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,  2745,   240,  2765,   144,  5689,  8409, 14619,   210,   229,\n",
      "         22280,   417,  9193,  1407,  4222,   452,   221,   123,  7122,  5233,\n",
      "           304, 22280,   118,   176,   229, 22280, 18424,  1342,   180,  8156,\n",
      "          2684,   320,  1338,   171,  1722,   117,  2541,   860,   653,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,   146,  7343,  6447,   171,  8574,  4412, 22278, 14672,   179,\n",
      "           229, 22280,  2036,  8098, 22280, 21568,   141,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   122,   170,  3265,  3803,   303,   117,   660,   146,  4222,\n",
      "           452,   347,   117,   926, 22278, 13917,   179,   123,  1706,   122,\n",
      "           327,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   607,  2978,   179,   820,  8743, 22280,  1257,   298,   532,\n",
      "          4761,  1089,  2798,  1971,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 21 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.03183023864236011\n",
      " Coesão Score Final: 0.5159151193211801\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'assim', 'se', 'nem', 'ou', 'como', 'para que', 'tanto', 'se nao', 'por isso']\n",
      " Número de conectivos: 12\n",
      " Número de sentenças: 21\n",
      "======================\n",
      "Resultados para preprocessado_dom_casmurro_machado_cap_1.json:\n",
      "{'coesao_score': np.float64(0.52), 'conectivos_encontrados': ['e', 'mas', 'porem', 'assim', 'se', 'nem', 'ou', 'como', 'para que', 'tanto', 'se nao', 'por isso'], 'num_conectivos': 12, 'proporcao_conectivos': 0.032, 'similaridade_media': np.float64(1.0), 'num_sentencas': 21}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  2535,   179,  2078,  3533, 22283,   146,  4222,   452,   117,\n",
      "          6793,   123,  4766,   146,  1722,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,  1075,  1659,   117,   240,   210,   117,  2826,  3558,   259,\n",
      "          6593,   179,   311,  6792, 22287,   123,  7482,   229,   223, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[ 101, 3361,  331,  117,  170,  222, 2724,  102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   123,  1105,   173,   179,  1623, 22280,   122,  2004, 22278,\n",
      "         10692,   118,  1084,  4902,   125, 19284,   373,   117,  6751,   125,\n",
      "           222,  6532,   316, 22280,  2754,   179,   311,   873,  2037,   525,\n",
      "          3198, 22283,   118,  1340,   117,   449,  1447,  1084,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[101, 222, 644, 102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   607,  2780, 22281,   481,   117,  5069,   748,   118,   311,\n",
      "         20927,   202,  3815, 22280,  1160,   123,  1105,   173,   179,   311,\n",
      "           786,   244,   229,  2557,  4768,   125,  6629,   118,  9701,   117,\n",
      "          3951,   118,  2036,   146,   653,  7316,   122,  3338,  7970,  1858,\n",
      "           117,   179, 14778,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   893,   428,   122,  7090,  9050,   228,  1004,   260,  4837,\n",
      "           303,   143,   179,  7707,  3283,   122,   146,   653,  9078,   247,\n",
      "          1990,  1609,  6859,   117,  1510, 22281,  9751,   125,  2375,   117,\n",
      "         17935,   404,   320,  4707,   117,   260,  7477, 15460,  1671,   122,\n",
      "          7289,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   229,  1310,  5809,   117,   123,  4501,   171,  1835,   183,\n",
      "           122,   366,  7367,   122,   325,   291,  1528,  4209,   117, 11368,\n",
      "          9247,  7134,   591,   125,  2968, 14689,  6915, 22281,   122,  1491,\n",
      "          3852,   128,   179,   260,  5267, 22287,   538,  9786,   117,   125,\n",
      "          1632,   303,   123,  1632,   303,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   538,  1256, 18694,   171,  1835,   183,   260,  5969,   366,\n",
      "           418,   303,   143,   117,   122,   320,  1997,   366,  7367,   259,\n",
      "         21496, 22280,   143,   125,  2992,  8889,   117,   527,  3209,   183,\n",
      "           117, 21310, 22280,   122,  8050,  2949, 22281,   375,   117,   170,\n",
      "           259,  3360,   240,  3378,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   229, 22280, 20482,  3048,   123,  3083, 13793,   125,  2571,\n",
      "          2641,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[ 101,  625,  227,  793,  221,  123, 1105,  125, 6629,  118, 9701,  117,\n",
      "         1941,  740, 1011, 1016, 3916,  671, 8940,  171,  693,  194,  247, 2095,\n",
      "          102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101, 12931,   495, 10303,   171,   596,  3285,   140, 15107,  1548,\n",
      "           319,   122,  5969,  5227,   173,  6807, 11190,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,   146,   325,   122, 14619,   210,  3469,   763,   122, 16408,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101, 15212,  1690,   934,   508,   117,  2968,   117,  2653,  2766,\n",
      "           117,   230,   504, 21304,   387,   117,   222,   302,   303,   122,\n",
      "         11348, 17351,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[ 101, 1700, 7406,  304, 7492,  122, 8556,  151, 7492,  102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101, 17878,   117,  2535,   117,   271, 19888,   117,   607,  5863,\n",
      "           146,   653,  9266,   180,  1069,  2699,   117,   179,   122, 12164,\n",
      "          7137,   117,   170,   123,  5169,   117,   179,   122,  1315,   474,\n",
      "         22278,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   146,  7343,  1338, 10790,   495,  1316, 22282,   260,   924,\n",
      "         19576,   180,  1069,   117,   122, 15334,   229,  2189,  4095,   289,\n",
      "           123,  6945,  3292,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,  1502,   117,  7258,   117,   229, 22280,  1104, 22283,  9126,\n",
      "           141,   146,   179,   262,  2798,   146,   179,   572, 22283,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,   173,  2745,   117,   176,   146,  9169,   122,  4209,   117,\n",
      "           123, 15578,  4275,  4322,   122,  3575,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   176,   331,   311,  3207, 22281,  6867,   259,   736,   117,\n",
      "          1447,   222,  2397,  5411, 22278,   118,   176,   325,   291,  1528,\n",
      "           366,  1101,   179,  2243,   325,   988,   183,  2779,   653,   117,\n",
      "           122,   418, 15530,  2666,   122,  2745,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   146,   179,  5863,   418,   122,   117,  3002,  3286,   214,\n",
      "           117,  4327,   123,  4501,   179,   176,  6792,   229, 17897,   122,\n",
      "           538, 13841,   117,   122,   179,   820, 14223,   146, 19877, 22280,\n",
      "         11792,   117,   271,   176,  1331,   529,  1846,  2742,   562,   146,\n",
      "          8280,   229, 22280,  6205,   796, 17681,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,   230,   738,  3679, 22280,   179,   311,  2811,  4698,   481,\n",
      "           125,  2169,  2686, 13441,   159,   259, 14848, 22281,   117,   271,\n",
      "           944,   259,  5590, 15740, 22281,   117,   449,   229, 22280,   123,\n",
      "          9726,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   259,  3667,   179,   311,  2226, 22287,   629, 22280,   125,\n",
      "          2788,  6240,   944,   259,  3845,   506,  5964,   123, 15488,  4984,\n",
      "           298,  5097,   118, 14125,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  2249,   260, 19763,   117,  1450, 17032,   125,  8184,   481,\n",
      "           117,  1028,   125,  1528,   117,   122,  1821,  1485,  3960,   210,\n",
      "           229,   390,  2420,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,   924,   291,  1510, 22281, 11610, 22287,  3960, 22282,  9918,\n",
      "           712,   736,   117,   449,   123,  3182, 22278,   179, 14936, 18237,\n",
      "          5747,   576,   123, 16700, 22282,   259,   434,  9223,   501,   117,\n",
      "           122,  1815,  1864,  3292,   122,   822,   375,   567,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,  5147,   117,  1069,  3575,   229, 22280,  3189,  4640,  1069,\n",
      "          8562,   117,   122,  1858,   144,  5424,   123,  8128,  3953, 22281,\n",
      "           117,  7583,  1069,  2557,  3379,   118,   311, 12989,   328,   125,\n",
      "          1415, 16473,   128,   179,  2036,   417,  9193,   449,   122, 14619,\n",
      "           210, 20811,   179,  3763,   785,   730,   994,   179,   123,  1191,\n",
      "          3848,  1009,   117,   122,   117,   125, 14876,   151,   117,  5249,\n",
      "         22280,  3933, 18127,   304, 22280, 11152,   122, 18190,   289,   364,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   173,  3295,   117,  1695,  1183,   303,   122,  1528,   988,\n",
      "         22280,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101, 22238,   303,   143, 16099,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,   146,   325,   171,   596,   122, 20639,   173,  3428,   578,\n",
      "           117,  1941, 18304,  2623,   122,  9784,   271,  1004,   122,   229,\n",
      "         22280,   623, 22282,   283,  3002,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101, 11032,   117,   271,  2745,   822,   375,   117,   418, 10537,\n",
      "           897,   151,  2467,   240,  3899, 10866, 22282,   118,   311, 14619,\n",
      "           210,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,  8204, 15611,   117,   122,  5069,   748,   118,   311,  4766,\n",
      "           222,  1722,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101, 11255, 12999,  3992,   340,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,  5422,   122, 12067,   232,   417,  1797,  1429,   118,   311,\n",
      "           117,   449,   229, 22280,   311,   417,  1797,  1429,   260,   344,\n",
      "          1149,  1395, 14348,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   700,   117,  4174,   244,   173,  1434,   230, 22443, 22443,\n",
      "          4131, 22278,   298,   695, 11364,   501,   112,   112,  1528,  8742,\n",
      "           179,   260, 14876,   562,   171,  7148,   599,   145,  3746,  2028,\n",
      "         22290,   933,   298, 14125, 17233,   123,   651,   495,  1706, 18913,\n",
      "           154,   117,   449, 19764,  5590,   122, 10995,   271, 21941,   143,\n",
      "           117,  2745,   388,   286,   122,  1639,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,   262,   318, 13793,   179,   259,  2231,   382, 20367, 22281,\n",
      "           529,  7367,  8880,   123,  5961,   118,   311,   122,   123,  4640,\n",
      "           118,   311,   179,   117,   230,   576,   179,  1061,   229, 22280,\n",
      "         20482,  2028,   692, 19252,   569,   307,   118,   311,   259,  3492,\n",
      "         10788,   117, 15854, 22281,   236,   180,  7482,   122, 11169,   236,\n",
      "          1089,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,  5787,   123,  5233,   304, 22280,   311,  2811,   123, 14306,\n",
      "         13793,   117,   122,   260, 15419, 22281,  7762, 22281,  6867,   337,\n",
      "         15394, 22282, 10032, 22281,   117,   271,   320,  6447,   117,   229,\n",
      "         22280,   146,   171,  8574,   117,   449,   146,   171,   395, 11011,\n",
      "           123, 22283,  4778,  1111,  1858,   576,   117, 11233, 10359, 22281,\n",
      "         15419, 22281,   136,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101, 10692, 19623,   316, 22280, 20073,   170,   418,  3138,   117,\n",
      "           179,   744,  2535,   311,  8574, 22279,   123,  7482,   229,   223,\n",
      "         22280,   102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,  1141,   117, 21310, 22280,   117,   527,  3209,   183,   117,\n",
      "          8050,  2949, 22281,   375,   117,   122,  5023,   117,   739,  2992,\n",
      "          8889,   117,   179,   311,   563,  1025,   123,  1434,   259, 17080,\n",
      "         13052,   501,   117, 13406,   303,   118,   962, 22281,   146,  6865,\n",
      "           117,   122, 17891, 20933,   578,   320,  1798,   260, 10200,  2949,\n",
      "         22281, 12773,   784,   179,   311,  7762,   684,  6661,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,  2166,  2277,   117,  5212,   244,   146,   179,  2569, 22283,\n",
      "           117,   122,  3791,  2443,   244,   123,   223, 22280,   221,  3933,\n",
      "          1706,   125,   636,  5902, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,   122,   151,   117,   662,  2812,   128,   123, 16117,   304,\n",
      "           304, 22280,   240,   230, 13783, 22279,  1373,   125,  1617,   117,\n",
      "           179,  2364,   311,  6969,   685,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,  4435, 22279,  1028,  1615,   117,  2980,   117,   122, 18967,\n",
      "           117,   449,  7583,  2364,   176,   311, 17896,   203,   171,  5791,\n",
      "           183,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,   122,   146,   179,  2541, 22281,  9050,   117, 20601, 22280,\n",
      "           102]])\n",
      "DEBUG ======================\n",
      " len vetores 42 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.023284313696955498\n",
      " Coesão Score Final: 0.5116421568484778\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'entretanto', 'assim', 'logo', 'pois', 'uma vez que', 'quando', 'se', 'nem', 'ou', 'ora', 'quer', 'como', 'quanto', 'uma vez que', 'tanto', 'quanto']\n",
      " Número de conectivos: 19\n",
      " Número de sentenças: 42\n",
      "======================\n",
      "Resultados para preprocessado_dom_casmurro_machado_cap_2.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'porem', 'entretanto', 'assim', 'logo', 'pois', 'uma vez que', 'quando', 'se', 'nem', 'ou', 'ora', 'quer', 'como', 'quanto', 'uma vez que', 'tanto', 'quanto'], 'num_conectivos': 19, 'proporcao_conectivos': 0.023, 'similaridade_media': np.float64(1.0), 'num_sentencas': 42}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  8544,  4270,   229,  4767,   125, 12338,   117,   625,  8362,\n",
      "         17782,   307,   146,  7343,   655,   122,  3235, 10773,   118,   311,\n",
      "          7521,   180,  4303,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   123,  1105,   495,   123,   180,  4768,   125,  6629,   118,\n",
      "          9701,   117,   146,   454,  1617,   117,   146,   622,   122,   179,\n",
      "           122,   222,  1971,  2873,   183,   117,   449,  2779,   229, 22280,\n",
      "          2678, 22283,   125, 16326,   260, 10995,   123,  7122,  1069,   331,\n",
      "           221, 15397,   159,   260,  1101,   179,   229, 22280,  3330, 22287,\n",
      "          4131,   138, 20991,   146,   622,   495,   125,  3692, 22337,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101, 20838,   151,   117,   123, 17704,  4588,   544,   229,  3138,\n",
      "           125,  3285,   140,   146,  7275,  7489,  5321,   202, 11504, 17551,\n",
      "           136,   122,   325,   179,   596,   117,   122,  1941,  2535,   706,\n",
      "          5110,   230,  7614,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[ 101,  118,  118,  179, 7614,  136,  118,  118,  230,  739, 7614,  102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,  7122,   223, 22279,  8204,  4945,   146,   179,   495,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,  4141,   236,  1564,   117,   700,   125,  1089, 16423,   143,\n",
      "           125, 14777,   304, 22280,   117,  3429,   792,   176,  1021,   614,\n",
      "           210,   202, 14148,   229, 22280,  2002,   240,  9726,   117,  2927,\n",
      "           122,   117,  3508, 12190,   243,   123,  4410,   117,  1996,   179,\n",
      "           123,  7614,  1011,   229,  1105,   320,   766,   117,   123,  9349,\n",
      "           171,  1852,  3611,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   118,   118,   123,  9349,   171,  1852,  3611,   136,   118,\n",
      "           118,   607,  3179,   596, 12044,   221,  2036,  4640,  3413,   117,\n",
      "           449,   229, 22280,   311,   361,   130,   619,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   229, 22280,   311,  4048, 22003,   179,   146,  7275,  7489,\n",
      "          5321,  1961, 22279,  3285,   286,   538, 18694,   170,   123,  2267,\n",
      "           171, 19889,   421,   117,   122,   418,   122,   123,  7614,   117,\n",
      "          2113,   176,  1061, 15854, 22287,   125, 20401,   117,   123, 17704,\n",
      "          8743,   785,   179,  6680,   221,  2531,   118, 15887,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   118,   118,   229, 22280, 17386,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,  3285,   474,   538, 18694,   136,   118,   118,   122,   222,\n",
      "          2277,   125,  5961,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[ 101,  173, 7513, 6436,  683,  117, 1684, 5062,  102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  7489,  5321,  1821,   179,   229, 22280,  8625,   125,  1084,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   123,  3113,   122,   230,   273, 22259, 15770,   146,  1568,\n",
      "           659,   179,   229, 22280,   873,  5357, 22278,   368,   179,   260,\n",
      "           144,  5424, 22281,  1820, 22281,  6867,   125,  2821,   117,   179,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,  4620,   214,   146,   347, 22000,   123, 17704,   229, 22280,\n",
      "          3960,   173,  2571, 10315,   128,   117,  4048,   118,  2036,   179,\n",
      "           944,   376,   123,  8410,  2452,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   118,   118,   449,   117,   139, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,  4141,   236,  1564,   117, 15212,  3382,   259,  4385,  5911,\n",
      "          2746,   117,   122,  2364,  1976,  3874,   179, 21719, 18224, 22282,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[ 101, 1587,  154,  123, 2169, 7489, 5321, 3002,  376, 8184,  481,  102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101, 12109, 22288,  1191, 17279,   123,  2767, 19023,   629, 22280,\n",
      "          3687, 22281,   854,  3048,   966,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   229, 22280,   176,  6969,   304,   179,   506,  7329,  5062,\n",
      "           117,  1065,  7583,   739, 16386,   403,   117,   607,  1027,   481,\n",
      "           117,   173,   179,   123, 20027,  1852,  3611,  3763, 19020,   144,\n",
      "          5424,   180, 22283,  8198,   260, 16594,   689,   303,   143,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,  1502,  2779,  2678, 22283,   125,  3960, 22282,   136,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101, 10018, 13636, 22279,   117,  2354, 22279,   179,  7093,   136,\n",
      "          7392, 13636, 22279,  9396,   170,   222, 22443, 22443, 11032,   112,\n",
      "           112,   179,   117, 13078,   173, 21928,   117,  4750,  4640, 22443,\n",
      "         22443,   629, 22280, 12223,  3391, 22280,   143,   171,  4141,   236,\n",
      "          1564,   259,  4385,  7465,  2097,   118,   176,   117,  2779,   901,\n",
      "           307,   183,   118,   311,   582,   418,   146, 11240, 22280,   136,\n",
      "           112,   112,   118,   118,  1141,   117,  3960,   247,   179,   146,\n",
      "          7258,   418, 13441,   201,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   118,   118,   706,   333,  7122, 17704,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  6368,   840,  5900,  3083, 13793,   449,  3960,   151,   179,\n",
      "           229, 22280,  4763, 22283,  3133, 13793,   700,   125,   785, 11367,\n",
      "          2623,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,   118,   118,   173,  1364,  1652,   117,  2541,   660,   596,\n",
      "           117,  8082, 13665,  7122,   223, 22279, 17891,  8364,   125,  3285,\n",
      "         22279,   118,  1340,   202, 11504, 17551,  2249,  1075,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   118,   118,  1004,   117,   230,   576,   179,   229, 22280,\n",
      "          3763,   123,  3138,   125,   146,  1434,  7148,   117,   376,   118,\n",
      "           176, 12650,   146,  1310,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,  7489,  5321,   607,   125, 21619,   259, 16617,   125,   327,\n",
      "           223, 22279,   122,   700,   123,  2567,  2509,   376,  7188, 18261,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,   229, 22280,  6969,  2227,   128,   179,   222,  4864, 19080,\n",
      "           123,  8883,   175,   117,   122,   179,   146,  7148,  2996,  1286,\n",
      "         15705,   146, 16421,   247,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,   118,   118,  1161,   271,   123,  1354,  2461,  1316, 22290,\n",
      "          2598,  7392, 13636, 22279,   117,  9262,  1825,   123,  3845, 21568,\n",
      "          2841, 12067,   560,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,   118,   118,  5112, 22280,   117, 14914,   117,   229, 22280,\n",
      "         12044, 14530, 16241,  1537, 22287,   117, 12044, 17860,   146,   179,\n",
      "          2779, 18691,   122,  4640,   179,   146, 14445,   744,   376,   739,\n",
      "          1798,   202,  1010,   215,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,   118,   118,  2354, 22279,   146,   179,  3189,   122,   222,\n",
      "           853,  3356, 22279,  1961, 22279,   117,  1447, 10467,   146, 11240,\n",
      "         22280,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101,  2249,   320,  3265,   117,   176,   376,   125,   333,  7148,\n",
      "           117,  5034,   122,  1407,   179,   229, 22280,   662,   289,   123,\n",
      "          4640, 13544,  7521,   366,  6929,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,   449,   117,  2389,   296,   329,   117,  1062, 22278, 20838,\n",
      "           151,   117,   607,   653,  4096,   125,  5637,   118,  1340,  7148,\n",
      "           136,   118,   118,   122, 13246,   117,   607,   125, 10241,   118,\n",
      "           176,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   118,   118, 18661,   179,  2354, 22279,  1191, 13246,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,   449,   230, 13246,  1016,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,   229, 22280, 18661,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[ 101, 3960,  247,  179,  117, 1004, 4047,  243,  102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,  2354, 22279,   179,  7093,   117,  5495, 13012,   324,   136,\n",
      "           118,   118,  2779,   136,   118,   118,  3295,   122,   179,  1078,\n",
      "           222,  4178,  1407,   125,   898,   117,  3449,  7392, 13636, 22279,\n",
      "           118,  4023,   122,   179,  4178,   125,   944,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,  6033,   117,   230, 13246,   125, 14730,   481,   102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,   449,   117,   179,   122,  1257,   117,  1062, 22278, 20838,\n",
      "           151,   136,   418,  5334,  2537,   136, 11032,   418,  1502,  3413,\n",
      "           122,   144,  5424,   125,  1084, 20739,   136,  7122,   223, 22279,\n",
      "          1990,   203,   118,   176,   834, 12304,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,  5495, 13012,   324,  3960,   247,   179,   176, 17760,   122,\n",
      "           262,   370,   170,   740,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,  5793,   118,   176,   222,  2979, 22243, 22280,   117,   726,\n",
      "           146,   615,  7932, 22279,   123, 13779,   455,   125,  4270,   229,\n",
      "          4767,   117,   449,  1858,   344,   304,   636,   117,  1858,  5088,\n",
      "           304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,   229, 22280, 10340, 22279,  9226,   260,  3724,   179,  7392,\n",
      "         13636, 22279,  3033,   123,  4640,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101,  5495, 13012,   324,   294,   890,   256, 22443, 22443,  5495,\n",
      "         20838,   151,  5495, 20838,   151,   112,   112,  4141,   236,  1564,\n",
      "         13201,   321,   256,   118,   176, 22443, 22443,   176, 13256, 22281,\n",
      "           236,   117,   229, 22280,  2471, 14412,   117,   449,  4763, 22283,\n",
      "           412,  1214,   371,   304, 22280,   117,   412,  5948,   117,   423,\n",
      "         14477, 22280,   117,   221, 10241,   222,  2643,  8650,   339,   117,\n",
      "           222,  2643,  8650,   613,  1211,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 43 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.029062087147870428\n",
      " Coesão Score Final: 0.5145310435739352\n",
      " Conectivos encontrados: ['e', 'mas', 'contudo', 'assim', 'pois', 'porque', 'uma vez que', 'quando', 'se', 'caso', 'isto e', 'ou', 'ora', 'quer', 'senao', 'como', 'quanto', 'porque', 'uma vez que', 'realmente', 'tanto', 'quanto']\n",
      " Número de conectivos: 22\n",
      " Número de sentenças: 45\n",
      "======================\n",
      "Resultados para preprocessado_dom_casmurro_machado_cap_3.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'contudo', 'assim', 'pois', 'porque', 'uma vez que', 'quando', 'se', 'caso', 'isto e', 'ou', 'ora', 'quer', 'senao', 'como', 'quanto', 'porque', 'uma vez que', 'realmente', 'tanto', 'quanto'], 'num_conectivos': 22, 'proporcao_conectivos': 0.029, 'similaridade_media': np.float64(1.0), 'num_sentencas': 45}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[ 101, 4141,  236, 1564, 3330,  256,  259, 1229,  266, 1100,  102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   495,   222,  2277,   125,  2822,  2996,   304, 22280, 19265,\n",
      "           260,  5365,   229, 22280,   260, 11020,   117, 15724,   123, 14051,\n",
      "           159,   260, 16264,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101, 17760,   118,   176,   221,   925, 10467,   146, 11240, 22280,\n",
      "           117,   179,  1011,   202,  2699,   180,  1105,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,  2252, 22283,   118,   311,   785,   123,  8130,   117,   122,\n",
      "          1976,   118,   146,  3852,   170,   260,   675, 14790,   138, 11563,\n",
      "          2787,   703,   401,   117,   543, 10661,   117, 10849,   455,   122,\n",
      "         14967,   154,   125,  3848, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   262,   298,   169, 14116, 22281,   179, 14827,   543, 10661,\n",
      "           202,  2187,   125,  1543,   117,   122,  5787,  3102,  1147,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101, 16555,   260, 14790,   138, 10647,   221,   179,  2036,  1968,\n",
      "         22281,  6867,  1004,  1241,  4851,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   123, 14967,   154,   125,  2992,  2762,  7967,   117,   170,\n",
      "           222,  7055,   125,  1169,   240,  1839,   117, 13175, 16754,   118,\n",
      "          2036,   146, 13227,   303,   495,   318, 13793,  7800,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   146, 10849,   455,   125,  1224,   343,   117,  4677, 22279,\n",
      "           504,   556,   122,  7211,   117,  9821,  8807,   230,  1105,   304,\n",
      "           125,  4569,  7595,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   495,  5923,   157,   117,  4273,  9783,   117,   170,   222,\n",
      "           905,   247,   125,  1945,   256,  2471,   259,   532, 11330,   122,\n",
      "          1685,   481,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101, 17760,   118,   176,   170,   146,  6793,  5926,  6031,   171,\n",
      "         12215,   117,   229, 22280,  6086,  5926, 22282,  9721,   487,   176,\n",
      "           495,   298,   466, 14960,   942,   128,   117,   449,   222,  5926,\n",
      "         22282, 10315,   201,   122,   125, 16493,   117,   222,  7296,   439,\n",
      "           714,  6104,   117,   123, 22202,  1075,   180,  5395,  3292,   117,\n",
      "           123,  5395,  3292,  1075,   180,  5700, 13793,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[ 101,  222, 2643, 8650,  613, 1211,  102]])\n",
      "DEBUG ======================\n",
      " len vetores 11 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.02808988748264108\n",
      " Coesão Score Final: 0.5140449437413206\n",
      " Conectivos encontrados: ['e', 'mas', 'se', 'ou', 'para que']\n",
      " Número de conectivos: 5\n",
      " Número de sentenças: 11\n",
      "======================\n",
      "Resultados para preprocessado_dom_casmurro_machado_cap_4.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'se', 'ou', 'para que'], 'num_conectivos': 5, 'proporcao_conectivos': 0.028, 'similaridade_media': np.float64(1.0), 'num_sentencas': 11}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101, 14049, 20167,  4332,  3629,   125,  7122,  2480,  5356,   117,\n",
      "           582,  8954,   123,  6124,   285,   151,   529,  2143,  1029,   180,\n",
      "         18757,  1382,   581, 14049, 20167,   179,  6788,   145,   271, 17558,\n",
      "           328,   188, 14788,   285,   712, 10414,   171,   969, 16972,   117,\n",
      "           337, 12440,  4212,   260, 21672,   138, 12553,  1748,   703,  1609,\n",
      "           591,   125,   144, 19428,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   333,   912, 22283, 14049, 20167,   117,   122,  1369,   375,\n",
      "         22283, 11152,   246,   123,  5926,  6554, 18711,   375,   117,   221,\n",
      "           179,   146, 10695,  5429,  3750,   397, 12019, 22280,   398,   810,\n",
      "         22279,   123, 12252,   366,  6205,   138,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   582,  2541,   123,   870,  8776, 22278,  6124,  3281,   117,\n",
      "           179,  5308,  6372,   328,   123,  4203,  2992, 15396,   117,  6628,\n",
      "           320, 12837,   303,  2480, 22290,   123,   739, 20262,   136,   582,\n",
      "          2541,   271,  7352,  1809, 10913, 13784,   146, 15998,   430, 22280,\n",
      "          9019,   247,   529, 12475, 22280,   143,   171, 10951,   136,  1510,\n",
      "         22281,  3486, 22281,  9815, 22287,   498,   146,  6247,   215, 16936,\n",
      "           268,   179,  2541, 10417,  2537,  3300, 22279,   117,   528,   173,\n",
      "          1796,   222,  2656, 18263,  3561,   437, 22305,  7352,   229, 22280,\n",
      "         16450,   146,  5052,  3725,   230,   854,  2028,   122,   222,   646,\n",
      "          7629,   157,   179, 12061,   123,  3377,   202, 10420,   303,   366,\n",
      "          9647,   117,   122,  5911,  2227,   925,   148,   128,   117,  2292,\n",
      "          2592,   180,  1589,  2480, 16853,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   123,   599,  1165,   285,   558, 18680,   175,  3889,   180,\n",
      "          7815,   222, 17382, 11744, 11891,   117,   179,  5381,  3632,   420,\n",
      "           146,   528,  1259,   366, 10398,  2010, 11865,  2812, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   146,   390,   303, 18263,   117, 18119,   487,   320,   449,\n",
      "           552,   117,  1904,   259,  5708,  7925,   229, 15419,  4173, 16291,\n",
      "           180,  2480,   123,  1632,   942,   146, 11552,  4136, 14838,   240,\n",
      "          1744,  3897,  1084, 12092,   148, 11314,   498,   146,   274,   364,\n",
      "         22288,   117,   582,  3370,  7567,   260,   924, 14808,   639, 13535,\n",
      "           117, 20216, 22281,   125,   347,   851,  2532, 19206, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,  3876,  2182,   146, 18908,   247, 15700, 22278,   121, 22361,\n",
      "          8410,   222, 10342, 13449, 19088,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   179,  4314, 22278,   368,   229,  2480,   171, 10638,   247,\n",
      "           136,   230,  4131, 22278,   179,   311,  7283,   228,   529,  1863,\n",
      "           591,  8525,  1182,   138,   582,   529,   218,   117,   123,  1945,\n",
      "           251,   180,  2954,   117,   625,   123, 13943,  7282,  2836,   202,\n",
      "          2992, 22288,  7908, 22279,   348,   259,  5097,   117,   122,   123,\n",
      "           235,  1046, 22278,  1315,  2245,  5723,   538,  1877,   956,   143,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[ 101, 1314,  207,  304,  146, 9059,  102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   146,  1315,   326,   366, 10398,  8163,   343,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   146, 10695,  1390,   154,   498,   260,  8950, 20378,   202,\n",
      "         21228,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,  7674,   118,   176,   123, 15235,   292,   298, 20167,   122,\n",
      "           123, 16468, 22281,   304, 19876,   421,   117,   271,   146,  1109,\n",
      "           141,   117,   260,  2360,  1149, 10180,   498,   146,   472,   714,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,  4023,   437,  7211,   123, 13982,   117,   235,  1408, 22280,\n",
      "           122,   313,   697, 10695,   117,   240,   420,   260, 10398, 17464,\n",
      "           117,   122,   437,   302,   537,   229, 22290, 20657, 22278,  1748,\n",
      "         22279,   251,  8932,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,   331,   827, 22287,   221,   964,   260,  4332,  1705,   527,\n",
      "           277,   122,   221,   964,  1941, 22281,   269,   794,   123,  7390,\n",
      "         20798, 20167,   125,  9372,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,  1139,   962,  1557,  1016,   123,  2620,  8253, 22280,   171,\n",
      "          9059,   117,   123,  1058, 22280, 10695,   117,   781,   256,   260,\n",
      "         11563, 11912, 22281,   123, 10428,   261,   117,   179,   437,  6082,\n",
      "           117,   449,   229, 22280,   176,   670,   180,  2480,   582, 14556,\n",
      "         22278,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 14 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.033426183750902\n",
      " Coesão Score Final: 0.516713091875451\n",
      " Conectivos encontrados: ['e', 'mas', 'assim', 'quando', 'enquanto', 'se', 'ou', 'ora', 'como', 'quanto', 'para que', 'quanto']\n",
      " Número de conectivos: 12\n",
      " Número de sentenças: 14\n",
      "======================\n",
      "Resultados para preprocessado_iracema_jose_de_alencar_cap_1.json:\n",
      "{'coesao_score': np.float64(0.52), 'conectivos_encontrados': ['e', 'mas', 'assim', 'quando', 'enquanto', 'se', 'ou', 'ora', 'como', 'quanto', 'para que', 'quanto'], 'num_conectivos': 12, 'proporcao_conectivos': 0.033, 'similaridade_media': np.float64(1.0), 'num_sentencas': 14}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  1202, 22287,   117,   785,  1202, 22287,  7970, 10530,   117,\n",
      "           179,   744,  5580, 22278,   202, 21228,   117,  2714, 11865,  2812,\n",
      "         22278,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101, 11865,  2812, 22278,   117,   123,  2477,   705,   298, 18908,\n",
      "           501,   125,   949,   117,   179,   978,   259, 13841,   325,  7769,\n",
      "           179,   123, 14114,   180,  5032,   324,   117,   122,   325, 10620,\n",
      "          5790, 13665,  1815,   296,   125,  1877, 10443,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   146,   395,   378,   180,  1941,   193,   229, 22280,   495,\n",
      "         11152,   271,   347, 13449, 19088,  2798,   123,   475, 19206, 22290,\n",
      "           252,   746,  1399,   202, 17849,   455,   271,   347,   607,  3093,\n",
      "           183,  3980,   809,   243,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   325,  6372,   328,   179,   123,   549,   304, 16853,   117,\n",
      "           123,  1623,   912,  2477,   705,  4063,   151,   146,   333,   154,\n",
      "         22280,   122,   260, 21021,   171,   254,   862,   117,   582,  2238,\n",
      "          2836,   327, 14739,  1272,  9750,   117,   180,   739,   229,   304,\n",
      "         22280,   316,   581, 17376,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   146,   766,   416,  1697,   122,  1444,   117,  3002,   577,\n",
      "          2746,   117,  1369,   375,   256,   820,   123,  6183,   279,  8497,\n",
      "           151,   179, 12232, 22278, 13038,   124,   170,   260,  2796,  6205,\n",
      "           138,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   222,   644,   117,   320, 13718, 22280,   171,   969,   117,\n",
      "           740, 17520,   375,   256,   173,   222,  6054,   180,  7321,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101, 10137,  2836,   118,  2036,   146,  1831,   123, 15419,   180,\n",
      "           146,  5137, 15586,   117,   325,  1198, 22281,   304,   171,   179,\n",
      "           146,   438,  5505,   180,  2954,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   259, 10752,   180,  1334,   351,  7296,   933,  1186, 17700,\n",
      "         12051, 22287,  2968,   498,   259,   222,   474, 13841,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101, 20559, 22281,   324, 14121,   705,   259,  3852,   128, 13501,\n",
      "           945,   692,   146,  2242,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101, 11865,  2812, 22278,  5127,   171, 17052,   146,   313,  1286,\n",
      "          6774,   121, 22361,  6205, 22278,   744,   123,   577,   130,   524,\n",
      "           117,   271,   123, 11152,  8037, 12764, 22278,   179,  5553, 22288,\n",
      "           173,  1062,   252,   125,  9856,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,  1139, 17520,   375,   117,  4276,   411,   148,   366, 14131,\n",
      "           171,  2727,   260, 16240,  5949,   125,   347,  7055,   117,   122,\n",
      "          7346,   154,   170,   146,  9679,   180,  6629,   117,  1074, 13550,\n",
      "           202,  1187,   268,  6730, 22280,   117,   146,  2242, 21339,  7485,\n",
      "         22279,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   123,   416, 15633, 11997,   117,   327, 20216,   122,  8932,\n",
      "           117,  5911,   304,  1982,  3914,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,   260,  1176, 19423,   712, 10752,   180,   388,  4056,   122,\n",
      "           125,  1084,  3196,  4042, 22282,   705,   423,   655,  1028, 11935,\n",
      "          2650,   146, 13223,   125, 19278,  8788,  2530,   117,   582,  3889,\n",
      "           123, 16853,   532,  3980,  7362,   117,   259, 14874, 19873,   171,\n",
      "          2564,  3239,   117,   260,  6205, 22290,   842,   180,   717,  5105,\n",
      "           170,   179,  1835, 22279,   123,  6472,   117,   122,   260, 17681,\n",
      "         22281,   125,   179,  8788,  2446,   146,  3233,   285, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,  7257, 22282, 21358,  6642,   123, 11152, 14676,   180, 19988,\n",
      "           154,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101, 15568,   123,  2477,   705,   259,  5708,   117,   179,   146,\n",
      "           969,   229, 22280, 17579,  2755,   124,   327,  3122,   526, 21994,\n",
      "           118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,  4271,  3914,   122,  1364,   123,  4108,   266,   118,  1084,\n",
      "           418,   222, 18263, 14848,   117,   176,   122, 18263,   122,   229,\n",
      "         22280,  3179, 11775,  5791,   183,   180, 14151,  2841,   154,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   376,   529, 21347,   146,  4712,   366, 11912, 22281,   179,\n",
      "         18995, 22287,   146,   528,   538,  5708,   146,  5580, 12448,   366,\n",
      "          6205,   138, 16087,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,  9392,   470, 14505,   138,   122, 10128,  9392,   382,  9312,\n",
      "         22287,   118,  2036,   146,  1831,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,   262,  6372,   286,   117,   271,   146, 11552,   117,   146,\n",
      "         22000,   125, 11865,  2812, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   123, 16240,  2034,   173,   483,  3301,   285,   202,  7055,\n",
      "          8383,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,  3746,   470,   125,  5052, 10527,  6605,   546,   229,  8630,\n",
      "           171, 10846,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,   125,   652,  6554,   183,   117,   123,   223, 22280,  4878,\n",
      "           154,  6600,   498,   123,  3466,   180,  9531,   449,  2044, 13449,\n",
      "           172, 22288,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   146,   390,   303, 18263, 13308,  3505, 15249,  1096, 22280,\n",
      "           125,   327,   223, 22279,   117,   582,   123,  2606,   122,  8602,\n",
      "         22280,   125,   370, 13397,   124,   122,  3165,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  4808,   325,   121, 22361,  8410,   179,   180,  1718,   328,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,   146, 10289,   179,   368,   429,   538,  5708,   122,   202,\n",
      "          9169,   117,   229, 22280,   146, 18661,  2779,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   240,   210,   123,  2477,   705, 18253,  1353,   125,   898,\n",
      "           146,  7055,   122,   123,   169,   364, 13714,   117, 17382, 22282,\n",
      "         19870,   221,   146, 18263,   117, 20987,   285,   180,  5923,  3632,\n",
      "           179,  8408, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   123,   223, 22280,   179,  6372,   328,  1718,   364,   117,\n",
      "           418,  3048, 22288,   325,  6372,   328,   122,  4968, 22281, 12569,\n",
      "           256,   146,  5052,   179,  3746,   185,   524,   256,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,   700, 11865,  2812, 22278, 13935,   870,  3355,   252, 14621,\n",
      "           328,  2002,   123, 21639,   320, 10846,   117,  5825,   214, 11631,\n",
      "           123,  8993,  5546, 14004,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,   146, 18263, 11234,  2010,  6642, 22281,   170,  1039,   123,\n",
      "         16240,  2034,   180,  4527,   136,  2010,  1977,   437, 16948,   117,\n",
      "         18263,  4712,   117,   123,  4616,   125, 17080,   925,   148,   128,\n",
      "           136,   171,   323,  7762,  7485, 22279,   123,  3769, 21021,   117,\n",
      "           179,  2364, 12343, 22287,  1342, 18263,   271,  5023,   136,  2010,\n",
      "          1214,   268,   125,  1004,  5533,   117,  2267,   366,  9647,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,  1214,   268,   366,  3145,   179,   437,   249,   925,   148,\n",
      "           128,  1941,  8771,   228,   117,   122,  1790,   376,   259, 17080,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,  2010,  1004,   118,  6661,  1547,   146, 10334,   712,  5097,\n",
      "           298,   316,   581, 17376, 22281,   117, 14415,   366, 12378,   117,\n",
      "           122,   123,  5351,   795,   125, 11997, 20346,   117,  1568, 20933,\n",
      "         10780, 21101,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 31 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.02325581391188584\n",
      " Coesão Score Final: 0.511627906955943\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'logo', 'pois', 'enquanto', 'se', 'nem', 'ou', 'seja', 'como', 'quanto', 'quanto']\n",
      " Número de conectivos: 13\n",
      " Número de sentenças: 31\n",
      "======================\n",
      "Resultados para preprocessado_iracema_jose_de_alencar_cap_2.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'porem', 'logo', 'pois', 'enquanto', 'se', 'nem', 'ou', 'seja', 'como', 'quanto', 'quanto'], 'num_conectivos': 13, 'proporcao_conectivos': 0.023, 'similaridade_media': np.float64(1.0), 'num_sentencas': 31}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,   146, 10334,  5793,   123,  2477,   705,  1032,   143,   180,\n",
      "          7321,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   625,   146,   969,   273,  2227,   581,   256,   498,   123,\n",
      "          7045,   298, 20578,   117,   122,   123,   577,   266,  1950,  5723,\n",
      "           171,  4707,   180,  6629,   259,  1867, 12825, 22290,   683,   117,\n",
      "          1061, 15731,   202,  5488,   123,   739,   316,   581,   122,   325,\n",
      "          5533,   117, 15231, 21794,   202, 15998,   430, 22280,   117,   123,\n",
      "         15419,   298,  7188,   717, 11837,  1044,   117,   123,  5351,   795,\n",
      "           243,   372,   537,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   146,  7889,   151, 22280, 19016,   256,   123,  4303,   117,\n",
      "         21541,   229,   860,   364,   125, 18757,  1382,   581,   117, 13750,\n",
      "          1552,   259, 15797, 22281, 20457, 22281,   125,  5023,   321,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   146,  1744,  3897,   331,  1703,   180,  2042,   375,   505,\n",
      "          2890,   256,   117,   271,  2143,   942,   125,  3233,   285, 22280,\n",
      "           117,   259,   408,  9539, 22281,   122, 16734, 13841,  8618,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   125,  1031,  1281,   178,   179,  1011,   117, 20158,   151,\n",
      "           123,  1069,   701,  5708,   329,  2370,   122,   529,  1315,  1557,\n",
      "         16087,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   146,   372,   537,  1340,  2042,  2904,   259,   682,  5184,\n",
      "           382,   179,  1938, 20798,   692,  4450,   203,   792,   123, 15419,\n",
      "           125,   230,   388,  4056,   969,   931,   151,   179,  8940,  3600,\n",
      "         12674,   214,   118,   176,   423,  5488,  1796,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   625,   259, 19204,  8880,   229, 15592,  2377,  2755,   124,\n",
      "           171, 17849,   455,   117,   318, 13793,   347, 11552,   271,   146,\n",
      "           171, 16815,   130,   117,  4613,   373,   260,  1510,  1671,   117,\n",
      "          6233, 11865,  2812, 22278,   122,  4970,   179,   123, 16246,   222,\n",
      "          2656, 18263,   117,   125, 17253,   646,   304,   122,  5533, 22281,\n",
      "          3145,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   260,  8217,   316,   581, 17376, 22281,   117,   121, 22361,\n",
      "          1202, 22287,  8251,   151,   321,   581,   117, 18402, 22287,   125,\n",
      "           230,   940,   646,   304,   125, 14449,   117, 14874,   271,  2968,\n",
      "           125, 16468, 22281,   304,   117,  2374,   557, 22281,   125,  2873,\n",
      "           154,  1269,   421,   260,  7414,   171,   311,  1620, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   146,  7889,   151, 22280, 17662,   179,  2589,   222, 18263,\n",
      "          4327,   117,  6086,   179, 17573,  2836,   259,  5097,  9283,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101, 20885, 22280,   117, 14657,   203,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   123,  2477,   705, 12110,   221,   146, 10334,   122,  1331,\n",
      "          2010,   368,  3429,   117,  1568,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[ 101, 2010, 3429, 1004,  102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,   122,  5023,   321,   179,  3889,   146,  9730, 22279,   123,\n",
      "          5351,   795,   125, 11997, 20346,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,  1016,  4111,   117,   146,   372,   537,  1367,   146, 13850,\n",
      "         22178,   320, 10334,   122,  8880,  2592,   229,  5351,   795,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   146,  9922,  2152, 22280,  1636,   203,   118,   176,   229,\n",
      "          2551,  1310,   117, 20083, 22278,   202,  1997,   180,  1967,   304,\n",
      "         22280,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101, 11865,  2812, 22278, 10049,  2071,   146,  4848,   180,  5627,\n",
      "           292,   122,  5626,   146,   179,  1021,   125, 18025, 22280,   143,\n",
      "           221, 21619,   123, 11062,   122,   123,  2496,  4250,  2650,   146,\n",
      "          4745,   180,   329,   304,   117,   123, 20080,   118,   121, 22361,\n",
      "          6205, 22278,   117,   259, 11380,  7296,   933, 11060,   117,   259,\n",
      "           395,  2370,   125,   949,   122,   146, 10832,   125,   329,   741,\n",
      "           122,  9480,  1343,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   700,   123,  2477,   705,  3033,   170,   123,   254,   421,\n",
      "         13714,   117,   179, 16386,   371,   229,  3944,  6730, 22278,   125,\n",
      "          6205, 22278, 12837,   304,   221, 11348, 22282,   146,  9169,   122,\n",
      "           260,   223,   128,   243, 10334,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,   625,   146, 18263,  3303,   123, 19538,   304, 22280,   117,\n",
      "           146,  4575,   372,   537, 17896,   203,   146, 13850, 22178,   122,\n",
      "         11234,  2010,  7762,  7485, 22279,   136,  2010,  1976, 22287,   117,\n",
      "          9396,   146, 10846,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,  2010,  1004,  7762,  7485, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   146, 10334,   122,  7258,   229,  5351,   795,   125, 11997,\n",
      "         20346,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   259,   316,   581, 17376, 22281,   376,   592, 14449,   221,\n",
      "         10591,   118,  1340,   117,   173,   209,  2210,   834,  1284,   221,\n",
      "          1312,   118,  1340,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,  1331, 22279,   117,   122,   944,   437, 13486, 18003, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,  2010,   372,   537,   117,  2779,   437, 13406,   303,   146,\n",
      "           762,  7591, 22290,   268,   179,   311,  2166,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  2044,   179,   146,   969, 16737,   117,  4314,   244,  5023,\n",
      "         22278,  5351,   795,   122,   437,   249,  5097, 13793,   323,  1976,\n",
      "         22287,  7955,   449,   229, 22280,  7427,  5308,   118, 15887,   834,\n",
      "          4640,   118,   437,  1977,   122,   146, 18263,   117,   179,  3283,\n",
      "           677,  3695,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,  2010,   262,   123,  5023,   321,   179,   146,   372,   537,\n",
      "          5083,   368,   437,  5626,   117,   368,   437,  4915, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101, 11997, 20346,  3874,  1191,   423,  9730, 22279,   229, 22280,\n",
      "         10573,   171,   323,  2158,   117,   122,   625,  2541,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   176,  3189,   143, 18165,   117,   273,  2227,   498,   964,\n",
      "           259, 13994, 20073, 22281,   176,  3189,   143,  5961,   117,   437,\n",
      "         22288,  9730, 22279, 13083,   154,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,   146, 10334,  1996,  2010,  7206,   298, 14449,  8618,   117,\n",
      "           179, 14748,   228,   123,   316,   581,   529,  7414,   171,  1941,\n",
      "         12470,  7057,   117,  3047,   171,   528,   117,   582,  1967, 22287,\n",
      "           259, 19147,  1185,  9777,   117,  5999,   125,  5023, 22278,   229,\n",
      "           304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,  7343,   655,   122,   528,  2762,   117,   179,   229,  5023,\n",
      "         22278,  3182, 22278,  1331,   271,  1417,   125, 18263,  7343,  5052,\n",
      "           117,   146,   243,   739,  2049,   179,   652,  4970,   260,  3145,\n",
      "           125,  5023, 22278,  9019,   151,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,  1941, 17080, 16863,  2799,  9288, 11306,   240,   528,   260,\n",
      "           956,  1495,   171,   221, 10224,   117,   125,   582,  8198,   122,\n",
      "           146,  3795,   117,  1950, 10490,   825,   298,   532,   117, 13910,\n",
      "          2535,   259, 14985, 22281,   333,   183,   143,   171,  1178,  1125,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,   331,  2779,   125, 14730, 10692, 19623,   117,  2113,  1011,\n",
      "           420,   259, 13779,   193, 12470,   138,   125,  1334,   124, 22288,\n",
      "           117,   229,  5351,   795,   171,  4332,   378,  4928, 22283,   117,\n",
      "           925,   148, 22280,   125,  1941,  5132,   324,   117,   179, 14055,\n",
      "           654,   170,  1039,   123,   388,  4056,   180,  8286,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101,   607,  1510, 22281,   331,   145, 18318,   793,   221,   123,\n",
      "           329,   304,   122,  7955,   298, 17080,   117,  1976, 22287,   712,\n",
      "          5097,   298,   154,   581, 17376, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,  2010,   262,  3179, 11775,  5791,   183,   180,  7321,   179,\n",
      "          2992,  2904,   146, 18263,  4712,   202, 13389,   180,  6629,   117,\n",
      "          9396,   146,  7889,   151, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   123, 17649, 22278, 17554, 22288,   117,  1202, 22287,   117,\n",
      "           229,  4493,   171,  5488,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101, 11314, 22278,   123,  2954,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 35 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.023696682427019458\n",
      " Coesão Score Final: 0.5118483412135097\n",
      " Conectivos encontrados: ['e', 'mas', 'assim', 'logo', 'pois', 'porque', 'quando', 'se', 'ou', 'ora', 'quer', 'como', 'logo que', 'porque', 'tanto']\n",
      " Número de conectivos: 15\n",
      " Número de sentenças: 35\n",
      "======================\n",
      "Resultados para preprocessado_iracema_jose_de_alencar_cap_3.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'assim', 'logo', 'pois', 'porque', 'quando', 'se', 'ou', 'ora', 'quer', 'como', 'logo que', 'porque', 'tanto'], 'num_conectivos': 15, 'proporcao_conectivos': 0.024, 'similaridade_media': np.float64(1.0), 'num_sentencas': 35}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,   146,   372,   537, 11744,   748,   146, 15273,   304,   117,\n",
      "           122,  5127,   180,  5351,   795,   117,   240,   210,   146, 10334,\n",
      "           229, 22280,  1767,   331,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101, 11865,  2812, 22278,  4706, 22278,   170,   260,  2459,  7000,\n",
      "           221,  6202,   146,  9730, 22279,   125, 11997, 20346,   117,   122,\n",
      "           259, 14449, 13545,   221, 12126, 11237,   943,   118,  2036,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,  2010, 18263,  4712,   117,  1996,   123,  2477,   705,   117,\n",
      "           146, 15537, 14368, 22279,  5023, 22278,  2551,   726,   123,  2954,\n",
      "           122,   146,   969, 12424, 22278,  3377,   123,   437,   249,  5708,\n",
      "           117, 14161,   123,  5023, 22278,  8410,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   122,  1016,  4111,   117, 11865,  2812, 22278,   978,   146,\n",
      "         18908,   247,  8574,   452,   117,   122,   222,   328,   123,  1877,\n",
      "           269,  1609,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,  2010,  5023,   311,  5308, 22281,   136, 16795,   528,  2762,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,  2010,   260,   325, 18636,  2459,   180,   739,   316,   581,\n",
      "           336,  1039,  8679,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,  2010,   221,  2859,   123,  2267,   125, 11997, 20346,   229,\n",
      "         22280, 12444,   370, 19440,   146,  9730, 22279,   123,  5351,   795,\n",
      "           171,   372,   537,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,  2010, 10334,   117, 11865,  2812, 22278,   229, 22280,   706,\n",
      "           333,  5023, 22278,   333,   256,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   122,   740,   179,  5825,   146, 11021,   180,  3436, 21101,\n",
      "           122,   146,  9250,   247,   171,  8846,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   327,   148, 22280,  7875,   221,   146,   372,   537,   123,\n",
      "         14422,   125,  5023,   321,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   146, 18263,  7045, 22280,  7570,   203,   123,  5351,   795,\n",
      "           122, 20158,   456,   118,   176,   229,  1510,   256,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   123,   739,   316,   581,  8004,   151,   118,   176,   202,\n",
      "          4707,   171,  5488,   117, 16188,   251,   954,  6611,   128,   180,\n",
      "         14161,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  1315,  1096,   146, 15273,   304,   320,  8420,   157, 15106,\n",
      "           171,  6916, 22280, 16853,   117,  3985,   151,   123,  4654,   304,\n",
      "           173,  4899,   123,  1315,   272,  6846,  3292,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   146,   372,   537,  8424,  4337,   151,   146, 15797,  5849,\n",
      "          8631,   122, 10355,   320,   264,   378,  3960,   175,   259, 21774,\n",
      "           125,  5023,   321,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   146,   636,  3795,   180,   229,   304, 22280,   316,   581,\n",
      "         17376,   117, 11865,   862, 22278,   117,  7433,   124,   171,  2979,\n",
      "           180, 10530,  8251,   151,   321,   581,   117,   221,  4915,   260,\n",
      "          8217,   171,   333,   154, 22280,   598,   146,  2949,  3111,   339,\n",
      "         13779,   193, 12470, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   259, 14449,   171,  5488, 17337,  2872,   123, 13109,   171,\n",
      "          3795,   117,   122,   146,  6730, 22280,  3076,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   146,  9922,  2152, 22280,  7045, 22280,  4970,  5533,   146,\n",
      "          4982, 22280,   180,  4939,   117,   122,  1367,  1202, 22287,   117,\n",
      "           122,  9834, 22288,   146,  2992, 22288,  5580,   834, 15553,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,   123,  4595,  8221,   117,   179,   318, 13793,  6788,   256,\n",
      "           498,   123,  1034, 11312,   180,  7321,   117,   866,  4373,   347,\n",
      "          6793, 11948,   221,   260, 12837,  1149,  7414,   171,  1334,   124,\n",
      "         22288,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,   625,   368,  2153,   618,   203,   146,  5488,   122,  8544,\n",
      "         11092, 14754,   229,  6629,   117,   146,  5184,   183,   125, 11865,\n",
      "          2812, 22278,  4751,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   123,  2477,   705,  3866, 22278,   146, 10334,  3255,   123,\n",
      "           235,  1046, 22278,   233,   630,   179,   398,   810, 22278,   834,\n",
      "           362, 22282,  4643,   130,  4765,   240,   420,   123,  6847,   984,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,  2010,   240,   179,   117,  1996,   740,   117,   146, 10334,\n",
      "         18370,   123,  5351,   795,  9730,   556,   834,  4915,   146,  2981,\n",
      "           180,  1359,   136,  1977,  1191, 17356, 22280, 18263,  4712,   229,\n",
      "          2480,   298,   316,   581, 17376, 22281,   136,   146,  7045, 22280,\n",
      "         10733,  2249,   495, 19219,   123,   179,  1445,   122, 12524,   118,\n",
      "           176,  1486, 21308,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,  2010, 16241,  1537, 22287,  1191,  3002,   320,   437, 22288,\n",
      "          9730, 22279,   117,  2267,   125, 11997, 20346,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   495,   146,  6532,   125,   792,   532,  3667,   179,   146,\n",
      "          4415,  5723,   298, 17151, 22281,   298,   316,   581, 17376, 22281,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,   229, 22280, 16403,   146,  2981,   180,  1359,   449,  1904,\n",
      "           173,   327,  8410,   123, 10201,  2028,   125, 11865,  2812, 22278,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,  2010,   176,   123, 10201,  2028,   125, 11865,  2812, 22278,\n",
      "         10606,   149, 22361,  8410,   171, 10334,   117,   740,   229, 22280,\n",
      "           146,  4314,   151,  1018,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   146,  9059,   229, 22280,  1904,   123, 11912,   285,  8525,\n",
      "          1182, 22278,   117,   625,   123, 11912,  5167, 22279,   123,  6205,\n",
      "         22278,   180,  9856,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   123,  2477,   705,  4217,  5461,  2010, 18263,  4712,   117,\n",
      "          2521,   179, 17649,  3301,   781,   185,   180,   329,   304,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,   146,   925,   148, 22280,   125, 11865,  2812, 22278,   376,\n",
      "           146, 12728,   233,   630,   179,  7525,   403,   472, 22280,  1095,\n",
      "          7270, 22278,   420,   259, 12484,   180,  6629,   122,   146, 11552,\n",
      "           171,   146,  5137,   492,   179,   873,  1407,   229,  1510,   256,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,   368,   437, 13375,   124,   260,  7414,   171,  2187,   366,\n",
      "           702,  1149,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,  2010,  2249,   596,   176,  3852, 22278,  1075,   179,   146,\n",
      "           925,   148, 22280,   125, 11865,  2812, 22278, 10726,   125,  1359,\n",
      "           229,  5351,   795,   125, 11997, 20346,   136,  2010,   146,   969,\n",
      "           117,   179,  2541, 16737,   117,  2962, 22278,   170,   146, 18263,\n",
      "         17649,  3301,   712,  5097,   171,   254,   862,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,  2010,   437, 22288,  9730, 22279,  2521,   117,  2267,   125,\n",
      "         11997, 20346,   449,   176,   146,   969,  2862,   229, 22280,  5626,\n",
      "         22282,   146,   925,   148, 22280,   125, 11865,  2812, 22278,   117,\n",
      "           368,  4915, 22278,   146, 14739,  1339,  4712,   123,   316,   581,\n",
      "           298, 13779,   193, 12470,   138,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[ 101,  528, 2762, 2927,  123, 5351,  795,  171,  372,  537,  102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,   123, 21672, 22278,  2551,   179, 11865,  2812, 22278,  3980,\n",
      "           809,   124,   170,   123,   398,   387,   171,  7489,  1286,   314,\n",
      "          5825,   256,   118,  2036,   222, 11334,  1945,   283,   122, 11152,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   146,  7045, 22280,  2251, 22282,   155,   685,  8362,   214,\n",
      "          4217,   364, 22282,   117,   420,   259,   362, 22282,  4643,  1408,\n",
      "           180,  7321,   117,   146,  2242,   223,  3629, 22280,   180,  2477,\n",
      "           705, 19345,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 34 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.018998272851471033\n",
      " Coesão Score Final: 0.5094991364257355\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'assim', 'quando', 'se', 'ou', 'como', 'quanto', 'antes que', 'quanto']\n",
      " Número de conectivos: 11\n",
      " Número de sentenças: 34\n",
      "======================\n",
      "Resultados para preprocessado_iracema_jose_de_alencar_cap_4.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'porem', 'assim', 'quando', 'se', 'ou', 'como', 'quanto', 'antes que', 'quanto'], 'num_conectivos': 11, 'proporcao_conectivos': 0.019, 'similaridade_media': np.float64(1.0), 'num_sentencas': 34}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  4141, 13793,  2415, 22280,   262,   117,   298,  9967,   712,\n",
      "          4698,   122,  1685,   481,   117, 12531,   125,   222, 12447,   397,\n",
      "           179, 20613,   685,   420,   260,  1256,  7367,   125,   230,  5980,\n",
      "         22278,   122, 21597,   392,   316,   391,   324,   538,  1314,   211,\n",
      "           683,   171,  2907,   171, 11967, 10953,   763,   122,  1971,  7569,\n",
      "          3770,   171,  1695,   179,  5076, 22278,  4922,   623, 12051,   125,\n",
      "           481,   117,   179,   117,   320,  9215,   118,   176,   146,  9019,\n",
      "         13793,   221,   123,  2480,   117,  2036,  2789,   117,   173,  7855,\n",
      "           125, 11263, 22281, 11938, 22281,   117,  2798,   331,   123,  5304,\n",
      "           170,   146,   179,  1011,  1839,   117,   271,   744,   222, 12059,\n",
      "           122, 22191,  5292,   173,  3495,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,  4996, 17551,   122,  8350,   240,   327,  1284,   117,   146,\n",
      "         13254,  7304,   748,   118,   176,   123, 18908,  3239,   304, 22280,\n",
      "           744,   170,   325,   388,   947,   117, 13601,   118,   176,   125,\n",
      "          1815,  2607,  2032, 22280,   125, 20613,   943,   117,   179,  9620,\n",
      "         21206,   398,  4861,   243,   260,   325,  3072, 22281, 11118,   303,\n",
      "           143,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,  4678,  4322,   498,   146,  3568,   304, 22280,   180,  2004,\n",
      "         22278,  5304,   117,   173,  5530,   125,   230,   860,   364,   117,\n",
      "          2636, 17343,   458,   125,   222,   629,   303,   125,  9466,   321,\n",
      "         13140,   125, 19278,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   123,  9652,  5646,   524,   256,   118,   219,   252,   117,\n",
      "          9402,  1256, 18384,  7911,   240,   644,   117,   230,   163,  8860,\n",
      "         17124,   327, 13219,   117,   123, 10420, 18398,   852,   117,  3336,\n",
      "           266, 19125, 11769,   117, 17479,   125,   222,  4575,  2992,   339,\n",
      "         17642,   173,  7472,   125,  1796,   122,  8932,   285,   170,   222,\n",
      "          1456,   143,   179,   978,   230,  3883,   304,   125,   223, 22280,\n",
      "           122,  5057,   958,   555,   229,   651,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101, 10420, 18398,   852, 14619,   210, 10454,  2124,   123,   327,\n",
      "           163,  8860,   285,   495,   123,   325,  1004,   870,  2127,  3897,\n",
      "         20733,   171,  2907,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   125,  1062,   252,  5133,   151,  5450, 22288,   117,   122,\n",
      "           123,  2954, 11371, 10310,   373,   122,   847,  1149,   125, 10692,\n",
      "          1655,  9141,   256,   125,  1955,   123,   347,  8517,  4698,   592,\n",
      "           118,  7911,   240,   454,   117,   122,   117,  2440,  1659,   117,\n",
      "           978,   125,   670,  1821,   179,   146,  1395, 17551,   221,   123,\n",
      "           313,   512,   322,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   222,   644,   117,   240,   210,   117,   146,   347,  2397,\n",
      "           117,   700,   125, 13239,  5899,  2653,  3611,   117, 14537,   348,\n",
      "           230,  5890,  2886,   260,   675,   344,  1149,   117,  6600,  4807,\n",
      "           229,  4768,   117,   320,  1341,   180,  3883,   304,   117, 15872,\n",
      "         10490,   243,   271,   230,  5294,  8849,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,  4141, 13793,  2415, 22280,  6075,   739,  3316,   240,   418,\n",
      "           273,   522,   304,   117,  1191,   118,   176,  2684, 17469,  7600,\n",
      "           298, 14632, 22281,   180, 13219,   117,   122,   170,  3846, 20045,\n",
      "         22280,   123, 19068, 14995,   117,   179,   123,  3264,  2606,   146,\n",
      "          9916,   221, 14809,  9238,   366,   675,   273, 15256, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,  6540,   118,   176,   170,   368,   117,  5222,   118,  2036,\n",
      "           123,   327,  1069,   125, 14283,  7557,  3391, 22280,   143,   122,\n",
      "          5781,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,  1112,   347,  7258,   170,   151,   118,  2036,   123,  6614,\n",
      "           171,  1831,   229, 22280,   495,  5911, 10838, 22280,   221,   230,\n",
      "          6754,  2606,   370,   125,  3240, 22282,   900,   363, 22361,  1369,\n",
      "           117,   944,   259,  2112,   117,  4698,   592,   118,  7911,   173,\n",
      "          3495, 22354,   122, 11021, 22288,   118,  2036,   318, 13793,   146,\n",
      "           179,   978, 10506,   243,   221,   123,   327,  4676,   122,  2467,\n",
      "         11765,   320, 12447,   397,   179,  2036, 15242,   236,   260,  3338,\n",
      "         22281,   117,  2113,  1941,   125,  5288,   576,  1796,  6641,   251,\n",
      "           240,  1956,  3563,   128,   179,  2036,  8880,   229,   163,  8860,\n",
      "           285,   954,  8001,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   180, 22283,   173,  4271,   117,  4141, 13793,  2415, 22280,\n",
      "          1204,   118,   176,   146,  8097,   117,   146, 18737,   122,   146,\n",
      "         12781,   180,  3336,   266,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   202,  1338,   125,  1695,   596,   495,   368,  1977,  5267,\n",
      "           256,  1284,   125,  2745,   179,   740,  1271,   151,   122,   495,\n",
      "         14619,   210,  1977,  5078,   252,   122,  1598, 13808,   298,   532,\n",
      "         12651,   501,   117,   122,  1977,   176, 12771,  2836,   125, 11935,\n",
      "           367,   320,  7258,   259,  4698,   592,   118,  7911,  3509,   441,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  6540,   118,  2036,  2044,   230,  1284,  5820,   117,   122,\n",
      "           123,   163,  8860, 17124,   117,   625, 11736,   125,  3495,   221,\n",
      "          1569,  5664,   117, 10348,   222,  5995, 22280,  2684,   123,  5304,\n",
      "           122, 16653,   118,   146,   366,   223,   128,   171, 12447,   397,\n",
      "           117,   125,  1112,   347,  4141, 13793, 22354,   117,   271,   740,\n",
      "         10355,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   347,  4141, 13793,   125,  1584,   256, 11637,  1780,  3867,\n",
      "          4296, 17088, 22281,  1362,  6846,   140,  4029, 21102,   117,   173,\n",
      "          3561,  1400,   125,  1798,   332,   243, 19449, 22278,   118,   176,\n",
      "           117,  3002,  2685,   122,   173,  4824, 20299, 22281,   125,  1955,\n",
      "          1112,  8748,   122,   723,  1430,   125, 10420, 18398,   852, 22354,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   122,   240,  1815,   547,   262,   146,   316,   391,   514,\n",
      "           397,  7551, 12908,  2028,   202,  5791,   183,   180,  2606,   117,\n",
      "           179,   418,   870,   509,  3874,   325,  2709,   619,   331,   240,\n",
      "           898,   117,   122,  5205,   256,  2461,   117,  2992, 18004,   175,\n",
      "           117,  1364,   122,  1569, 11717,   342,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   240,   169, 14116,   117,   176,   614,   210, 11736,  8364,\n",
      "           170,   740,  1569,  3907,   523,   117,  2798,   325,   176, 10348,\n",
      "           320,  1223,   125,  2863,   118,  1084,   117,  8544,  2044,  2368,\n",
      "           123,  4141, 13793,  2415, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[ 101,  625, 7320, 1154, 2072, 8932,  442,  102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,   368, 19284,   118,  2036, 11460,   210,  5062,   122,   740,\n",
      "         11353,   125,  4332,   942, 13059,   117,  8540,   173,  3285,   140,\n",
      "           118,   176,   125,  1160,   170,   222,  1456,   143,   117,  2113,\n",
      "           117,   271,  1719,   123,   329,  7882,   852,   117, 10420, 18398,\n",
      "           852,   229, 22280,  4750, 22076, 22282,   118,   176,   123,  7769,\n",
      "           122,  2863,   256, 16589, 15500,   692,   403,   146,  2397,  1532,\n",
      "           646,   304,  2886,   123,   327,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,  4141, 13793,  2415, 22280, 10107,   318, 13793,   117,   170,\n",
      "           260,  3338, 22281,   180,  8932,   117,  1089,  1877,   793,   125,\n",
      "          5856,   320,  1341,  9657,   180,  5304,   117,   122, 17760,   230,\n",
      "           504,   508,   125,   924,  6929,   117,  7398,   320,  1423, 20114,\n",
      "           246,   123,  4768,   117,   660,   123,   670,   180,  2375, 11826,\n",
      "           123,   163,  8860,   285,   122,   123,   171,  4707,   221,   222,\n",
      "          4678,  8476,  2758, 22280,   179,   176, 15580, 22288,   170,   259,\n",
      "           329, 17738,   942,   125, 10420, 18398,   852,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,  1021,   117,  1202, 22287,   180,  9461,   117,   230,   271,\n",
      "           285,   125,  1941,  5105,   404,   785,  7492,   170, 12361,  2912,\n",
      "           138,   125,  4437, 11229,  1941,   528, 22279,   401,   117,   222,\n",
      "         11032,   428,   247, 13140,   125, 14125,   122,   344,   825,   125,\n",
      "          1798,   125,   549,   117,   222,   475, 22288,   739,   125, 16064,\n",
      "          4793,   316,  2041,   201,   117,   682,  2764, 19161, 22281,   125,\n",
      "         16341,  5159,   125,   230,   331,  6514, 22278,   122,   222,   928,\n",
      "           328,   415,  5351,   878,   125,   466,   702,   229,  8130,   117,\n",
      "           170,   123,   327,  7740,   403, 13218,   125, 17541,   683,   125,\n",
      "          1224,   343,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   146, 12447,   397,  2364, 18424, 22278, 19020,  8556,   151,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,  2010,  2535,   117,  1996,   368,   123,  3336,   266,   117,\n",
      "           260,  4486,  1447, 22280, 13239,  1407,   221,  2354, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,  2354, 22279,  2541,  4412,   344,   124,  2779, 18450,   170,\n",
      "           146,   179,  3207,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  3876,   644,   368,  5127,   785,   123,  4768,   117,   122,\n",
      "           230,  2767,   700,  4169,   170,   230, 14121,   125,  1798,  1719,\n",
      "          3321,   117,   179, 16625,   173,  4410,  2729,   123, 20216,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,  2010,  2354, 22279,  2535,   229, 22280,   376,   325,  7258,\n",
      "          4492,   173,  2590,   123,  8092,   117,   179,   740, 16143,   420,\n",
      "          1084, 20739, 13406,  7345,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[ 101, 2535,  418, 3039,  102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,  4678,  2836,   175,   146,   179,  2354, 22279,  3283,   140,\n",
      "           122,   331,   347,   122,   325,   125,   532,  2292,   117,   176,\n",
      "           259, 18424,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,  2467,   118,   176,   146, 16195,   125,  7198,   259,  4698,\n",
      "           592,   118,  7911,   123,  1143,   185,   171,  2992,   339,  2010,\n",
      "           144,  2640,   123,  9349,   176,   179,  1445,   122,   180,  7716,\n",
      "           368,   117,   271,  7343,  7258,   117, 19764,   146,  1955,   117,\n",
      "         19764,   146,   179,   495,   347,  2010,   347,   291,   229, 22280,\n",
      "           347,   117,  2467,   118,   176,   122,  1069,   940,   598,  1364,\n",
      "           146, 12215,   117,  6540,   118,   176,  3876,   644,   230, 16317,\n",
      "          1165,   125, 10832,   171,  5457,   117,   122,   259,   682, 19938,\n",
      "           228,   118,   229,   173,  6320,   320,   739, 14611,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,  5147,   117,   123,  1815,  3743,   125,  4676,   495,  1706,\n",
      "           171,  2004, 22280,  4141, 13793,  2415, 22280,   117,   122,  2798,\n",
      "           653,   146,  9264,   117,   179,   368, 18047, 22288,   125,  1143,\n",
      "           269,   702,   118,  2036,   173,  5530,   117,   221,  2822,   123,\n",
      "          9066,   266,   636,  6322,   292,   117, 15010, 15891,   375,  2113,\n",
      "           146, 14657,   183, 17670, 22278,   230, 22107,  5822,  1941, 19946,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,   146,  7258,   125, 10420, 18398,   852,   229, 22280,  1023,\n",
      "         14019,  3482,   171,  2099,   146,   179,  2036,   380,   654,   117,\n",
      "          1141,   117,   262,   179,   123,   327, 17479,  2036,  1021,  4173,\n",
      "           286,   221,   123,   475,  2832,   700,   180,  1386,   171,  3695,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[ 101, 2010,  146, 2992,  339,  179, 1214,  252, 3344,  118, 1084, 5863,\n",
      "          117,  176,  344, 4051,  102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101,  9898, 22288,   146, 12447,   397,   125,   898,   221,   898,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,   368,   179, 11314, 22278,  4922,   122, 20840,   176,   376,\n",
      "           291,   229, 22280,  1174,   337,   138,   229, 22280, 14940,   117,\n",
      "           331,  1767, 20885, 22280,   125,  1364,   180, 22283,   123,  1510,\n",
      "         22281,  2112,   117,   625,  2036,   380,   654,   123,  1386,   171,\n",
      "          4575,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   123, 17479,  3852, 22278, 12931,   173,  2316, 20798,   123,\n",
      "          1569,   298,  2292,   171,  4807,   449,   117,   240,  2983,   117,\n",
      "          3874,  1021,   179,   746,   159,   682,  9196,   272,  1289,   125,\n",
      "          2405,   636,   179,   117,  4276,   211,  3281,   123, 20172, 22278,\n",
      "           117, 13917,   923,   125,  2745,   117,  1528,   125, 20949,   118,\n",
      "           176,   229,  7062,   125,   230,  3336,   266,   123,  1977,   229,\n",
      "         22280,  1413, 22287,   125,  1415,   481,  7583,   670,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,  1112, 11032,  1587,  5723,  1941,   117,   122,   229, 22280,\n",
      "           495,  1695,   117,   146,   179,  2036,  2365,  4816,   201,   726,\n",
      "          1971,   596, 22354, 10420, 18398,   852, 15010,  2535,   320,  1341,\n",
      "           125,  4141, 13793,  2415, 22280,   146,  1798,  5849, 17377,   125,\n",
      "         11314,  2650,   397,   117,   125,  3293,   122,   125, 11003,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,   390,  3750,   524,   256,   123,  5488, 22282,   117,   449,\n",
      "           125,  1354, 20073,   260,  1256,   180, 12495,  1011,  1941,   229,\n",
      "           395,   387,   125,   944,   259,  1564,   117,  4042,   348,   146,\n",
      "         19935,   221,   259,   958,  2118,   143,   122,   700, 19866,   146,\n",
      "         16960,   303,   221,   259,  5684,   125,   230,  1449,  1272,   179,\n",
      "          1021,   221,  1202, 22287,   125,   222,   739,   853, 18983,   162,\n",
      "           712,  8001,   180,  5304,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101,  8525,   322,   123,  1105,   117, 13159,   256,   117,  5133,\n",
      "           151,   320,  3568,   304, 22280,   229,   316,   391,   324,   117,\n",
      "           625,   146,  3695,  6952,   256,  9955,  1084,   240,  1796,  5057,\n",
      "           123,   327,   163,  8860,   285,   726,   146,   644,   202,  9556,\n",
      "           125,   736,  1312,   942,   117,   122,   123,  2954,  9882,   118,\n",
      "           176,   221,   123,  4303,   180,  5304,   117,   122,   117,   975,\n",
      "          1885,   185,   125,   222,   227,   702,   458,   125, 13954,   117,\n",
      "         10310, 11541, 10692,  1655,   122, 10310, 10009, 22278,  9344,  6436,\n",
      "           842,   117,   179,  2415, 22280,  8544,   412,  1062,   252,   117,\n",
      "           173,  8037,   138,   125,  7924,   117,   125,  3541,   942,   122,\n",
      "           834,  5899, 22281,   117,  8977,   123,  7815,   171, 11371,   102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,   122,   146,  3174, 13733,   180,  2606,   744,  7339,   596,\n",
      "           221, 11348, 22282,   122, 18486,   578,   117,  1202, 22287,   180,\n",
      "           327,   117,   123, 11451,   171,   347,  2397,   117,   179,   418,\n",
      "           117,  1201,   252,   123,  3295,   117,   229, 22280,   495, 19020,\n",
      "           122,  2364,  9882,   173,  1364,   146,   454,   125,  1089, 10115,\n",
      "           125, 14790,   138,   125,  1757, 21304,   185,   122,  1028, 17783,\n",
      "          7924, 22281,   125,  3979,  1196,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,  4141, 13793,  2415, 22280,   229, 22280,  8625, 22278,  2364,\n",
      "           123, 17611,   117,  2798,  8544,   123, 13544,   712, 18433,  2745,\n",
      "           179, 10971,   151,   123,   327,  5304,   122,   325,   123,   163,\n",
      "          8860,   285, 16246,   641,  5137, 21102,   221,   123,  8097,  7569,\n",
      "           232,   122,   180, 22283,   318, 13793,   221,   146,  6465,   102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,  1971,  1016,   179,   117,   222,   622,   700,   180,  8402,\n",
      "           232, 22280,   180,  3336,   266,   117,  6738,   173,   607,  8849,\n",
      "         10033,  1450,  4332,  1149,   125,  2480, 18349,   320,  4707,   180,\n",
      "           316,   391,   324,   117, 13860, 14273, 22288,   118,   260,  2044,\n",
      "           122, 20242,   117,   834,  5112,   125,   596,   117,   125,  4902,\n",
      "          1510, 22281,   504,  4242,   125,  4303,   122, 11471,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,   179, 12935,   207,   125, 14657,  7919,   122,   125,  3338,\n",
      "           229, 22280,  4172,   368,  4922,   893,   304, 22280, 15724,   125,\n",
      "          1449,  1339,   117,  3330, 22281,   375,   256,   122, 17754,   256,\n",
      "         13954,   117,  6642,   256,  5028,  5028,   117,   179,   146,  7492,\n",
      "           303,   117,  1796,   125,  2856,   117,  1982,   170,   123,  8932,\n",
      "           117, 16463, 11814,   123,  1449,  1272,   171,  4707,   117,   180,\n",
      "          1589,   547,   179,   695,   436,   923,   146,  3028,   366,  4103,\n",
      "           173,  1706,   179,  1021,   240,  1369,  3047,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,  2983, 16463,   382,  1085,  5159,   170,  1485,   260, 17649,\n",
      "          6204, 22281,   122,  1684, 18728, 22281,   171,  1407,  1518,   117,\n",
      "           416,  1149,   123,  7892,  2148,   351,   125,   179,  3876,   596,\n",
      "           123,   661,  1144,   229, 22280,   176, 15245,   785,   240, 11665,\n",
      "          2847, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,  4141, 13793,  2415, 22280,  5521,   256,   726,   146,   644,\n",
      "          1647,   260,  1860,   173,   179,  9086,  3028,   221,   146,   644,\n",
      "          1457,   117,   122,   123,  2954,  1084,  1011,   368,  5302,   185,\n",
      "           117,   325,   123, 10420, 18398,   852,   117,   123, 16374,   210,\n",
      "         14038,   138,   117, 19108,   117,  4117,   842,   117,   629,   942,\n",
      "           125,  1945,   117,   221,   146,  1423,   180,  4768,   117,   170,\n",
      "          3541,   252,  7349,   179,   176,   229, 22280,  8362, 22278,  1003,\n",
      "           411, 20732,   125,  7257, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101,   700,   117,   222,  5267,   256,   230,  5890,   122, 18318,\n",
      "         22278,   221,  1105,   117,  1139,   146,  1342,  9086,   125, 20482,\n",
      "         14036,   320,  1341,   171,  4745,   117, 13610,   123,  2822,  4227,\n",
      "           117,   173,  1652,   125,  9538,   122,   117,   625,   146,   179,\n",
      "           978, 19480,  1359,   256,   117, 16246,   318, 13793,   146, 10110,\n",
      "           117, 21617,   240,   327,   576,   102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101,  3874,  7707, 15832,   256,   117,  2798,   653,   260, 21733,\n",
      "           298,  1449,  8855,   117,   259,  9701,   125, 16341,   117,   146,\n",
      "          6465,   291,   123, 11383,   298,   528, 12773,  1044,   102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[  101,   122,   146,  2099,   122,   179, 11665,  1510, 22281,   504,\n",
      "          4242,   117,   316, 22280, 21613,  3514,   893,   649,   117,   506,\n",
      "           146,  2009,   125,  2816,   171,   739,   549,   713,   125,   629,\n",
      "         22280,  2415, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[  101,  1790,  1256,  4332,  1149,   125,  2480,   117, 22032,   252,\n",
      "          2139,   117,   700,   325,  1028,   117,  8544,   146, 12447,   397,\n",
      "          9822,  1364,   146,  5856,   179,   176,   860,  1399,   954,  8001,\n",
      "           180,   327,  1151,   272,   421,   122,   117,   123,  3606,   304,\n",
      "         22280,   179,   146,  2562,   256,   117,  9487,   923,   118,   176,\n",
      "           259, 15050,   122,   146,  5492, 22280,   125,  6310,   102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101,  1684,   173,  8037,   138,   125,  7924,   117,   834, 11388,\n",
      "          2798,   644, 11842,   117,   229, 22280,  6757,  2364,   123,  2880,\n",
      "           151, 22280,   125,  3791, 21102,   952, 22282,   118,   176,   171,\n",
      "           313,  9193, 22280,   117,  4513,   125,  7198,  1485,   260,  1176,\n",
      "           179,  4207,   122,  2364,  4513,   125,  3859,   117, 13441,   348,\n",
      "           259,   958,  2118,   143,   117,  6641,   348,   538, 16455,   122,\n",
      "           529,  5772,   117,  6009,   214,   240,  1027,  7911,   125,   949,\n",
      "           144,   201,   146,   179,   259,  5976, 16463, 11814,   180,  1105,\n",
      "           298,   532, 14415,   117,  7837,  1552,  1078,   576,   325,   260,\n",
      "          2004,   138, 14520,   117,  4276,  5822,   214, 11118,   303,   143,\n",
      "           498, 11118,   303,   143,   117,  6564,   122,   325,   123,  8932,\n",
      "           271,   230, 10506,   125,  1151,   145,   117,  4141, 13793,  2415,\n",
      "         22280,  3429,   870,   509,   123,  8977,   230,  3264,   670,   180,\n",
      "         11791,  1449,  1272,   117,   179,   368,   117,   944,   259,  1564,\n",
      "           117,   320,  9322,   180,  1373,   117,  3791,  4409,   222, 16423,\n",
      "         22279,   123,  4303,   180,  5304,   117,  4108, 21307,   125,  5533,\n",
      "           170,   222,   398,  4861,   243, 11552,   125,  8087,   232,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101,   429,  1084,  2139,  2217,   123, 14195,   210,  5028,   122,\n",
      "           736,  2139,   123,  1434,   210,  1084,   537,   442,   122,  4405,\n",
      "           583,  6720,  2055,   128,   117,   122,   318, 13793,   905,  4373,\n",
      "           123,  5076,   173, 15618,   293,   117,   316, 22280,   173, 15618,\n",
      "           293,   179,   117,  1839,   125,   622,   122,  1423,   117, 13860,\n",
      "          7137,   256,  1941,  1364,   146,  1632,   303,  4620,  2828,   420,\n",
      "           260,   675,   504,  4242,   122,   123,  1449,  1272,   117,  3413,\n",
      "           122,   117, 11368, 17784,  4332,  1149,   125,  4707,   498,  4698,\n",
      "           125,  2375,   173,  3204, 13540,  5567,   122,  4929,   907, 22280,\n",
      "           221,  4902,   102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101, 13776,   240,  1921,  2880,   151, 22280,  6414,   118,   176,\n",
      "         14619,   210,   222,   425,   825,   179,  9086,   123,  5065,   180,\n",
      "          5304,   117, 12544,  1014,   820,   240, 11665,  4698,  4332,  1149,\n",
      "           125,  7716,   179,  1364,   146, 13185,   303,  9657,   171,  9078,\n",
      "           247,   117,  5664,   125,  7226,  4698,   122, 14730,  2390,   117,\n",
      "         15891,   524,   256,   221,   146,  5856,   171, 12447,   397,   260,\n",
      "           675,  4167,  9751,   125, 11979,   892,   102]])\n",
      "DEBUG: Tokenized sentence 50: tensor([[  101, 10107,   118,   146,   222,  1815, 15796,   404,   117, 19317,\n",
      "           175,  1456,   143,   117,  8350,   229,  4768,   171, 11014,  9258,\n",
      "           170,   230,  7841,   125, 12466,   240, 13379,   102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[  101,  5728,   230, 15703,  1250,   202,  6759, 13793,   117,  5698,\n",
      "           118,   176,   118,  8544,   368,   221,  1084,   170,   123, 20027,\n",
      "           117,  1502,   179,   123,  2606,   117, 10502,   860,   266,   117,\n",
      "         17704, 20494,  6044,   122,   170, 19016,  1149,   125,  9825,   117,\n",
      "          1941,   229, 22280,  4207, 15515,   123, 18489,   340,   202,  1997,\n",
      "           180,   651,   117,   271, 14619,   210,   327,  9586,   117,   123,\n",
      "          1757,   209,  7902,   508,   117,  1664,   351,   785,  1877,   328,\n",
      "           122, 11736,   125,  2716, 11694, 22278,   221,   432,   172,  4765,\n",
      "           122,  5357,  1831,   102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[  101,  3413,   262,   146,   179,  1996,   146, 15796,   404,   712,\n",
      "          8229,   117,   240,   210,   123,  5907,  2318,   180,  5896,  2028,\n",
      "          1011,   229,  4096,   117,   179,   368,  3009,   151,  5657, 11259,\n",
      "           117,   125, 16083, 10502,   860,   266,   171,  9007,   298,   532,\n",
      "         11314,  2650,  1058,   102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[  101, 10502,   860,   266,   495,   230,  2606,  7248, 11067,   180,\n",
      "          4312,   304, 16995,   118,   176, 12222,  1021,  9967,   481,   122,\n",
      "           726,  1966,   596,  1270, 22278,   320,  4170,  1719,  7716,   125,\n",
      "           273,  1289,   382,   102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101,   744,  1075,   125,  8794,   146,   995,   622,   125, 15813,\n",
      "          3942,   247,   117,   146, 15796,   404, 13779, 22290,  2598,   118,\n",
      "           123,   173,  7229,   522,   175,  2607,   373,   125,  4183,   367,\n",
      "           247,  1767, 18928,  1212,   122,   146,   347,   652, 13419,   262,\n",
      "           125,  3497,   118,  1084,   221,   146,   644,   492,  1982,   170,\n",
      "           146,  4395, 17377,   449,   123,   327,  1105,  2791, 13649,   118,\n",
      "           176,   170,   146,   171,   185,   179,   740,  5626,   124,   117,\n",
      "          7226, 17784,  9342,   173,  9078,   501,   122,  1169,   143,   180,\n",
      "          2450, 22278, 10033,   117,   125,   179,   176, 19203,   146,   273,\n",
      "           522,  1196,  1971,  2249,  2036, 12122,   146,  4130,   171,   477,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101,  1202, 22287,   125,   179,   117,   222, 11103,  1637,  5782,\n",
      "         22281,   303,  1467,  1706,   221,  3240,   404,   326,   117,   122,\n",
      "           117,   995,   123,   327,  5012,   151, 22280,   117,  1569,  3240,\n",
      "           404,   326,  2350,   687,   319,  9086,   785,  3002,   123,   222,\n",
      "         19317,   175,   125,  5288,  2601,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101,   466,   852,   256,   117,  3364,   125,  2745,   117,   123,\n",
      "           327,  3602,   304, 22280,  1979,   122,  8574,   151,   331,   170,\n",
      "           123,  3138,   125,   792,   118,   176,  1993,  6754,   117,   834,\n",
      "          3353,   122,   834, 16151,   221,  5725,   934,   123,  1069,   117,\n",
      "           700,   125,   176,  5110, 19877,  3611,   243,   123, 11368, 17783,\n",
      "           494,  8397,   138,   122,  4613,   373,   123,  5260,  2042,  1076,\n",
      "           125,  1456,   143,  8797,   179,  1941,   229, 22280,   376,  9019,\n",
      "           151,   229,  8768, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[  101,  1169,  9990,   201,   975,  1885,   185,  4461, 18320,  2949,\n",
      "           128,   117,  1519,  2610,   118,   176,   170,   230,  2281,  2531,\n",
      "           304, 22280,   125, 22004,   117,   122,   259,   682,  4058,   123,\n",
      "         18165,   173, 15050, 11053,   102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[  101,   229, 22280,   170,   923,  5062,   117,   122,  3002,  4708,\n",
      "           692,   420,   898,   230,   291,  1858,  3661,   380,   436,   833,\n",
      "           328,   117,   625,  1569, 14313,   243, 19937,   259,  2259,   151,\n",
      "           123,   598,  1289,   183,   102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[101, 146, 679, 692, 118, 176, 102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101,  1078,   615, 16280,   423,  1342,   222, 12186, 10968,  2256,\n",
      "           117,   179,  1695,   123,  1695,   176,   262, 12963,   173,  7729,\n",
      "         10766,   340,  5443,   102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101,   146,  5774,   125,  1757,   209,  7186,  3429,  4305,  2858,\n",
      "           744,   325,   123,  8121,   304, 22280,   123,  6754,   854,  2028,\n",
      "           117,   173,   576,   125,  6202,   125,  4129,   712,   682,   851,\n",
      "          5325,   143,   117,   262,  1075,   222,  1160, 17993, 22282,   179,\n",
      "           176,  5931,   420,  1061,   102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101,   860,   266,  3330,   256,   118,   123,  1528,   171,   179,\n",
      "          2036, 21315,   146, 16589,   234, 17688,   240, 10262, 22280,   118,\n",
      "          1084,  2267,   171,  4170,   117,   122,   860,   123,  4343,  8849,\n",
      "           256,  2113,   978, 18164,   304, 22280,   125,   229, 22280,   333,\n",
      "           347,  1568,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[  101,   230, 11791,  2954,   117,   240,   210,   117,   146, 15796,\n",
      "           404,   117,   179,   495,  2397,   125,  5052, 14657,   183,   122,\n",
      "           438,   304,   256,   318, 13793,   954,   532,  7187,   122,  1685,\n",
      "           481,   117, 10733,   118,   176,   173, 10671,  4496, 20383,   178,\n",
      "          1177,   125,   599,  2042,  2420,   102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[  101,   495,  1373,  1941,   122,   229, 22280,  1021,   173,  1105,\n",
      "          3933,  3293,   179,  2036,  7119,  5488, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[  101,  5069,   748,   118,   176,   180,  2606,   117,   449, 16278,\n",
      "           456,  2044,   418,  3138,   170,   440,  5345,  1522, 22278,  7729,\n",
      "         10766,   340,   102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101, 14372,   123,   146,   679,   118,  1084,   102]])\n",
      "DEBUG: Tokenized sentence 67: tensor([[  101,  5147,   860,   653,  2099,   125, 18237,   304, 22280,   173,\n",
      "           179,   368,   176,  7199,   125,   229, 22280,  6202,   118,   176,\n",
      "          3914,   117,   123,  5296,  1758,   125, 10968,   852,   118,  1084,\n",
      "           117,   271,   179,   744,   325,  2036,   502,  1051,   256,   146,\n",
      "          6532,   180,  7714,   117,  2636,   180,  2772,   851,  1995,   222,\n",
      "         10388, 15318,   102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[  101,   870,   509,   117,  5664, 13310,   117,  4460,   179,  6496,\n",
      "           246,  3874, 16652, 22281,   236,   123,   327,  7729, 10766,   340,\n",
      "           412,   337,   741,   124,   117,   262,   370,   320,  3147,  3914,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101,   123,  2606,  4678,  4322,   123, 11334,   969,   183,   102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[  101, 15796,   404,  3033,   766,  3221,   766,   122, 21161,   118,\n",
      "           176,   180,  9461,   102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101,  1112, 12444,  4706,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101, 17662,   102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101,   229, 22280,  2036,  9086,  1004, 11972,   102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101, 22354,   449,   146,  5052,  5313,   524,   256,   118,  2036,\n",
      "           117, 15158,   214,   118,   123,   102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[  101,   744, 18540,  3529,   222, 16423, 22279,   117,  1031,  1281,\n",
      "           178,   117,   123,  4108,   266,   118,  1084,   202,   347,  6532,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101,   860,   266,   117,   271,   176,   146, 11552,   171,  4170,\n",
      "          2036,   305,   162,  5585, 22279,   146,  1831,   117,  8861, 22288,\n",
      "           118,   176,   498,   146,  1896,   892,   180,  4573,   117,  7729,\n",
      "          2037,   214,   170,   260,   144,  3706,   146, 16936,  3210,   221,\n",
      "           123,  2375,   122, 10063,   348,   230,   872, 22281,   421,   125,\n",
      "          1444, 15182,  9466,  1165,   285,   122,  7352,   102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[  101,   146, 15796,   404,   229, 22280,   706, 17061,   117,  7304,\n",
      "           748,   118,   176,   598,   740,   117,   179,   117,  1362,  3265,\n",
      "           498, 22281, 10557,   183,   117,   325,   125, 10013,   179,   125,\n",
      "          6838,   117, 16668, 22288,   118,   176,   117,  2862,  2044,   122,\n",
      "         18105,   170,   146,  4170,   102]])\n",
      "DEBUG: Tokenized sentence 78: tensor([[  101,   122,  2789,   118,   176,  4276,   211,   702,   954, 11989,\n",
      "         22281,   117,   125,  5708, 19977,   117, 16044,   557,   179, 14372,\n",
      "           123, 18165,   117,   834,   123,  2892,  5594,  3292,   125,  2745,\n",
      "         11972,   102]])\n",
      "DEBUG: Tokenized sentence 79: tensor([[  101,   123, 22296,   740,  7719,   271,  4863,   179,   146, 15211,\n",
      "         22280,   117,  1065,   179,   229, 22280,  1023, 16151,   125, 14661,\n",
      "           118,   176,   125,  1105,   117,  1021,   117,   325,  8545,   291,\n",
      "           325,  1373,   117,   125,  2863,   118,  1084,   125,  1160,   102]])\n",
      "DEBUG: Tokenized sentence 80: tensor([[  101, 18574,   118,  2036,   146,  2829,   310,   117,  2124,   221,\n",
      "          8781, 22282,   122, 13141,   221, 17061,   320,  6532,   102]])\n",
      "DEBUG: Tokenized sentence 81: tensor([[  101,  5149,   201,   146,  2607,   373,   117,   146,  6320,   243,\n",
      "         19317,   175, 10733,   118,   176,   374, 22290,  4095,   243,   125,\n",
      "           792, 12268,   122,  3456,  2040,  1637,   102]])\n",
      "DEBUG: Tokenized sentence 82: tensor([[  101,   229, 22280,  1023,  6015, 22280,   125,  2822,  3661,   117,\n",
      "           122,  9883,   118,   176,  1480,  7485,   181,   268,   122,   362,\n",
      "         22282,  2014,   221,   146,   347,  3147,   125,   273, 10214,   243,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 83: tensor([[  101,   146, 22296,   271,  2036,   171,   151,  2535,   146,   179,\n",
      "          4364,   256,   125, 15707,   229,  2992,  8172,   180,   327, 20957,\n",
      "           292,   102]])\n",
      "DEBUG: Tokenized sentence 84: tensor([[ 101, 2010,  179, 3049, 2382,  102]])\n",
      "DEBUG: Tokenized sentence 85: tensor([[  101, 10355,   368,   762,  2640,   102]])\n",
      "DEBUG: Tokenized sentence 86: tensor([[ 101,  179,  928,  328,  415, 3049, 2382,  102]])\n",
      "DEBUG: Tokenized sentence 87: tensor([[  101,   202,   644,  1457,   117,   259,   682, 12061,   118,   176,\n",
      "           122,  4365,   228,   118,   176,   173, 22243, 22280,   117,   271,\n",
      "           176,  3874,   125, 10376,  2623,   247, 20075, 22278,   420,  1061,\n",
      "         18198,   229,  5693,   574,   102]])\n",
      "DEBUG: Tokenized sentence 88: tensor([[  101,   434, 22282,   118,   176,   118,  8544,  2684,   179,   117,\n",
      "           700,  7970,  1366,   340,   117,   146, 15796,   404, 16280, 11251,\n",
      "           146,   347,   146,   635,   598,   123,  2772,   102]])\n",
      "DEBUG: Tokenized sentence 89: tensor([[  101,   122,   117,   123,  2954,  2811,   653,   644,   117,   625,\n",
      "           176, 12524, 10580,   229,   327,  9461, 12555,   117,  3436,   203,\n",
      "           592,  1176,   712,   532,   235,  1408,  2364,   325,   117,  2364,\n",
      "           325,   117, 15707,  4327,  7406,  7870,   102]])\n",
      "DEBUG: Tokenized sentence 90: tensor([[  101,   449,   117,   180, 22283,   123,   222,   454,   117,   146,\n",
      "          6754,  2397,   117,  1169, 18377,   286,   125,   222,  1160,  2831,\n",
      "           125, 19366,   322,   117,  2927,   320,  3147,   180,  2606,   102]])\n",
      "DEBUG: Tokenized sentence 91: tensor([[  101,   860,   266,  1678,   118,   146,  1014,   576,   271,   180,\n",
      "           681,   117, 16044,   557,   179,   229, 22280, 19994,   256,   229,\n",
      "          2880,   151, 22280,   117,   240,   210,   117,   173,   179,   368,\n",
      "           176,  1178,   545,  2836,  3914,  1154,  2042,  2507,   117,   123,\n",
      "          1301,  1721,   117,   834,   176,   926,  8984,   117,   969,   654,\n",
      "           118,  2036,   173, 13140,   598,   146,  9169,   230,  6742,  1187,\n",
      "          4419,   179,   123,  5155,   331,   269,  2836,   102]])\n",
      "DEBUG: Tokenized sentence 92: tensor([[  101,   146,  6754,   118,   644,   492, 15310,   519,   203,   117,\n",
      "          2643,   138,  3240,   404,  3093,   852,   243,   117,   331,  3307,\n",
      "          3897,   214,   118,   176,   117,  5782, 22281,   303,   117,  1362,\n",
      "          1048,  4549,  8879,   125,  2448, 20342,   326, 19994,   243,   170,\n",
      "          6430,   340,   102]])\n",
      "DEBUG: Tokenized sentence 93: tensor([[  101,   123,  2606, 13053,   123,  8121,   304, 22280,   122,   229,\n",
      "         22280,  2036,  2002,   596,   221,  7912,  1367,   118,  2036,  6372,\n",
      "           286,   260, 11351,   240,  5530,   122,   117,   768, 12717,   243,\n",
      "           118,   176,   118,  2036,   320,  1831,   117,  2992,  2904,   118,\n",
      "           146,   170,   230, 18595,  4419,   125, 10786,  3103,   102]])\n",
      "DEBUG: Tokenized sentence 94: tensor([[  101,   229, 22280,   176,  5961,   228,   102]])\n",
      "DEBUG: Tokenized sentence 95: tensor([[  101, 15796,   404,  2364,   123, 18424, 22278,   117,  2798,  2364,\n",
      "           123,  5401,   117,  1016,   316, 22280, 16353,   202, 15537,   102]])\n",
      "DEBUG: Tokenized sentence 96: tensor([[  101, 14848, 22288,   118,   123,   102]])\n",
      "DEBUG: Tokenized sentence 97: tensor([[  101,   870,  1185,   748,   118,   176,   118,  2036,  2765,   538,\n",
      "          4332,   942,   125,   230, 11003, 20409,  6920,  9918,   146, 12109,\n",
      "          1212, 16473, 22280,   170,   179,   538,   173,   483,   483,  9398,\n",
      "           260, 10089,   138, 13501,  7485,  2208,   229, 13747,  3292,   171,\n",
      "          3746,  2256,  1214,   499, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 98: tensor([[  101,  6920,   118,  2036,   202,   765,   397,   180,  6614,   122,\n",
      "           202,   765,   397,   298, 13841,  3980,  7362,   179,  2364,  2036,\n",
      "         10500, 22278, 15415,   118,  2036,  1342,   607,  3093,   183,   117,\n",
      "          1342,  4081,   538,  2510, 13513, 22281,   122,   538,  4217,  1058,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 99: tensor([[  101,   122,  3746,  2256, 22288,   118,   123,   117,  3746,  2256,\n",
      "         22288,   118,   123,  7406,  2227,   403,   117,   170,  2607,  2032,\n",
      "         22280,   117,   170,  5907, 12458,   304, 22280,   125,  6032,   202,\n",
      "         13747, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 100: tensor([[  101,   122,   740, 14619,   210,   117,   740, 14619,   210,  3746,\n",
      "          2256, 22288,   117, 16405,   251,   240,  7583,  7892,  2148,   351,\n",
      "         12307,  7909,   171,  5381,   265,  1637,   179,   259,   273, 19206,\n",
      "         22278,  3746,  2256, 22288,   123,   273,  4315,  3679,   272,  4566,\n",
      "          5291,   179,   123,  2592,  1334,  7134, 15736,   712,  5708,   222,\n",
      "           171,  1342,  9466, 22282,   685,   118,   176,  1719,   117,   646,\n",
      "         20010,   214,   259, 12141,   117,   768, 22285, 13732,   243,   117,\n",
      "         15702,  4566,   347,  6631,   146,   679,   243,   117,  7093,   214,\n",
      "           118,   146, 14619,   210,  2535,   117,   271,  2397,   117,  1407,\n",
      "           179,  2364,   117,  9697,   451,   348,   118,   146,   538,   532,\n",
      "          4332,   942,  1444, 22281,   117,  3285,  1825,   118,  2036,   412,\n",
      "          9463,   123,  3182, 22278,   222,   328,   122,   173,  1010, 22278,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 101: tensor([[  101,   700,   117,   222, 15700, 22280,   125,  1831,  7821,   117,\n",
      "           170,   222,  7078,   303,   866,  1441,   122,  5404,  2794,   117,\n",
      "          5512, 17675,   185,   122,  5572,  4617, 22278,   117,  8173,   821,\n",
      "         22288,   118,   176,  1362, 15051,   125, 11351,   122,  4332,   942,\n",
      "         13059,   117,   123,  3049,   304,   221,   146,  1341,   117,   259,\n",
      "          5708,  1623,   457,  5895, 22281,   122,  5334,  6031, 22281,   117,\n",
      "          1719,   740,   762,  8723, 10922,   185,   117,   271,   176,   123,\n",
      "         12012,  4793, 16242,   201,   229,  9461,   102]])\n",
      "DEBUG: Tokenized sentence 102: tensor([[  101,   123,  1018,  2990,  2954,   117,   180,   615,   331,   412,\n",
      "          1062,   252,   146, 15796,   404,   176,  9883,   171,  3147,   180,\n",
      "          2606,   117,  5931,   118,   176,   420,  1061,   146, 19877, 22280,\n",
      "           125,   230, 15685,  5185,   117,   316, 22280,  5443,   271,   744,\n",
      "           229, 22280,   123,  2365, 16102,   487,   117,  4460,   179,   202,\n",
      "         14353, 22280,   125,  1078,   222,  4588, 10899, 22281,   236,   598,\n",
      "           146,  1342,   123,  1589,  7729, 10766,   340,  6496,   173,  3874,\n",
      "         16009,  2647,   102]])\n",
      "DEBUG: Tokenized sentence 103: tensor([[  101,   726,  1027,   481, 19581,   785,  1004,  7444, 22281,  2535,\n",
      "           117,   240,   210,   117,  1971,   596,   700,   180,   681,   851,\n",
      "         14266,  1711,  1187,   117,   122,  2535,   179,   146, 19317,   175,\n",
      "          1941,   229, 22280,   495,  1169, 18377,   286,   316, 22280,  3745,\n",
      "           240, 11665, 18695,   179,   146,  7096,   524,   692,  1796,   125,\n",
      "          2856,   320,  4678,  8476,  2758, 22280,   125, 10502,   860,   266,\n",
      "          2535,   117,   122,   145,   179,   123,  1301,  1721,  9821,  4213,\n",
      "           154,   123, 14887,   218, 13133,   229, 12526,   117,  3951, 15445,\n",
      "           712, 11314,  2650,  1058,   171,  4170,   117,   229,  2880,   151,\n",
      "         22280,   173,   179,  2983,   695,   923,   221, 16960,   934,   291,\n",
      "         14890,   102]])\n",
      "DEBUG: Tokenized sentence 104: tensor([[  101,   262,   240,  1257,   179,   146, 15796,   404, 10107,   146,\n",
      "          9078,   247, 12034,   123,  4141, 13793,  2415, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 105: tensor([[  101,   123,  1105,   495,  3264,   347,   877,   319,  2455,   373,\n",
      "          1011,   229, 17138,   171,  6151, 22290,   449,   221,  1257,  1021,\n",
      "         11935,   635,   170,   785,  1695,  6009,   692,   118,   176, 11368,\n",
      "          1027,  4332,  1149,  4566,  5856,   171,  4707,   179,  8544,  2684,\n",
      "           123,  1449,  1272,   117,   122,   325,  7226,  1027,   291,  8184,\n",
      "          1877,   793,   171,  1341,   173,   179,  9086,   123,  5304,   102]])\n",
      "DEBUG: Tokenized sentence 106: tensor([[  101, 15796,   404,   262,  2044,  9050,   118,   176,   170,   146,\n",
      "          2415, 22280,   122, 19284,   118,  2036,  3907,   523,   102]])\n",
      "DEBUG: Tokenized sentence 107: tensor([[ 101,  146,  316,  391,  514,  397, 6719, 9911,  102]])\n",
      "DEBUG: Tokenized sentence 108: tensor([[  101, 15796,   404,   601,  5432,   102]])\n",
      "DEBUG: Tokenized sentence 109: tensor([[  101,  2010,   146,  7258,  2243,   347,   596,   122,   347,  7020,\n",
      "         19652,   381,  1353,   146,  3695,   125, 10420, 18398,   852,   102]])\n",
      "DEBUG: Tokenized sentence 110: tensor([[  101,  2798,   331,   229, 22280,  8545,   230, 12804,  3281,   171,\n",
      "          7343,  5856,   117,   271,   744,  2036,  9176,   117,   176,   390,\n",
      "          8204,   140,  8868,   117,  6086,  7094,   303,   179,  2036,  1968,\n",
      "           320,  4707,   180,  1105,  2010,   146,  6151, 22290,   136,  2010,\n",
      "           122, 20811,   102]])\n",
      "DEBUG: Tokenized sentence 111: tensor([[  101,  2010,  1502,  2354, 22279,  3189,   179,  2779, 10692,   455,\n",
      "           834,  1690,  5105,   117,   834,  9186,   117,   834,  3874,   136,\n",
      "          2010,   221,  9726,   495,   125,  7246,   102]])\n",
      "DEBUG: Tokenized sentence 112: tensor([[  101,  2010, 11032,   117,  1114, 22279,   118,   176,  1659,   117,\n",
      "          2397,   117,   122,  2826, 22278,  1084,  2249,  3189,   423,   179,\n",
      "          2036,  2671,   249,   102]])\n",
      "DEBUG: Tokenized sentence 113: tensor([[ 101, 2010, 1941, 1996,  146,  179,  978,  123, 4640,  102]])\n",
      "DEBUG: Tokenized sentence 114: tensor([[  101,  2010,  9262, 22278,   118,   311,   318, 13793,   320,  1528,\n",
      "           260,  1027,  4332,  1149,   171,  4707,   102]])\n",
      "DEBUG: Tokenized sentence 115: tensor([[  101,  2010,  2798,  1423,  1877,   283,  2010,  1257,   122,  3002,\n",
      "          1076,   125,   327,   670,   117,  4178,   136,  2779,   117,   176,\n",
      "           395,   303,  3846, 20045, 22280,   117,   122,   412,  7122,  3113,\n",
      "           117,   179,  3804,   117,   144,  3249,   117,   125,   222,  1695,\n",
      "           125,  1632,   303,   221,  9456,   702,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 116: tensor([[  101,  2010,   122,  2779,   229, 22280,  8545,   117,  2113,  8911,\n",
      "           171,  7343,  5856,  2010, 11032,   615,   179,   644,   492,   706,\n",
      "          1084,  2354, 22279,  1434,  1369,   136,   230,   240, 15035,   125,\n",
      "           222,  7094,   303,   125,  5856,  1821,   768,  6859,   320, 21173,\n",
      "           122,   712,  8001,   125,  7122,  1105,   625,  2354, 22279,   117,\n",
      "          1369,   138,   117,  4267, 22280, 22279,   125,  1971,  1632,   303,\n",
      "           744,  2010,  2678, 22283,   125,  2036,  6515,   176, 15212,   291,\n",
      "           229, 22280,   146,   179,  1434,  1369,  2010,   122,   179,  2354,\n",
      "         22279,   122,   437,  3556, 22280,  2389,   296,   117,   176,   311,\n",
      "          9262,  1249,   260,  1027,  4332,  1149,   171,  4707,   117,   123,\n",
      "           327,   670, 15109, 20299,   173,  1999,  8145,  2684,   123,  1449,\n",
      "          1272,   117,   122, 13083,   375,   256,  2779,   125,  4412,   170,\n",
      "           230,  3508,   125,  5856,   313,  9193, 22280,   123,  3285,   140,\n",
      "           118,   176,   423,  7343,   102]])\n",
      "DEBUG: Tokenized sentence 117: tensor([[  101,  3189,  4945,   136,   229, 22280,  1052,   791,   146,  6151,\n",
      "         22290,   834,  2354, 22279, 11650,   118,   176,  2010,   318, 13793,\n",
      "          4412, 22278,   170,   146,  6151, 22290,   221,  1684,   834, 16909,\n",
      "           117,  2113,   146,   179,   978,   123,  4640,  1941,  1996,  2010,\n",
      "           449,   117,  2397,   125,  4023,   117,   179,   644,   492,  4174,\n",
      "         22279,   222,  1695,  2354, 22279,  1369,   229, 22280,   706,  4902,\n",
      "          3874,   291,  9767, 22278,   179,  2036,  4314,   244,  8925,  9751,\n",
      "           498,   146,  7343,  6151, 22290,   102]])\n",
      "DEBUG: Tokenized sentence 118: tensor([[  101,  2010,   229, 22280,  8911,  8925,  9751,   498,   146,  6151,\n",
      "         22290,   125, 16241,  1537, 22287,  2010,  2798, 22033,   203,   303,\n",
      "          2036,  4314,   244, 14748,  8130,   117,   316,  7039,   243,   118,\n",
      "           311,   260,  9751,   180,  4573,  2010,   229, 22280,  8911, 14748,\n",
      "          8130,  2811,  1341,   102]])\n",
      "DEBUG: Tokenized sentence 119: tensor([[  101,  2010,   318, 13793,   179,   644,   492,  2541,  2354, 22279,\n",
      "          1434,   125,  1364,   860,  5856,   136,   102]])\n",
      "DEBUG: Tokenized sentence 120: tensor([[  101,  2010,   123, 22296,  1257,  2535,   122,   329,   170,  1039,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 121: tensor([[  101,   146,   179,   344,   331,   186,  2010,  1502,  3960,   151,\n",
      "           179,   176,  3456,  2040, 22279,   125,   229, 22280,   311, 21761,\n",
      "           146,  5856,   102]])\n",
      "DEBUG: Tokenized sentence 122: tensor([[  101,  2010,   176,   311,  3456,  2040,   140,   117,  4500,  3292,\n",
      "           331,  2036,  2826, 22280,   122,   179,   785,  3002,   176,  5197,\n",
      "         22278,  1977,  8204,   140,  3285,   140,   118,   176,   329,   170,\n",
      "           123,  7122,  1069,  2010,  7282,  1004,  2010,  3605,   249, 12682,\n",
      "           203,   118,   176,   318, 13793,   230,  4840, 22278,  5302,  4095,\n",
      "           285,   122,  1401,   285,   420,   146,  1456,   143, 19317,   175,\n",
      "           125, 12466,   240, 13379,   122,   146,  1456,   143, 19317,   175,\n",
      "           125, 20237,   122,  3848,  6913,   102]])\n",
      "DEBUG: Tokenized sentence 123: tensor([[  101,  6086,   229, 22280,   176,  2709,   619,   123,  1434,   146,\n",
      "         16909,   171,  6151, 22290,   117,   834,   370, 20482,  2028,   243,\n",
      "           146,  7094,   303,   125,  5856,   179,   146,  2531,   256,   171,\n",
      "         21173,   122,   146,  1342,   117,   240,   347,  1341,   117,   229,\n",
      "         22280,  4363,   151,   123,  2521,  2028,   125,   305,  1051, 22282,\n",
      "           118,  2036,   744,   117,   423,  1528,   117,   924,   291,  1510,\n",
      "         22281,  4332,  1149,   712,  8001,   180,  1105,   670,   418,   179,\n",
      "           117,  4762,   259,   532, 10315,   128,   117,  5488,   322,  2987,\n",
      "           117,   230,   576,  3064,   146,   739,  1778,   179,   169,  8388,\n",
      "           246,   146, 16555, 18862,  2010,   123,   854,   304, 22280,   125,\n",
      "           230,   418, 15976,   173,  2009,  5223,   117,   230,   418, 15976,\n",
      "         15218,   117,   834,  1416,   117, 11826,   123,  6074,  1719,  7583,\n",
      "         14689,  8497,  3613,   125,   549,  1107,   179, 19310,   436,   692,\n",
      "           240, 11967, 10953,   763,   102]])\n",
      "DEBUG: Tokenized sentence 124: tensor([[ 101,  495,  860,  146,  347, 7503,  102]])\n",
      "DEBUG: Tokenized sentence 125: tensor([[  101,  1021,   785,   179,  4141, 13793,  2415, 22280,  9584,  6856,\n",
      "           221,  1921,  3138,  2448, 15736,   170,   740,  1485,   260, 13674,\n",
      "         10850,   351,   123,   944,   259,  2241,   326,   143,   125,  4918,\n",
      "           125,   893,   304, 22280, 13860,  7137,   256,  4561,   579,  1941,\n",
      "          8227, 22281,  6009,   256,  4117,   252,   173,  1448,   223, 22280,\n",
      "          5057,  6514, 13732,  5949,   125,  1945,   122, 19108,   146,   179,\n",
      "           495,  2745, 12174,  2640,   202,   347, 17733,  1690, 22280, 16026,\n",
      "           117,  3596,  7316,  5267,   256,   173,  5255,   146,  1354,   367,\n",
      "         14848,   125,   230,  5223,  1923,  8253,   285,   117,  1815,   495,\n",
      "           123,  5402,   298,  4674,   179,  1369,   176,   305,   508,   692,\n",
      "          6039, 10887, 14038,   138,   122,  9344,   124,  6137,   117, 14793,\n",
      "         22281,   125,   388,  4056,   117,   449,   534,   125,  4449,   117,\n",
      "         11314, 15546,   117,  8197,   125,  3883,  1149,   117,   849,  9580,\n",
      "           125, 13954,   122,   125,  3050,   117,  4848,   143, 12156,   175,\n",
      "         15069,   117, 18720, 22281,   122, 18720, 22281,   125, 19108,   125,\n",
      "           944,   259, 18190,   128,   117,  1923, 21818,   125, 13747,   310,\n",
      "           117, 20578,   125, 11912,   122,  2480,  9850,   117, 11621, 10780,\n",
      "         22280,   143,   125,  4117,   842, 20991,   117, 21733,  4814,   117,\n",
      "         12174,   721,   125,  1945,   117,   146,   644,   492, 17878,   320,\n",
      "           179,   368,   117,   179,  9679, 15363,   271,  3867,  4486,   176,\n",
      "         16463, 11814,   117,   398, 10750,   256,   117,   969,  1552,   123,\n",
      "          2954,   222,   928,   328,   415,   329, 22280,   125,   450, 22278,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 126: tensor([[  101,   860,   329, 22280,   495,  4252,  2122,   125, 19731, 22281,\n",
      "           398,  7173, 22281,   170,   123,  9349,   171, 15796,   404,   117,\n",
      "           123,  3596,  6151, 22290, 16241,  1537, 22287,   125,  1105,  4207,\n",
      "          7433, 22282,   117,   700,   366,  1027,  2856,   180,  2954,   117,\n",
      "           834, 13239,   146,  5121,   125,   333, 14408,   487,   412,  1718,\n",
      "         22278,   102]])\n",
      "DEBUG: Tokenized sentence 127: tensor([[  101,  2010,   122,  1434,   146, 16909, 10355,   146,  4141, 13793,\n",
      "          2415, 22280,   117,   629,   830,  6436,   243,   259, 20462, 22281,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 128: tensor([[  101,  2010,   229, 22280,   395,   303,  3898,  1436,   256,   146,\n",
      "          1342,   102]])\n",
      "DEBUG: Tokenized sentence 129: tensor([[  101,   176,   368,   122,  5790,   154, 22280,   125,   853,  6522,\n",
      "         22280,  2779, 14619,   210, 15212,   853,  6522, 22280,   173, 10894,\n",
      "          3391, 13793,   117,   229, 22280, 11314, 22278,   202,  6151, 22290,\n",
      "           171, 15796,   404,  3922,   508,   291,  6251,   339,   117,  4173,\n",
      "           474,   171, 17477,   171, 12447,   397,   117,   179,   229, 22280,\n",
      "          1904, 22281,   236, 11007, 20158,   319,   102]])\n",
      "DEBUG: Tokenized sentence 130: tensor([[  101,  4141, 13793,  2415, 22280,  3947,  5723,   598,   146, 18144,\n",
      "           173,  3401, 19641,   117,  7845,   214,  7903, 20798, 22281,  1749,\n",
      "           909,   117, 12402,   173,  2822, 12785,   102]])\n",
      "DEBUG: Tokenized sentence 131: tensor([[  101,  2010,  1502,   122,  1434,   222, 16909,   202,  3922,  2825,\n",
      "          3898,   618,  2836,   146,  4170,   125,   860,   266,   102]])\n",
      "DEBUG: Tokenized sentence 132: tensor([[  101,   180, 22283,   123,  1089,  2112,   117,  4141, 13793,  2415,\n",
      "         22280,   117,   700,   125,  5172,   222,  1270,  8008,   397,  3803,\n",
      "           303,   221,  4914,  1450,  4332,  1149,   171,  6151, 22290,   171,\n",
      "         12034,   117,  9565,   905,  2048,   260,  1860,   180,   418, 15976,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 133: tensor([[  101,  2010,  5308,  2765,   117, 10415,   256,   368,   229,  9461,\n",
      "           170,   123, 10420, 18398,   852,  5308,  2765,   179,   744,  2036,\n",
      "          2678, 22283,   125,  4270,   954,  8001,   180,  1105,   117,   176,\n",
      "           122,   179,   229, 22280,  2036,   420,   412,  2375,   325,  8545,\n",
      "           291,   325,  1373,   271,   118,  2036,   117,   229, 22280,   924,\n",
      "          4332,  1149,   117,   449,  2139,   117,  3003,   117,  1364,   146,\n",
      "          6151, 22290,   122,  2684,   146,  2004, 22280,   425,   825,  5787,\n",
      "           122, 10355,  3413,   170,   230, 18164,   304, 22280,   125,  1977,\n",
      "          2745,   706,   122,  2745,  2521,   180,   327,  4588,  8990,   371,\n",
      "          2028,   117,   171,   347,  3803,   303, 11233,  2152,   587,   154,\n",
      "           415,   122,   180, 22109,   292,   258,  8066,  6044,   171,   347,\n",
      "          3495,   117,  3495,   179,   331,  2036,  8625, 22278,   366,   877,\n",
      "           842,   221,  4706, 19386,   201,   102]])\n",
      "DEBUG: Tokenized sentence 134: tensor([[  101,  1065,   179,   123, 14594,   125,  8771,   176,  1178,   545,\n",
      "           203,  2461,  4171,   117,   944,   259,   532,  7823,   117,   944,\n",
      "           117,  2589,   146,   325,  2281,   117, 22158, 22287,   222,  3316,\n",
      "         11977,  2264,   342,   102]])\n",
      "DEBUG: Tokenized sentence 135: tensor([[  101,   331,   978,   230,  5811,   304, 22280,  5683,   259,  6109,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 136: tensor([[  101,   366,   675,  3428,   470, 12401,   151,   221,   898,   122,\n",
      "           221,   123, 20216,   259, 18967,  2653,  7362,   117,  5022,   179,\n",
      "           117,   240, 14697,   117, 16241,  1537, 22287,  8977,   151,   260,\n",
      "           675,  3922,  4242,  1271,   923,   785,   122,   368,   229, 22280,\n",
      "           170,   151,   222, 18181,   117,   171,   179,   202,  1325, 14971,\n",
      "         21024,  5133,   151,   118,   259,   944,   122,  1519, 21206,   118,\n",
      "           176,   170,   259,  8197,   180,  9652,   298,  5684,   102]])\n",
      "DEBUG: Tokenized sentence 137: tensor([[  101, 11972,  1941,   229, 22280,   495, 16122, 13793,   117,   495,\n",
      "           230,  3848,   687,   151,  8089,  1337,   117,   230,  7406,  7870,\n",
      "           117,   222, 19781, 11097,   125,  6039,   711,   125,  8333,  2745,\n",
      "           123,  9245,   102]])\n",
      "DEBUG: Tokenized sentence 138: tensor([[  101,   122,   347,  1903,  3378,   185,   117,   331,  1196,   117,\n",
      "           125, 13841,   123,  3235,  3711,   252,   117,   123, 17897,  1684,\n",
      "           240,  1434,   117,  8544,   122,  8940,   180,  1449,  1272,   221,\n",
      "           123,  5304,   117,   180,  5304,   260,  3428,   470,   122,   320,\n",
      "           853, 18983,   162,   117,  1684,   173,  8037,   138,   125,  7924,\n",
      "           117,   125,  3541,   942,   117,   834,  5899, 22281,   117,  2389,\n",
      "          7831,   221,   944,   259,  6615,   117,   170,   146,   347,  2337,\n",
      "           140,   300,   388,   125,  8087,   232,   117,  1178,   545,   348,\n",
      "           118,   176,   117,   170,   259,  5708,   117,   125,  2745, 11972,\n",
      "           125,   179,   368,   229, 22280,  4207,  1178,   545,   159,   118,\n",
      "           176,  2044,   170,   260,   877,   842,   102]])\n",
      "DEBUG: Tokenized sentence 139: tensor([[ 101, 5147,  117,  123, 4768, 1084, 1796, 2049, 2836,  118,  176,  125,\n",
      "          222, 2277, 8296,  415,  102]])\n",
      "DEBUG: Tokenized sentence 140: tensor([[  101,   893,   151,   118,   176,  3002,   117,   240,   210,   785,\n",
      "          2366,   923,  1690,   507,   122,   504,  4242,   180,  2954,   221,\n",
      "           146,   644,   695,   923,   259,  5231,  1537,   145,   260,  5077,\n",
      "         15277,   692,   125,  2261,   102]])\n",
      "DEBUG: Tokenized sentence 141: tensor([[  101, 17340, 22278,   118,   176,   230,  7875,   125, 11282, 18836,\n",
      "           122,  1858,   125, 20262, 22281,   117,   122,   259,  5684, 19407,\n",
      "           125,  1062,   252,   122,   260, 15252,   118, 22232,   138,   117,\n",
      "           122,   123,   636,   670,  2866,  8544,  1847,   123,  1105,   125,\n",
      "         14016,   179,  4141, 13793,  2415, 22280,  5646, 17376,   712,  8001,\n",
      "           180,   327, 17935,   404,   102]])\n",
      "DEBUG: Tokenized sentence 142: tensor([[  101, 19914,   118,   176,  2534,   316,   391,  1343,  3963,   117,\n",
      "           240,   210,   117, 17002,   333,   316, 22280,   870,  2127,  3897,\n",
      "         20733,   271,   123,  2461,   102]])\n",
      "DEBUG: Tokenized sentence 143: tensor([[  101,  2364,   146,   347,  3907,   523,  1796,   316, 22280,  1004,\n",
      "           117,  2364,   146,  2135,  2758, 22280,  8868, 22278,  1971,  5133,\n",
      "           151,   325,  2535,   117,   785,   325,   117,   179,   538,   481,\n",
      "          3860,   102]])\n",
      "DEBUG: Tokenized sentence 144: tensor([[  101,  1023,  2684,   125, 18689,   307, 11314,  2650,  1058,   102]])\n",
      "DEBUG: Tokenized sentence 145: tensor([[  101,   260, 12817,   229, 22280,  2036,   221,   692,   529,  1174,\n",
      "          1832,  1908,   146,  3568,   304, 22280,  1011,  1078,   576,   325,\n",
      "         16814,   534, 22280,   117,   325, 20639,   102]])\n",
      "DEBUG: Tokenized sentence 146: tensor([[  101,   122,   146,  3495,   123, 13718,   702,   117,  4698, 22287,\n",
      "           240,  4698, 22287,   117,  1839,   180,  1956, 21305, 22278,   117,\n",
      "           122,   123,  3235, 22282,  6070,   180,  1956, 21305, 22278,   221,\n",
      "           123,  6688,   117,   712, 11330,   122,   712,  5830,   592,   118,\n",
      "          7911,   117,   122,   180,  9066,   124,   221,   146,  6465,   117,\n",
      "           712,  9342,   122,   712,  9342,   102]])\n",
      "DEBUG: Tokenized sentence 147: tensor([[  101,   870,   509,   117,  1941,  2036,   229, 22280,  1587,  5723,\n",
      "         13449,   779,   146,   347,  7616,   538,  9098, 22281,  6996,  2247,\n",
      "           662,  1353,   123,  3859,  1089, 16453,   128,  4188,   180,  8768,\n",
      "         22278,   146, 10832,   117,   240,  1416,   117,   179,   368,  4654,\n",
      "           555,  6009,   256,   712,  5841, 22281,   529,  4103,   125, 13379,\n",
      "           117,  8940,   118,  2036,  2535,   125, 18615,  1187,   260, 13779,\n",
      "          1708,   117,   122,   125,  1078,   230,  5057,  1510, 22281,   170,\n",
      "          6205, 22278,   122, 13850,  3391, 22278,   122, 19537,  2034,   256,\n",
      "         19971, 22281,   125,  1923,  1046,   125,  3001,   945,   117,   125,\n",
      "         17730,   125, 14223,   117, 11314,  1604,   143,   125,  2360, 13331,\n",
      "         22281,   117,  3673, 20480,   117, 20427, 22281,   117,  7406,   304,\n",
      "           122,  1615,  1028, 12817,   102]])\n",
      "DEBUG: Tokenized sentence 148: tensor([[  101,  3336,  9098, 22281,   221, 12174,   373,   117,  8806,   456,\n",
      "           123,   163,  8860,   285,   122, 10564,   146,  4678,  8476,  2758,\n",
      "         22280,   117, 17887,   146,  1632,   303,   221, 17086,   123,  5304,\n",
      "           117,   179, 21244, 22288,   125,  3846,   122,  2473,   325,   924,\n",
      "          6929,   102]])\n",
      "DEBUG: Tokenized sentence 149: tensor([[  101,  1941,   229, 22280,   495,   230,  2281,   316,   391,   324,\n",
      "           117,   495,   222,   475,  4803,   173,   179,   176,  7339,   125,\n",
      "          2745,   117,  4674,   125,  4857,  6812,   117,  5643,  1495,   117,\n",
      "           240,  1927,  3503,   117,   169,  2050,  9760,   128,   125,  4223,\n",
      "           247,   117, 11451,   125,  3979,  1196,   221,   259,  5684,   117,\n",
      "          7698,   221, 11451,   125,  2606,   117,  1690,  7817, 22281,   125,\n",
      "         19278,  2004,   128,   221,   146,  1312,   303,   320,   969,   117,\n",
      "          3980,   809,  1313, 11037,   470,   117,  2377,   555,   125,  1224,\n",
      "           357,   130,   117, 16936,   942,   170, 11454,   125,  3165,   117,\n",
      "           122, 15631,   145,   122,  5911,   942,   125,  4437,  4189,  2623,\n",
      "           247,   102]])\n",
      "DEBUG: Tokenized sentence 150: tensor([[  101,   122,  1719,   123, 14350,  3613,  7970, 22281, 13722, 11157,\n",
      "           138,  8544,  9322,  1084,   117,   291,   318, 13793,  1369,   320,\n",
      "          1341,   117,   229,  1105,   125, 14016,   117,   582,   259, 11309,\n",
      "           501,   366,  7875, 22281,   122,   259,  5684,   180,  1449,  1272,\n",
      "           176,  2259,   923,   700,   171,  1312,   303,   117,   122, 19001,\n",
      "          5167,  1825,   122, 10415,   214,  2684,   260,  1027,  2856,   180,\n",
      "          2954,   117,   420,   146, 15021, 22280,   572,   283,   298, 13850,\n",
      "         22178, 22281,   117,   171, 11371, 10310,   373,   173,  3673, 20480,\n",
      "           122,   298, 19068, 14881,   143,   125, 18691,  2615, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 151: tensor([[  101,   495,  4141, 13793,  2415, 22280,  1977,  7707,  4049,   351,\n",
      "          2745,   117,  2745,   117,  2684,  3495,  8684,  4409,   117,   625,\n",
      "          3179, 11736,   102]])\n",
      "DEBUG: Tokenized sentence 152: tensor([[  101,   240,  1369,   229, 22280,   176,  7339,  1955,   458,   117,\n",
      "          3596, 11263,   229, 22280,  2589,   627,  2032, 21102,  9079,   260,\n",
      "           223,   128,   171,  7492,   303,   102]])\n",
      "DEBUG: Tokenized sentence 153: tensor([[  101,   122,   498,   860,  9312,   117,  1821,  1684, 13122,   712,\n",
      "           374,  7485, 22280,   143,   117, 15210,   256, 19075,   125,  3003,\n",
      "           240,  9143,   320,   454,   117,   222,  1695,   325,   171,   179,\n",
      "         16403,   712,   179, 13649, 22287,   123,  2450, 22278,   170,  2377,\n",
      "         12166,   125,  2987,   291,  5731,   102]])\n",
      "DEBUG: Tokenized sentence 154: tensor([[  101,   229, 22280, 14940,   117,   260,   504,  4242,   171,   549,\n",
      "           713,   117,   123,  3606,   304, 22280,   179,   176,  1316,   464,\n",
      "           304,   692,   117, 16386,   923,   118,   176,  2044,   117,   834,\n",
      "           653,  2822,   596,   123,   179,   260, 17681, 22281, 19660,  6867,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 155: tensor([[  101,  1021,   739,  4042, 15182,   173,  5231,   421,   118, 10497,\n",
      "          6086,   495,   146,  1407,  2009,   171,  2907,   221,   123,  9349,\n",
      "           171,  1223,   102]])\n",
      "DEBUG: Tokenized sentence 156: tensor([[  101,   259, 11677,   180,  1449,  1272,  5674,   923,   944, 11460,\n",
      "          1084,   117,  2113, 19001,   123,   682, 10674,   180, 18237,   304,\n",
      "         22280,   102]])\n",
      "DEBUG: Tokenized sentence 157: tensor([[  101,   146, 15796,   404, 13734,   796,   256,   125, 16001,   102]])\n",
      "DEBUG: Tokenized sentence 158: tensor([[  101,  2010,   222,   549,   713,  2638,  4839,   256,   368,   117,\n",
      "          4720, 22281,   293,   102]])\n",
      "DEBUG: Tokenized sentence 159: tensor([[  101,   222,   549,   713,  3002, 11646,  1547,  6086, 12447,   397,\n",
      "           125,   944,   259,   644,  3471,  1434,   118,   311,   222,   549,\n",
      "           713, 15702,   366,  9751,   102]])\n",
      "DEBUG: Tokenized sentence 160: tensor([[  101,  3254,  2904,   118,   311,   123,  1105,   117,   146,  3002,\n",
      "          2525,   122,   962,  8476,  2836,  1174,  1557,   117,  7845,   214,\n",
      "           179,  1021,   125, 16695,   118,   176,   117,   122,  3947,  1552,\n",
      "           712, 10420,   444,   598,   146,   302,   179,  2036,  5808,   151,\n",
      "           173,  8950,   260,  7289,   117,   122,   598,   146,  3752,  7134,\n",
      "         11037, 22290,   268,   298,  1449,  8855,   122,   505,  6592, 11369,\n",
      "           179, 16403, 22287,   123,   528, 20871,   125,   969,   123,   969,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 161: tensor([[  101,   146,   179,  1369,   138,   229, 22280, 14546,   179,   260,\n",
      "           504,  4242,  4390, 22281,  6867,   123, 13431,   117,   230,  4230,\n",
      "          1858,   117,   122,  5327,  2044,   176, 16386,  1825,   117,   123,\n",
      "         18195,   210,   118,   176,   877,   649,   240,  1369,   123,  1796,\n",
      "           117,  1065,   123,  5304,  2684,  1821,   320, 21173,   117,   122,\n",
      "           700, 15277, 22281,  6867,   221,   146,  1341,   171, 15796,   404,\n",
      "           122,  1938, 20798, 22281,  6867,   498,   146,  6151, 22290,  2166,\n",
      "           117,   179,  9821,  3444,  1196,   240,  7583,   333, 15407,   125,\n",
      "          5028,   122,  1945,   102]])\n",
      "DEBUG: Tokenized sentence 162: tensor([[  101,   146, 15796,   404,  9673,  2044, 14748,   146, 16909,   102]])\n",
      "DEBUG: Tokenized sentence 163: tensor([[  101,  3874,  6086,  3174, 13733,   495,  4051,   125, 17687,   118,\n",
      "          2036,   123,  1105,  2684,   123,  4767,   125, 12338,   122,   259,\n",
      "         15050,   171,   549,   713,  9079,   228, 17878,   125,  5480,   320,\n",
      "         16909,   171, 19317,   175,   117,  6335,   170,   123,  4390,   304,\n",
      "         22280,   180,  1105,  2166,   222,   739,  1896,   892,  1962, 22280,\n",
      "           117,  4446, 22279,   125,  4286,   247,   125, 13974,   117,   582,\n",
      "          4207,  4883,   222,  2971, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 164: tensor([[  101,  4167,   539,   122,  1685,   504,  4242,  9654,   654,   123,\n",
      "         16031,   418, 15976,   102]])\n",
      "DEBUG: Tokenized sentence 165: tensor([[  101, 20466, 22281,   117,  4141, 13793,  2415, 22280,  9673, 14748,\n",
      "           229,  2375,   117,   529,  4698,  4332,  1149,   179,  2531,   692,\n",
      "           123,  5304,   171,   425,   825,   171, 15796,   404,   117,   222,\n",
      "         15618,   293, 16909,   125,  1027,  1877,   793,   125,  2847,   117,\n",
      "         18728,   125,   329,   942,   125, 10809,   122,  8001,   125, 16317,\n",
      "          1165,   117,   122,   170,   222,   739,  4303, 22280,   202,  1997,\n",
      "           117,   582,   176, 10090, 18417,   230, 18253,  2830,   125, 20699,\n",
      "         10780,   138, 17445,   117,   240,  5530,   125,   230, 14038,  4316,\n",
      "         15403,   117,   173,   179,   176, 19449, 22278,   146,  1457,   117,\n",
      "          2685,   123, 17681,  4619, 18416,   122,   834, 14656,  3057,  1112,\n",
      "           418, 15976,   125,   629, 22280,  2415, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 166: tensor([[  101,  5231,  7567,   118,   176,   504,  4242,   122,   964,  1343,\n",
      "           221, 11348, 17124, 22281, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 167: tensor([[  101,   260,   504,  4242,  1085,  5231,  6839,   240,   454,   122,\n",
      "           260,   964,  1343,   240,   644,  2745, 12659,  8684,  4409,   102]])\n",
      "DEBUG: Tokenized sentence 168: tensor([[  101,   146, 14701,   125,  1078,   964,   324,   117,  3285,  1825,\n",
      "           123,  6205, 22278,   117, 22191,  5292,  7911,  6459, 13793,   123,\n",
      "           670,   102]])\n",
      "DEBUG: Tokenized sentence 169: tensor([[  101,   260,  9824, 13314,   171,   549,   713,  2365, 13521,   340,\n",
      "           122,   229, 22280,  9141,   692,  3874,   221, 11348, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 170: tensor([[  101,   416,  1149,   123, 19164,   340,   180,  6205, 22278,   179,\n",
      "          1084,  1021,   117,   271,   173,  3963,  1858,   670,   117,   122,\n",
      "           416,  1149,   320,   785,  1632,   303,   125,   179,   176,  1598,\n",
      "         13808,   202,   549,   713,   221, 18195,   123, 11451,   117,   123,\n",
      "          5277,   340,   260,   964,  1343,   229, 22280,   176,  1191, 12449,\n",
      "           417,  1797,  1429, 11348, 17124, 22281,   125,   944,   259,  2038,\n",
      "           180,   651,   117,   420,  2859,  1450, 20330,   125,  1004,  5533,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 171: tensor([[  101,   122,   117,  3002,  5926,   256,   230,   366,   504,  4242,\n",
      "           117,   291,   222,  3147,   117,   222,  2242,   582,   144,  8102,\n",
      "         22281,   236,   222,  1207,  2034, 22280,   117,  2366,   151,   230,\n",
      "         18637,   125, 11179,   358,   123,  3623,   118, 15887,   102]])\n",
      "DEBUG: Tokenized sentence 172: tensor([[  101,   122, 11972,   176,   262, 16895,  1532,   739, 11348,  1217,\n",
      "           151,   117,   762,  3249,   122,  1923,   209,  7494,   154,   117,\n",
      "           170,   260,   675,  1384, 22281,   125, 17935, 22281,   117,   260,\n",
      "           675,  3428,   477,   585,  6183, 17675,   555,   122,   259,   532,\n",
      "          1941, 18304, 18983,  2552,   125,  1510, 22281,   122,  1256,  1877,\n",
      "           793,   117,   179,  1183,  6451,   271, 18048, 20073, 22281,   240,\n",
      "           420,   123,  2128,   381,   124,   366,  1743,  3832,   964,  1343,\n",
      "           669,  3666, 12717,   555,   122,   146,  7871, 20318,   366, 18592,\n",
      "          6688,  1149,   125,  3233,   285, 22280,  4793,   117, 11823,   498,\n",
      "           259, 16814,   534,   128,  9213,   125, 11348, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 173: tensor([[  101,   122,   259,  3746,   185, 17675,   555,   274,   364,   249,\n",
      "           117, 20648,   125, 11451,  3848,  4419,   117, 11214,  1228,   692,\n",
      "           320,   969,   117,   179,  2798, 14542,   125,  4437,  4712,   102]])\n",
      "DEBUG: Tokenized sentence 174: tensor([[  101,   122,  6621,  2480, 16386,   159,  2382,   122,   572,   155,\n",
      "         10768,   117,  6621, 15682,  8833,   122,  1340,   442, 22278,   117,\n",
      "           662,  1353,   123,  1439,   268,   934,   117,   123, 15518,  1370,\n",
      "          5822, 22282,   117,   123, 11251,   117,   222,  1147,   117,   230,\n",
      "          5664, 12043,   117,   230,  1841,   304, 22280,   117,   179,  9821,\n",
      "         11934,   578, 13609,  2492, 22278,   117,  1369,   653,   117,  4566,\n",
      "         19068,   458,   117,   122, 19386,   159,   118,   176,   271, 19785,\n",
      "           202, 16758,   303,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 175 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.007366151633602919\n",
      " Coesão Score Final: 0.5036830758168015\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'isto e', 'nao obstante', 'nem', 'tampouco', 'ou', 'ora', 'quer', 'seja', 'como', 'tanto quanto', 'quanto', 'em vez de', 'desde que', 'assim que', 'porque', 'posto que', 'gracas a', 'apesar disso', 'no entanto', 'por exemplo', 'alias', 'acima de tudo', 'como tambem', 'tanto', 'quanto', 'se nao', 'a proporcao que', 'de sorte que', 'por isso']\n",
      " Número de conectivos: 41\n",
      " Número de sentenças: 175\n",
      "======================\n",
      "Resultados para preprocessado_o_cortico_aluisio_azevedo_cap_1.json:\n",
      "{'coesao_score': np.float64(0.5), 'conectivos_encontrados': ['e', 'mas', 'porem', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'isto e', 'nao obstante', 'nem', 'tampouco', 'ou', 'ora', 'quer', 'seja', 'como', 'tanto quanto', 'quanto', 'em vez de', 'desde que', 'assim que', 'porque', 'posto que', 'gracas a', 'apesar disso', 'no entanto', 'por exemplo', 'alias', 'acima de tudo', 'como tambem', 'tanto', 'quanto', 'se nao', 'a proporcao que', 'de sorte que', 'por isso'], 'num_conectivos': 41, 'proporcao_conectivos': 0.007, 'similaridade_media': np.float64(1.0), 'num_sentencas': 175}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,   726,   682,   481,   146,   549,   713,  4041, 10955,   125,\n",
      "           644,   221,   644,   117,  7551,   344,  1149,   117,   331,  2746,\n",
      "           118,   176,   125,  9349,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   122,   320,  1341,   146, 15796,   404, 17154,  5723,   118,\n",
      "           176,   117, 11233, 13194, 22280,   170,  7583,   294,  8102,   672,\n",
      "           151, 18997,   125,  1069,   117, 13038,   825,   975,  1885,   185,\n",
      "          7970,  7321,  4955,   304,   415,   179,  2036,  1664,   351,  1982,\n",
      "           180,  1105,   117,   240, 15702,   366,  9751,   117,   122,  8764,\n",
      "         10957,   143,   117, 18967,   122,   325, 15618,  1509,   171,   179,\n",
      "           333, 15407, 22281,   117, 16387,   692,   240,  1719,   123,   670,\n",
      "           117,  3444,  2746, 13734,   796, 22282,   146,  1690, 22280,   173,\n",
      "          3443,  3914,   117,   646, 17425,   243,   146,  3037,   122, 17409,\n",
      "           348,  2745,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,  4460,   179,  1084,   229,  4768,   171, 11014,  9258,   259,\n",
      "           532,  3907,  2698,   229, 22280,  1820, 22281,  6867,  3002,   117,\n",
      "          2803,  5723,   118,  2036,   123,  9533,   123,  3240,   404,  1159,\n",
      "         22278, 12464,   171, 12447,   397,  1112,  6086,  1903,   222,  3061,\n",
      "           371,   415,   117,   222,  5980, 22280,   117,   179,   229, 22280,\n",
      "           879, 11448, 22278,  2364,   222, 12581,   183,   117,   122,   179,\n",
      "          9584,   125,  9461,   122,  9317,   170,   230, 10105, 22354,   123,\n",
      "          2954,   122,   712, 18433,   744,   325,  7491,  1111,   351,   146,\n",
      "           347,  3673,   430,  2766,   117,   625,   368,   117, 12401,  1825,\n",
      "           118,   176,   395,   193,  1655,   171,  1312,   303,   117,  5308,\n",
      "           256,   118,   176,  4412,   860,  2828,  1532,   466, 14960,   942,\n",
      "         22278,   117,  1982,   123,  9317,   180,  4767,   125, 14890,   117,\n",
      "           122,  8362, 22278,   117,   123,   598,  1289,   183,   117,   146,\n",
      "         15618, 15335,   157,  7257, 22282,   179,  8940,   180,   418, 15976,\n",
      "          1532, 19455,  3391, 13793,  2124,   125,  3155,   822, 13550, 22281,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   229, 22280,  4207,  3767,   123, 11471,   834,  3859,   202,\n",
      "          9169,  6086,   475,  1783,   117,  8833,   122, 20957,   117,   179,\n",
      "           146,   173,   483,   483,   285,   256,   170,   146,   347,  5546,\n",
      "         10848,   125,  5294,  8849, 22281,   202,   144,   373,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   122,   700,   117, 10371,   202,  3147,   125, 18165,   117,\n",
      "         21692,   175,   122, 19877,  3611,   243,   260,   516, 14652, 22281,\n",
      "         18757,   441,   180,  2606,   117,   847,   265, 22280,  1941,   298,\n",
      "         19262, 22281,   498, 22281, 10557,   382,   179,  2036,  9144,   117,\n",
      "           123,   368,   117,  1718,   391,   146,  5052,   122,  5986,   123,\n",
      "           338,  7153,   795,   117,   495,   744,   123, 16521,   171, 12034,\n",
      "           146,   179,  2036,  1670,  6152,   256,   146,  5791,   183,   117,\n",
      "           432, 22279,   846,  2146,   118,  2036,   123,  8410,   170,   222,\n",
      "          2996, 22280,  5381,   265,  1637,   125, 21736,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   978,  2401, 22279,   524,   171,  1342,   117,  4566,  1342,\n",
      "          1456,   143,   179,  3283,   371, 12464,   117,   834,  3804, 22282,\n",
      "           577,   140,  3484,  1224,   357,   130,  4566,  1342,   179,   117,\n",
      "           221,   333,   325,  8797,  1510, 22281,  1176,   171,   179,   368,\n",
      "           117,   229, 22280,  1023,   125,  6759,   170,   123,  2267,   171,\n",
      "          9019, 13793,   291,   170,   123,  1587,   578,   285,   125,  3179,\n",
      "         22082,   958,  2118,   180,  1105,   449,   318, 13793,   117,   368,\n",
      "         15796,   404,   117,   179,   176, 10262,  3638,   123,   169,  8388,\n",
      "          9434, 22280,   180, 13503,   387,   705,   122,   180, 14657,  7919,\n",
      "           368,   117,   179,   117,  2044,   700,   171,   347,  2982,   117,\n",
      "         12416,   214,   221, 18615,  1187,   123,   222,   294,   118,  9741,\n",
      "           179,   146, 10840,  1095,  5723,   117,  1996,   124,   179,   146,\n",
      "          1010,   215,   495,   230,  2840,   421,  9217, 17754,   285,   125,\n",
      "          3495,   117,  8764,  2551,   138,   222,  2397,  2135, 22280,  4276,\n",
      "           211,  8041,  6530,   368,   117,   179,   176,   978,   229,  1284,\n",
      "           125,  2401,  4211,   415, 12600,   458,   117,   229, 22280,  9882,\n",
      "           870,   509,   125,   222,  7094,   303,   125,   260,   300, 12103,\n",
      "           170,   146,   347, 12034,  9767, 22278,  1434,   118,   176,  7258,\n",
      "           171,  1010,   215,   122,  3283,   371,   118,   176, 15316,   125,\n",
      "           230,  2509,  3002,   118,  6974,   285,   122,   834,   440,  5345,\n",
      "          1522,   125,  9429, 12223,   186,   118,   176,  1815,  2895,   221,\n",
      "          1491, 10470,   117,   122,   229, 22280,  9882,   125,   230,  1746,\n",
      "           699, 17850,  2951, 22278,   122,  2588,  5271,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,  1141,   202,  1338,   125, 11169,   615,  1796,   123,   327,\n",
      "          5566, 22278,   136,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101, 20613, 18003,   222,  1695,   117,   122,  3295,   117,   449,\n",
      "           271,   136,   123,   179, 14701,   136,  9935, 12766,   214,   118,\n",
      "           176,   123,   222,   644,   492,   117,   179,  2036,  5626,   124,\n",
      "         17784,  9342,   125,  7911,   117,   449,   563,   162,  5187,   909,\n",
      "         10985,   143,   125,   273,  1289,   382,   122,   792, 12268, 22281,\n",
      "          5646, 17376,   123,  1069,   117,  1141,   117,   449,  1023,   125,\n",
      "           829,   900, 19731,   246,   230,  2606,   179,   368,   146,   679,\n",
      "           256,   122,   171,   179,   870,   509,  2036, 17670,  2745,  1257,\n",
      "           136,   615,   495,   870,   509,   123,   327,   739,  1622,   340,\n",
      "           136,   171, 21929,   180,  1105,   221,   146,   879, 22282, 11290,\n",
      "          2758, 22280,   171,  1223,   122,  2700,   118,  1246, 22278,  2401,\n",
      "         22279,   524,   415,  7716,   117,   229, 22280,  1021,   623,  2100,\n",
      "           229, 20703,  1337, 15756,  7919,   125,   179,  1757,   209,  7186,\n",
      "          2589,   327,  2267,   117,   146,   273,   522,  1196,  2798, 14019,\n",
      "          3746,   852,   256,   146, 15537,   125,   333,  1568,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   176,   740,   117,   173,   576,   125, 16737,   125,   860,\n",
      "           266,   117,  1796,   230,   432,   537,   343,  6436,   252, 12401,\n",
      "           328,   240,   368,   117,   122,  2711,   179,   123,  3330, 22281,\n",
      "           236,   122,   318, 13793,   123,  1069,  2036, 13239,   151,   125,\n",
      "          1342,  2277,   449,  6621, 22281,  1109,   319,   143,   117,   123,\n",
      "          6754,   854,  2028,  3874,   325, 15010,   179,   146,  6875,  3361,\n",
      "           171,   599,  1125,  2042, 22280, 17688,   117,   122,   146, 15796,\n",
      "           404,   860,  1399,  2684,   123, 14808,   403,  7248,   121,   112,\n",
      "          5566, 22278,   146,   146,   635,   179, 12595,   256,   598,   123,\n",
      "          2772,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,   230,   730,   945,   123,  1815,   180,   327,  1069,  2010,\n",
      "           572, 22283,   230,  5294,  8849, 15912,   456,   368,   117,   173,\n",
      "          4410,  2729,   117, 15975,   348,   118,   176,   180,  9461,   117,\n",
      "           582,   176,  1021, 12401,   286,   238, 14600,   246,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   122,   429,   118,   176,   123,  7282,   159,   202,  3147,\n",
      "           834,  6272,   125, 18165,   117, 21128,   179,   123, 14594,  7970,\n",
      "          2401, 22279,   524,  2036,  9466, 22282,  8253,   256,   259, 14689,\n",
      "          5198,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,  8540,   122, 14657,   183,   495,   146,  4141, 13793,  2415,\n",
      "         22280,  1966,   117,  1141,   117,  7258,   221,  1966,   122,   179,\n",
      "          1021,   125,   333,   123,  1069,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  1417,   180,   223, 22279,   117,   179,  1011,  1790,   316,\n",
      "         22280,  3039,   122,  7259,  3391,   201,   271,   202,   644,   173,\n",
      "           179,  2080,   180,  2480,   834,   222,  4698, 22287,   125,   347,\n",
      "          1966,   117,  1141,   117,   179,   495,   390,   303,   122,  4207,\n",
      "           744,  3746,  4803,   785,   117,  2113,   625,   653,  7762, 22281,\n",
      "           236,   123,  6759,   122,   123,  2606,  2036,  8625, 22281,   236,\n",
      "           230,  1858,   860,   266,   495,   331,  3497,   118,  1084,   221,\n",
      "           146,   644,   492,   170,   222,  8993,   269,  4207,  5637,   118,\n",
      "          1340,   221,  1966,   122,   179,   495,   146,  1010,   215,  2010,\n",
      "           572, 22283,   230,  5294,  8849,  3898,  1145,   256,   368,   834,\n",
      "          4914, 18079,   159,   118,   176,   170,   123, 15685,   171, 12447,\n",
      "           397,   102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   230, 16318,   613,   699,   202,  1338,   125, 11169,   179,\n",
      "           644,   492,  1047, 22280,  2779,   136,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   230,  1105,   125,  3907,   523,   117,   180,   615,   229,\n",
      "         22280, 21174, 14661,   118,   311,   834,  7766,   367,   146,   179,\n",
      "          1084,   418, 11272,   222,  1855,  3285,   286,  1532,  2551,   125,\n",
      "           669,  3391, 22280,   143,   179,   229, 22280,   176, 17558,   328,\n",
      "         22287,  2364,   117,   122,  1078,   576,   325,   176, 12543,   228,\n",
      "           122,   325,   311,   768,  9398,   320, 21964,   141,  1014,  2480,\n",
      "           117,   582,  4314,   244,   123, 15520,   179, 15212,   125,  7343,\n",
      "           117,   176,   123,  8410,   171,  7343,  6884,   373,   122,   146,\n",
      "           171,   185,   117,   179,   311,  5626,  7583,   834,   118,   792,\n",
      "         12268,   122,   179,   123,   740,   311,   466,   323,   271,   123,\n",
      "          1143,   185,   180,  1105,  2791,   311,   466,   323,   123,   418,\n",
      "          4203,   121, 22361,  5566, 22278,   136,   262,   180, 10262,   392,\n",
      "           304, 22280,  1154,  3679,  5809,  5365,   179,   176,  5192,   202,\n",
      "         16450,   304, 22280, 16026,   171, 15796,   404,   222,  1160,  7503,\n",
      "          2010,   146,  4222,   452,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,  3207,   214,   118,  2036,  2829,   310,  2004, 22280,   221,\n",
      "           259, 21853,   501,  5248,   179, 16386,   210,   123,  1069,   125,\n",
      "           222,  2397,   834, 20027,   117,   123,  1977,  8650,   122,   834,\n",
      "         12223,  3391, 13793,   221,   926,  3746,  4803,   170,   260, 11920,\n",
      "           470,   117,   146, 20456,  8080, 22280, 18825,   748,   118,   176,\n",
      "          7583, 14038, 22278,   117,   271,   222,   762,  8723, 10922,   185,\n",
      "           117, 19054,   180,  1386,   117,   179,   176, 15975,   421,   123,\n",
      "          2521,  2028,   125,   230,  1069,  9021,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,   123,  2541,  1076,   125,   860,   266,   117,   179,   123,\n",
      "           905,   247,  2036,  4551,   256,   298, 18908,   501, 15856,   379,\n",
      "          1159, 13449, 19088, 22281,   125,   390,  1165,   117,  2535,  2036,\n",
      "          6009, 12051,   123,  5546,   154,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101, 11556, 11688,   578,   118,   176,   125,   179,   740,   170,\n",
      "          3901,  4841,   186,  5052,  5284,   117,   179,   368,   117,   240,\n",
      "           327,   576,   117,   176,   229, 22280,   146,   978,  4841,   201,\n",
      "           117,  5626,   124,   118,   146,   240,  3402,  2004, 22278,   117,\n",
      "           146,   179, 12444,  5488, 22282,   325,   744,   122,  1065,   318,\n",
      "         13793,   905,  4373,   123,  2448,  2430,   170,   222,  1923,  1464,\n",
      "           117,  2636,  1659,   146,  4947,  3189,   286,   180,   327,  1622,\n",
      "           340,   117,   785,  8174,   788,   202, 14353, 22280,   240,   370,\n",
      "           870,   509,  8085,   230,  5664,   173,   179,  4207, 19226, 22282,\n",
      "          3495,   117,   834,   370,   117,  2364,   325,   117,   125,   398,\n",
      "           569, 22283,   118,  1340,   123,  2606,   117,  2798,   370,   125,\n",
      "          5308,   118,  1340,   123,  2760,  3933,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,  4327,  5811,   304, 22280,  1261,  5990,   118,   146,   173,\n",
      "          8887,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,  2002,  2044,   221, 16044,   307,   118,   176, 15316,   366,\n",
      "          3038,  6034,   784,   117, 19519,   214,   440,  5345,  1522,  3290,\n",
      "           117,  4276, 20057, 13114,   243,   118,   176,  2249,  4207,   122,\n",
      "         12746,  2746,   123,   327,  2401, 22279,   524,   423, 12034,   170,\n",
      "           222,  1065, 21102,   293,   388,   125, 20283,  4917, 22281,  3824,\n",
      "           403,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,   320,  3852,   118,  2036,   944,   259,  1564,   412,  5304,\n",
      "           117,  7104,  2349,   256,   118,   146,   170,  2561,   304, 22280,\n",
      "           117, 13449, 11252,   834,  1984, 22282,   122,  3030,  7831,  2044,\n",
      "           123,  1354,   173,  2590,   117,   785,   333,   247,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,  2391,   259,  1867, 10674,   221,   123,  6009,   171,  4222,\n",
      "           452,  6540,   123,  1105,   122,  2002,  8118,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   123,  2606,   117,  4460,   179,  2036, 12110, 22281,  6867,\n",
      "          1941,   259, 13841,  8618,   117,  4912,  1939,  1808, 22288,   170,\n",
      "          1257,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  1757,   209,  7186,   978,   318, 13793,  6136,   221,  9967,\n",
      "           481,   122,   495,   146,  1903, 17609,   180,  2922,   485,  1451,\n",
      "          1877,   328,   117,  5923, 11324,   117,   170,  1283,   194,   986,\n",
      "         18048,   577,  3706,   529,   362,   942,   138,   171, 17749,   117,\n",
      "           366,  1877,   269,  4323,   122,   298, 18908,   501,   117, 21347,\n",
      "         20474,  2157,   162,  6839,   125,  9344,   591,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,  9815,   256,   146,  5902,   222,   286,   366,  2968, 13152,\n",
      "          1343,   117,   230,  2566,   392, 16224,   125,  4929, 21563, 13841,\n",
      "         17291,   268,   118,  6054, 22281,   117,   223,   128,  1821, 19408,\n",
      "           358,   117,   877,   842,  3848,   143,   122, 10647,   117,   271,\n",
      "           260,   180,   223, 22279,   117, 12141,  1695,   325,  6054, 22281,\n",
      "           171,   179,   123,  1034,  4530,   171,  9169,   117,  1143,  1283,\n",
      "           194,  1674,   117,  1896,   892, 15541,   449,   259,  5708,  1491,\n",
      "           117,  7769,   117,  9585,   122,  3002,  9258,  1409,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   240,  1921,  1405,   451, 22278,   117, 13776,   117, 19288,\n",
      "           125, 10802,   117, 16302,   243,   320,  1568,  3914,   117,   146,\n",
      "          1417,   125,   222, 22082,  1168, 18745, 14908, 22280,   179, 10348,\n",
      "         15152, 22281, 16228,   123,  1105,  2791,   125, 15796,   404,   122,\n",
      "           179,   495,  5787,   146,  1407,   958,  2118,   179,   860,  1776,\n",
      "         22278,   202,  2699,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   146, 13254, 13028,   118,   176,  2678, 22285,  3132,   117,\n",
      "           978,  8184,   481,   122,  8940,  8794,   229,  3952,  1089,  3058,\n",
      "           428,   501,   179,  2036,  3207,   692,   221,  4270,   229, 14164,\n",
      "           125,  7930,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101, 15796,   404,  9730,   203,   118,   146,   202,   347,   425,\n",
      "           825,   180,  4768,   171, 11014,  9258,   449,   146,  8609,   179,\n",
      "          1391, 22288,   118,   176,   117,   202,  1338,   125,  1089,  1564,\n",
      "           117,   125,   179,   123, 22283,  9086,  3002, 13851,  6859,   117,\n",
      "           122,   146, 19317,   175,   117,   123,  1977,   229, 22280,  5572,\n",
      "           508, 16550,   671, 22282,   118,  2036,   117,  8404,   203,   170,\n",
      "           368,   221,   123,   327, 18489,   340,  2754,   125, 11967, 10953,\n",
      "           763,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,  2678, 22285,  3132,   495,  7390,  5137, 21102,   117, 13140,\n",
      "           125,  1334, 13808,   579,   117,   170, 11368, 15020,   261,  6471,\n",
      "           125,  9586,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,  9821,   785,  8170, 22280,   298,   532,  2595,   122,   316,\n",
      "         22280,  1695,  3084,   256, 10768,   122,  3598,  5592,   117,   179,\n",
      "           229, 22280, 15891,  1399,   222,  4698, 22287,  1796,   366,  4096,\n",
      "           125,   681,  5657,  3756,   351,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,   125,  4745,   117,   123,   229, 22280,   333,   125,  1062,\n",
      "           252,   221,   260,  6880,   117,   179,  8544,  1684,   170,   146,\n",
      "         15796,   404,   117,   229, 22280,  3456,   285,   256,   766,   125,\n",
      "          1105,  3133, 13793,   173,  4067,   180, 20027,   117,  2166,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101, 10502,   860,   266,   117,   202,  6365,   125,  1695,   596,\n",
      "           117,  6075,   240,   368,  5948,  1821, 17883, 22290,   122, 12771,\n",
      "           203,   118,   176,   125,  5357,  1284,   180,   327,  9317,   285,\n",
      "           117,  9317,   285, 13608, 22278,   423, 19317,   175,   117,  3382,\n",
      "           179,   146,  2678, 22285, 13538,   994,   978,  2601,  1546, 22278,\n",
      "           171,  1568,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,  2364, 21315,  3495,   625, 11736,   125,  1569,  5664,   117,\n",
      "         15158,   256,   118,   123,   125, 10502,   860,   266,   117,   179,\n",
      "           240,   327,   576, 12771,  2836,   146,  4170,   125,  6009,   118,\n",
      "          1084,   117,   660,   146,  4947, 18253,  1196,   229,  1284,   171,\n",
      "         22082,   170,   230,   170,  6501, 22280,   125,  3644,   900,   247,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   327,  9730,   984,  2803,  5723,   623, 15048,   122, 11330,\n",
      "           592,   118,  7911,   240,   454,   117,   171,   179,   368, 17226,\n",
      "           229, 22280,   978,  3482,   117,  2798,  4750,   370,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,  3874,  2036,  3207,   256,   117,   122,   259,  7329,   180,\n",
      "          1105,   146, 16598,   692,   271,   123,   222,  1417,   171,  2004,\n",
      "         22280,  7258,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,   123,  2954,   117,   260,  1176,   117,   625,   146,   596,\n",
      "          1011,  4062,   117, 10502,   860,   266,  8625, 22278,   170,   368,\n",
      "           117,   123,  2267,   122,   222,  3848, 17714, 22279,   117,   146,\n",
      "          5488, 15500, 22287,   117,   123,  2822,   210,   230,  1359,  2684,\n",
      "           123,  7815,   122,   117,   173,  1226,  8431,   221,  1569,  4939,\n",
      "           173,  1105,   366, 19763,   117, 16403,   118,   146,   173,   327,\n",
      "          4067,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101,   123,  3293,   705,   180, 20027,   117,   171, 15796,   404,\n",
      "          5048, 13808,   118,   176,   125,   847,  1382,   124,   117,  1124,\n",
      "          7137,   744,   390,   304,   117,  3848,   458,   324,   122,   374,\n",
      "           266,   117,   179,  3598,  5723,  1364,   146,  4698, 22285,  6199,\n",
      "           179, 18720,   256,   173,  8977,   853,   634,   229,  5304,   125,\n",
      "          4141, 13793,  2415, 22280,   230,  2128, 11324,  2477,   705,   117,\n",
      "          2052,   447,  4275, 22282,   117,   785, 10032,   122, 12043,   117,\n",
      "         21990, 22278,   122,  8742,   271,   222,  3848, 17714, 22279,   117,\n",
      "          7422,   214,   125, 17722,   252,   117,   834,  2036,  3207, 22282,\n",
      "           222,  2476,   117,   123, 11281,  4277,   180, 21597,   194,   292,\n",
      "           117,   122,  4111,   117,  1684,   179,   259, 11314,  2650,  1058,\n",
      "           291,   259,   958,  2118,   143,   180,   316,   391,   324,   117,\n",
      "           331,   221,   311,  2650, 22282,   170,   740,   117,  2036, 21280,\n",
      "          3106,   304,   303,   143,  1112,   146,   151,   117,   179,  2779,\n",
      "           311,   179,  1391,   320,  7472,   125,   438,   780, 22354,   117,\n",
      "           122,  3448,   146,  1815,  5488, 15500, 22287,   117,  1417,   125,\n",
      "           230, 17479,   179,   262,   125, 10502,   860,   266,   122,   123,\n",
      "          1977,   418,  1021,   313,   512, 14996,   102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,   123,  2606,   171, 15796,   404,   978,   240,   860,  3848,\n",
      "         17714, 22279,   230,  4613,   232, 22280,   834,  6559, 10348,   118,\n",
      "          2036,  1719,   123,  4676,   117,  3495,   117,  4400,   117, 16403,\n",
      "           118,   146, 11631,   123, 17611,   117, 16555,   118,   146,  1004,\n",
      "         12413,   122,  5747,   576,  2080,   123,  1434, 13747,  7362,   123,\n",
      "          2267,   117,   125,   316, 22280,  6358,   343,   179,   176, 15245,\n",
      "           170,   368,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,  1502,   176,   123,   853,  6522,  1337, 17704,   646, 22290,\n",
      "         15736,   170,  1757,   209,  7186,   240,  2318,   171,  2128,  6812,\n",
      "          1502,   117,   176,   625,   176,   179,  1445,   692,   259,   682,\n",
      "           117,   222,   598,   146,  1342,   117,   740,  2364, 10348,  3083,\n",
      "         13793,   123,  2267,  1502,   176,   146,   179,  1021,   125,  1407,\n",
      "           229,  1105,   495,   221,   146,  5488, 15500, 22287,  1502,   117,\n",
      "           176,   625,   262,   860, 13379,   125,  5294, 14271,  1557,   122,\n",
      "           146, 15796,   404,   117,  2440,   366, 10262, 21014,   122,   298,\n",
      "          9958,   180,  2772,   117,  9673,   118,   146,   221,   222,  5627,\n",
      "           117, 10502,   860,   266,  5334,  5630,   944,   259,  1564,   122,\n",
      "           726,   123, 12298,   340,  2461,   229, 22280, 10539,  7670,   117,\n",
      "          2798,  9537,   117,  2798,  6075,   259, 12141,   123, 16241,  1537,\n",
      "         22287,   136,   122,   146,  6754, 15796,   404,   117,   176,   229,\n",
      "         22280,  4750,  9533, 16421,  1245,  3292, 22281,   180,  2606,   122,\n",
      "          9226,  3260, 12764,  3318,   975,  1885,   185,   298,  7329,   117,\n",
      "           978,   125,  2822,   320,  3848, 17714, 22279,  1719,   123,   996,\n",
      "           304, 22280,   122,  1434,   118,  2036, 21323,   246,  1485,   260,\n",
      "          6272, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,  1021,   744,   117,   425,   260,  4117,   842,   171, 19317,\n",
      "           175,   117,   222,  1342,  9730, 22279,  1202, 22287,   171,  2678,\n",
      "         22285,  3132,   117,   146,  4575, 11967,  1215,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,   860,   117,   240,   210,   117,   229,  3322,   125, 11718,\n",
      "           343,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,   495,   222,  6754,   118,   644,   492, 18597,   214,   221,\n",
      "           259, 17585,   481,   117,  6267,   321,   713,   117, 10881,  4712,\n",
      "           117,  6995,   122, 16293,   117,   271,  3235,   256,   117, 17897,\n",
      "           122,  4351,   339,   272,   171,   653, 11609,   785, 13376,  3173,\n",
      "           183,   117,   170,  7226, 11224,   128, 13722,  3324,   179,  2036,\n",
      "          8101,   692,   146,  3846,   180,   879,  6720,   266,   122, 21280,\n",
      "           118,  2036,   123,  1354,   230,  9434, 22280,   125, 16526,  1186,\n",
      "           117, 15363,   125,  1365,   170,   146,   347, 17749,   602,   309,\n",
      "           303,   122,   170,   123,   327,  9463,   834, 18908,   501,  1413,\n",
      "         22287,   118,   176,   118,  2036,   744,   944,   259, 12141,   117,\n",
      "           449,   117,   316, 22280, 13478,   117,   179,  9821, 22287,  1743,\n",
      "           308,  2684,   320,  1423,   102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,  6952,   256,  1684,   125,  7967,   117,   170,   222,  5825,\n",
      "           118,  9856, 15702,   171,  4332,   303,   122,   222,  1690,  7817,\n",
      "           125,  4332,   421, 11272,   529, 17722,   842,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101,  1796,   173,   347,   596, 12531,   171,  1847,   523,   117,\n",
      "           700, 17478, 22282,   125,  5976,  7719,   653,   179, 13956, 22278,\n",
      "           325,   125,   230,   576,   229,  5566, 22278, 19317,   214,  7769,\n",
      "           240,   327,  1284,   102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101,  7304,   748,   118,   176,   785,   260, 20519,  3391, 22280,\n",
      "           143,   726,   123,  1658,   171,   221,  4894,   744,  5076, 22278,\n",
      "          2124,   117,  5168,   123,   333,  1004,  8797,   449,   123, 10849,\n",
      "          1950,   214, 22288,   122,   117,   125,  3002,   439,   157,   173,\n",
      "          3002,   439,   157,   117,   262,   118,  2036, 15832,   214,  2745,\n",
      "           240,   420,   260,   675, 16317, 22281,   125, 15252,   125,  6372,\n",
      "           387,   102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[  101,   122,  2535,   117,   144,  2640,   117,  1941,  4575,   117,\n",
      "           170,   286,   125,   273,   215,  5587,   143,   117, 13140,   125,\n",
      "          8394,  6702,   649,   117,  1413,   118,   176,  4171,   834,  3353,\n",
      "           122,  4655,  5723,   123, 15419,   171, 15796,   285,   117,   170,\n",
      "          1977,   240,  1415,   481,  3954,   173, 13254,   117,   425,   260,\n",
      "          6107,   171,   653,  9019, 13793,   117,   122,   125,  1977,   176,\n",
      "         14223,   124,  3695,   117,   123,   905,   247,   240, 19937,   122,\n",
      "           325,  1373,   240,  4096,   102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[  101,  7427,  5630,   118,   146,   117,  2954,   122,   644,   117,\n",
      "           230,  4955,   304,   415,  8650,  7474,   117,   230,  1401,   285,\n",
      "         12448,   852,   125, 11938,   117,   222, 19781, 11097,   525,  3356,\n",
      "           403,   117,   598,  2745,   122,   598,   944,   117,   240,   229,\n",
      "         22280,  2036,   370,   908,   668,  4812,  4276,   211,   702,   146,\n",
      "          1147,   170,   260,   675,   223,   128,  1790,   238,  1973,   244,\n",
      "         22281,   122,  8574,  2191,   102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101,   122,   117,   271,   146,   347,  2233,  1177,   125,  3061,\n",
      "          1994,   229, 22280,  2036, 12122,  8925,   598, 16241,  1537, 22287,\n",
      "           146, 21115,   117,  1950,   581,  1165,   256,  1746,  1499,   371,\n",
      "           214,   260,  5365,   180,  1405,   451, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101,  1016,   117,  1085,   260,  1176,   785, 13278,   260,   498,\n",
      "          1056,   138,   171, 15796,   404,   117,   625,   117,   420,   736,\n",
      "          6578,  1877, 19147, 12822,   117,  8940,   123,  5489, 22281,   375,\n",
      "         22280,   146,  2115,  8806, 21544,   179,   905,   151,   256,   123,\n",
      "          4883,   118,   176,   173,  3443,   180,  2241,  2187,  4712,   102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101,   318, 13793,   146, 11967,  1215,  9086,  4720, 22281,   293,\n",
      "           122,   962,  8476,  2836, 16264,  1749,   909,   117,   221,   123,\n",
      "          5065,   122,   221,   123,  4573,   117,   271,  1977, 10907, 12785,\n",
      "           834,  1434,  6568,   117,   122,  2354, 14406,  2836,  4019,   304,\n",
      "           303,   143,   117, 17887,  7583,  1201, 22292,   565,   221,  5452,\n",
      "           439,   159,   146,  4575,   146,   635,  6039,  2794,  1839,  2461,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 50: tensor([[  101,  2010,  8490,   474, 10420,  5630,  1178,  3066,   713,   102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[  101,   329,  3252, 22278,   125,  1390,   185,   801,   122,   146,\n",
      "           347, 21568,   141, 15888,   679,   256,   118,  2036,   298,  5708,\n",
      "           173, 15351,   138, 15755,   912,   591,   117, 12625,  9804,  2858,\n",
      "           118,   176,   173,  1485,   260,  2566,  3082,   122,   173,  1485,\n",
      "           260, 20390,   589,   102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[  101,   123,  9429,   117,   123,  7828,   117,   146,  9861,   117,\n",
      "           123,   390,  2420,   117,   123,   344,   304,   117,   123, 10428,\n",
      "         22279,   117,   122,  1953,   123, 12464,   117,   122,   145,   146,\n",
      "           179,   368,   229, 22280, 16759,  2836,   123, 16241,  1537, 22287,\n",
      "           117,  3330,  2876,   319,   348,  1364,  6086,   179, 17002,   146,\n",
      "           179,   368,   229, 22280,   463,  6758,   124,   179,  3746,   852,\n",
      "           256,   146,   179,   368,   229, 22280, 16102, 19749,   179,  9679,\n",
      "           146,   179,   368,   229, 22280, 10698, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[  101,   122,   117,   221,  6553,  2854,   146,  4947,   171,   347,\n",
      "           146,   635,   117,  1359,   256,   118,   176,   598,   146,  1010,\n",
      "           215,   117,  1921,  2480,   179,   117,   229,   327,  5012,   151,\n",
      "         22280,   117,   331,   978,   230,  7947, 15500, 22278, 20613,   943,\n",
      "           259,  4966,   117,   122,   179,   117,   202,  1325,   117,   146,\n",
      "          4314, 22278,   117,   123,   368,   117,   229,  2377, 10866, 22278,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101,   532,  1564,  1085,  5149,   474,   171,  1457,  2277, 19994,\n",
      "           256,   260,  3003,   180,  1062,   252,   117, 11348,   256,   118,\n",
      "           176,   653,   202,  3147,   170,   230,   374,  3613,  3848,  4419,\n",
      "           173,  5791,   183,   125, 10832,   700,  8544,  9784,   259,  6349,\n",
      "           221,   123,  4767,   125, 14890,   117,   123,  2521,   171, 16960,\n",
      "           303, 16960,   304,   256,   122,  4767,   117,  5267,   256,   146,\n",
      "          7390,   272,   122,  8544,   641,  5137, 21102,   221,   230, 11628,\n",
      "          3239,   322,   180,  4768,   171, 12728, 22282,   117,   582, 17003,\n",
      "          4412,  3791,  4409,  2684,   260,  2856,   171, 14890,   117,   420,\n",
      "          1672,   123,  4640,  3002,   366,  1101,   179, 19407,  1084,  1796,\n",
      "           117,   975,  1885,   185,  2461,   102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101,   978,   123, 20494, 13793,   125,  8223,  1364,   146,  2187,\n",
      "           125,  1543,   122,   259,   302, 21510, 22281,   125,  1078,   222,\n",
      "           173,  2754,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101,   260,  1176,   117,  6387,   117, 10502,   860,   266, 12771,\n",
      "          2836,   118,   146,   125,  1434,  4296, 18937,   125,  4857,  6812,\n",
      "           117,   146,   179,   146, 11967,  1215, 19005,   256,  1407,   179,\n",
      "         16241,  1537, 22287,   136,   449,   123,   327,   739,  1568,  2037,\n",
      "         22280,   117,   146,   347, 13141,   117,   495,   123,  5546,   285,\n",
      "           117,  2251,  5630,  2745,   179,  1996, 22281,   236,  3953,   123,\n",
      "          2260,   714,   117,  4460,   179, 18424, 22278,  1684,  2401,  4211,\n",
      "           415,  7672,   260,  3191,   125,  1569,  4446, 22279,   117,  1623,\n",
      "           246,   260,   125,  4848,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[  101,   229, 22280,  4207,  9226, 10907, 22282,  3047,   125,   898,\n",
      "           230,   730,  7173, 18304, 22278,   117, 10552,   148,   256,   118,\n",
      "           176,   240,   210,   170,  2745,   179,   765,  1402,   236,   123,\n",
      "          1658,   123,   543,   194,   304,   125,   222,  1538,   173,   739,\n",
      "          9470,  4551,   256,   118,  2036,  1084, 20739,   125,   271,   304,\n",
      "         22280, 18574,   229,  8993,   180,  3182, 22278,   146,   179,   176,\n",
      "          1822,   151,   123,  1069,   125, 13974,  7194,   151,   320,   652,\n",
      "         21574,   125,  5708,   146,  4460,   122,   146,  1831,   123,   179,\n",
      "          7898,  1569, 11793,   122,   117,  2440,   298,   532,  7093,  1374,\n",
      "           117,   495,  9226,  6374,   229,  4768,   123,   549,  2912, 22278,\n",
      "           291,   146, 14619,   141,  4337,   557,   146,  2971, 22280,   117,\n",
      "          9086,  2044,   202,   388,   117,   122,   117,  5747,   576,   117,\n",
      "           625, 10348,   240,   898,   117,  5057,   670,   298,   179,  6082,\n",
      "           692,   123, 19540,   102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[  101,   318, 13793,   117,   229, 22280, 19674,   221,  1105,  1139,\n",
      "           259,  3218, 10775,   176, 12401,  4368,   102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[  101,  1821,  1684,  1359,   256,  2990,  7406,  7870,   260,  2139,\n",
      "           180,  1373,   117,   390,   286,   123,  1434,   171,   117,   834,\n",
      "           926,   370,   118,   176,   529, 11351,   117, 15872, 10490,   243,\n",
      "           125, 10803, 22282,  2856,   122,  2856,   320,  4081,   180,  2293,\n",
      "         22278,   125,  9196,  2382,   322,   102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101,   122,   146,   325, 11236,   122,   179,   368,   117,   320,\n",
      "          2477,   118,  2036,   123,  3655,   304, 22280,   117,  6838,   256,\n",
      "           118,   176, 18928,  1212,   598,   146,  3002, 11646,  5278,   179,\n",
      "           146, 18237,   256,  7583,  9466, 14004,   117,  4956,   146,  2971,\n",
      "         22280,   240,   230,  9211,   292,   125,  5207,   122,  2636,   125,\n",
      "         19284,   373,   146,  3420,   325,  1639,   102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101,  2010,   331,  4048,   117, 19068,   796,   256,   118,   176,\n",
      "           368,   117,   179,   123,  4388,   304, 22280,  4566,  3002,  2525,\n",
      "           495,  2822,   118,   311,  6365,   180,  6614, 11032,   873,  2872,\n",
      "          1510, 22281,  2856,   125, 11978, 22279,   118, 11978, 22279,   240,\n",
      "           230,   331,  4943,   556,   125,   944,   259,   644,  3471,   230,\n",
      "           366,  4351, 22282,   277,   325,   170,   585,   171, 11967,  1215,\n",
      "           495,   146,   347,   146,   635,   423,  5488, 15500, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101,   146,  3848, 17714, 22279,  2318,   256,   118,  2036, 14594,\n",
      "           170,   260,   675,   766,   226,  1053,   784,   125,  9726,  4943,\n",
      "         22280,   117,   122,   117,  7492,   303,   117,  5945,   214,  2249,\n",
      "          2859,   146, 12239, 11814,   117,   744,   325, 18420,  2836,   117,\n",
      "         11120,   229,  2561,   304, 22280,   125, 10502,   860,   266,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[  101,   146, 11718,   343,   125,   785,   179,   146,  2471,  5404,\n",
      "          2794,   117,   176,   229, 22280,  1796,   123,  4096,   125, 15397,\n",
      "           159,   123, 10502,   180,  1105,   102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[  101, 11967,  1215, 18574,   260,  3207, 22281,   125,   860,   266,\n",
      "           271,   260,  1877,   486,   180,  2004, 22278,   223, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[  101,   146, 15796,   404,   653,   117,   179,   146,  1413,   173,\n",
      "          1284,   125,  3695, 11293,   117,  1615,   122,  1615,  1176,   219,\n",
      "           842, 12908,   124,   173,  2880,   247,   143, 15834,   591,   125,\n",
      "          1950,   581,  1783,   117, 17141,  1546,  3514,   146,  2249,   202,\n",
      "         14353, 22280,   123, 10968,   852,   256,   122,   123,  3083, 13793,\n",
      "           240,   179,   229, 22280,   123,  5078,   252,   229,  4768,   712,\n",
      "          8993,  2817,   102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101,   122,   146, 11967,  1215, 10348,   118,  2036,  1719,   123,\n",
      "          3083, 13793,  3486,  1399, 14619,   210,   179,   259,   333,   501,\n",
      "          6399,  5201,  2072,  3364,   125,  2745,   102]])\n",
      "DEBUG: Tokenized sentence 67: tensor([[  101,  2010,   230,  2606,  6621, 22281,  1109,   319,   143,   117,\n",
      "         10355,   368, 18164,   183,   117,  2120,  3874,  1528,   179,   146,\n",
      "          1855,   117,   122,   222,  1855,   173,  1652,  3484,   123,  9349,\n",
      "         10968,   852,  2535,   117,  2354, 22279,   146,   179, 12444,   495,\n",
      "          2364,  3767,   118,   176,   221,   740,   102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[  101,  2010, 11032,  6834,   256,   146,  4170,   102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101,  2779,   311,   898, 22282,   378,  3914,   271,  1977,   176,\n",
      "          7947,   125,   230,  3240, 22282,  8008,   364,   146, 11718,   343,\n",
      "           117,  8540,   240,   792,  2249,   146,  3695,  4042, 22290,  5723,\n",
      "           123,  2606,   117, 13130,   256,   173,  2745, 20824,   117,  3951,\n",
      "           118,  2036,   222,   505,  2552, 22280, 12631,   303,   125,  8296,\n",
      "           304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[  101,   449,   240,  1342,  1341,   117,   625,  8362, 22278,   860,\n",
      "           266,  5961,   171,  4170,   117,   170, 18522,  1065, 22287,   122,\n",
      "          2684,   170,  7355, 22280,   117,   744,   325,  1428,  2001,   383,\n",
      "           151,   125,  1519,   175,   102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101,  2010,  2354, 22279,  3189,  4945,   136, 18601,   740,   117,\n",
      "          2779,  1004, 11094, 22280,  2249,  6086,   338,  7485, 22279,   171,\n",
      "          7258,  7343,  4170,   311,  4343,  8849,   117,   449,  1257,  1971,\n",
      "           176,   311,   180,   271,   123,   681,  7924,   179, 12232,   273,\n",
      "           522,  2382,   246,   221,   538,   117,  2459,   125,  2707,   117,\n",
      "           229, 22280,  7105,  5212,   834, 15211, 22280,   117,   625,  4081,\n",
      "           128, 12222, 22281,   125,   547,   179, 15212,   125,   829,   900,\n",
      "           146,   179,   311,  6600,   173,  7716,   117,  3189,  8766,   185,\n",
      "          2461,  3189,   229, 22280,  8766,   185,  3436, 22280,   118,  2036,\n",
      "           117,   240,   210,   117,   179,   117,   176,   380,  2720,   179,\n",
      "           146, 15796,   404,   176,  1120,  3897,   260,  1176,   221,  9726,\n",
      "           117,   122,  2113,  3486,   214,   179,  9141,   325,   123,  7482,\n",
      "         21761,   171,   179, 14537,   159,  5489, 22281,   375, 22280,   170,\n",
      "           230,  5294,  8849,  7970,  2601,   146, 11967,  1215,   117,   170,\n",
      "           123,   327,  9221,   514,  2647,  2000,  3292,   171,  1147,   117,\n",
      "          2364,  4686,   151,   123,  3484,   298,   682,   146,   179,  1078,\n",
      "           615,  2036, 10355,   598,   146,  1342,  1971,  1016,   179,   117,\n",
      "          5288,  2880,   151, 22280,   117, 12401,  1825,   118,   176,   123,\n",
      "          1105, 20990,  6859,   117,   173,  5314,   179,   229, 22280,   495,\n",
      "           171,   347, 12215,   117, 16143,   117,   320,  3852,   423,  6151,\n",
      "         22290,   117,  2082, 14473,   444,   125, 12514,  3508,  1165,   591,\n",
      "           179,  9821, 22287,  2477,   125,   222,  2242,   870,   439,   201,\n",
      "           125,  3088,   392,   117,   582,   173,  1250,   229, 22280,  8544,\n",
      "         16241,  1537, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101, 16924,   203,   118,   176,   221,  1084,   173, 21115, 22281,\n",
      "           125,  1143,   122,   117,   834,   333, 11094,   286,   117,  6920,\n",
      "           860,   266,   318,   840,   285,   420,   146, 16909,   122,   146,\n",
      "          2678, 22285,  3132,   102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101,  2789,   118,   176,  4412,   730, 17598,   117,   834,  5023,\n",
      "          2245, 22282,  2798,   362,  2245, 22282,   117,   122,   117,   331,\n",
      "           625,   259,   682,   176, 14661,   228,   117,   262,   179,   368,\n",
      "           176,  6075,   102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101,   123, 17704,   969,   654,   222,  3265,  9247,   183,   117,\n",
      "           122,   146, 13254,   117,   125,  6367,   179,  1011,   117,  1191,\n",
      "           118,   176,   549,   125, 21340,   449,   146, 11967,  1215, 11556,\n",
      "         20885,  2446,   118, 15887,   117,  4111,   173,  4410,  8932,   122,\n",
      "         20004,  2010,  1257,   122,   230,   525,   381,  2346,   351,   146,\n",
      "           179,  2354,   143,   418, 22280,  2636,   102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[  101,  3769,  4486,   229, 22280,   122,  2166,  2277,   179,   176,\n",
      "          5646,  2872,  1016,   271,   572, 22283,  2779,   117,  4207,   333,\n",
      "          1858,  2760,   102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101,  1502,  1532,  1105,   173,   179,   607, 14730, 15050,   117,\n",
      "           122,  1084,  8911,  2477,  3285,  5194,   118,   176,  3102,  2242,\n",
      "           171,  6151, 22290,   136,   102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[  101,  2010,   538,   229, 22280,  2072,   128,  2636,  3874,  1996,\n",
      "           860,   266,   117,  3477,   214,   146,  5052,   118, 10343,   102]])\n",
      "DEBUG: Tokenized sentence 78: tensor([[  101,  2010,   123, 22296,  1204,   146,  4575,   117,  1183,  5974,\n",
      "         20158, 22280,  3953,   318, 13793, 13201,   269,   117,  4174,   244,\n",
      "           179, 19864,   102]])\n",
      "DEBUG: Tokenized sentence 79: tensor([[  101,   122,  2389,   296,   179,   117,   176,  1016,  2589,   117,\n",
      "           221,  9726,  1467,   146,   653,   117,  2113, 17386,  1257,   123,\n",
      "          5664,   325,  2711,   171,  1147,   122,  3486,   214,   179,  1014,\n",
      "          1069,   123,  9349,   331,  1904,   146,   179,   662,   102]])\n",
      "DEBUG: Tokenized sentence 80: tensor([[  101,   176,  1976,   117,  3960,   151,   117,   262,   271,   176,\n",
      "          3874,  1003,   236,   117,  2113,  3874, 15212,   123,   765,   364,\n",
      "         22282,   170,   123,  1069,   125,  1078,   222,   102]])\n",
      "DEBUG: Tokenized sentence 81: tensor([[  101,   123, 17704,   418,   390,   304,   117,   418,   229,   344,\n",
      "           304,   298,   481,   347,  4170,   229, 22280,   123, 12458, 22305,\n",
      "           117,   122, 20198,   179,   146,  1747, 22278,   240,  1342,   123,\n",
      "         22296,  3413,   122,   146,  1147,   117,   122,   117,   176,   122,\n",
      "           516,   183,   117,   229, 22280,   227,   793,   538,   179,   146,\n",
      "          3283,  5835,   516,   183,   102]])\n",
      "DEBUG: Tokenized sentence 82: tensor([[  101,  2684,  5288,  2169,   944,  7273,  1839,   222,  7395, 13732,\n",
      "           268,   118,   505,  6592,  3120,   117,   179,   122,  8911,  6074,\n",
      "           117,  1075,   179,   368,   538,  8788, 22279,   229, 22280,  7707,\n",
      "           171,   228,   260,   223,   128,   102]])\n",
      "DEBUG: Tokenized sentence 83: tensor([[  101,   820, 17386,   179,   117,   221,  1858,   576,   117,  3921,\n",
      "           370,   222,  1074, 19161,   325,   125, 12245,   122,   102]])\n",
      "DEBUG: Tokenized sentence 84: tensor([[ 101, 2010,  418, 4062, 1587,  154, 7415,  860,  266,  102]])\n",
      "DEBUG: Tokenized sentence 85: tensor([[  101,  2010,  5112, 22280,  2779,   117,   176,  2826, 22280,  3413,\n",
      "           117,   122,   221,  5308,   118, 15887,  1004, 20885,   128,   123,\n",
      "          7343,  3953,   102]])\n",
      "DEBUG: Tokenized sentence 86: tensor([[  101,   229, 22280, 18691,   117,  2798,   240, 15419,   117,   179,\n",
      "           176, 20469,   228,   125,   179,   102]])\n",
      "DEBUG: Tokenized sentence 87: tensor([[  101,   146,  2678, 22285,  3132,  1316, 22290,  2598,   117,   170,\n",
      "           123,  4410,   744,   271,  2100,  2010,   449,   117,  2547,   593,\n",
      "           117,   347, 11967,  1215,   117,   179,   102]])\n",
      "DEBUG: Tokenized sentence 88: tensor([[  101,   146,  4575,  8082, 13665,   118,   146, 14619,   210,   240,\n",
      "           327,   576,   117,  3891,   118,  2036,   123,   223, 22280,   202,\n",
      "         20462,   122,  4415,  1552,   118,   146, 11631,  2010,   229, 22280,\n",
      "          2660,   746,   247,   117,   179,   229, 22280,   146,  7766,   367,\n",
      "           244,   117,  9778,   122,   117,   271,  1941, 19864, 15262,   125,\n",
      "           860,   266,   117, 11021, 22288,   118,  2036,   173,  5902, 21839,\n",
      "          2010,   229, 22280,   745, 22279,   123,  1434,  3413,  1016,   117,\n",
      "           179,  2354, 22279,   176,  3254,   421,   102]])\n",
      "DEBUG: Tokenized sentence 89: tensor([[  101,  2389,   296,   271,  2036,  8574,   210,   260, 11351, 10502,\n",
      "           860,   266, 14009,   118,   259,   123, 18175,   117,  5926, 11681,\n",
      "           246,   117, 19519,   214,  5811,   304, 22280,   173,  9654,   222,\n",
      "          6847,  4943,   735, 22279,   117,  8764,  2968,   740,  8544, 13301,\n",
      "          1825,   170,  5747,   416,   304,   117, 11032,  1719,   792,  3281,\n",
      "           498,   260,  4924,  5874, 13004,   117, 11032,   302,   214,   118,\n",
      "           176,   229,  9530,   508,   298,  1143,   221, 20482, 15690,   259,\n",
      "          7920,   247,   552, 14881, 22281,   122,   259,  1062,  3391,   138,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 90: tensor([[  101,  2678, 22285,  3132,  5793,   146, 11967,  1215,  2684,   320,\n",
      "          3147,  2166,   117, 10415,   214,   834,  5698,   125,  6797,   102]])\n",
      "DEBUG: Tokenized sentence 91: tensor([[  101,  2010,  2354, 22279,   318, 13793,   229, 22280,  3887,   149,\n",
      "          3411,   117,  2678,   147,   136,  7845,   136, 16795,   118,  2036,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 92: tensor([[  101,   146,  4575,   978,  1941, 10941,   117,   123,  1984, 22282,\n",
      "           117,   179,   259, 18720,   124,   173,  7229,   522,   175,   122,\n",
      "           179,  4412, 22278,  4062,   596,   123,   730,  8393,   154,   102]])\n",
      "DEBUG: Tokenized sentence 93: tensor([[ 101, 2010, 5961,  146,  179,  117,  347,  374,  326,  136,  102]])\n",
      "DEBUG: Tokenized sentence 94: tensor([[  101,  1502,   318, 13793,  1977,  4047,  2354, 22279,   179,  2779,\n",
      "          7206,   136,   102]])\n",
      "DEBUG: Tokenized sentence 95: tensor([[  101,   331,  8925,   244,   146, 21115,   176,  2354, 22279,   311,\n",
      "          1270,  5607,   221,  1257,   117,   449, 12044, 20089,   179,   229,\n",
      "         22280,  2822, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 96: tensor([[  101,  3189,  4945,   136,  2779,  2684, 12060,  2185, 22280,   785,\n",
      "           170,  2354, 22279,   117,  2678, 22285,  3132, 17386,   179,  2354,\n",
      "         22279,   122,   222,  9235,  9778,   117,   230, 12252,   122,  2826,\n",
      "         22280,   118,  2036,   325,  2678, 22283,   125,  7163,   259,   532,\n",
      "          3907,  2698,   170, 10502,   860,   266,   102]])\n",
      "DEBUG: Tokenized sentence 97: tensor([[  101, 12402,  1016,   117,   978,   118,  2036, 12585,   260,   223,\n",
      "           128,   122,   870,   468,  2836,   118,   260,   102]])\n",
      "DEBUG: Tokenized sentence 98: tensor([[  101,  2010,  2389,   296,   117,  3449,   117,  1334,  1653, 17598,\n",
      "           118,   146,  1684,   229, 22280,   176,  5436,   170,   171, 22285,\n",
      "         14837,   138,   117, 18047,   136,   102]])\n",
      "DEBUG: Tokenized sentence 99: tensor([[  101,   629, 22280,   146,   644,   492,   240,   180,   329,  7583,\n",
      "         19278,  1968,   222,  2397,   173,   305,  4418,  2535,  2249,   260,\n",
      "          1028,   117,  5562, 22280,   170,  2859,   229, 22280,  3422, 22279,\n",
      "          3963,   320,  3459, 17551,   117,  2798,  2036,   171, 22278,   123,\n",
      "          3049,   304,   117,  2113,   117,   202,  1338,   125, 11169,   117,\n",
      "           529,  7892,  2148,   784,   125, 10502,   860,   266,   117,   122,\n",
      "          2684,   222,   739,  1312,   303,   179,  2354, 22279,  2036,   659,\n",
      "          7343,  8797, 19421,  4838, 21102,   117,   625,   230,  2606,  1941,\n",
      "          1367,   298,  7187,   122, 18720,   123, 13071,   222, 13254,   373,\n",
      "           180,   327,  2169,   117,   122,   271,   176,  3582, 22281,   236,\n",
      "          2987,   173,   302,  4178,   118,  2036,   123,  1956,  1025, 10692,\n",
      "           455,   318, 13793, 14672,   125,   179,   229, 22280,   122,   331,\n",
      "           123,   740,   179,  2354, 22279,   659,   146,  1670, 17714,   247,\n",
      "           117,   449, 14619,   210,   320,  4170,  2249,   325,  3235,  2858,\n",
      "           118,  2036,  2354, 22279,   123,  2606,   117,  1407,   740,  4412,\n",
      "         22278,   125,  1677,   247,   117,   122,   240,  1104,  1017,  1407,\n",
      "           333, 22278,   221,   146,  6754,  2397,   117,   144,  2640,   179,\n",
      "           376,  1941,  2780,   170,   179,   176, 10363, 19356,   140,  1084,\n",
      "           240,  3378,   117,   170,   259,   532,  3907,  2698,   117,   122,\n",
      "          3804,   125,   222,  1695,   125, 21158,   625,  1359,   171,  1312,\n",
      "           303,   122,  3285, 22279,   118,   176,   173,  1105,  3235,   301,\n",
      "           118,   123,   117,  3235,   301,   118,   123,   179,   123,   240,\n",
      "         22278, 13376, 22278,   179,  2798,  2189,  1350,   146,   179,   122,\n",
      "          8911,   122,   785,  7472,   994,   117,  5945,   136,   229, 22280,\n",
      "         21719,  1858,   854,  2028,   285,   271,   123,   125,  1790,   122,\n",
      "          1236, 22279,   221,  4271,   117,   229, 22280,   331,   170,   740,\n",
      "           117,   449,   170,  1485,   260,   179,  2036,  9322,   210, 15702,\n",
      "           180, 14114,  1447,  3891,  1528,   260,   125,  1105,  6628,   117,\n",
      "           179,  1257,   122, 19199,   240,  2318,   366,  3848,   687,   562,\n",
      "          2798, 22033,   203,   303,   171, 22285, 14837,   138,   229, 22280,\n",
      "           176,  5436,   170,   123,  1757,   209,  7186,   122,  3960,   151,\n",
      "           179,  2036,   988, 22280,  1016,   117,  2113,  7206,   347,  3695,\n",
      "           117,  2113,   146, 17386, 12060,   713,   117,  2113,   146, 17386,\n",
      "         22003,   122,  1334,  6812, 22288,   118,   146,   316, 22280, 12043,\n",
      "           246,  2990,   576,   117,   179,   146,  8609,   117,  4173,   557,\n",
      "           118,  2036,   366,   223,   128,   117, 18518,   118,   176,   170,\n",
      "           222, 22000,   125,  7729, 10766,   340,   122, 10968,  2256,   117,\n",
      "          1139,   146,  4575,  2036, 10355,   173,  4410,   408,  3198,   328,\n",
      "          2010,  2389,   252,  2521,  3539,   329,  2354, 22279,   122, 18224,\n",
      "           243,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 100 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.010039177274721063\n",
      " Coesão Score Final: 0.5050195886373605\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'isto e', 'todavia', 'de forma que', 'por conseguinte', 'nem', 'tampouco', 'ou', 'ora', 'quer', 'senao', 'assim como', 'como', 'quanto', 'em vez de', 'antes que', 'assim que', 'porque', 'visto que', 'posto que', 'por outro lado', 'no entanto', 'com efeito', 'acima de tudo', 'principalmente', 'nao so', 'mas tambem', 'tanto', 'quanto', 'se nao', 'de forma que']\n",
      " Número de conectivos: 41\n",
      " Número de sentenças: 100\n",
      "======================\n",
      "Resultados para preprocessado_o_cortico_aluisio_azevedo_cap_2.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'porem', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'isto e', 'todavia', 'de forma que', 'por conseguinte', 'nem', 'tampouco', 'ou', 'ora', 'quer', 'senao', 'assim como', 'como', 'quanto', 'em vez de', 'antes que', 'assim que', 'porque', 'visto que', 'posto que', 'por outro lado', 'no entanto', 'com efeito', 'acima de tudo', 'principalmente', 'nao so', 'mas tambem', 'tanto', 'quanto', 'se nao', 'de forma que'], 'num_conectivos': 41, 'proporcao_conectivos': 0.01, 'similaridade_media': np.float64(1.0), 'num_sentencas': 100}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  1085,  1685,  2856,   180,  1062,   252,   122,   146,   549,\n",
      "           713, 19994,   256,   117, 14094,   117,   229, 22280,   259,  5708,\n",
      "           117,   449,   123,   327,  9211,   292,   125,  6929,   122,  9751,\n",
      "         17132,   591,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,   222, 19994, 22282, 20073,   122,  5546,   183,   125,  1977,\n",
      "          4678,  3111, 22288,   125,   230,  3791,  8440,  1118,  2856,   125,\n",
      "         20201,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   271,   179,   176, 16280, 22287,   744,   229,  6738,  3173,\n",
      "           351,   125,   872, 15617, 22278,   260,  1270,  8008,  1402,  7663,\n",
      "           180,   169,  8388,  6594,   180,  2954, 18431,   403,   117,  6784,\n",
      "          2579,   118,   176,   123,  3377,  7406,   124,   122,  1744,   124,\n",
      "           180,   527,  4626, 22278,   117,   179,  2798,   222,  4217,   397,\n",
      "           125, 10428,   261,  7955,   173,  2480,   313,  9193, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[  101,   123, 11451, 11348,   285,   117,   179,  4412, 22278,   125,\n",
      "          5693,   574,   538, 16450, 17351, 22281,   117,   222, 11237,   351,\n",
      "           146,   388,   122,  5078,   252,   118,  2036,   222,  5546,   183,\n",
      "           417,   130,   125,  6459, 13793,  4189,  2623,   247,   102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,   260,  8483,   171,  1690, 22280,   117,   188,  1609, 22285,\n",
      "          8156,  4851,   202,  1247,   180, 18853,   122,   173,  1089,  2038,\n",
      "          5580,   401,   423,   360,   215,   117, 15245, 22287,   230,  1877,\n",
      "          8973,  9247, 10557,   252,   122, 12448,   117,  2850,   125,  6039,\n",
      "           565,   303,   143,   125,   730, 16584, 19660,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,  5147,   117,   366,  6929,  2366,   923,  3049,  1149,  9301,\n",
      "           687,   484,   401,   125, 11334,  8362,   228,   118,   176,  9621,\n",
      "         22281,  1151,   289,  3103,   117,  5248,   271,   146,   528, 10575,\n",
      "         22282,   366,  8950, 19417, 13147,  2836,   118,   176, 15618,   293,\n",
      "           240,  1719,   123,   670,   662,   304,   692,   260,  3043, 18775,\n",
      "           138,   123,   964,  3552,   578,   146,   765,   397,  8833,   171,\n",
      "         19935, 10017,   351,   117, 10262,  1053,  1552,   944,   259,   736,\n",
      "          4708,   692,   118,   176,   125, 11471,   221, 11471,   260,  2796,\n",
      "          3724,   117,   259,  9361,   118,  1564,  3655, 11814,   118,   176,\n",
      "         19229, 18467, 22281,   123,  2954,   123,  3113,   285,   329,  1796,\n",
      "           338, 11198,   256,  1941,   117,   122,  1084,  1839,   366,  4103,\n",
      "         16819,  5334,   444,  3508,  1165,   442,   125,   854,  2028, 22281,\n",
      "           179,   744,   229, 22280,  6952, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,   202,   915,  5587,  7257, 22282,   179,   176,   547,   256,\n",
      "           117,  5873,   692,   118,   176,  3979,   128,   117, 10195,   125,\n",
      "         12514,   179, 11518,   304,   692,   117,   834,   176,  4945,   582,\n",
      "           117,   416, 22281, 10656,   125,   528, 19356,   128,   117,  8032,\n",
      "           125,  3922,   128,   117,   329, 17738,  4765,   125,  3922,  4242,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,   125,  1089, 15050,  8625,   228,  2459,   179, 16819, 15231,\n",
      "         20973,   329,  1796,   117,   229,  8130,   117,   123,  1956,   247,\n",
      "           266,   171,  5848,   421,   247,   117,   122,   259,  7406,   444,\n",
      "           117,   123,  3098,  2028,   298, 16551,   117,  7104,  2349,   692,\n",
      "           118,   176,  1315,   474,  3514,   117,  1632,   514,  9420,   118,\n",
      "           176,   123,  3377,   940,   171,   644,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,   180, 22283,   123,  1695,   117,   173,  1359,   366,  7395,\n",
      "           138,   495,   222,  1757,   309,  9437, 22287,  6478,   230, 11621,\n",
      "         10780, 13793, 17856, 18711,   375,   125, 11024,   122,  1154,  2890,\n",
      "         22281,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,  7226,   117,  4230,   736,   117, 11348,   692,   123,  1354,\n",
      "           117, 20990,  9468,   117, 15702,   171, 13831,   125,  6205, 22278,\n",
      "           179,  3235, 22282,   322,   180,  2847,   125,  7226,  1685,  1877,\n",
      "           793,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,   146,  1690, 22280, 15718,  2836,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,   260,  2459, 11736, 22287,  1941, 21254,   260,  8625,   138,\n",
      "           420,   260,   144,  3706,   221,   229, 22280,   260,  3848,  2430,\n",
      "          1413,   118,   176,   118,  7707,   123,   374,  8849,   285,  1444,\n",
      "         15182,   298,  4332,   942,   122,   171, 13227,   303,   117,   179,\n",
      "          2859, 12989,   923,   117, 16322,  2986,   146, 10881,  1364,   221,\n",
      "           146,  2979,   171,   504,   303,   259,  2217,   117,  3636,   229,\n",
      "         22280,   176,  5811,   692,   173,   229, 22280,  3848,  2430,   146,\n",
      "           423,   117,   320,   598,   342,  3285,   923,   123,  3049,   304,\n",
      "          1004, 15702,   180,  6205, 22278,   122, 15518,  2127, 14533,   170,\n",
      "           344,   304,   260,  9427,   138,   122,   260, 17897, 22281,   117,\n",
      "          5148,   348,   122,   968,  4212,   598,   260,  1877,   486,   180,\n",
      "           223, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,   260,  6929,   366,  4840, 12065, 22281,   229, 22280, 12604,\n",
      "           375,   692,   117,   495,   222,  8925,   122, 16325,   125,  1078,\n",
      "         16423, 22279,   117,   222,  4270,   122,  5197,   834,  1510,  2635,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   229, 22280,   176,  3174, 18444,  1084,  1839,   122, 16819,\n",
      "           744,  8650,  2537,   260, 14790,   138,   291,   260,  8625,   138,\n",
      "           260,   854,  2028, 22281,   229, 22280,   176, 21280,   320,  1223,\n",
      "           125,  1084,   925,   117, 19537,  2034,   692,   118,   176,  1369,\n",
      "           653,   117,   202,   853, 18983,   162,   298,  8001,   117,   240,\n",
      "           125,   839,   180,   418, 15976,   291,   202, 19726,   234,   366,\n",
      "          3428,   470,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   146,  7257, 22282,  1664,   351,   117, 21057,   348,   118,\n",
      "           176,   146,  1757,   309,  9437, 22287,   125,   944,   259,  1564,\n",
      "         11534,  2836,   118,   176,  1941,   176,   229, 22280,  5873,   692,\n",
      "         12514,  9694,   138,   117,   449,   222,   331,  1315,   286, 16949,\n",
      "           179, 16386,   151,  1364,   146,   549,   713,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   662,   304,   692,   123,  1434, 18937,   229,  5304,  5925,\n",
      "          2901,   692,   118,   176,  5489, 22281,   293,   143,   122,   398,\n",
      "          7173, 22281,  8362,   228,   118,   176,  6742,  1187,  8757,   122,\n",
      "          1174,  1557,  1941,   176,   229, 22280, 18402,   117,  9247,  5723,\n",
      "           118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101, 16280,   118,   176,  6621,  1718,  2349,   304, 22280, 13402,\n",
      "           789, 22278,   117,  6621,   866,   266, 21853,  1337,   125,  4924,\n",
      "          5874, 13004,   179, 15478,   546,   259,  1143,  8773,  2935,   229,\n",
      "         19068, 22278, 13305,   122, 12874,   403,   180,  1069,   117,   146,\n",
      "         15537,  6032,   125,  9205,   117,   123,  8727, 11074, 12458,   304,\n",
      "         22280,   125,  9815, 22282,   498,   123,  2480,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,   180,  4303,   180,  5304,   179, 10348,   221,   146,   549,\n",
      "           713, 16420,   122, 16819,   271,   928,  3670,  2636, 18937,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,   924,  9751,   171, 15796,   404, 19914,   118,   176,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,  4169,  1532,   123,   847,  1382,   124,   117,   179,   176,\n",
      "          1598, 13808,   123,   662,   934,   123, 15703,   180,  1105,   102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101,  2010,   149,   252,   623, 12674,  9247,   654,   740,   221,\n",
      "          3378,   117,   123,   629,   830, 13133,   222, 12617,   125,  9317,\n",
      "           176,  2354, 22279,   376,  2803,   830, 22305,   125, 10985,  1790,\n",
      "           117,  3985, 22278,   229,  4303,   117, 16143,   136,   123,   447,\n",
      "          4275, 22282,  4751,  2044, 14619,   210,   117, 13586, 17598, 12898,\n",
      "         22278,   123,  1354,  6592,   252,   240,   420,   146, 13227,   303,\n",
      "           122,   146, 20462,   180,  1124,  7137,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,   146,  1852,   458,  3033,   229,   418, 15976,   117,   170,\n",
      "           123,   327,   739,  2992,  8849,   123,  3049,   304,   122,   146,\n",
      "           347,  6465,   125, 16341, 10371, 15702,   171,  4332,   303,   117,\n",
      "           122,   262,  9994,   159,   173,  1423,   171,  4286,   247,   117,\n",
      "           123,  2521,   298,   958,  2118,   143,   117,  1074, 22087,   243,\n",
      "           123, 12588,  7485,   124,   498,   146,  2840,   735, 22279,   179,\n",
      "           368,  3748,   203, 20466,   246,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   173,  5255,  1011, 17477,   240,   230, 18637,   125,  9349,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,   260,   854,  2028, 22281,  4183, 14533,   118,   202,   117,\n",
      "           122,   117,   123,  3606,   304, 22280,   179,  1078,  2606,   291,\n",
      "          1078,  2397, 16653,   146,   372, 22280,   117, 10907,   256,   221,\n",
      "          1105,   170,   860, 12631,  1196,   598,   146, 11979,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,   230, 13348, 22278,   117,  2590,   240,   222,  5294,  2964,\n",
      "           157,  3165,   285,  1196,   117,  8544,   117,   964,  3552,  1552,\n",
      "         12448,   246,   146,   347,  5334,  2848,   268,   117,   125,  4303,\n",
      "           173,  4303,   117, 13375,   285,   240,   222,  2397, 21617,   125,\n",
      "          4372,  5822,   155,   125, 14121,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   146,  1757,   309,  9437, 22287, 19288,   320,   347,  1178,\n",
      "          7382,   102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   123,  7875,   125, 11282, 18836,   117,  1369,   653,   180,\n",
      "         13219,  2028,   117,   662,  1353,   123,  3212,   117,  2787,  7447,\n",
      "           348,   146,  1923,  1259,   170,   146,   347,   388,  6774, 10537,\n",
      "          8468,   125,   223, 11198,   123,  8886,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101,   260,  9393,  2684,   123,  5304,  9487,   923,   118,   176,\n",
      "           117, 12963,   118,   176,  1362,   792,  1817, 22282,  4060,   125,\n",
      "           928,  1185,   458,   502,  1051,   243,   102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,  2535,   117,   202,  1247,   366,  7395,   138,   305,   508,\n",
      "           692,   118,   176,  4840,   138,   125,   944,   259, 18190,   128,\n",
      "           117,   498, 22281,   375,   557,   260,   125, 18691,  2615, 22279,\n",
      "           170,   222,  4332,   303,   125,  4561,   173,  5530, 16280,   118,\n",
      "           176,   146,   338,   269,  4765,   180,  6205, 22278, 15356,   229,\n",
      "         14121,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,  1450, 11348, 17124, 22281, 16386,   923,  1941,   260,   675,\n",
      "           964,  1343,  1028,   860, 12396,   538, 16450, 17351, 22281,   123,\n",
      "         11451,   179,  4412, 22278,   125,  3848,   268,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[ 101,  905,  151,  256,  146, 1223,  102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101, 11103,   923,   366,  6742, 20666, 22281,   259, 22229,   128,\n",
      "          4966,   122,   260,  1261,  4242,  7649,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,   222,  3883,   304, 22280,   125, 14575,  3033,   170,   739,\n",
      "          1923,  1259,   125, 12446,   229,  5028,   117,  4431,   125,   230,\n",
      "           313, 10441,   159,   124,  7672, 13808,   313, 12371,   619,   285,\n",
      "           423,  3883, 20635,   598,   146,  9066,   157,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,   122,   117,   726,   785,   596,   117,  1191,   118,   176,\n",
      "           222,  2541,  2158,   125, 11641,   143,   102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101, 11502,   259, 20860, 22281,   125,  7714, 12837,   304,   122,\n",
      "           736,   125,  5849,   138,   122,  8446,   125,  1151, 22283,   331,\n",
      "           229, 22280, 16819,  3428,   477,   585,   117,  2113,  1021,  1615,\n",
      "          3428,   470,   202,   549,   713,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,  8198,   259,  1315,   474,   128,   449, 18001,   143,   117,\n",
      "           170,   260,   675,  4840,   138,   125,  6799,  8572,  2430,   151,\n",
      "           117,   170,   260,   675, 17730,   125,   822,   272,  1044,   122,\n",
      "          4674,   125, 10809,   122,   170,   146,   347, 14664,   125,   329,\n",
      "           934,  6684,   122,  5334, 19334,  1402,   117,   125, 14121,   118,\n",
      "           125,   118, 13185, 21510, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101,  1078, 12447,   947,   978,   146,   347,  2277,  1199,   125,\n",
      "          2533,   339,   159,   117, 11054,   118,   176,   146,  2397,   366,\n",
      "          9344,  6436,   842,   117,   170,   260,  2992,  8849, 22281,   171,\n",
      "         11371, 10090, 21794, 22281,   117,   123,  7800,   125, 17772,  2028,\n",
      "           117,   125,   222, 16341,   179,   368, 16555,   320, 20462,   102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,  3874,   325,   262,  8911,   171,   179,   146,   347,   652,\n",
      "           866,  2668,   268, 15777,  9238,   122,   866,  1441,   221, 13431,\n",
      "           210,  2044,   117,   271,   240, 16473, 22280,   117,   230,  5223,\n",
      "          5402,   125, 20543,   117,   179,  8198, 17586,  9365, 22282,   118,\n",
      "           176,  2461,   170,   739,  8043,   292,   117,   577,  2746,   118,\n",
      "           176,   118,  2036,   529, 11351,  3456,   421,  4851,   122, 14689,\n",
      "           348, 10262,  1436,  2411,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[  101,   146,  9344,  6436, 16510,   259,  4415,  5723,   170,   146,\n",
      "           766,   117,  1139,  5133,   151,   146,   347, 11371,   123,  4303,\n",
      "           366,   504,  4242,   117,   449,   259,  7395,  3148,   128,   229,\n",
      "         22280,   273, 20941,   122, 14372, 22287,   123, 21822,  1516, 22282,\n",
      "           117,  5646,  7831,   259,  2992,  7485,   128,   179,   146,  2397,\n",
      "          8170,  3514,   316,   321,   256,  3002, 15724,   320,   958,  2118,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101,   221,   792,   118,   176,  3039,   240,   222, 16423, 22279,\n",
      "           298,  3229,  3932,   495,  1395, 17551, 20949,   221,  1004,  5533,\n",
      "           222,  5078,  2895,   125,  9344,  6436,   842,   117,   498,   146,\n",
      "           615,   176,  8163, 11541,  2044,   117,   712,  5995,   128,   117,\n",
      "           146,   939,   298,  1449,  2668,   268,   143,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,   123,   681,   179,   176,   429,   123, 11348, 22282,   262,\n",
      "           123,   447,  1403,   124,   117,   240, 20025,   123,  1112, 16803,\n",
      "           324, 22354,   117,  3649, 18781, 22305,   117, 10420, 20193,   117,\n",
      "          5995,  1409, 11753, 20225,   122, 15618,  1409,   117,  7889, 22278,\n",
      "           125,  6032,   171,  2244,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,   978,   924,  8447,   117,   230, 12222,   122, 14136,   171,\n",
      "          4170,   117,  9480,   366, 19659,   117,   123,  1977,   331, 21612,\n",
      "           123,  1112,   366, 19659, 22354,   122,  1858,   171, 22285, 14837,\n",
      "         22278,   744,   117,   123,   872,   514, 22285,   117,   122,   325,\n",
      "           222,  1417,   117,   146,   762,   128,  5321,   117,  9778,  6751,\n",
      "           298,   644,  3471,   117,   179,  9247,  5723,  1971,   291,  1407,\n",
      "           179,   123,   223, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,   123,   366, 19659, 19575,   173,   327,   504,   508,   123,\n",
      "           670,   117,   449,  1719,   123, 20027,  1967,   256,   202,   549,\n",
      "           713,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[  101, 16241,  1537, 22287,  1369,  9679,   320,  4863,   176,   123,\n",
      "         16803,   324,   495,  4970,   256,   291,   273, 10214,   285,   259,\n",
      "          2292,   229, 22280,   176,  9821, 22287,  7226,   170,   259,   736,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101,   123,   366, 19659,   117,  1141,   117, 18601, 22287,   179,\n",
      "          1796, 12222,   122,   179,  7453,   124,   146,  4170,   221,  3285,\n",
      "           140,   118,   176,   170,   222,  2397,   171,  1847,   523,   122,\n",
      "           179,   860,   117, 20813,   214,   118,   176,   221,   123,  2480,\n",
      "           122,   229, 22280, 19608,   969,   154,   118,  1084,   320,  1950,\n",
      "         10490,   157,   117,  4314, 22278,   146, 17796,   173,   347,  1247,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[ 101, 2471, 4698,  122, 1685,  481,  102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[  101,   872,   514, 22285, 17668,   102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101,   730,   945,   285,   117,  6251,  8149,   324,   122,  2124,\n",
      "           117,   170,   230,   258, 11837,   508,   125, 17275,   180,   327,\n",
      "          2477,  4602,  1076,   117, 15832,   214,   271,  2787,  4838, 22278,\n",
      "           240,   420,   259, 17084,   298, 13254,   143,   179,   123, 11666,\n",
      "           834,   333,   221,  6759,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101,  2787,   703,  2836,  1004,   122,  9679,  1434, 11451,  7352,\n",
      "           125,  2397,   170,  5747, 15156,   304, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101,   320,  1341,   180,   447,  1403,   124,   262,  7201,   118,\n",
      "           176,   123,   327,   964,   324,   123,   527,  3209,   154,  7714,\n",
      "           118,  3848, 22279,   117,  2509,   117,  7352,   117,  2606,   125,\n",
      "          1202,  2037,   158,   130,   117,   222,  1124, 14273,   125,  9836,\n",
      "           481,   117, 11793,   125,   661,  1144,   117,   337,   701,   713,\n",
      "           117,   125,   739,  4351,   339,   272,  7967,   117,   179,  1391,\n",
      "          1684,  3240, 21102,   201,   122,   222, 13702,   125, 14790,   138,\n",
      "         11563,  2787,   703,   401,   122, 11967, 22280,   143,  1743,   459,\n",
      "           229,  5546,   285,   117,   625,  1011,   125,  1312,   303,   102]])\n",
      "DEBUG: Tokenized sentence 50: tensor([[  101, 14619,   210,  2365,  2292,   117,   449,   744,  4385,   117,\n",
      "           222,   298,  1647,   117,   123,   717,   741,   117,  9584,   229,\n",
      "           651,   170,   123,  5836, 11324,   179,   176, 12771,  2836,  3914,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[  101,   418,  5836, 11324,   495,   230,   144,  8351,   125,  7187,\n",
      "           592,   118,  7911,   221,  5530,   117,   123,   447,  8723, 22279,\n",
      "           117,   170,   425,   825,   229,   651,   102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[ 101, 5160, 2346,  351, 4871,  102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[  101,  1202,  2037,   158,   130,   117,   173,  1105,   117,   123,\n",
      "          5314,   125, 21158,   117,   538,   532,  4004,  3152,   122,   229,\n",
      "           327,  7924,  1950, 17789,  3632,   285,   117,   495,   785,  1690,\n",
      "         22280,   170,   259,  9288,   125,   418, 15976,   117, 10415,   256,\n",
      "           117, 18419,   122,  5911,   304,   256,   117,   449, 19876,  4212,\n",
      "           146,  9470,   117,   568,   371,   214,   146,  4351,   339,   272,\n",
      "           122,  4276,  3638,   214,   123,   327,  1224, 10224,   154,   117,\n",
      "           170,   179,   978,   146, 12215,   125, 13526,   193,   702,   260,\n",
      "         14790,   138,   125,   235,  3198,   117, 16241,  1537, 22287,   325,\n",
      "          2036,  1413,   259, 12141,   122,   318, 13793,   123,   944, 18402,\n",
      "          3689, 22280,   122,   240,  5530,   171, 20462,   102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101,   123,  2606,   117,   123,  1977,   368,   331, 10348,  5023,\n",
      "           625,   229, 22280,  1011,  5546,  6859,   117,   495,   125,   230,\n",
      "          3578,   687,   292, 21605,  4456, 22290,   202,   549,   713,   117,\n",
      "          3578,   687,   292,   834,  1707,   373,   117,  2113,  8940,   180,\n",
      "          6738,  3173,   351,   171,   347,  2829,   310,   122,   229, 22280,\n",
      "           171, 11717,   342,   171,   347,  1354,   367,   102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101,  1982,  3914,   429,   118,   176,   123,  3212,   123,   447,\n",
      "           451,  9147,   117,  2606,   125,   222,  1718,  1339,  1565,  5782,\n",
      "           300,   117,  3649,  3113,   122,   331,  2382,   117,   125,  7714,\n",
      "         22281,  3072, 22281,   117,   170,   230,  7989,  1749,   415,   125,\n",
      "          1301,  1721,   420,   260,   675, 13511,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101, 16246,   118,   176,   123,  4735, 22278,   117,   230,  6365,\n",
      "         15439,  7492,   117,  1423,  6220,   154,   117,   123,  1977, 16598,\n",
      "           692,   944,  1676,  9429, 22281,   125,   179,   331,   740,  1598,\n",
      "         13808,   221,  7489,  2964,  2317, 15266,  1440,   138,   122, 16633,\n",
      "         14594, 22281,   240,  1423,   125,   184,  6471,   122, 18190, 15035,\n",
      "         22281,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[  101,   495,  5762,  2996, 22278,   117, 15618,   375,   117, 12448,\n",
      "           117,   170,  5708,   273,   256,   364,   442,   117, 12141, 17584,\n",
      "         22281,   123,  9324,   252,   117,  6335,  8993,   117,   271, 12141,\n",
      "           125,   329, 22280,   117, 13841, 21990,   128,   117,  3235, 22282,\n",
      "          9539, 22281,   122,   744,  2582,   850,  2440,   180,  2169,   102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[  101, 21612,   118,  2036,  1112,  5782,  2037, 22354,   102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[  101,   700, 16246, 22287,   118,   176,   123,   528, 16527,   122,\n",
      "           325,   123,   327,  2267, 12252,   624,   102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101,   123,   681,   117,  1124,  7137,  2557,   117,   785,  1467,\n",
      "           122,  3791,   251,   173, 14155,  1080,   123,   327,  1105,  1011,\n",
      "          1684,   222,   328,   366, 14269, 11348,  1495,   102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101,   173,  2036,   305,  1051,   214,   146, 11775,  6590,  5078,\n",
      "           252,   118,   176,  2044,   123,  1632, 10656,   117,   123,  8525,\n",
      "          6070,  1154,  2042,  2507,   117,   122,   117,   625,   123, 16001,\n",
      "           495,   739,   117,  4063,   151,   123, 10467,   222,  3568,   272,\n",
      "           125,  6205, 22278,   122,  7496,  2127,  2836,   118,   146,   170,\n",
      "         18928, 22278,   423,  1690, 22280,   180,  4767,   102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101,   123,  2267,   978,  8184,   481,   117,   123,  6614,   125,\n",
      "           222,  1623,  1460,  8833,   117, 10786,   942,  3260,  2301,   117,\n",
      "         22003, 22281, 12141,   117,  5708, 19366,  1408,   128,   125, 12361,\n",
      "           304,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[  101,  1719,   740,  1011,   123, 10381,  2397,   117,   449, 12595,\n",
      "           256,   744,   123,   327,  2477,  4602,  1076,   122,   229, 22280,\n",
      "          9262,   151,   117,  2798,   123,   223, 22280,   125,  4023,  7148,\n",
      "           117,   712,   577,  1289,   125,  4141, 13793,  2415, 22280,   117,\n",
      "           179,   123, 17090,   305,  1051, 22282,   123,  1073,   303,   125,\n",
      "          4296, 21386, 22280,   143,   229,  3659,   122,   202,  5274,   366,\n",
      "         18937,   179, 12252,   624,  5057, 14316,   123,  5304,   102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[  101,   700,  1413,   118,   176,   123,  7492,   847, 12764,   178,\n",
      "           117,  3413,   122,   117, 10502,   847, 12764,   178,   117,  2113,\n",
      "          1369,   229,   418, 15976,  7707, 12119, 14533,   944,  5288,   996,\n",
      "           304, 22280,   117, 15627,   251,  1676,   675, 11402, 10278,   125,\n",
      "          2760,   179,  1941,  1023,  3896,   230,  6754,  2606,  9652,   125,\n",
      "           273,  1289,   382,   102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[  101,  1796, 12222,   170,   146,  8517,   125,   230,  1105,   125,\n",
      "          1690,  7817, 22281,   117,   179, 13935,   122,  7231, 12334,   118,\n",
      "           176,   117,  4513,   118,  2036,   230,  2267,   785,   171,   265,\n",
      "           508,   122, 14578,   117,   123,  1977,   847, 12764,   178, 19978,\n",
      "           203,  2745,   221,  6974, 22282,   117,  3951,   118,  2036,  5686,\n",
      "          2684,   125,  1546,   143,   102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101,   978,   230,  1354, 13376,  3173,   154,   125,  7492,  3649,\n",
      "          7427,   154,   117,   179,  1941,   262, 13717,   285,   117,  1151,\n",
      "          2041,  5949,  3848,   143,   125,   412,  2028, 22281,   570,  6021,\n",
      "         14004, 22281,   117,   179,  2036, 15231,   923,   298, 18694,   180,\n",
      "          9463,   271,   629, 19161, 22281, 16026, 22281, 19873,  7769,   202,\n",
      "           179,  1391,   117,  5708, 17291,   683,   117,  1684,  5334,  6031,\n",
      "         22281,  2787,  8551,   442,  1676,  1877,   269,  4323,   102]])\n",
      "DEBUG: Tokenized sentence 67: tensor([[  101, 14537,  2836,   173, 20674, 22281,   498,   260,  4137,   146,\n",
      "          3240, 22281,   293, 10881,  9247, 10557,   268,   877,   487,   125,\n",
      "          2389, 22279, 22280,   125, 13501,   214,   138, 20460,   102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[  101,   625,  8625, 22278,   123,  4768,  5078,   252,   222,  2337,\n",
      "           140,   300, 12413,   125, 17678, 13305,   117,  7093,  4238, 11538,\n",
      "           285,   117,  3561,  8625, 22278,   229, 22280,  5057,  1315,  1557,\n",
      "           117,   122,   222,  3043,  1531,  4619, 14838,   179,  2036, 10348,\n",
      "           123,  1364,   146,  1831,   222, 18190, 22280,  8685, 11540, 22290,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101,   180,   327, 19023, 18550,   331,  2036,  4412, 22278,   230,\n",
      "          8097,   125,  6372, 22279,   125,  2987,   117,   229,   615,   123,\n",
      "         10152,   715,   415, 17704, 13779,  3502, 22279,  2836,  2535,   117,\n",
      "          4217,   364,   214,   123,  1078, 13779,  1378,   102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[  101,   123,  2267,   495,   123, 12252,   171,   549,   713,   102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101, 21612,   118,  2036,   302,   282,   508,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101, 19543,   117,  4460,   179,  9525,  1694,   122,  8089,  1337,\n",
      "           320,   169, 14116,  2009,  7406,   124,   117,   785,  1877,   328,\n",
      "           117,   170,  7226, 12400,   125,  9586,   125,  3264, 20027,   102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101,   123,   223, 22279,   229, 22280,  2036, 12122, 11348, 22282,\n",
      "           117,  2798,  2787,   703,   159,   117,   653,  2113,   146,  6815,\n",
      "         22280,   123,  4535,   364,  9434,   246,   102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101,   978,   146,   347, 20743,   117,   146,  4141, 13793,   180,\n",
      "          4203,   117,   390,   303,   171,  1847,   523,   117, 14031,   171,\n",
      "          9019, 13793,   122,   298,  8229,   117,   170,   785,  3753,   117,\n",
      "           122,   179,   123,  2251,  5630,   122, 18574,  1065,  1283,   194,\n",
      "           343,   449, 10502,   847, 12764,   178,   229, 22280,  4750,   179,\n",
      "           146,  2982,   176, 19636,  1941,   102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[  101,   122,   179,   302,   282,   508,   117,   438,  2746,  1369,\n",
      "           138,   954, 13417,   481,   117,   229, 22280,   978,   744, 12659,\n",
      "           123,  3402,   146,  4793,   265, 22280, 14399,   180,   879,   604,\n",
      "          1076,   117,  2440,   171,  1757,   821,   180,  7492,   122,   298,\n",
      "         19978,   501,   179,   418,  5057,   221, 10241,   123,  3979,   304,\n",
      "           260,   543,  1360,   303,   143,   171,  6815, 22280,   122,   229,\n",
      "         22280,  3207, 22282,   123,  2267,   146,  2892,   273,   415, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101,   202,  1325,   117,   144,  7647,  4566,  2982, 10090,   151,\n",
      "           123, 15685,   125,  4903,   117,  2113,   146,  4203,   117,  1004,\n",
      "         12531,   271,   176, 16995,   173,  1105,   125,   222,  7392,   347,\n",
      "           117,   125,  1977,   325,  1373,  1021,   125,   333, 17796,   117,\n",
      "          1744, 16959,   256,   117,  2044,   179,  5896, 22281,   236,   125,\n",
      "          1177,   117,   398,   569, 22283,   118, 10497,   320,   347, 19262,\n",
      "         16146, 22280,  1979,   102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[  101,   123,  6754,  7492, 15834,   256,   118,   176,   170,   146,\n",
      "          2099,   122, 21315,   123,  4023,   117,  1485,   260, 13674,   117,\n",
      "          1075,   125, 18165,   117,   179,   260, 16889, 22281,   236,   122,\n",
      "          7940,  4886,   123,  2267,   230,   416,   304,   316, 22280,  2281,\n",
      "           179,   368,  5057,   117,   834,  7523,   304, 22280,   125,  9607,\n",
      "           951,   117,   123,  2523,   138, 10173,  6943, 22281,  1021,   423,\n",
      "          1147,   449,   117,   123, 21736,   125,  3846, 20045, 22280,   117,\n",
      "           240,  5664,  3963,  1014,  1069,  2380, 15500,   322,   179,   123,\n",
      "           327,  3113,  4103,   236,  1075,   125,  1112,   333,  2606, 22354,\n",
      "           117,   271, 10355,   740,   102]])\n",
      "DEBUG: Tokenized sentence 78: tensor([[  101,   122,  1112,   179,  5308, 22281,  6867,  1084,  5961,   146,\n",
      "         14914,   117,  3486,  1399,   179,   229, 22280,   495,   693,   403,\n",
      "           117,  2798,   978, 13071,   117,  2822,  2397,   123,   230,   390,\n",
      "           304,   179,   744,   229, 22280,  1796,  3248,   285,  1676,  5080,\n",
      "           229, 22280,  1075,   873,   118,  1084,   969,  7700,  1719,   123,\n",
      "          1069,   122,  4412,   210,  4903,  1273, 10742,   221,  1684,  6086,\n",
      "         21929,   180,   418, 15976, 22354,  1084,   202,   549,   713,  2072,\n",
      "           944,   123,   332,  1014,  4131, 22278,   229, 22280,   495, 11021,\n",
      "           221, 16241,  1537, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 79: tensor([[  101,   122,   229, 22280,   176,  9882,   222,   644,   179,   229,\n",
      "         22280,  6185,  1557,  6867,   924,   122,  1510, 22281,  1176,   123,\n",
      "          7492,   170,  3769, 16264,  2010,   318, 13793,   136,  1941,  3429,\n",
      "           136,  2010,   240,   179,   229, 22280,  2137,   259, 17052, 22281,\n",
      "           125,   528,   136,  2010,   240,   179,   229, 22280,  3196,  1342,\n",
      "          6815, 22280,   136,  2010,  2779,   117,   176,  2589,   123, 17704,\n",
      "           117,  1105,   256,   118,   259,  1016,   653,   123,  7492,  3989,\n",
      "          1399,  4111,   179,   123, 15685,   229, 22280,   176,  3283,   371,\n",
      "           221,   740,   102]])\n",
      "DEBUG: Tokenized sentence 80: tensor([[ 101,  122, 4217,  364,  256,  398, 4861,  285,  102]])\n",
      "DEBUG: Tokenized sentence 81: tensor([[  101,   625,   146,  4203,  1183,   351,   700,   180,   327, 18237,\n",
      "           304, 22280,   221, 10964,   123, 14308,   117,   259,  6310,   180,\n",
      "           418, 15976,  7104,  2349,   692,   118,   202,   173, 22243, 22280,\n",
      "           170,   222,  3953,   293,   388,   125, 10497,  2896,   122, 13779,\n",
      "          1200,   117, 20045,   308,   316,   218,  1121,   240,  6086, 11314,\n",
      "           657,   714,   117,   598,   146,   615,   229, 22280,  1201,   923,\n",
      "          2798,   653,   260,  9429, 22281,   180,  5782,  2037,   102]])\n",
      "DEBUG: Tokenized sentence 82: tensor([[ 101,  302,  282,  508,  495,  785, 3189,  328,  240, 1719, 7583, 9349,\n",
      "          102]])\n",
      "DEBUG: Tokenized sentence 83: tensor([[  101,   495,  1977,  2036,  1633,   619,   260,  6536,  1977,   173,\n",
      "          1250,  5057,   146,   577, 22290,   221,   260, 11348, 17124, 22281,\n",
      "          1977,  4551,   256,   260, 11169,  1977, 19449, 22278,   146,  1955,\n",
      "           221,   259,   179,  8204,  4368,  9226,   102]])\n",
      "DEBUG: Tokenized sentence 84: tensor([[  101,   466,   852,   692,   118,   229,   170,   785,  3953,   122,\n",
      "         21280,   118,  2036,  4400,   117,   146,   179,  2036, 12122,  4863,\n",
      "         13702, 15404,   102]])\n",
      "DEBUG: Tokenized sentence 85: tensor([[  101,  6952,   256,  1684,   125, 11967,  4242,   291, 14230,  5321,\n",
      "         22281,   170,  5899, 22281,   125,   549,   117,   347, 12413,   125,\n",
      "          1224,   343,  2787,   703,   201,   978,   260,   675,  4141,   151,\n",
      "          7248, 22281,   221,  5197,   123,  4768,   117,   122,   117,   712,\n",
      "         18433,   117,  1977,   123,  1377, 22281,   236,   123, 13544,   229,\n",
      "          2567,   125,   629, 22280,  4141, 13793,  3985,   384,   117,   229,\n",
      "         22280,  1467,  4051,   125, 18224, 22282,   179,   740, 19575,   173,\n",
      "           549,   713,   102]])\n",
      "DEBUG: Tokenized sentence 86: tensor([[  101,  3030, 15736,   123,   450, 22278,   366,  2796, 11348, 17124,\n",
      "         22281,   117,   146,   313, 11284, 22280,   117,   222, 10738,  4613,\n",
      "          1817,   243,   117, 13141,   117,   549,   125, 17700,   339, 11899,\n",
      "           286,   122,   170,   222, 11753,   994, 17291,   268,   117, 17579,\n",
      "          2836,   243,   122,  6754,   117,   179,  2036, 11314, 22278,   117,\n",
      "          1532,   331,  1999,   117,  2684,   320, 13227, 11661,   268,  3848,\n",
      "         22279,   122,  2135, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 87: tensor([[  101,   495, 11348, 17892,   122,  9584,  1684,   420,   260,  2459,\n",
      "           117,   170,  1977,  1941,  1011,   316, 22280,  8043,  2303,   179,\n",
      "          2859,   146, 14915, 22287,   271,   123,   230,  2760,   171,   653,\n",
      "          5497,   173,   543,   194,   304,  2461, 18402, 22287,   125,  4486,\n",
      "           179,   229, 22280,  5924,   923,   173,   543,   194,   304,   125,\n",
      "          1342,  2397,  9144,   118,   202,  2684, 14809,  9238,   298,   532,\n",
      "          3165,   143,   122,   366,   675,   851, 14266, 22281,   117,   170,\n",
      "           230,  6251, 17932, 22278,   179,   146,   229, 22280,  6838,   256,\n",
      "           117,  2798,   271,   619,   102]])\n",
      "DEBUG: Tokenized sentence 88: tensor([[  101,   625,   222,  4672, 13672,   256,   291,   924, 19763,   176,\n",
      "          3623,   692,   117,   495,  1684,   313, 11284, 22280,  1977, 14915,\n",
      "           125, 16048,   151,   118, 15887,   117,   294,   890,   214,   260,\n",
      "          2459,   123,  5997,   151,   102]])\n",
      "DEBUG: Tokenized sentence 89: tensor([[  101,  4654,   555, 12771,  2836,   118,   176,   125, 15210, 22282,\n",
      "           146,   577, 22290,   366,  8229,   117,   240,  3330,  1758,   449,\n",
      "           230,   576,   117,  6738,   123,   230,  6527, 22278,   125,  5090,\n",
      "           117,  7320,   118,  2036,  1084,   117, 16241,  1537, 22287,  9679,\n",
      "           240,   179,   117,   230,   623, 12051,   125,  4102,   128,   117,\n",
      "           122,   146,  6754,   118,   644,   492,  3436,   203,   318, 13793,\n",
      "           117,   420,  1084, 20739,   122,  7078,   942,   117,   179,  2364,\n",
      "           325,   176, 20241,  7518,   125,  3859,   259,   577,   145,   102]])\n",
      "DEBUG: Tokenized sentence 90: tensor([[  101,   122,   180, 22283,   173,  4271,   117,   170,  3901,   117,\n",
      "           229, 22280,  3456,   285,   256,   259,   766,  6199, 22281,   171,\n",
      "           549,   713,   117,   123,   229, 22280,   333,   538,  1564,   125,\n",
      "          7095,   117,   173,   179,  8544,   117, 12413,   125,  4654,   934,\n",
      "           387,   117,  7282,   159,   123,  1373,  1676,  5207,   122,   123,\n",
      "          2954,  4654,   934,   538, 21145, 22281,   298, 17863,   102]])\n",
      "DEBUG: Tokenized sentence 91: tensor([[  101,   978,  5907,  1568,  2037, 22280,   240,  1966,  7465, 12411,\n",
      "          1442, 21206,  3495,   726,   146,   622,   221,  3598,   578,  1364,\n",
      "           170,   123,   449,  5105,   285,   102]])\n",
      "DEBUG: Tokenized sentence 92: tensor([[  101,   122, 16241,  1537, 22287,   146,  7339,   117, 11388,   291,\n",
      "           644,   125,  2767,   117, 11348,   214,   291, 12604, 22087,   243,\n",
      "           117,   179,   229, 22280, 10606,   170,   123,   327, 14790, 22278,\n",
      "          7352,  2787,   703,   251,   117,   123,   327,  7924,  1743,   321,\n",
      "           117,   222, 16936,   303,   320, 13227,   303,   117,   122,   117,\n",
      "          8650,   825,   123, 11214, 22278,   117,   222,  5429,   162,   179,\n",
      "          2036, 11314, 22278,   498,   260, 11351,   271,   230,  8625, 22278,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 93: tensor([[  101,   229, 22280, 19016,   256,   117,   229, 22280,  5167,   151,\n",
      "          5791,   382,   122, 16555,  1684,   260,   223,   128, 13605,   401,\n",
      "           122,   222,   649,   102]])\n",
      "DEBUG: Tokenized sentence 94: tensor([[  101,  6621,  1062,   252, 14748, 22278,   118,   176,   744,   222,\n",
      "          1695,   325, 18253, 14960,   243,   179,   171, 12215,   117,  2113,\n",
      "          3852, 22278,  3002,   123,  2954,   102]])\n",
      "DEBUG: Tokenized sentence 95: tensor([[  101,   123,  7492,   847, 12764,   178,   117,   179,  2036,  9086,\n",
      "           320,  1341,  9657,   117,  8362,   214,   118,   146,  4217,   364,\n",
      "         22282,   170,   601,   544,   340,   117, 16795,   118,  2036,   146,\n",
      "           179,   978,   102]])\n",
      "DEBUG: Tokenized sentence 96: tensor([[  101,   123, 22296,  5747,  3848,  2993,   125,  1831,   122,   230,\n",
      "          8993,   285,   171, 16026,   179,   146,   229, 22280,  5308,   256,\n",
      "           123,  7492,   746,  3529,  2086, 11935,  1530,   117,   122,  5157,\n",
      "           259,   682,   117,   202,  1423,   125,  1719,  7583,  1069,   117,\n",
      "           123,  5961, 12448,   246,   498,  3848,   687,   562,   102]])\n",
      "DEBUG: Tokenized sentence 97: tensor([[  101,   122,   117,  1139,   117,   202,  4745,   180, 15433,   364,\n",
      "           117,   123, 16803,   324,   117,   123,   527,  3209,   154,   117,\n",
      "           123,   447,   451,  9147,   117,   123,  5782,  2037,   117,   123,\n",
      "           528, 16527,   122,   327,  2267, 10415,   692,   125,   964,   324,\n",
      "           123,   964,   324,   117, 10420,  2537,   122,  1821,   834,   176,\n",
      "          9226,   210,   117,   123,  4410,   222,  1971,   822, 20733,  1941,\n",
      "           423,  1312,   303,   117,   975,  1885,   185,  4687,   117, 12544,\n",
      "           954,   274,   364,   249,   117,   547,   256,   118,   176,   222,\n",
      "          1160,  5302,   455,   125, 11348, 17124, 22281,   117,   179,   417,\n",
      "          1797,   923,   125,  1796,   117, 17754,   591,   125,  5359,  3706,\n",
      "           117,   122, 16420,  1315,   474,  3514, 12086, 19277,   320,  1341,\n",
      "         11368,   366,  1028,   117,   420,   230,   762,   343,   304, 22280,\n",
      "           834,  1510,  2635,   117,   582,   176,   229, 22280,  7194,   151,\n",
      "           146,   179,   495,  3922,   268,  1165,   122,   146,   179,   495,\n",
      "         13672,   102]])\n",
      "DEBUG: Tokenized sentence 98: tensor([[  101,   230,   123,   230, 19022, 22287,   118,   176,  1485,   260,\n",
      "           964,  1343,   102]])\n",
      "DEBUG: Tokenized sentence 99: tensor([[  101,   122,   125,   944,   259,   504,  1522,   171,   549,   713,\n",
      "          8625,   228,  2217,   221,   260,   675, 18237,   303,   143,   102]])\n",
      "DEBUG: Tokenized sentence 100: tensor([[  101,   240,   230,  4303,   179,  1021,   320,  4707,   180,   418,\n",
      "         15976,  4285,  6451,   259,  5684,   180,  1449,  1272,   117,   171,\n",
      "           323,  8940,  2535,   146,  2582,  4029, 22282,   298, 21672,   247,\n",
      "           143,   122,   366, 12307,   823,   470,   102]])\n",
      "DEBUG: Tokenized sentence 101: tensor([[  101,   146, 15796,   404,   117,   125, 14790,   138,   125,   235,\n",
      "          3198,   117,  1690,  7817,  2979,   122,   498,  1149,  3391, 22278,\n",
      "         13305,   117,  1367,  1084,  1796,   117,   173,  3420,   221,   146,\n",
      "          5209,   210,   117,  8582,   423,  2678, 22285,  3132,   179,  8544,\n",
      "           221,   260,  6880,   102]])\n",
      "DEBUG: Tokenized sentence 102: tensor([[  101,   146,  1202,  2037,   158,   130,   117,   179, 13956, 22278,\n",
      "           125,  1312,   303,  1921, 12495,   117,  3033, 18618,   117,  7570,\n",
      "           203,   146,  4286,   247,   117,   834,  5961,   123, 16241,  1537,\n",
      "         22287,   117,  2798,   653,   123,  2606,   117,   122, 12401, 13665,\n",
      "           118,   176,   123,  1105,   117,   221, 18165,   102]])\n",
      "DEBUG: Tokenized sentence 103: tensor([[  101,   222,   939,   125,   449, 18001,   143,   117,   146,  2607,\n",
      "          3906,   117,   146,   302, 17168, 22280,   117,   146,  1546,  8747,\n",
      "           122,   146,  1961,   952,   117, 15078,  1078,   615,   170,   123,\n",
      "           327,   739,  8097,   125, 10187,  2245, 21947,   138,   117,  5127,\n",
      "           221,   123, 13693,  3391, 13793,   125,   944,   259,  1564,   117,\n",
      "         11518,  2746,   122,  1174,  1537,  9420,   173,  4693,   102]])\n",
      "DEBUG: Tokenized sentence 104: tensor([[  101,   222, 13254,   373,   125, 12581,   183,  3033,   180,  4768,\n",
      "           122,   262, 10573, 22282,   123, 16803,   324,   412,   149,   252,\n",
      "          6188, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 105: tensor([[  101,  2010,   123,  6188, 22278,   475,  1721,   136, 18661,   329,\n",
      "           659, 22032,   252,  3003,  1564,   179,   740,   388,   172, 16681,\n",
      "           123,   447,   451,  9147, 11535,  2044,   179,   123,  1124,  7137,\n",
      "          1011,   170, 11304,   125,  9196,   272,   421,   170,   146,  5101,\n",
      "           283,   102]])\n",
      "DEBUG: Tokenized sentence 106: tensor([[ 101, 2010,  179, 5101,  283,  136, 6185, 2904,  527, 3209,  154,  102]])\n",
      "DEBUG: Tokenized sentence 107: tensor([[  101,  2010,  6086,  5351,  5630, 22281,   303,   179,   176,  3285,\n",
      "           151,   260,  1176,   123, 22283,   170,   740,   102]])\n",
      "DEBUG: Tokenized sentence 108: tensor([[ 101, 1331,  179,  122,  745,  458,  102]])\n",
      "DEBUG: Tokenized sentence 109: tensor([[  101,  2010,   740,  3171,   118,   176,   136, 16795,   146,  3265,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 110: tensor([[  101,  2010,   229, 22280,   117,  1996,   123, 16803,   324,   146,\n",
      "          3147,   418, 10371,   117,   449,   123,  1124,  7137,   376,  4486,\n",
      "          1084,   102]])\n",
      "DEBUG: Tokenized sentence 111: tensor([[  101,  2354, 22279,   146,   179,  4750,   136,  2010,  8940, 10467,\n",
      "           230, 11451,   179,   418,   170,   740,   102]])\n",
      "DEBUG: Tokenized sentence 112: tensor([[  101,  2010,   229, 22280, 18661,   117,  1417,   117, 10573,   229,\n",
      "          5304,   320,  4141, 13793,  2415, 22280,   117,   179,  5787,   437,\n",
      "          6022,  4640,  3933,  5664,   102]])\n",
      "DEBUG: Tokenized sentence 113: tensor([[  101,  2010,  1369,   136,  2010,  1141,   117,  3265,   117,  6621,\n",
      "          4303,   117,   582,   123, 13305,   171, 20860,   418, 13394,   146,\n",
      "           644,   492,  2389,   252,   179, 12249,   138,   123, 21370,   304,\n",
      "           125,   360,   215,  1941,   176,  4970,   179,  7716,   136,  4048,\n",
      "           179,   229, 22280,   873,   582, 12249, 22278,   860,  9849,   125,\n",
      "           854,  2028,   122,   117,  4428,   214,   179,   146,  1417,   117,\n",
      "           146,   762,   128,  5321,   117,   176,  4621,   256,   221,  5357,\n",
      "           146,  1247,   171,  1342,   179,  1941,   176,  8544,  2010,  8625,\n",
      "           180, 22283,   117,  5023, 14619,   210,   117,  1143,   185,  1941,\n",
      "           905,   562,   229, 15831,   304, 22280,   125,   944,   259,  1564,\n",
      "           136,  3539,   221,   329,   117,   179,  1904, 22281,   449,   117,\n",
      "           122,  3295,   117,   179,  5637, 22281,  5023,   179,   229, 22280,\n",
      "          2541, 22281,   494,   159,   123,  3428,   154,   171,   662, 14455,\n",
      "           136,  2010,   368,  1996,  3185,  2097,   179,  2779,  2535,  2589,\n",
      "           123,  1373,   117,   179,   495,  1407,   102]])\n",
      "DEBUG: Tokenized sentence 114: tensor([[  101,  2010,   123, 22296,   122, 22032,   252,   117,   229, 22280,\n",
      "           437,  6969,  1149,   117,  1216,   259,   682,   592,   118,  7911,\n",
      "           117,   179,   122,  1338,   171,   454,   102]])\n",
      "DEBUG: Tokenized sentence 115: tensor([[  101,  2389,   252,  2541,  1084,  1839,   122,  1331,   123,   872,\n",
      "           514, 22285,   179,   437,  8853,   123, 11451,   179,  3429,  3185,\n",
      "          2097,   123,  2954,   102]])\n",
      "DEBUG: Tokenized sentence 116: tensor([[  101,   146,  3265, 18518,   118,   176,   125,  1742,   117,   122,\n",
      "           740,  2036,  9247,   654,   229,  7062,  2010,   122,   179,   229,\n",
      "         22280,   302, 13808,   146,  1314,   439,   201,   202,  4848,   834,\n",
      "          2779,   370,  1084, 19480,   230, 10415,   738,   671, 12682,   186,\n",
      "           118,   176,   202,  4745,   180,   450, 22278,   125, 11348, 17124,\n",
      "         22281,   123,  3953,   180,  6188, 22278,   475,  1721,   102]])\n",
      "DEBUG: Tokenized sentence 117: tensor([[ 101, 2010,  122,  171,  328,  653,  102]])\n",
      "DEBUG: Tokenized sentence 118: tensor([[  101, 11638,   256,   527,  3209,   154,   102]])\n",
      "DEBUG: Tokenized sentence 119: tensor([[  101,  3285,   140,   118,   176,   229,  9196,   272,   421,   834,\n",
      "          2822,  1284,   180, 11451,   179,  2036, 13030,   228,   102]])\n",
      "DEBUG: Tokenized sentence 120: tensor([[ 101, 1016,  607,  125, 4412,  834,  222,  958, 2118,  102]])\n",
      "DEBUG: Tokenized sentence 121: tensor([[  101,  2010,  7583,   229, 22280,  8530,  1235,   343,   325,   102]])\n",
      "DEBUG: Tokenized sentence 122: tensor([[ 101, 1078,  576, 1968, 2684,  325,  502, 1051,  285,  102]])\n",
      "DEBUG: Tokenized sentence 123: tensor([[  101,  4048,   179,   376,  4848,   202, 19036, 22280,   706,  5110,\n",
      "           146,  1312,   303,   179, 20075,   117, 15921, 12659,   272,   117,\n",
      "          2541,  2745,   258,  1341,  2389,   252,   146,   179,  5127,   146,\n",
      "           622,  3714,   170,   123,  4939,   180,  2377,   252,   102]])\n",
      "DEBUG: Tokenized sentence 124: tensor([[  101,  2010,   318, 13793,  2535,   117,   170,   860,  1124, 14273,\n",
      "           117,   146,  5101,   283,   117,   122,   230,  7666,   118,   792,\n",
      "         12268,   334, 22361,   577,   644,   117,  1502,  2354, 22279,   229,\n",
      "         22280,  4970,   136,  6777,   123, 22283,  1532,  5167, 11237,   364,\n",
      "           117,   123,  4654,   934,   122,  8032,   123, 14643,   117,   179,\n",
      "          2798, 18661,   146,   179,  9821,  4023,   437,  3039,  2010,   221,\n",
      "          2745,   607,  2856,   122,   607,  1564,   102]])\n",
      "DEBUG: Tokenized sentence 125: tensor([[  101,  2010,   221,   123,  6188, 22278,   944,   259,  1564,   629,\n",
      "         22280,  1564, 14125,   123,  5790,   154, 22280,   122,  8264,  1977,\n",
      "         14537, 22279,   240,   740,  2010,   744,  1016,   229, 22280,   122,\n",
      "           223, 15799,   102]])\n",
      "DEBUG: Tokenized sentence 126: tensor([[ 101, 4551,  175,  146, 2455,  373,  180, 1447,  679,  705,  102]])\n",
      "DEBUG: Tokenized sentence 127: tensor([[  101,  2010,  4062, 16450,   304, 22280,   376,   740,   117,  2684,\n",
      "          3547,   117,   179,   229, 22280,  5825,   222,  4698, 22287,   258,\n",
      "           644,   125, 22032,   252,   102]])\n",
      "DEBUG: Tokenized sentence 128: tensor([[  101,  4048,   179,   146,  3495,  2036,   659,   170,  1361, 13793,\n",
      "           202,  1831,  2010,   700,   122,   179,   629, 22280,  2859,   102]])\n",
      "DEBUG: Tokenized sentence 129: tensor([[  101,   146,  4141, 13793,  2415, 22280,  1941,  2036,   229, 22280,\n",
      "         10692, 22278,  2010,  1502,  2389,   296,   179,   123,  6188, 22278,\n",
      "          2036,   376, 16386,   286,  1004,   260,   223,   128,   625,   740,\n",
      "           376,  3495,   122,  2113,   146,  3598,   154,   653,   122,   260,\n",
      "         11348, 17124, 22281,   229, 22280,   176,  1945, 14533,   117,  1684,\n",
      "           123, 15518,  2127,   159,   117,   122,   123,  5818,   117,   122,\n",
      "           123,  8861, 22282,  7924, 22281,   122,   738,  6278,   138,   117,\n",
      "         15518,   439,  3897,   401,  1941,   423,  6569,  9258,   102]])\n",
      "DEBUG: Tokenized sentence 130: tensor([[  101,   320,  6793,   179,   117,   173,  3443,   180,   327,   316,\n",
      "           702, 15249,   289,   117,   146,   549,   713,   176,   173, 16979,\n",
      "           556,   256,  1364,   125, 11451,  3848,  4419,   117,   125,   582,\n",
      "           146,   969,  4551,   256, 11214,  1228,   303,   143,   125,  5731,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 131: tensor([[ 101, 2072,  173, 1512,  122,  146,  644,  495,  388, 9238,  102]])\n",
      "DEBUG: Tokenized sentence 132: tensor([[  101,   123, 14286,   298, 16450, 17351, 22281,   978, 19673, 22281,\n",
      "           188, 14788,  6436,   128,   260,  7367,   179, 21280,  2375,   320,\n",
      "         16972,   117, 11314, 10416, 13808, 22281,   125,  1160,   117,  7871,\n",
      "           604, 14533, 16188,   401,   117,   586,   249,  2746,   123,  3122,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 133: tensor([[  101,   173,   230,   366,  9751,   180,  4767,   125, 14890,   171,\n",
      "         15796,   404,   117, 10502,   860,   266,   122,  1757,   209,  7186,\n",
      "           117,  4903, 12232,   591,   125,  6054,   122,  4903,   123,  1743,\n",
      "           823, 22287,   260,   877,   842,   117, 10415,   692,   173,  4410,\n",
      "          1401,   285,   117, 21692,   358,   123,   762,   343,   304, 22280,\n",
      "           179,  8544,  1084,  6805, 22280,   117,   785,  6969,  7345,   229,\n",
      "           327, 20885,   292,   125,  3486, 22281,  8540,   143,   102]])\n",
      "DEBUG: Tokenized sentence 134: tensor([[  101,  5147,   117,  2535,   146,   636,  2115,   495,   229,  5304,\n",
      "           123,  3302,   180,   418, 15976,   102]])\n",
      "DEBUG: Tokenized sentence 135: tensor([[  101, 21280,  4167,  2856,   122,   259, 11309,   501,   366,  7875,\n",
      "         22281, 19288, 22287,   118,   176,   221,   146, 16960,   303,   102]])\n",
      "DEBUG: Tokenized sentence 136: tensor([[  101,   320,  3568,   304, 22280,   146, 18433,   122,   146,  5009,\n",
      "           178,   229, 22280,  2365,   223,   128,   123, 15191,   170,   123,\n",
      "          3293,   705,   180, 13219,  2028,   259,   173, 18271, 22290,   683,\n",
      "           125,  1798, 11229,  4897,   923,   118,   176,   117,   122,   146,\n",
      "          3495, 13718,  8041,   834,   558, 18680,   340,  1839,   180,  1956,\n",
      "         21305, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 137: tensor([[  101,  2010,  1423, 21080,   125, 11902,  2010,   222,   374,  8849,\n",
      "         22280,   125,   417,  8497,   159,  2010,   230, 16317,  1165,   125,\n",
      "          4778, 22175,  2010,   682,   528,  2846, 22281,   125, 10832,  2010,\n",
      "           682,  4698,   255,   125,   572,   283,  2010,  1256,   125,  6459,\n",
      "         13793,   122,   259,  9247,   382,  8963,   923,   118,   176,  1532,\n",
      "          4682,   125, 12514,   125,   944,   259, 14378,   102]])\n",
      "DEBUG: Tokenized sentence 138: tensor([[  101,  8362,   228,   118,   176,  9958,   420,   259, 16007,   207,\n",
      "          2010,   311,  4042, 22279,   117,   347, 18433,  2779,  1114,   244,\n",
      "           123,  9652,   202,  4848,  2010,   146,  1143,   185,   180,   329,\n",
      "           260,  3985,  7796,   117,   179,  2779, 15212,   325,   146,   179,\n",
      "          1434,  2010,   347,  5009,   178,   117,   229, 22280,   311,  3174,\n",
      "           130,  1921,  3001,   945,   320,  1341,   117,   229,   504,   508,\n",
      "           125, 14016,   117,   123, 10420, 18398,   852,   117,   125,  8625,\n",
      "           138,  3456,  1539,   591,   202,  1896,   892,   117,   146, 13850,\n",
      "          3391, 22280, 15618,   293,   122,  8018,   117, 15908,  8149,   214,\n",
      "           125,   233,   141,   117,  8544,   122,  8940,   125,   230,  9196,\n",
      "           747,   123,  1858,   117,  2636, 16032,   117,   179,  4141, 13793,\n",
      "          2415, 22280, 16403,   125,  1742,   712,  5684,  3791,  8052,  1362,\n",
      "          4308, 12411,  1982,   102]])\n",
      "DEBUG: Tokenized sentence 139: tensor([[  101, 18689,   364,   118,   176,   222,  1160, 11314,  2650,   397,\n",
      "           117,   331,   221,   146,   958,   640,   117,   122,   146, 13254,\n",
      "           117,   123,  1078,   662,  2524, 22290,   179,  8544,  5168,   117,\n",
      "          8825,  5723,   117,   173,  5902,  8954,   243,   122, 15777,  9238,\n",
      "           117,   123,   327,   558,  1817,   415,  2925,   366,  9652, 22281,\n",
      "           179,  1021,   102]])\n",
      "DEBUG: Tokenized sentence 140: tensor([[  101,   222,   765,   397,  2124,   125,  3673, 20480, 10310,   373,\n",
      "          9267,   256,   102]])\n",
      "DEBUG: Tokenized sentence 141: tensor([[  101,   146,   221,   193, 17364,   256,   240,  1485,   260,  9317,\n",
      "         22281,   117,   122,  1078,   822,   383, 22278,   125, 19935,   117,\n",
      "           125,  7406,   304, 15021, 22278,   117,  8004,   151,   222, 12349,\n",
      "         13793,   125,   572,   283,  1510, 22087, 12717,   243,   123, 10985,\n",
      "         12909,   243,   102]])\n",
      "DEBUG: Tokenized sentence 142: tensor([[  101,   230,   313, 10441,   159,   124,  7672, 13808,   117,   173,\n",
      "           179, 16241,  1537, 22287,   176,  3486,  1399,  9993,   692,   118,\n",
      "           176, 19229,   173,  1485,   260, 15512, 22280,   143,   117,  5489,\n",
      "          2916,   118,   176,   123, 10420,   444,   117,   170,  5488,   358,\n",
      "          5078,  8757,   498,   260,  9317, 22281,   102]])\n",
      "DEBUG: Tokenized sentence 143: tensor([[  101,   122,  1684,   123,  5197,   117,   122,  1684,   123,  4270,\n",
      "          9349,   117,   122,   259,   179,  8625,   228,   117,   700,  7970,\n",
      "           662,   852,   387, 15618,   375,   117, 16420,  7612,   358,   125,\n",
      "          1519,  6423,   117,   170,   123,  1923,  6943,  1004, 13086,   117,\n",
      "           123,  7096,   578,   102]])\n",
      "DEBUG: Tokenized sentence 144: tensor([[  101,  1362,  6465,   125, 16341,   374, 22281,   303,   117,   179,\n",
      "          8921,   171,  1341,   125,  1796,   117,  1982,   123,  8130,   122,\n",
      "          3047,   180,  5304,   117,   222,  2397,   117,   125, 14790, 22278,\n",
      "           122,  7924,   125,  1757, 21304,   185,   117,  4004,  3152,   125,\n",
      "         16064,  4793,   117, 11690,   117,  1021,  1941,   230,  3264,  5314,\n",
      "           117,   221,  5961,   170,   146, 12447,   397,   102]])\n",
      "DEBUG: Tokenized sentence 145: tensor([[  101,   495,   222,  1456,   143,   125,   532,  7187,   122,  1685,\n",
      "           123,  9836,   481,   117,  2979,   117,  9531,  1350,   117, 17897,\n",
      "         22281,   260, 13001,   117, 13841,  7967, 22281,   122,  3002,   436,\n",
      "           720, 15356,   118,  2036,   498,   123, 15719,   117,   240, 15702,\n",
      "           125,   222,  1690,  7817,   125, 10840,   552,  4189,  2623,   247,\n",
      "         13227,   303,   125, 18016, 22280,   122,  1354,   125,  2316, 17768,\n",
      "           117,   229,   615,   259,  5708, 17226,   117, 21323, 22281,   271,\n",
      "           259,  5708,   125,   222,  1151, 22283,   125,   822,   421,   117,\n",
      "          2468,  3198,   923, 20885, 22278,  7390,  1076,   102]])\n",
      "DEBUG: Tokenized sentence 146: tensor([[  101,  2010,   318, 13793,   744,   229, 22280,   176,   706,  5961,\n",
      "           320,  2397,   136, 16795,   368,   117,  6738,   320,  3568,   304,\n",
      "         22280,  9050,   118,   176,   170,   146, 18433,   102]])\n",
      "DEBUG: Tokenized sentence 147: tensor([[  101,  2010,   146,  9019, 13793,   418,  2535,   785,  9955,   102]])\n",
      "DEBUG: Tokenized sentence 148: tensor([[  101, 14657, 22279,  2010,   449,   629, 22280,  1821,  1027,  2856,\n",
      "           122, 12044,   170,   222, 11166,   125, 19935,   202,  9466, 16606,\n",
      "          2010,   781,   185,  2044,  2010,  1623, 22280,   229,   651,   940,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 149: tensor([[  101,   122,   222,  1241,   124, 22280,   180,  8156,   146, 11314,\n",
      "          2650,   397,  9247,   654,   318, 13793,   221,   123, 13159,   117,\n",
      "           834,  8082,   140,   146,   179,  5057,  2010,   146,  2397,   179,\n",
      "           123, 22283,   418,   117,   347,  4141, 13793,   117,  1331,   179,\n",
      "           176,  2541,  1853,  2010,   368,   179, 14657, 22279,   222,  1695,\n",
      "           117,   179,  1941,  2036,   988, 22280,  9396,   146, 12447,   397,\n",
      "           202,  1423,   125,   230,  1742,   102]])\n",
      "DEBUG: Tokenized sentence 150: tensor([[  101,  2826, 22278,   118,  2036,   179,   229, 22280,  1447,  2010,\n",
      "           449,   122,   179,   744,   229, 22280, 16960,   289, 22283,   122,\n",
      "         12044,  5863,   123,   964,  4029, 22282,   102]])\n",
      "DEBUG: Tokenized sentence 151: tensor([[  101, 10719,   146,  2316, 17768,   170,   123,   327,  4410, 15618,\n",
      "           375,   122,  5276,   102]])\n",
      "DEBUG: Tokenized sentence 152: tensor([[  101,  2010,   146,  1417,   117, 16960,   289,   123, 22283,   653,\n",
      "          5863,   146,   179,   229, 22280,  3207,   122,   125,  1847,   102]])\n",
      "DEBUG: Tokenized sentence 153: tensor([[  101,  1941,  4207,  2765,  4042,   201,  2010,  1502,  1447,  1084,\n",
      "          9565,   146,  1054, 22285,  4803,   124, 22280,   117, 13685,   180,\n",
      "          5304,   221,  4270,   229,  1105,   125, 14016,   117,   582,   259,\n",
      "           179,  1084,   176, 16995, 22287,   146,  7307,   170,   388, 12898,\n",
      "         22280,   117, 13750,   214,   118,   146,   180,  3049,   304,   712,\n",
      "          1143,   117,   271,  9144,  1684,   170,   944,   259,   179,   123,\n",
      "         22283,   176, 20254,   412,   681,   576,   102]])\n",
      "DEBUG: Tokenized sentence 154: tensor([[  101,   122, 20582, 22288,   118,   176,   123,   230,   366,   454,\n",
      "          4242,   117,  6661,  2044,   146, 11314,  2650,   397,  8032,   118,\n",
      "          2036,   123,  2925,   298, 16032,   102]])\n",
      "DEBUG: Tokenized sentence 155: tensor([[  101,  2010, 12424, 22278,  1084,   146,  9805,   243,   170,  3985,\n",
      "          7796,   122,   873,   524,   222,   528,  2846,   125, 10832,   102]])\n",
      "DEBUG: Tokenized sentence 156: tensor([[  101,  2010,  3189,  6183,   291,  2477,   705,   136,  2010,  1214,\n",
      "           252,   146,  6183,   449,  6952,   170,  1257,   117,  1417,   117,\n",
      "           179,  1941,   229, 22280,  3539,   834,   596,   102]])\n",
      "DEBUG ======================\n",
      " len vetores 157 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.007079646016132821\n",
      " Coesão Score Final: 0.5035398230080664\n",
      " Conectivos encontrados: ['e', 'mas', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'embora', 'isto e', 'todavia', 'nem', 'ou', 'ora', 'quer', 'seja', 'como', 'quanto', 'ao passo que', 'logo que', 'porque', 'posto que', 'ao contrario', 'no entanto', 'com efeito', 'alias', 'tanto', 'quanto', 'se nao', 'a proporcao que']\n",
      " Número de conectivos: 32\n",
      " Número de sentenças: 157\n",
      "======================\n",
      "Resultados para preprocessado_o_cortico_aluisio_azevedo_cap_3.json:\n",
      "{'coesao_score': np.float64(0.5), 'conectivos_encontrados': ['e', 'mas', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'embora', 'isto e', 'todavia', 'nem', 'ou', 'ora', 'quer', 'seja', 'como', 'quanto', 'ao passo que', 'logo que', 'porque', 'posto que', 'ao contrario', 'no entanto', 'com efeito', 'alias', 'tanto', 'quanto', 'se nao', 'a proporcao que'], 'num_conectivos': 32, 'proporcao_conectivos': 0.007, 'similaridade_media': np.float64(1.0), 'num_sentencas': 157}\n",
      "\n",
      "\n",
      "DEBUG: Tokenized sentence 0: tensor([[  101,  5899,  5314,   700,   117,   625,  4141, 13793,  2415, 22280,\n",
      "           176,  4970,  1528,  9955,   117,   262,   370,   170,   146, 10738,\n",
      "           179,   146,  2863,   256,   122, 20582, 22288,   118,   176,   975,\n",
      "          1885,   185,  2461,   117, 15356,   125, 22229,   945,   117,   449,\n",
      "           834,   176,   179,  1445, 22282,   117,  2798,   176,  2036,   338,\n",
      "           307,   123, 15578,  4275,  4322,   146,  2892,  6804,   148,   125,\n",
      "           822,   375,   303,   102]])\n",
      "DEBUG: Tokenized sentence 1: tensor([[  101,  2010,  2354, 22279,  3539,   180,   670,   171, 18546,  1149,\n",
      "           136, 16795,   118,  2036,   102]])\n",
      "DEBUG: Tokenized sentence 2: tensor([[  101,   368, 11234,   118,   311,   125,   222,  2397,   179,  4178,\n",
      "         14790,   159,  5028,   117, 10497,   934,  4848,   122,  1434,  1084,\n",
      "           537,   243,   102]])\n",
      "DEBUG: Tokenized sentence 3: tensor([[ 101, 2010, 7206, 2779,  102]])\n",
      "DEBUG: Tokenized sentence 4: tensor([[  101,  2010,  1011, 12531,   173,  1858,  1449,  1272,   136,  2010,\n",
      "          1011,   122, 12044,   102]])\n",
      "DEBUG: Tokenized sentence 5: tensor([[  101,   229,   125,   629, 22280,   434,   763,   117,   449,   273,\n",
      "          1289,   185, 22283,   118,   311,  3914,   122, 18691,  3852, 14339,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 6: tensor([[  101,  2010,  2249,  2036,   180, 22280,  1084,   136,  2010, 17585,\n",
      "           592,   118,  7911,   102]])\n",
      "DEBUG: Tokenized sentence 7: tensor([[  101,  2010,   146, 22296,  1257,   122,   222, 10907,   185,  2010,\n",
      "           229, 22280,  1223,   240,  1528,   102]])\n",
      "DEBUG: Tokenized sentence 8: tensor([[  101,  2010,  2779,   117,   146,   636, 11263,   179,   395,   303,\n",
      "           122,   125, 11330,   102]])\n",
      "DEBUG: Tokenized sentence 9: tensor([[  101,  2010, 11330,  4624,   222, 12361, 13981,   102]])\n",
      "DEBUG: Tokenized sentence 10: tensor([[  101,  2010, 11032, 15212,   123, 22283,  1415,  5684,   125,  1084,\n",
      "           537,   243,   240,  1966, 14701,  2010,   623,   857,   179,   543,\n",
      "          2097,  4230,   183,   123,   223, 22280,  5065,   173,   271,   146,\n",
      "          7258,   229, 22280,  1377,   240, 11330,   592,   118,  7911,  1977,\n",
      "         13105,   524,   123, 11934,   304,   117,  1143, 22279,   123,   661,\n",
      "          8254,   122, 10497,   455,  4848,   117,   834,  2036,  3254,   702,\n",
      "           123,  5028,   122,   834,  1434, 14096, 22281,  2010,  1141,   117,\n",
      "           449, 17585,   592,   118,  7911,   122,   222, 11263,  6944,  4812,\n",
      "          2010,  3876,  1652, 17891,   271,  1976, 22287,   102]])\n",
      "DEBUG: Tokenized sentence 11: tensor([[  101,  1968,   146,  6775,   240,   229, 22280,  6775,  2010, 17585,\n",
      "           592,   118,  7911,   122,   785,  3495,   102]])\n",
      "DEBUG: Tokenized sentence 12: tensor([[  101,  2010,   329,   240,  9726,   117,  3486,   214,   179,  5488,\n",
      "           123,  7482,  7198,   325,   222,  1695,   123,   222, 17509,  4062,\n",
      "           117,   171,   179,  2765,   123,  9533, 14096, 22281,   117,   271,\n",
      "           146,   179,  4808,   327,  1449,  1272,   123,  2767, 19023,   229,\n",
      "         22280, 12402,   229,  1069,   171,  6754,   125,  2544, 22280,   179,\n",
      "          1767, 15702,   180,  5028,  2010,   123, 22296,   146, 18546,  1149,\n",
      "         11234,   118,  2036,   202, 14096,   136,  2010,  5222,   118,   390,\n",
      "           117,  1141,  7258,   117,   122,   146, 14096,   229, 22280,  9963,\n",
      "           151,   176,   146,  2397, 13256, 22281,   236,  1434,   146,  1312,\n",
      "           303,  2010,   449, 17585,   592,   118,  7911,   122,  6944,  4812,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 13: tensor([[  101,   273,   304,   222,  1695,  2010,   240,  1528,   229, 22280,\n",
      "           311,  7947,   102]])\n",
      "DEBUG: Tokenized sentence 14: tensor([[  101,   122, 13083,   375,   793,   125,  3598,   578,  3724,  2010,\n",
      "          2354, 22279,  7422,   123,  1449,  1272,   136,  2010,  2364,   123,\n",
      "          1976,   125,  3047,   117,   449,  8204,   311, 11639,   179,   122,\n",
      "          3264,   102]])\n",
      "DEBUG: Tokenized sentence 15: tensor([[  101,   125,  5533,   765,  5461,   118,   311,   123, 20300,   102]])\n",
      "DEBUG: Tokenized sentence 16: tensor([[  101,  2010, 14657, 22279,   222, 16423, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 17: tensor([[  101,  4141, 13793,  2415, 22280,  2002,   222,  5995, 22280,   123,\n",
      "          5304,   117,  2789,  1450,  6107,   117,  3486, 22282,   748,   222,\n",
      "          1690,  7817,   229,  3049,   304,   122,  2927,   123,   370,   170,\n",
      "           146,  1342,   102]])\n",
      "DEBUG: Tokenized sentence 18: tensor([[  101,  2010,  1961, 22279,   123,   792,  9247,   654,   118,  2036,\n",
      "           180,  4303,   171,   958,   640,   117,   179,   123,  1695,   122,\n",
      "          1695,   176,   188,   256, 12051,   124,   125,  1364,   102]])\n",
      "DEBUG: Tokenized sentence 19: tensor([[  101,   146,   329,  5458, 13981, 20945,  6136,  4698,   255,   423,\n",
      "           347, 16960,   303,   122, 14009,   118,   146,   173, 22243, 22280,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 20: tensor([[  101, 17347,   228,   146,   549,   713,   102]])\n",
      "DEBUG: Tokenized sentence 21: tensor([[  101,   123, 18908,  3239,   304, 22280, 14372,   102]])\n",
      "DEBUG: Tokenized sentence 22: tensor([[  101,   260, 11348, 17124, 22281,  2365,  1941, 19480, 16960,   934,\n",
      "           122,  2365, 12520,   125,  1160,   221,   146,  1223,   102]])\n",
      "DEBUG: Tokenized sentence 23: tensor([[  101,  2535,  2072,  1485,   125,  1690,  7817,   125, 19278,   117,\n",
      "          2440,   366,   374,  2876,   138,   179,   176,  4857,   288,   102]])\n",
      "DEBUG: Tokenized sentence 24: tensor([[  101,   222,  6938,   125,  1248,   713,  1623,   679,   118,  7707,\n",
      "           259,  8589,  1107,   173,  1010, 22278,   122, 11214,  1228,   358,\n",
      "           125,   233,   141,   102]])\n",
      "DEBUG: Tokenized sentence 25: tensor([[  101,   222,  1177,  1154,  2042, 22290,  1178,   545,  2836,   118,\n",
      "           176,  4687,  4900,   398,  2848,   243,  7583,  2826,  1009, 22280,\n",
      "          2850,   320,   969,  1718,  2349,   256,   118,  7707,   146,  5052,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 26: tensor([[  101,   123, 16803,   324, 11518,   304,   256,   170,   230, 13305,\n",
      "           179,  1796, 15158, 22282,   222,   332,   125,  5899, 22281,   122,\n",
      "         16863,   934,   230,  7924,   123,   527,  3209,   154,   117,   785,\n",
      "          3848, 22279,   498,   123,   327, 14038, 22278,   125, 11348, 22282,\n",
      "           117,  9821,  1270, 20509, 22282,   118,   176,   271,   176,   492,\n",
      "           123,   447,   451,  9147,  7453,   256,   125,   576,   173,   625,\n",
      "           123, 11451,   122,   146,  6459, 13793,   221,   144,   934,   260,\n",
      "           170,  1361, 22280,   143,   171,  1896,   892,   122,   366,  2477,\n",
      "         10661,   117,   502,  1051,   591,   423,  1623, 16093,   123,  5782,\n",
      "          2037, 10537, 11933,  2836,   117,   398,  4549,  4212,  1532,   601,\n",
      "           544,   340,   125,  6220,   154,   117,   320,  1341,   180,   528,\n",
      "         16527,   179,   117,   170,   146,   347,  1903,   125,  1124,  7137,\n",
      "          7492,   117,   222, 13850, 22178,   320,  2242,   180,  9463,   117,\n",
      "          8954,   256,   374,   401, 10537, 11769, 22281,   171,   333,   154,\n",
      "         22280,  1112, 22232,  1149,   316, 22232,  5870,   214,   117, 22232,\n",
      "          1149,   316, 22232,  5870,   214,   117,   229,  1425,   640,   171,\n",
      "         18419,  2014, 22232,  1149,   316, 22232,  5870,   214,   102]])\n",
      "DEBUG: Tokenized sentence 27: tensor([[  101, 22354,   123, 12252,   624,   117, 20073,   117, 15363,  1004,\n",
      "           170,   146,  9708,   171,   969,   117,   123, 14966,  7177,   834,\n",
      "         22229,  3670,   117,  1990,   619,   256,   259,  5334,  2283,   122,\n",
      "           599,  8859, 22281,   179,   176, 17318, 22287,   229,   418, 15976,\n",
      "           117,   122,  1982,  3914,   117,   123,   949,  1601,  8551,   304,\n",
      "         17704, 10502,   847, 12764,   178,  4217,   364,   256,   117, 15518,\n",
      "          2127,   348,   123,   327, 11451,  1839,   180,   964,   324,   117,\n",
      "         13549,   117,   271,   222,  9038,   123,  3212,   202,  3053,   247,\n",
      "           320,  6793,   179,   146,   313, 11284, 22280,   117,  9344,  3391,\n",
      "          3356, 22279,   348,   259,   532,  1896,  1046,  8699,   125,  2397,\n",
      "          1863,  1165,   713,   117,  3985,   151,   229, 14038, 22278,   222,\n",
      "           332,   125, 14790,   138,   117,   202,  7881,  6846,  6882,   122,\n",
      "         14689,  1350,   125,   222, 11899,  2825,   123,  5818,  4351,  6644,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 28: tensor([[  101,   146,  1831,  8574,   151,   118,  2036,  1364,   117,   122,\n",
      "           368,   117,   125,   576,   173,   625,   117, 16322,  1399,   146,\n",
      "         16936,   303,   171, 13227,   303,   221, 13540, 19328,   159,   123,\n",
      "          3317,   117,   122,   318, 13793,   222,  2510, 13513,  4217,   364,\n",
      "           243,   695,   151,   118,  2036,   712, 18908,   501,   102]])\n",
      "DEBUG: Tokenized sentence 29: tensor([[  101,   180,   504,   508,  5492, 22280,  1015,  8940,   222, 18612,\n",
      "           735, 22279,  6205,   243,   117,   449,   870,  1862,   102]])\n",
      "DEBUG: Tokenized sentence 30: tensor([[  101,   495,   123,   366, 19659,   179,   905,   151,   256,   146,\n",
      "           347,  1312,   303,   229, 22280,  9679,  2787,   703,   159,   834,\n",
      "          8032,   102]])\n",
      "DEBUG: Tokenized sentence 31: tensor([[  101,   202,  5492, 22280,   977,   872,   514, 22285,  8032,   715,\n",
      "           256,   173,  5902,   785,   325,  3378,   122,   125,   222,   298,\n",
      "         15050,   171,  4707,   180,   418, 15976,  8625, 22278,   125,  1632,\n",
      "           303,   123,  1632,   303,   230,  4428,   260,   574,   125,  1073,\n",
      "          7052,   514,   102]])\n",
      "DEBUG: Tokenized sentence 32: tensor([[  101,   146, 12447,   397,   117,   320,  3852,   240,   125,   839,\n",
      "           125, 12252,   624,   117,   179,   202,  2182,   305,  1051,   256,\n",
      "         11451,   171,  1690, 22280,   117,  3050, 22288,   118,  2036,   230,\n",
      "          1877,  2555,   229,   670,   171,  1831,   318, 13793,   325,   173,\n",
      "         18374,   102]])\n",
      "DEBUG: Tokenized sentence 33: tensor([[  101,  2010,   229, 22280, 21595, 22278,   117,  2678,   147,   136,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 34: tensor([[  101,  9247,   654,   740,   117,  6372,   286,   117, 15568,   214,\n",
      "           118,   176,  3689, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 35: tensor([[  101,   122,   117,  3951,   170,  4141, 13793,  2415, 22280,  2010,\n",
      "          2779,  2044,  1976,   102]])\n",
      "DEBUG: Tokenized sentence 36: tensor([[  101,  1904,  8893,   214,  5863,   170,   123,  9349,   122,   700,\n",
      "           117,  2541,   118,   176,  8977,   229,  5304,   117,   146,   629,\n",
      "          1165,   243,  6641, 22278,   202,  5274,   644,   492,   171, 13588,\n",
      "           339,  2779,   229, 22280,   437, 18691,   117,  4178,   136,   146,\n",
      "         12447,   397,   969,   654,   118,  2036,   940,  1877,  2555,   170,\n",
      "           325,   344,   304,   122, 11579,   117,  2113,   740,   176,  4857,\n",
      "           124,   170,   222,   494,   500, 13140,   125,  6205, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 37: tensor([[  101,  2010,  3539,  1174,   329,   117,   176,   188,  4051,   644,\n",
      "           492,   180,  1143,   185,  4141, 13793,  2415, 22280,  1941,   176,\n",
      "          1021, 14426,   170,   146,   329,  5458, 13981,   102]])\n",
      "DEBUG: Tokenized sentence 38: tensor([[ 101, 2010,  146, 7258,  376, 5863, 5747, 9349,  102]])\n",
      "DEBUG: Tokenized sentence 39: tensor([[  101, 10719,   118,  2036,   860,   102]])\n",
      "DEBUG: Tokenized sentence 40: tensor([[  101,  2010,   146, 22296,  1191,   146,  1342,   117,   629,   830,\n",
      "          6436,   243,   259, 20462, 22281,   117,   122,  1996,   700,   170,\n",
      "          4136,  1323,  2010, 15098,   325,  5830, 15050,   179, 16330, 13140,\n",
      "         22281,   449,   122,  2745,  9349,  1467,   229, 22280,   607,  4004,\n",
      "         12886,   255,  3660,   418, 15976,   176,  3379,   230,  3277,   421,\n",
      "           117,  2779,  1120, 22280,   117,   122,  2745,  4364,  2044,  2364,\n",
      "           538,  3033,   329,   123,   661,  1144,   117,  2798,  2364,   123,\n",
      "          4314,  5835,  4270,   122,  2389,   296,   179,   176,  7465,  2097,\n",
      "          1004,   170,   260,   675, 14643, 22281,  2745,  9349,  5747,  3264,\n",
      "          2365, 12319,   320,  1338,   171,  4286,   247,   171,   549,   713,\n",
      "           122,   117,   700,   125,  4636,   210,   230,  4303,   179,   176,\n",
      "          3030, 15736,   170,   222,  5274,  8650,   825,   123,   230, 15445,\n",
      "           117, 16512,   228,   118,   176,   202,   853, 18983,   162,   179,\n",
      "          1021,  1075,   180,  1449,  1272,   102]])\n",
      "DEBUG: Tokenized sentence 41: tensor([[  101,  2010, 19821,   240,  5863,   653,   179,   122,   325,  3047,\n",
      "           117, 12910,  2598,   146, 12447,   397,   102]])\n",
      "DEBUG: Tokenized sentence 42: tensor([[  101,   122,   259,   682,   117,   173,   576,   125, 10469,   210,\n",
      "           123,  5675,   117, 17347,   228,   146,   853,   314,  8833,   122,\n",
      "          1510, 22281,  2848,  7909,   102]])\n",
      "DEBUG: Tokenized sentence 43: tensor([[ 101, 1423,  118,  644,  173, 2009,  102]])\n",
      "DEBUG: Tokenized sentence 44: tensor([[  101,   146,   969,  1011,   123, 13718, 22280,  2745,  7871,   604,\n",
      "          2836,   123,  3377, 10984, 11847,  4146,   415,   125,  1512,   117,\n",
      "          1362,   644,   834, 15553,   102]])\n",
      "DEBUG: Tokenized sentence 45: tensor([[  101,   123,  1449,  1272,   117,   173,   179,   740,  3985,   151,\n",
      "           125, 14982,   173,  5530,   117,  2992,  8041,  2389,  4419,   125,\n",
      "          2375,   102]])\n",
      "DEBUG: Tokenized sentence 46: tensor([[  101,   495,  8911,   528,   779,  2854,   123,  3122,   221,  7544,\n",
      "           260,  1444, 20798, 22281,   180,  5028,  3874,   325,   179,   230,\n",
      "           739,  9922,   252,  7352,   122, 17637, 22278,   117,  8638,   412,\n",
      "           670,   125,  3378,   202,  1690, 22280, 14021,   125, 15520, 22290,\n",
      "           268, 14689,  1350,   117,   179,   320,  5533,  1271,   151,   146,\n",
      "          3901,   125,   222,  5294, 10848, 22279,  1348, 11577,   183,   117,\n",
      "           122,   412,   670,   125,  5530,   229, 16704,  4968, 10394,   171,\n",
      "           388,  4056,   243,   117,   582,   176,   229, 22280,  7194,   923,\n",
      "           736, 14378,   325,   171,   179,   202,   243,   138, 20269,   117,\n",
      "          1004, 20269,   117,   498,   146,  6183,   118, 13389,   102]])\n",
      "DEBUG: Tokenized sentence 47: tensor([[  101,   123,  3606,   304, 22280,   179,   259,   682,   176,  4621,\n",
      "           692,   180, 19601,   403,  1449,  1272,   117,   146,  5856,  8544,\n",
      "           118,   176,  2862,   325,   122,   325, 15520, 22290,  6021,   243,\n",
      "           259, 21423,  9855, 11324,   692,   118,   176,   125,   230, 20076,\n",
      "          4982,   102]])\n",
      "DEBUG: Tokenized sentence 48: tensor([[  101,   325, 14339,   117,   240,  5863,   122,   240,  1369,   117,\n",
      "          1021,  1615,  3883,  1149,   117,  1450,   173,  2115,   117, 14537,\n",
      "           401,   123,  9066,   157,   122, 13086, 22281,   125,  1945, 15225,\n",
      "          6910,  1028,  1941, 20466, 22281,   221,  3866,   117,   123,  2521,\n",
      "           171,  6032,   117,   122,  1028, 17878,   170,   259,  4332,   942,\n",
      "           221,   146,   388,   117,   271,   176,  4364, 22281,  6867,   125,\n",
      "           333, 15891, 13199,  4900, 16423, 22279,   102]])\n",
      "DEBUG: Tokenized sentence 49: tensor([[  101,  2217, 18908,  3239,   692,   102]])\n",
      "DEBUG: Tokenized sentence 50: tensor([[  101,   123,  4573,   117,   240,  5530,   125,   222, 12232,  1699,\n",
      "           125,  2187,   117,   179,  9821,   370,   908,  5167,   286,   125,\n",
      "           222, 12424, 22280,   240,  6086,   969,  2496,   234,   117,  1021,\n",
      "           230,  5792,   125, 14038,   138,   117,   582,  1510, 22281,  4385,\n",
      "           117,  1821,  1444, 22281,   117, 10415,   692,  3791,  8052,   117,\n",
      "           834,  1434, 15419,   117, 16188,   308,   123,   363,  4387,   423,\n",
      "           969,   171,  1423,   118,   644,   102]])\n",
      "DEBUG: Tokenized sentence 51: tensor([[  101,   221, 14339,   117,   229,  1589, 15512, 13793,   117,  4063,\n",
      "           151,   222, 14985,  4117, 16510,   117,  4575,   122,  5980, 22280,\n",
      "           117, 16315,   243,   498,  9303,   125,  5028,   374, 22281,   304,\n",
      "           123, 22283,  1415,  4966, 18739,   125,  2069,   458,   117,   320,\n",
      "          1923,  1259,  4437,   319,   171, 12307, 13793,   179,  1718,   151,\n",
      "           146, 20300,   102]])\n",
      "DEBUG: Tokenized sentence 52: tensor([[  101,  2044,   173,  2590,   117,  2366,   151,   230, 17415,   125,\n",
      "          1718,  1339,   117,  1719,  1032, 20798,   285,   125, 16863,   942,\n",
      "           122,  4674, 19797, 22281,   117,   420,   259,  1647,  1938,  8521,\n",
      "         14533, 12446,   125,  3883,   173,  1359,   180,  4351,  5967,   324,\n",
      "           682,  2217,   117,   125,  1831,  1444,   117, 10137,   308,   125,\n",
      "           233,   141,   122,  5231,  4322,   442,   125,  6367,   271,   682,\n",
      "           644,  3471,   117,   528,  6204,   692,  6846,  4211,  7424,   498,\n",
      "           222,  7094,   303,   125,  3050,   173,  1010, 22278,   122,  1369,\n",
      "           653,   117,  3047,  2866,   117,   123,   344,   524,  3240, 15690,\n",
      "          2836,   230,  3746,   747,  3752,  7134,   117,   125,   582,  8625,\n",
      "           228,  4296,  3182,   138,   125,  4848,   117, 10984,  8156, 15268,\n",
      "           122,   866,  1159,   138,   102]])\n",
      "DEBUG: Tokenized sentence 53: tensor([[  101,  4141, 13793,  2415, 22280, 13683,   123,  3302,   180, 17415,\n",
      "           122,  9247,   654,   221,   222,   298,  1718,  8855,  2010,   146,\n",
      "          5782,   300,   229, 22280,   176,  6969,   304,   171, 17935, 22290,\n",
      "           180, 18253,  2830,   171,  4303, 22280,   259,   682,  2217, 16322,\n",
      "          7022,   240,   222, 16423, 22279,   146,  1223,   102]])\n",
      "DEBUG: Tokenized sentence 54: tensor([[  101,  2010,  1941,  1084,   572, 22283,   792,   117,  9396,   146,\n",
      "          5782,   300,   102]])\n",
      "DEBUG: Tokenized sentence 55: tensor([[  101,   229, 22280,  5488,   123,  7482, 18486,   154,   118,  1340,\n",
      "           418,  1364,   170,   286,   125,  5643,   705,   659,   118,   176,\n",
      "           118,  2036,   222,  1160,   117,   179,   122,  1407,  2010,  1502,\n",
      "           873,   524,  1084,  1257,   117,   179,   123, 18253,  2830,   418,\n",
      "           123,  9322,   122,   146, 12447,   397,  5793, 14339,   170,   146,\n",
      "          1342,   117,  1139,  7521,  5725,   304,   256,   146,   528, 20871,\n",
      "           498,   123,  4351,  5967,   324,   102]])\n",
      "DEBUG: Tokenized sentence 56: tensor([[  101,   173,  2590,  1413,   118,   176,   230,  3061,   371,   415,\n",
      "          1048,  2308,   151,   117, 13086,   125,   853,   314, 13427,   122,\n",
      "          2638,   684,   265, 22280,   125,  5294,  8849, 22281,   117,   170,\n",
      "          1247,   221,  5899,   623, 12051,   125,  3155,   102]])\n",
      "DEBUG: Tokenized sentence 57: tensor([[  101,  1011, 12649,   154,   117,   449,   117,   202,  3361,  5546,\n",
      "         10848, 19455,   201,   125,  1084,   117, 16280,   118,   176,   179,\n",
      "          1796, 17971,   744,  7583,  2954,   102]])\n",
      "DEBUG: Tokenized sentence 58: tensor([[  101,  1021,   700,   222, 12174,   373,   125,  4561, 22281,   117,\n",
      "         10568,   320,   653,   596,   125, 17415,   125,   505,  6592,  3120,\n",
      "           117,  1226,   123,  4303, 14793, 22281,   125,   388,  4056,   117,\n",
      "          1089,  1941, 10530,   442,   117,  1615, 14038,   138,  4276,  5822,\n",
      "           591,   117,  8197,   125, 20880,   122,   449,   534,   125,  4449,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 59: tensor([[  101,   180, 22283,   123,  1449,  1272,  2226,   692,   820,  7226,\n",
      "         11330, 10674,   122,   146,  1690, 22280,   495,  1941,  1364, 14021,\n",
      "           240,   230, 20080,   125,  5028,   390,   328,   179,  5980,  2836,\n",
      "           271,   123,  1945,   102]])\n",
      "DEBUG: Tokenized sentence 60: tensor([[  101,  5863,   117,  1369,   117,   240,  1719,   123,   670,   117,\n",
      "         12063,   118,   176,  5684,   117,  7226,   320,   969,   117,   736,\n",
      "         15702,   125,  4296,  6688,  1149,  4330,   125,  1041, 22278,   291,\n",
      "           125,  7438,   125,  1877, 10443,   102]])\n",
      "DEBUG: Tokenized sentence 61: tensor([[  101,   125,   222,  1341,  8981, 15736, 22287,  5028, 10714,   125,\n",
      "          1342,   123,  6642,   692,   123, 12307,   823,   154,   125,  1342,\n",
      "          4613,   319, 14533,  1084,   537,   442,   123,  8993,   125, 12307,\n",
      "         13793,   325, 14339,  9144,  4405,   583,  6720,  2055,   128,   123,\n",
      "          3235,  1703,   122,   223, 14524,   102]])\n",
      "DEBUG: Tokenized sentence 62: tensor([[  101,   122,  1364,  6086,  2582, 15500, 22287,   125,  8786,   117,\n",
      "           122,   146,   528, 20871,   180,   344,   524,   117,   122,   146,\n",
      "          5553,   298,   179,  1084,   173,  5530, 11934,   304,   692,   123,\n",
      "         13353,   221, 18253,   934,   118,  2036,  4848,   117,   122,   123,\n",
      "          1401,   285, 10273,   251,   320,  5533,   117,   179,  8940,   171,\n",
      "           549,   713,   117,   271,   125,   230,  6321,  9456,  2555,  2745,\n",
      "         10348,   123,  3138,   125,   230,  3546, 18781, 22305,   117,   125,\n",
      "           230,  1998,   125,  7903, 20798,   122,   125,   146,   635,   102]])\n",
      "DEBUG: Tokenized sentence 63: tensor([[  101,  5022,  2217,  3746,   185, 17675,   555,   125,   233,   141,\n",
      "           117,  5167,   308,   125,  6938,   117,   273,   256,   364,   442,\n",
      "           125,   601,   715,   304, 22280,   117,   123, 14195,   210,   117,\n",
      "           123,   730,   232, 17738, 22287,   117,   123, 10697,   684,   123,\n",
      "          5028,   117,  9821, 22287,   222,  5078,  2895,   125,  3174, 13733,\n",
      "         22281,  6838,   442,   229,   327,   525,  3356,  3292,   598,   146,\n",
      "          4764, 22281, 12569,   415, 12477,   179,   259,  4108, 21307,   170,\n",
      "         10968,  2256,   117, 16421, 21994,   415,   123,   944,   259, 16437,\n",
      "           122,   123,   944,   259, 12785,   179,  2036,  5510, 11211, 14533,\n",
      "           202,  4678,   293,   117,  4513,   834,   222,  2510, 13513,   179,\n",
      "          2036,  2751, 22281,  6867,   260,  3837, 13808, 22281,   125, 20300,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 64: tensor([[  101,   146,  3262, 18271,   243,   329,  5458, 13981,  1021, 12319,\n",
      "           123,  1925,  2876, 22278,   171, 17275,   293, 15218,   125,  5028,\n",
      "           978,   118,   146,  1354,   123,  1354,   117, 13750, 22288,   118,\n",
      "           146,   125,  2979,   123,  3378,   117,  7096, 10768,   117,  1362,\n",
      "          9898,  1401,   243,   102]])\n",
      "DEBUG: Tokenized sentence 65: tensor([[  101,   123,  1449,  1272, 15245,  3876,  2009,   125,  3122,   146,\n",
      "           347,  1341,   325, 19601,   403,   102]])\n",
      "DEBUG: Tokenized sentence 66: tensor([[  101,  1432, 13644,   154,   117,   170,   146,  4721, 20659,   243,\n",
      "         13185,   303, 16786,   320,   969,   117,  8004,   151,   118,   176,\n",
      "          2729,   514,   364,   122, 10151,   293,  3181,   285,   117,  9620,\n",
      "          5974,   146,  2992, 22288,   117,   785,  1486,   684, 22279,   117,\n",
      "         21990, 22278,   117,  5490, 12717,   185,   122, 13086,   125, 10286,\n",
      "           179,   454, 11147,   546,   403,  2036,  3235, 22282,  5283,   412,\n",
      "          5307, 16719, 22278,  1444, 15182,   170,   222,  3901,   125,   437,\n",
      "           562,   125, 11997, 13808,   102]])\n",
      "DEBUG: Tokenized sentence 67: tensor([[  101,   173,  8128,  4488,   117,   785,  2979,   171,  1690, 22280,\n",
      "           117,  2036,  3841,   734,   487,   313,  7557,   735,   143,   125,\n",
      "          3050,   117,  2101,  3273,   243,   117,   498,   222,  8163,  9258,\n",
      "           117,  3061,   371,   909, 14038,   138,   179,   117, 11004,   329,\n",
      "           125,  3378,   117,  9821, 22287,  1877,   721,   117,   449,   173,\n",
      "          5530,   366,  1647,  7226,   361,   130,  3453, 19417, 14649, 22281,\n",
      "           125,   547,  4570, 14208, 18444,   118,   176,   117,  5510, 11211,\n",
      "           348, 16437,   125, 12307,   823,   154,   598,   146, 12477,   102]])\n",
      "DEBUG: Tokenized sentence 68: tensor([[  101,   146,   329,  5458, 13981,  1462, 22279,   203,   123,  3049,\n",
      "           304,   170,   388,   125, 10497,  2896,   102]])\n",
      "DEBUG: Tokenized sentence 69: tensor([[  101,   146,   347, 22000, 17293,   256,   256,  1364,  6086,  1312,\n",
      "           303,   102]])\n",
      "DEBUG: Tokenized sentence 70: tensor([[  101,  2010,   873,   524,  1084,  1996,   368,   117, 12110,   214,\n",
      "           221,  4863,  2009,   180, 13353,   102]])\n",
      "DEBUG: Tokenized sentence 71: tensor([[  101,  2389,   296,   221, 11972,   327,  9349,   376, 19480,   260,\n",
      "          2992,  1557,   202,  1223,  1014,  1449,  1272,   102]])\n",
      "DEBUG: Tokenized sentence 72: tensor([[  101, 21494,  5855,   118,  1084, 13776,   240,  6086,  1342,  1341,\n",
      "           117,   221,   229, 22280,   598, 18250,   259,  3429, 22281,   180,\n",
      "          5028,   102]])\n",
      "DEBUG: Tokenized sentence 73: tensor([[  101,   418,   670,  5863,   122,  1719, 20300,   117,   122,   123,\n",
      "          1407,  1502,  2389,   296,   331,   146,   179,  1061,   376,  4551,\n",
      "           243,   125,  1084,  2010, 11368, 10497,  1149,   117,  7226,  1945,\n",
      "         15225,   179,   229, 22280, 13933,   221,  3874,   122,   230,  4678,\n",
      "           125, 16450,   304, 22280,   792,  3254,   702,  1016,   230,  6514,\n",
      "         22278,   316, 22280,  3264,  2535,   146,   179,   607, 22280,   125,\n",
      "          1434,  2990, 15520, 22290,  4419,   179,   123, 22283,   418,  3133,\n",
      "         13793, 12361,   942,   136,   122,  4332,   285,   712,  2992,   249,\n",
      "           117,  3960,   151,   370,  5028,  1014,  2601,   221, 19226,   118,\n",
      "          1084,   173, 12361,   942,   146, 12447,   397, 13083,  5723,   118,\n",
      "           146,   173, 22243, 22280,   117,  7837,  1552,   259, 10786,   942,\n",
      "           117, 10363, 19356,   286,   170,   123,  3138,  4566,  5932,  4815,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 74: tensor([[  101,  2010,   230,   240, 15035,   125,  1312,   303,  3449,   146,\n",
      "          1342,   102]])\n",
      "DEBUG: Tokenized sentence 75: tensor([[  101,  1369,   582,   418,  6086,  2397,   122,   179, 21494,   370,\n",
      "          2160,   123, 11934,   304,   117,  2113,   123,  6270, 13793,  5078,\n",
      "           252,  4133,  1719,   418,  3508,   179,   122, 14136,   240,   222,\n",
      "          3429,   102]])\n",
      "DEBUG: Tokenized sentence 76: tensor([[  101,   449,  1977,   376,   123, 22283,   146,  7258,  4051,   125,\n",
      "          1434,  1257,   136, 16241,  1537, 22287,  2113,   122,  8911,   222,\n",
      "         12531,   179,  8625,   581,   146,   179,   659,   179,   117,   176,\n",
      "           123,   661,  8254,   229, 22280,   344,   785,  1004,  3659,   117,\n",
      "          2798,   331,   229, 22280,   176,  7674,   146,  3429,   117,   271,\n",
      "           744,  4897, 22279,   320, 17509,   146,   653,   179, 12519,   320,\n",
      "          1342,   122,  8911,  8223,   785,  1004,   146,  1223,   221,   176,\n",
      "           926, 11076,  3243,  5565, 22278,  3103, 22280,  1014,  1449,  1272,\n",
      "          3264,   122,   740,   117,   449,   229, 22280,   529,   223,   128,\n",
      "           173,   179,   418,   122,   785, 22246,   529,  6270, 22280,   143,\n",
      "           122,   785,   173,   766,  1977,  2036, 10497,   934,  4848,   229,\n",
      "         22280,   706,  7912,  3133, 13793,   221,  5530,   412, 15445,   117,\n",
      "           122,   176,   146, 10738,   229, 22280,   344,  2135, 22280,  1904,\n",
      "           118,   146,   146,  3174,  7206,  2779,  1977,   146,  1331,   122,\n",
      "           700,   125,   230, 18827,   117, 14000,   117, 12086,   229,   327,\n",
      "           223, 22280,   117, 15618,   375,   271,   146,  2004, 22280, 15520,\n",
      "         22290,   268,   117,   222,  4405,   583,  6720,  2055, 22280,   179,\n",
      "          1011,   202,  1690, 22280,  2010,   179,  2826, 22280,  2779,   136,\n",
      "           329,   418, 12361,   942,   125, 20300,  3413,  2684,   122,   230,\n",
      "          5664,   179,  2983,  9066,   444, 21494, 12862,   240,   792, 12268,\n",
      "         19179,   123,  1449,  1272,   423,  1341,  2368,   122,  6456,   118,\n",
      "           123,   229,  1359,   179,   740, 10348,   700,   117,  6335,   222,\n",
      "          5450,   452,   463,  3621, 22280,   117,   122,   179,   176,  1413,\n",
      "          2249,   495,   739,   102]])\n",
      "DEBUG: Tokenized sentence 77: tensor([[ 101,  327,  256,  118,  176, 1004, 1075,  125, 3767,  320,  347, 6487,\n",
      "          170,  123, 6629,  102]])\n",
      "DEBUG: Tokenized sentence 78: tensor([[  101,  2010,   179, 16387,   125,  3495,   102]])\n",
      "DEBUG: Tokenized sentence 79: tensor([[  101, 10355,   146,  1054, 22285,  4803,   124, 22280,   117,   221,\n",
      "           214, 10552,  2646,   975,  1885,   185,   171,  1160, 12617,   125,\n",
      "         13353, 12043,   179,   176,   273,   243,  1609,   256,   229,   543,\n",
      "           194,   304,  2461,   102]])\n",
      "DEBUG: Tokenized sentence 80: tensor([[  101,  2010,  1719,   418,   670,   179,   176,  5229,  2535,   117,\n",
      "          4492,  4141, 13793,  2415, 22280,   117,   744,   229, 22280,   122,\n",
      "          7122,   102]])\n",
      "DEBUG: Tokenized sentence 81: tensor([[ 101,  122, 8500,  123, 8054,  221, 4271,  102]])\n",
      "DEBUG: Tokenized sentence 82: tensor([[  101,  2166,  1341, 19386, 14533,   118,   176,   260,  6688, 11147,\n",
      "           842,   259, 12361, 19428, 18739,   123, 15419,  4687,   117, 21692,\n",
      "           358,  5022,   682,   102]])\n",
      "DEBUG: Tokenized sentence 83: tensor([[  101,  1413, 22287,   118,   176,  9196,  2307,   320,  4848,   117,\n",
      "           498,  1256,  8483,   117,   320,   388,  3039,   117,   122, 13254,\n",
      "           721, 20788,   171, 14890,   298,  2639,   102]])\n",
      "DEBUG: Tokenized sentence 84: tensor([[ 101,  125, 2606, 2798, 4227,  102]])\n",
      "DEBUG: Tokenized sentence 85: tensor([[  101,   125,   576,   173,   625,   117,   229,  2377,  2755,   124,\n",
      "           125,   222,  1748,   703,   551,   125,  1041, 22278,   117, 10348,\n",
      "           118,   176,   170,   222,   939,   125,  2217,   117,   662,   214,\n",
      "           125,   144,  5635,   138,   975,  1885,   185,  7226,   298,   736,\n",
      "           117,   230,  9344,  6436,   252,   229,   223, 22280,  4573,   117,\n",
      "           222,   372, 22280,   229,  5065,   117,   320,  1341,   125,   230,\n",
      "         16317,  1165,   125,  6205, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 86: tensor([[ 101, 2010, 1684,  146,  653, 1312,  303, 3002, 8805,  122, 3002, 4542,\n",
      "          102]])\n",
      "DEBUG: Tokenized sentence 87: tensor([[  101,   398,  4549,  2904,   146,   329,  5458, 13981,   102]])\n",
      "DEBUG: Tokenized sentence 88: tensor([[  101,  5147,   117,   123,  1589,  3546,  9821, 15831, 22282,   240,\n",
      "          1719,   123,   670,   102]])\n",
      "DEBUG: Tokenized sentence 89: tensor([[  101,   449,   117,  1084,   202,  1338,   117, 15702,   298,   475,\n",
      "           282,   249,   179,  2405,   692,   146,  6487,   180,  1449,  1272,\n",
      "           117,  1089,  5684,  4678,  4322, 22287,   123, 15419,   117,   125,\n",
      "          5562, 22280,   221,   146,   388,   117,   123, 17897,   734,  1552,\n",
      "           221,   146,  2979,   117,   146, 13227,   303,  8697,  1056,  1859,\n",
      "           125,   549,   243,   301,   562, 15618,  1509,   271, 13540,   159,\n",
      "           784,   125,  4449,   117,   123,  9463,  6628,   117,   123,  9815,\n",
      "           304, 22280,  2124,   122, 20885, 22278,   125,  6032,   629,   635,\n",
      "           117,  1362,  8540,   122,  3974,   428,   319,   398, 13193,   702,\n",
      "           125,  5294,  8849,   822, 20733,   102]])\n",
      "DEBUG: Tokenized sentence 90: tensor([[  101,  2010,   179,   689,  2037,   310,   398,  4549,  2904,   125,\n",
      "          1160,   146,   329,  5458, 13981,   102]])\n",
      "DEBUG: Tokenized sentence 91: tensor([[  101,  2745,  3413,   418,   123, 15158, 22282,   222,  2397,  3689,\n",
      "         22280,   179,  2389,   296,   123,   333,   247,   221,   146,  1312,\n",
      "           303,  2010,  2779,  3874, 15212,   179,   792,   170,   860,  1341,\n",
      "         10719,  2415, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 92: tensor([[  101,  2010,   449,  1084,   180,   327,  1110,   607, 22280,   125,\n",
      "          1434,   146,   653,  2389,   186,  2010, 18420,   228,   117,  2113,\n",
      "         15212,   125, 11552,   423,  3907,   523,  1084,  1796,   102]])\n",
      "DEBUG: Tokenized sentence 93: tensor([[  101,  2010,   170,  1039,  5863,   122,   179,  1061,   229, 22280,\n",
      "         11610, 22287, 21340,   102]])\n",
      "DEBUG: Tokenized sentence 94: tensor([[  101,  1257,  3436, 22280,  2779,  3486,   214,   179,   146, 12531,\n",
      "          1981,   333,  1004, 12659,   117,   370,   221,   123,   327,  9652,\n",
      "           123,  5546,   154,   117,   146,   347, 11166,   125, 10832,   117,\n",
      "           449,   179,  1981,  1434,  1312,   303,   179,   176,   873,   524,\n",
      "           117,   291,   117,   318, 13793,   117,  4768,  4768,   117,   179,\n",
      "           229, 22280,  3207,   240,   123, 22283,  1977,   179,   364,  5076,\n",
      "          3495,  1368, 15134,   118,   311,   123, 11552,   240,  1061,   122,\n",
      "         20840,  2010,   146,   644,   492,   122,   179,  2354, 22279,  3189,\n",
      "         17585,   592,   118,  7911,   102]])\n",
      "DEBUG: Tokenized sentence 95: tensor([[  101,  4217,  5461,  4141, 13793,  2415, 22280,   102]])\n",
      "DEBUG: Tokenized sentence 96: tensor([[  101,  2010,   123, 22296,  2798,  1528,   222,  1752,   102]])\n",
      "DEBUG: Tokenized sentence 97: tensor([[  101,   449,   170,  1039,  5863,   607,   125,   792,   146,   179,\n",
      "          2036,   395,   303,  4270,   221,   313,  2245,   483,   364,  7273,\n",
      "           329,  5747,  9349,   179,   229, 22280,  3804,  2765,   102]])\n",
      "DEBUG: Tokenized sentence 98: tensor([[  101,   221,   179,  1971, 12361, 13981,   117,   240,  1416,   136,\n",
      "         11972,   122,  1312,   303,   221, 21158,   122,  1312,   303,   125,\n",
      "           854,  2028,   173,   576,   125,  1485, 11665,  4878,   486,   117,\n",
      "          9141, 22281,  5787,   123,  7187,   592,   118,  7911,   102]])\n",
      "DEBUG: Tokenized sentence 99: tensor([[  101,  2010,   122, 13776,  2249,  7707,  3687,   102]])\n",
      "DEBUG: Tokenized sentence 101: tensor([[  101,  1407,  1467,  5357,   682,  9361,  5684,   125, 11330,   117,\n",
      "           179,  4366,   146, 21244,   171,   179,  4366,  5022, 10537, 22281,\n",
      "           122,   179,  1146,  6202,   221,  1028,  4486,  4048,   179,  2364,\n",
      "         17070,  2389,   296,   117,   122,  1941,   123,  2914,   576,   179,\n",
      "          6086,   179,  1369,   418,  5308,  9322,   146,  3235,  1703,   170,\n",
      "          3901,  4141, 13793,  2415, 22280,  1767,  1945,   201,   117,   123,\n",
      "         17097,   956,   117,  1139,  1359,   692,   102]])\n",
      "DEBUG: Tokenized sentence 102: tensor([[  101, 16819,  2592,  4047,  1100,   102]])\n",
      "DEBUG: Tokenized sentence 103: tensor([[  101,  2010,   122,  2354, 22279,   117,   176,  2779,   146,  5357,\n",
      "           117,  1996,   700,   146, 12447,   397,   117,  5896,   118,   176,\n",
      "           329,   221,   123,   418, 15976,   136,   102]])\n",
      "DEBUG: Tokenized sentence 104: tensor([[  101,  2010, 12931,   229, 22280,  2678, 22283,   125,  4412,  1084,\n",
      "           229,   651,   940,   117,  1226,   146,  1312,   303,  5863,   102]])\n",
      "DEBUG: Tokenized sentence 105: tensor([[ 101, 2010,  122,  123, 9652,  117, 4049,  303,  118,  123, 2779,  136,\n",
      "          102]])\n",
      "DEBUG: Tokenized sentence 106: tensor([[  101,  2010,  1257,   122,   179,   123,  2606,   122,  1977,   123,\n",
      "           659,   449,   260, 18937, 17699,   118,  2036,   180,  5304,   102]])\n",
      "DEBUG: Tokenized sentence 107: tensor([[  101,  2010,  1502,   418, 10371,   146,  3907,   523,  2607,  8728,\n",
      "           203,  4141, 13793,  2415, 22280,   117, 20089,   125,   179,   229,\n",
      "         22280,  4207,   117,   240,  3338,   117, 12119,   159,   222,  2397,\n",
      "         13340,   102]])\n",
      "DEBUG: Tokenized sentence 108: tensor([[  101,   122, 17662,  1084,   125,   898,   221,   898,  1112,   259,\n",
      "         17080, 17585,   592,   118,  7911,  4706,   118,   311,   118,   320,\n",
      "           123,  1956, 21305, 22278,   102]])\n",
      "DEBUG: Tokenized sentence 109: tensor([[  101,  2745,   311,  1968,   173,  1105, 22354,  2010,   318, 13793,\n",
      "         15060, 21894, 22281,   136,   102]])\n",
      "DEBUG: Tokenized sentence 110: tensor([[  101,  2010, 15060, 21894, 22281,  2010, 21174, 22032,   252,  1434,\n",
      "           123,  5896,  2028,   136,  2010,  1790,   653,   117,   176,  8204,\n",
      "           140, 15212,   222,   271,   243,   179,  2036,   607,   125,  1945,\n",
      "          2430,   102]])\n",
      "DEBUG: Tokenized sentence 111: tensor([[  101,   122,   146,  5492, 22280,  5561,   102]])\n",
      "DEBUG: Tokenized sentence 112: tensor([[  101, 17891,  6515,   118,   219,   268,   102]])\n",
      "DEBUG: Tokenized sentence 113: tensor([[  101,   122,  1369,   640,   364,   214,   146,  6793,   117, 11092,\n",
      "         14754,   228,   229,  5675,   171,   853, 18983,   162,   170, 15512,\n",
      "         13793,   320,  4707,   171,   549,   713,   102]])\n",
      "DEBUG: Tokenized sentence 114: tensor([[  101,  2010,   123, 22296,   122,  3295,   271,  2354, 22279,   176,\n",
      "          3196,   136,  2010, 15605,  1885,  1211,   117,   221,   146,  6202,\n",
      "           102]])\n",
      "DEBUG: Tokenized sentence 115: tensor([[ 101, 2010, 6202,  123, 4023,  102]])\n",
      "DEBUG: Tokenized sentence 116: tensor([[  101,   327,  2606, 11348,   136,  2010,   122, 11348, 17124,   117,\n",
      "          1141,  7258,   102]])\n",
      "DEBUG: Tokenized sentence 117: tensor([[  101,  2010,  1004,   117, 11185,   128,   792,   118,  2036,   230,\n",
      "           964,   324,   102]])\n",
      "DEBUG: Tokenized sentence 118: tensor([[  101,   122,   146, 12447,   397, 13186,   748,   123,  4303,   171,\n",
      "          4707,   180,   418, 15976,   117,   125,   582, 20954,   117,   271,\n",
      "           125,   230,  9196,   747,  1718,  2579,   179,   176,  1014,   321,\n",
      "           117,   230,   475, 17647,   285,  8833,   117,  4410,  1994,  1510,\n",
      "         22087, 12717,   185,   123,  1718,  2349,   304, 22280,   125,   233,\n",
      "          2841,   122, 11451,  5925,  2204,   285,  8742,   214,   320,   969,\n",
      "           102]])\n",
      "DEBUG ======================\n",
      " len vetores 118 \n",
      "Similaridade média: 1.0\n",
      " Proporção de conectivos: 0.0100908173528117\n",
      " Coesão Score Final: 0.5050454086764059\n",
      " Conectivos encontrados: ['e', 'mas', 'porem', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'isto e', 'nem', 'ou', 'ora', 'quer', 'senao', 'como', 'quanto', 'em vez de', 'ao passo que', 'para que', 'porque', 'com efeito', 'por exemplo', 'tanto', 'quanto', 'se nao', 'a proporcao que']\n",
      " Número de conectivos: 30\n",
      " Número de sentenças: 119\n",
      "======================\n",
      "Resultados para preprocessado_o_cortico_aluisio_azevedo_cap_4.json:\n",
      "{'coesao_score': np.float64(0.51), 'conectivos_encontrados': ['e', 'mas', 'porem', 'entretanto', 'assim', 'logo', 'pois', 'porque', 'quando', 'enquanto', 'se', 'caso', 'isto e', 'nem', 'ou', 'ora', 'quer', 'senao', 'como', 'quanto', 'em vez de', 'ao passo que', 'para que', 'porque', 'com efeito', 'por exemplo', 'tanto', 'quanto', 'se nao', 'a proporcao que'], 'num_conectivos': 30, 'proporcao_conectivos': 0.01, 'similaridade_media': np.float64(1.0), 'num_sentencas': 119}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer  # Or BertTokenizer\n",
    "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
    "\n",
    "model = AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n",
    "\n",
    "def get_sentence_embeddings(sentences, model):\n",
    "    \"\"\"\n",
    "    Obtém o embedding médio para cada sentença usando um modelo pré-treinado.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): Lista de sentenças.\n",
    "        model: Modelo de embedding.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapeamento de sentença para vetor de embedding.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if not isinstance(sentence, str) or len(sentence) < 5:\n",
    "            continue\n",
    "        sentence_embedding = tokenizer.encode(sentence, return_tensors='pt')\n",
    "        print(f\"DEBUG: Tokenized sentence {i}: {sentence_embedding}\")  # DEBUG\n",
    "        embeddings[i] = sentence_embedding\n",
    "    return embeddings\n",
    "\n",
    "def analisar_coesao_sentences(sentences, words, embeddings):\n",
    "    \"\"\"\n",
    "    Analisa a coesão e coerência do texto de forma detalhada.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): Lista de frases do texto.\n",
    "        words (list): Lista de palavras do texto.\n",
    "        embeddings (dict): Embeddings das sentenças (mapeando texto -> vetor).\n",
    "\n",
    "    Returns:\n",
    "        dict: Resultados de análise de coesão, conectivos e similaridade semântica.\n",
    "    \"\"\"\n",
    "    # --- Análise de conectivos ---\n",
    "    conectivos = [\n",
    "        'e', 'mas', 'porem', 'contudo', 'entretanto', 'portanto', 'assim', 'logo',\n",
    "        'pois', 'porque', 'ja que', 'uma vez que', 'quando', 'enquanto', 'se', 'caso',\n",
    "        'embora', 'apesar de', 'alem disso', 'ademais', 'ou seja', 'isto e', 'todavia',\n",
    "        'nao obstante', 'ainda que', 'de modo que', 'de forma que', 'por conseguinte',\n",
    "        'dessa forma', 'desse modo', 'conquanto', 'sobretudo', 'inclusive', 'nem',\n",
    "        'tampouco', 'ou', 'ora', 'quer', 'seja', 'senao', 'assim como', 'bem como',\n",
    "        'como', 'tal como', 'tanto quanto', 'quanto', 'do mesmo modo', 'igualmente',\n",
    "        'em vez de', 'ao passo que', 'desde que', 'a fim de que', 'para que', 'antes que',\n",
    "        'logo que', 'assim que', 'tao logo', 'depois que', 'porque', 'porquanto', 'visto que',\n",
    "        'posto que', 'uma vez que', 'ja que', 'em virtude de', 'em razao de', 'gracas a',\n",
    "        'apesar disso', 'mesmo assim', 'de qualquer forma', 'de qualquer maneira', 'por outro lado',\n",
    "        'em contrapartida', 'em contraste', 'ao contrario', 'pelo contrario', 'no entanto',\n",
    "        'de fato', 'efetivamente', 'realmente', 'com efeito', 'por exemplo', 'alias', 'a proposito',\n",
    "        'inclusive', 'alem do mais', 'acima de tudo', 'principalmente', 'sobretudo', 'nao so',\n",
    "        'como tambem', 'bem como', 'nao apenas', 'mas tambem', 'tanto', 'quanto', 'se nao', 'caso contrario',\n",
    "        'a medida que', 'a proporcao que', 'de maneira que', 'de sorte que', 'de tal forma que',\n",
    "        'de tal modo que', 'de forma que', 'de jeito que', 'de maneira que', 'de modo que',\n",
    "        'em funcao de', 'em vista de', 'por causa de', 'por conta de', 'por motivo de', 'por razao de',\n",
    "        'em consequencia', 'em decorrencia', 'em resultado', 'em virtude', 'em vista disso', 'por isso',\n",
    "        'por essa razao', 'por esse motivo', 'por essa causa', 'por essa razao', 'por esse motivo'\n",
    "    ]\n",
    "    conectivos_encontrados = []\n",
    "    for c in conectivos:\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.lower()  # Normaliza para minúsculas\n",
    "            if c in sentence:\n",
    "                conectivos_encontrados.append(c)\n",
    "                break\n",
    "    num_conectivos = len(conectivos_encontrados)\n",
    "    num_tokens = len(words)\n",
    "    proporcao_conectivos = num_conectivos / (num_tokens + 1e-6)  # Evitar divisão por zero\n",
    "\n",
    "    # --- Análise de coesão semântica ---\n",
    "    # Garante que todos os vetores de embeddings sejam válidos\n",
    "    vetores = []\n",
    "    for vetor in embeddings.values():\n",
    "        if vetor is not None and len(vetor) > 0:\n",
    "            vetor = vetor.T\n",
    "            vetores.append(vetor)\n",
    "    if not vetores:\n",
    "        print(\"Nenhum vetor de embedding válido encontrado. Retornando coesão padrão.\")\n",
    "        return {\n",
    "            'coesao_score': 0.0,\n",
    "            'conectivos_encontrados': conectivos_encontrados,\n",
    "            'num_conectivos': num_conectivos,\n",
    "            'proporcao_conectivos': round(proporcao_conectivos, 3),\n",
    "            'similaridade_media': 0.0,\n",
    "            'num_sentencas': len(sentences)\n",
    "        }\n",
    "    \n",
    "    if len(vetores) < 2:\n",
    "        similaridade_media = 1.0  # Texto curto, assume alta coesão\n",
    "        print(\"isso aqui rodou.\\n\") # DEBUG\n",
    "    else:\n",
    "        similaridades = []\n",
    "        for i in range(len(vetores) - 1):\n",
    "            try:\n",
    "                sim = cosine_similarity(vetores[i], vetores[i+1])[0][0]\n",
    "                similaridades.append(sim)\n",
    "            except Exception as e:\n",
    "                print(f\"Vetores possuem os tamanhos diferentes: {len(vetores[i])} e {len(vetores[i+1])}. Erro: {e}\")\n",
    "        similaridade_media = np.mean(similaridades)\n",
    "\n",
    "    # --- Cálculo do Score Final ---\n",
    "    # Peso 50% conectivos, 50% semântica\n",
    "    score_conectivos = min(1, proporcao_conectivos)  # Ideal: 10% de conectivos\n",
    "    score_semantica = similaridade_media      # Garante entre 0 e 1\n",
    "\n",
    "    coesao_score_final = (score_conectivos + score_semantica) / 2\n",
    "    print(f\"DEBUG ======================\\n len vetores {len(vetores)} \\nSimilaridade média: {similaridade_media}\\n Proporção de conectivos: {proporcao_conectivos}\\n Coesão Score Final: {coesao_score_final}\\n Conectivos encontrados: {conectivos_encontrados}\\n Número de conectivos: {num_conectivos}\\n Número de sentenças: {len(sentences)}\\n======================\")  # DEBUG\n",
    "    return {\n",
    "        'coesao_score': round(coesao_score_final, 2),\n",
    "        'conectivos_encontrados': conectivos_encontrados,\n",
    "        'num_conectivos': num_conectivos,\n",
    "        'proporcao_conectivos': round(proporcao_conectivos, 3),\n",
    "        'similaridade_media': round(similaridade_media, 3),\n",
    "        'num_sentencas': len(sentences)\n",
    "    }\n",
    "\n",
    "# Iterar sobre os textos processados\n",
    "caminho_saida_pasta = \"data/caps_processados\"\n",
    "for arquivo_nome in os.listdir(caminho_saida_pasta):\n",
    "    caminho_arquivo = os.path.join(caminho_saida_pasta, arquivo_nome)\n",
    "    if os.path.isfile(caminho_arquivo) and arquivo_nome.endswith('.json'):\n",
    "        with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo:\n",
    "            dados = json.load(arquivo)\n",
    "            sentences = [\" \".join(tokens) for tokens in dados['tokens']]\n",
    "            tokens = [token for sublist in dados['tokens'] for token in sublist]\n",
    "            titulo = dados.get('titulo', 'Desconhecido')\n",
    "            # Obter embeddings e analisar coesão\n",
    "            embeddings = get_sentence_embeddings(sentences, model)\n",
    "            resultados = analisar_coesao_sentences(sentences=sentences, words=tokens, embeddings=embeddings)\n",
    "            \n",
    "            print(f\"Resultados para {arquivo_nome}:\")\n",
    "            print(resultados)\n",
    "            print(\"\\n\")\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            # Criando um gráfico consolidado para todas as métricas por livro\n",
    "            plt.figure(figsize=(12, 8))\n",
    "\n",
    "            # Adiciona os resultados do livro atual à lista\n",
    "            # Verifica se o texto já está presente no DataFrame\n",
    "            if titulo in df_resultados['titulo'].values:\n",
    "                # Atualiza a linha correspondente com os novos resultados\n",
    "                df_resultados.loc[df_resultados['titulo'] == titulo, ['proporcao_conectivos', 'similaridade_media', 'coesao_score', 'num_conectivos', 'num_sentencas']] = [\n",
    "                    resultados['proporcao_conectivos'],\n",
    "                    resultados['similaridade_media'],\n",
    "                    resultados['coesao_score'],\n",
    "                    resultados['num_conectivos'],\n",
    "                    resultados['num_sentencas']\n",
    "                ]\n",
    "            else:\n",
    "                # Adiciona uma nova linha com os resultados, incluindo as colunas ausentes\n",
    "                df_resultados = pd.concat([\n",
    "                    df_resultados,\n",
    "                    pd.DataFrame([{\n",
    "                        'titulo': titulo,\n",
    "                        'proporcao_conectivos': resultados['proporcao_conectivos'],\n",
    "                        'similaridade_media': resultados['similaridade_media'],\n",
    "                        'coesao_score': resultados['coesao_score'],\n",
    "                        'ttr': None,\n",
    "                        'score_ttr': None,\n",
    "                        'num_types': None,\n",
    "                        'num_tokens': None,\n",
    "                        'proporcao_hapax': None,\n",
    "                        'score_diversidade': None,\n",
    "                        'score_palavras_unicas': None\n",
    "                    }])\n",
    "                ], ignore_index=True)\n",
    "\n",
    "# Salva os resultados atualizados em um arquivo CSV\n",
    "caminho_csv = os.path.join(caminho_saida_pasta, 'resultados.csv')\n",
    "df_resultados.to_csv(caminho_csv, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "50b1b179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Contagem por arquivo:\n",
      "\n",
      "Arquivo: preprocessado_dom_casmurro_machado_cap_1.json\n",
      "  DET: 0.09411764705882353\n",
      "  NOUN: 0.22058823529411764\n",
      "  PRON: 0.10294117647058823\n",
      "  VERB: 0.20294117647058824\n",
      "  ADP: 0.11764705882352941\n",
      "  ADJ: 0.052941176470588235\n",
      "  ADV: 0.03823529411764706\n",
      "  CCONJ: 0.05\n",
      "  PROPN: 0.03529411764705882\n",
      "  AUX: 0.026470588235294117\n",
      "  SCONJ: 0.047058823529411764\n",
      "  NUM: 0.0058823529411764705\n",
      "  X: 0.0058823529411764705\n",
      "\n",
      "Arquivo: preprocessado_dom_casmurro_machado_cap_2.json\n",
      "  ADV: 0.050824175824175824\n",
      "  SCONJ: 0.027472527472527472\n",
      "  VERB: 0.1510989010989011\n",
      "  DET: 0.11538461538461539\n",
      "  NOUN: 0.24587912087912087\n",
      "  ADJ: 0.06456043956043957\n",
      "  PRON: 0.10164835164835165\n",
      "  PROPN: 0.017857142857142856\n",
      "  ADP: 0.11675824175824176\n",
      "  CCONJ: 0.08104395604395605\n",
      "  AUX: 0.017857142857142856\n",
      "  NUM: 0.008241758241758242\n",
      "  X: 0.0013736263736263737\n",
      "\n",
      "Arquivo: preprocessado_dom_casmurro_machado_cap_3.json\n",
      "  AUX: 0.02702702702702703\n",
      "  VERB: 0.19369369369369369\n",
      "  ADP: 0.07357357357357357\n",
      "  NOUN: 0.22672672672672672\n",
      "  SCONJ: 0.08408408408408409\n",
      "  DET: 0.12462462462462462\n",
      "  CCONJ: 0.04804804804804805\n",
      "  PRON: 0.07507507507507508\n",
      "  ADJ: 0.06906906906906907\n",
      "  NUM: 0.0045045045045045045\n",
      "  ADV: 0.03903903903903904\n",
      "  PROPN: 0.03153153153153153\n",
      "  SYM: 0.0015015015015015015\n",
      "  PUNCT: 0.0015015015015015015\n",
      "\n",
      "Arquivo: preprocessado_dom_casmurro_machado_cap_4.json\n",
      "  NOUN: 0.2795031055900621\n",
      "  PROPN: 0.018633540372670808\n",
      "  VERB: 0.14906832298136646\n",
      "  DET: 0.14285714285714285\n",
      "  AUX: 0.043478260869565216\n",
      "  SCONJ: 0.049689440993788817\n",
      "  ADJ: 0.055900621118012424\n",
      "  PRON: 0.031055900621118012\n",
      "  ADP: 0.13664596273291926\n",
      "  ADV: 0.037267080745341616\n",
      "  CCONJ: 0.043478260869565216\n",
      "  NUM: 0.012422360248447204\n",
      "\n",
      "Arquivo: preprocessado_iracema_jose_de_alencar_cap_1.json\n",
      "  NOUN: 0.2865853658536585\n",
      "  ADJ: 0.11890243902439024\n",
      "  ADP: 0.1524390243902439\n",
      "  DET: 0.14939024390243902\n",
      "  PRON: 0.0701219512195122\n",
      "  VERB: 0.14329268292682926\n",
      "  PROPN: 0.006097560975609756\n",
      "  ADV: 0.024390243902439025\n",
      "  CCONJ: 0.027439024390243903\n",
      "  SCONJ: 0.012195121951219513\n",
      "  AUX: 0.006097560975609756\n",
      "  NUM: 0.003048780487804878\n",
      "\n",
      "Arquivo: preprocessado_iracema_jose_de_alencar_cap_2.json\n",
      "  VERB: 0.14003944773175542\n",
      "  ADV: 0.03944773175542406\n",
      "  ADP: 0.14990138067061143\n",
      "  PROPN: 0.01972386587771203\n",
      "  PRON: 0.0670611439842209\n",
      "  NOUN: 0.2978303747534517\n",
      "  ADJ: 0.07692307692307693\n",
      "  DET: 0.14792899408284024\n",
      "  CCONJ: 0.03944773175542406\n",
      "  AUX: 0.007889546351084813\n",
      "  SCONJ: 0.009861932938856016\n",
      "  NUM: 0.0019723865877712033\n",
      "  X: 0.0019723865877712033\n",
      "\n",
      "Arquivo: preprocessado_iracema_jose_de_alencar_cap_3.json\n",
      "  DET: 0.13884007029876977\n",
      "  NOUN: 0.2741652021089631\n",
      "  VERB: 0.17926186291739896\n",
      "  ADP: 0.14586994727592267\n",
      "  ADV: 0.029876977152899824\n",
      "  CCONJ: 0.0421792618629174\n",
      "  ADJ: 0.06678383128295255\n",
      "  PRON: 0.05799648506151142\n",
      "  PROPN: 0.01757469244288225\n",
      "  SCONJ: 0.026362038664323375\n",
      "  AUX: 0.014059753954305799\n",
      "  NUM: 0.005272407732864675\n",
      "  X: 0.0017574692442882249\n",
      "\n",
      "Arquivo: preprocessado_iracema_jose_de_alencar_cap_4.json\n",
      "  DET: 0.15254237288135594\n",
      "  NOUN: 0.2919020715630885\n",
      "  VERB: 0.17702448210922786\n",
      "  CCONJ: 0.03389830508474576\n",
      "  ADP: 0.14124293785310735\n",
      "  SCONJ: 0.030131826741996232\n",
      "  ADJ: 0.06591337099811675\n",
      "  ADV: 0.022598870056497175\n",
      "  PROPN: 0.02071563088512241\n",
      "  PRON: 0.047080979284369114\n",
      "  AUX: 0.015065913370998116\n",
      "  X: 0.0018832391713747645\n",
      "\n",
      "Arquivo: preprocessado_o_cortico_aluisio_azevedo_cap_1.json\n",
      "  NOUN: 0.24814888933360016\n",
      "  PROPN: 0.01380828497098259\n",
      "  AUX: 0.015609365619371623\n",
      "  ADP: 0.1658995397238343\n",
      "  NUM: 0.01280768461076646\n",
      "  CCONJ: 0.048028817290374226\n",
      "  VERB: 0.155893536121673\n",
      "  DET: 0.12767660596357816\n",
      "  PRON: 0.0748449069441665\n",
      "  ADJ: 0.04922953772263358\n",
      "  ADV: 0.05123073844306584\n",
      "  SCONJ: 0.03622173303982389\n",
      "  X: 0.00020012007204322593\n",
      "  INTJ: 0.00040024014408645187\n",
      "\n",
      "Arquivo: preprocessado_o_cortico_aluisio_azevedo_cap_2.json\n",
      "  ADP: 0.14877847927532253\n",
      "  NUM: 0.005764479824320615\n",
      "  NOUN: 0.22920669777655778\n",
      "  DET: 0.1284655503705737\n",
      "  VERB: 0.16387592643425747\n",
      "  CCONJ: 0.04831183090859182\n",
      "  ADJ: 0.050782322261872084\n",
      "  ADV: 0.05050782322261872\n",
      "  PRON: 0.09607466373867692\n",
      "  SCONJ: 0.04721383475157837\n",
      "  PROPN: 0.012077957727147955\n",
      "  AUX: 0.01784243755146857\n",
      "  X: 0.0008234971177600879\n",
      "  INTJ: 0.0002744990392533626\n",
      "\n",
      "Arquivo: preprocessado_o_cortico_aluisio_azevedo_cap_3.json\n",
      "  AUX: 0.015888778550148957\n",
      "  NUM: 0.007696127110228402\n",
      "  NOUN: 0.25049652432969216\n",
      "  ADP: 0.15789473684210525\n",
      "  CCONJ: 0.04791459781529295\n",
      "  DET: 0.13505461767626614\n",
      "  VERB: 0.1641012909632572\n",
      "  SCONJ: 0.03450844091360477\n",
      "  PRON: 0.07274081429990069\n",
      "  ADV: 0.04344587884806356\n",
      "  PROPN: 0.013902681231380337\n",
      "  ADJ: 0.05561072492552135\n",
      "  X: 0.0004965243296921549\n",
      "  INTJ: 0.00024826216484607745\n",
      "\n",
      "Arquivo: preprocessado_o_cortico_aluisio_azevedo_cap_4.json\n",
      "  NUM: 0.01092690278824416\n",
      "  NOUN: 0.24642049736247174\n",
      "  ADV: 0.05124340617935192\n",
      "  PROPN: 0.01921627731725697\n",
      "  PRON: 0.07950263752825923\n",
      "  VERB: 0.17030896759608138\n",
      "  AUX: 0.016201959306706856\n",
      "  ADP: 0.1522230595327807\n",
      "  DET: 0.12735493594574226\n",
      "  CCONJ: 0.04822908816880181\n",
      "  SCONJ: 0.031650339110776186\n",
      "  ADJ: 0.04634513941220799\n",
      "  X: 0.00037678975131876413\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "caminho_textos_processados = \"data/caps_processados\" \n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "arquivos_json = [f for f in os.listdir(caminho_textos_processados) if f.endswith('.json')]\n",
    "\n",
    "def contar_pos_tags(texto):\n",
    "    doc = nlp(texto)\n",
    "    contagem = Counter()\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_space:\n",
    "            contagem[token.pos_] += 1\n",
    "    total_tokens = sum(contagem.values())\n",
    "    if total_tokens > 0:\n",
    "        proporcoes = {pos: qtd / total_tokens for pos, qtd in contagem.items()}\n",
    "    else:\n",
    "        proporcoes = {pos: 0 for pos in contagem}\n",
    "    return proporcoes\n",
    "\n",
    "contagens_por_arquivo = {}\n",
    "contagem_total = Counter()\n",
    "\n",
    "for nome_arquivo in arquivos_json:\n",
    "    caminho_completo = os.path.join(caminho_textos_processados, nome_arquivo)\n",
    "    with open(caminho_completo, 'r', encoding='utf-8') as f:\n",
    "        dados = json.load(f)\n",
    "        texto = \" \".join([token for sublist in dados['tokens'] for token in sublist])\n",
    "        contagem = contar_pos_tags(texto)\n",
    "        contagens_por_arquivo[nome_arquivo] = contagem\n",
    "        contagem_total.update(contagem)\n",
    "\n",
    "# Carregar o DataFrame existente\n",
    "caminho_csv = os.path.join(caminho_textos_processados, 'resultados.csv')\n",
    "df = pd.read_csv(caminho_csv)\n",
    "\n",
    "# Para cada arquivo, adicionar/atualizar as features POS no DataFrame\n",
    "for nome_arquivo, contagem in contagens_por_arquivo.items():\n",
    "    # Remover prefixo \"preprocessado_\" e sufixo \".json\" para bater com o campo 'titulo'\n",
    "    titulo = nome_arquivo.replace(\"preprocessado_\", \"\").replace(\".json\", \"\")\n",
    "    for pos, qtd in contagem.items():\n",
    "        # Adiciona a coluna se não existir\n",
    "        if pos == \"SYM\" or pos == \"PUNCT\" or pos == \"INTJ\":\n",
    "            continue\n",
    "        if pos not in df.columns:\n",
    "            df[pos] = 0.0\n",
    "        # Atualiza o valor na linha correspondente ao título\n",
    "        df.loc[df['titulo'] == titulo, pos] = qtd\n",
    "\n",
    "# Salvar o DataFrame atualizado\n",
    "df.to_csv(caminho_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\n Contagem por arquivo:\")\n",
    "for nome_arquivo, contagem in contagens_por_arquivo.items():\n",
    "    print(f\"\\nArquivo: {nome_arquivo}\")\n",
    "    for pos, qtd in contagem.items():\n",
    "        print(f\"  {pos}: {qtd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7f868719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             titulo  cluster\n",
      "0        dom_casmurro_machado_cap_1        0\n",
      "1        dom_casmurro_machado_cap_2        0\n",
      "2        dom_casmurro_machado_cap_3        0\n",
      "3        dom_casmurro_machado_cap_4        1\n",
      "4     iracema_jose_de_alencar_cap_1        1\n",
      "5     iracema_jose_de_alencar_cap_2        1\n",
      "6     iracema_jose_de_alencar_cap_3        1\n",
      "7     iracema_jose_de_alencar_cap_4        1\n",
      "8   o_cortico_aluisio_azevedo_cap_1        2\n",
      "9   o_cortico_aluisio_azevedo_cap_2        2\n",
      "10  o_cortico_aluisio_azevedo_cap_3        2\n",
      "11  o_cortico_aluisio_azevedo_cap_4        2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Carregar o DataFrame\n",
    "df = pd.read_csv('data/caps_processados/resultados.csv')\n",
    "\n",
    "# Selecionar apenas as colunas numéricas (ignorando 'titulo')\n",
    "X = df.drop(columns=['titulo'])\n",
    "\n",
    "# Padronizar os dados\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Definir o número de clusters (exemplo: 3, ajuste conforme necessário)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Adicionar o resultado ao DataFrame\n",
    "df['cluster'] = clusters\n",
    "\n",
    "# Exibir os clusters\n",
    "print(df[['titulo', 'cluster']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427b0d7",
   "metadata": {},
   "source": [
    "### Conclusão\n",
    "Primeiramente, um fator essencial que notamos é que teremos que tratar a sensibilidade que existe à diferença de tamanho entre os textos, essa é uma questão na qual vamos precisar pensar com cuidado, evitar usar números absolutos será relevante para tornas os parâmetros mais comparáveis. Por fim, nessa próxima etapa nossa intenção é de aumentar a quantidade de features e principalmente o escopo dos dados, utilizar mais autores e textos para nossa análise será essencial mais para frente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
